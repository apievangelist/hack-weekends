{"news":[{"title":"Montreal Hackathon Aims to Combat Government Corruption | TechPresident","details":"Hackons la Corruption 2012 (Hacking Corruption 2012), billed as North America\u2019s first anti-corruption hackathon, was held on November 10 and 11 at a community center in Montreals east end.  \n Participants from Quebec and elsewhere attended seminars, munched on pizza, and worked in teams to create projects aimed at exposing government behavior and improving open data access. \n \u201cThe climate in Quebec right now is one of astonishment at the amount of corruption. We think that in the 21st century, we need to use 21st century technology to fight corruption,\u201d said organizer Jonathan Brun, one of six co-founders of QuebecOuvert, the group behind the event. \n The hackathon, months in the planning, teamed hackers and challenged them to scrape existing information databases in hopes of raising public awareness regarding government accountability. \n Amongst the projects presented was DonsPolitiques, an effort to link political donations by individuals to the companies they work at. The idea was to track companies that might be encouraging their employees to give political donations, then reimbursing them by adding the money to their salaries or handing out bonuses (corporate political donations are illegal in Canada).  DonsPolitiques uses a database of political donations and LinkedIn to identify where people are working when they give political donations, and to find companies that might be trying to influence the legislative process or awarding of contracts.  \n Another platform, ContratsNet, provided visualization of how contracts are given out in Montreal by jurisdiction, by borough, or by type. \u201cWe are trying to build a very broad database of information from all the provincial legislators,\u201d says Michael Mulley, founder of OpenParliament.ca, a website that features easy-to-access information on the activities of Canadas federal government.  \n The timing for an anti-corruption hackathon is certainly right. Quebec has been rocked by a series of spectacular scandals over recent months. Long-rumored ties between provincial and municipal governments, the construction industry, and the mafia have been brought to light by various media reports \u2014 and, most notably, by the Charbonneau Commission  \u2014 named for presiding Justice France Charbonneau. Though still in its infancy, the commission has released one bombshell revelation after another. \n \u201cThe best is yet to come,\u201d declared Hackons la Corruption keynote speaker Jacques Duchesneau, referring to the Charbonneau Commission revelations.  The former Montreal Chief of Police and current provincial representative for the Qu\u00e9bec riding (electoral district) of Saint-Jer\u00f4me was hired by the provincial government to look into rumoured construction industry corruption in 2010.  In 2011, he blew the whistle on the relationship between the mob and Qu\u00e9bec officials with a shocking report he leaked to the media.  Duschesneau was subsequently fired.   \n \u201cWe\u2019ve only heard four witnesses.  Actually, we\u2019re expecting about 75, so hold your breath,\u201d Duscheneau told techPresident. \n So far, the commission has heard allegations about safes at Montreal City Hall so overstuffed with cash that the doors could not be closed; of cash payments to mobsters smuggled via sock-stuffing, and of a 3 percent kickback tax on construction contracts awarded under the stewardship of Montreal city engineer Gilles Surprenant. Surprenant claimed he spent large sums of these illegal payments at a Montreal casino, in an attempt to repay taxpayers. \n Perhaps most chillingly, the commission  heard about \u201cMr. Sidewalk\u201d a.k.a Nicolo Milioto, a construction company owner. According to Martin Dumont, a high level organizer for Montreal\u2019s governing Union Montreal party, Mr. Sidewalk threatened to make Dumont a part of his pavement if he asked too many questions. \n Over the past few days, top municipal officials have been falling like dominos.  Both Montreal mayor  \u202aG\u00e9rald Tremblay\u202c and Laval mayor Gilles Vaillancourt took medical leaves from work, then raided by Quebec anti-corruption squad.  Mascouche mayor Robert Marcotte C\u00f4te-des-Neiges-Notre-Dame-de-Grace and deputy to Mayor Tremblay, resigned from the citys executive committee. Applebaum cited a 2004 report \u2014 which, he says, proves Montreal has long been paying 30-40 percent more than other municipalities for road work.  \n All this comes on the heels of the Maple Spring, the student protest movement that erupted in February 2012, in response to the governments plan to raise university tuition fees by 75 percent.  The movement reached its zenith in May, when an estimated 400,000 citizens took to the streets, banging pots and pans. The government responded by passing Bill 78, an emergency law that required a police permit for demonstrations of more than 50 people and suspended classes at universities where students were on strike. The government also deployed riot police who were recorded using tear gas, carrying out mass arrests and initiating beatings to break up protests.  The Canadian Association for Civil Rights issued a strongly worded statement criticizing the Quebec government for violating fundamental rights, using excessive force and restricting civil liberties.\n So, how can a hackathon improve matters?  \n \u201cWell, for one thing, this allows me to be a little bit more optimistic, because it will replace cynicism that is taking root right now in the province of Quebec and replace it with something positive,\u201d says Jean Fortier, former President of Montreal\u2019s City Executive Committee. He says corruption at City Hall goes back a long time, and while he was \u201cnot at all\u201d surprised by the revelations that have been unearthed so far by the Charbonneau commission, having his worst suspicions confirmed moved him to tears one morning. \n \u201cI think the solution anywhere, either in private industry, or institutions, or public sectors, is activity-based management,\u201d says Fortier.  In order to do that, \u201cYou have to have basic, common data, that is available everywhere in the United States from town to town from city to city.\u201d \n In a speech delivered at the hackathon, the Sunlight Foundations* James Turk spoke about the OpenStates project, which provides data on U.S. state legislators. Turk said the platform was so popular that many governments prefer its apps for iPhone and ipad to their own software.  \n \u201cData that\u2019s in a book isn\u2019t truly accessible; it needs to be available online,\u201d said Turk. OpenStates uses only information that is in the public domain, although sometimes friendly informants may guide them to a certain obscure website, or provide them with lists of people who were invited to political fundraisers. Turk showed the crowd a model of the scrapers his team uses to obtain information. \u201cDon\u2019t be scared, it\u2019s actually pretty simple,\u201d he laughed. \n Optimism is a good thing, and Hackons la Corruption 2012 can be viewed at the very least as an interesting event \u2014 as well as a positive development . The hackathon was well covered by the local media and as such did succeed in raising awareness about government corruption, and the means of sourcing information to empower ordinary citizens, but the jury is very much out on the question of its overal impact. The developers who presented DonsPolitiques, the project that aspired to link political donations by individuals to the companies they worked for, did not produce an app at the event. But on the other hand, ContratsNet won the grand prize, with the developers \u2014 who were working on their project for two weeks before the hackathon \u2014 expecting to push forward toward a launch.\n But the overall attitude at the hackathon seemed to be be, better baby steps than no steps.  \u201cThe only way we can win over corruption is to get the information that we need,\u201d said Duschesneau. \u201cPeople have all the reasons in the world to get upset.  But a long journey always starts with a first step. Today, I think, is a first step in the right direction.\n *techPresident publisher Andrew Rasiej and editor Micah Sifry are senior advisors to the Sunlight Foundation. \n Personal Democracy Media is grateful to the Omidyar Network for its generous support of techPresidents WeGov section.","item_date":"Nov 13 2012 02:51:20","display_item_date":"11-12-2012","url":"http:\/\/techpresident.com\/news\/WeGov\/23123\/montreal-hackathon-aims-combat-government-corruption","source":"techpresident.com"},{"title":"Cleartrade Exchange delivers unique liquidity through open API","details":"Cleartrade Exchange, the leading OTC trading venue, is now live with an innovative solution enabling orders which are linked to an underlying commodity price to be automatically injected into the market. This unique development was successfully pioneered with lead broker member FIS and allows market-makers to enter, adjust, remove and trade multiple bunker fuel  swap orders in real-time, resulting in a firm, two-way spread on two of CLTX\u2019s three bunker contracts for the whole trading day.\n        FIS Bunker Broker, Ali Ersen, said: \u201cThe innovative technology developed by  Cleartrade Exchange has enabled price discovery and trading of these contracts in a way that was not possible before in this market sector. This market-leading initiative provides a unique service to our clients and is enabling us to continue bringing major developments to this important global market.\u201d\n  \u201cThis is another product development driven by feedback from our members,\u201d said Bob Antell, Global Business Development Director for Cleartrade Exchange. \u201cThe feedback we\u2019ve had so far has been very positive. We\u2019ve already seen an increase in traffic on the platform and we only expect this to grow further.\u201d\n  CLTX launched its single-lot Bunker Fuel contracts at the beginning of November and has since seen a great deal of interest. The product compliments CLTX\u2019s existing asset classes covering freight and commodities and is another development in the evolution of aggregating isolated individual OTC markets to a more relevant multi asset-class, interlinked risk management solution.","item_date":"Nov 12 2012 21:40:55","display_item_date":"11-12-2012","url":"http:\/\/www.commodities-now.com\/news\/technology\/13033-cleartrade-exchange-delivers-unique-liquidity-through-open-api.html","source":"www.commodities-now.com"},{"title":"Tweetro says it's 'completely crippled' by Twitter's strict 100,000 user token limit | The Verge","details":"Tweetro has fallen victim to Twitters strict new API policies that were announced earlier this year. According to an email we received from Tweetro developers, the app saw a huge spike in downloads after the release of Windows 8, and rapidly reached its 100,000 user token limit. Users now receive a cannot connect to service error when trying to authenticate the application, and Tweetro developers say the app is completely crippled as a result.\n  Twitter originally said that developers would have until January or March of 2013 to comply with the platforms API changes, so Tweetro developers are questioning why their app has been cut off so early \u2014 especially when Twitter has yet to release its own official app for Windows 8. Tweetro has reached out to Twitter to discuss the future of their app, and says it may pull the app from the Windows 8 store and re-launch with a paid app if Twitter does not agree to loosen its policy. Twitter declined to comment on the situation. \n                          Via         Windows Observer       \n                                                    Related Items          twitter api walled garden windows 8 apps tweetro third party app user token       \n         \n                                                             There are 116 Comments.","item_date":"Nov 12 2012 19:20:33","display_item_date":"11-12-2012","url":"http:\/\/www.theverge.com\/2012\/11\/11\/3631108\/tweetro-user-token-limit-api","source":"www.theverge.com"},{"title":"5 Rules For API Management","details":"APIs are the glue that connect apps. It\u2019s as true for consumer apps as it is for the enterprise. API management platforms have come into vogue as apps proliferate across the enterprise.\n As APIs rise in importance, so has the need for better practices in their creation, development and management. All the major API management services have built strategies that they use as guiding principles when working with customers.\n Mashery CEO\u00a0Oren Michels provided me with an overview of API management that I think also applies to other service providers such as Apigee, Layer 7, Mashape and SOA Software.\n These five rules are by no means the final say on API management, but they do provide context for the overall market:\n 1. Design\n Make the APIs accessible to different classes of developers and partners. Develop security policies, usage policies, selective access to data and services.\u00a0Both Layer 7 and SOA Software cite the need to serve the different constituencies of the enterprise.\n It needs to address each constituency or user group engaged in building and running an API. These include API Architects \/ Developers, Security Architects, IT Operations and Business Analysts (API marketers). The Layer 7 platform has a product component that addresses each of these stakeholders (API Gateway, API Identity Broker, API Service Manager, API Developer Portal).\n A platform by definition needs to be extensible. There needs to be a way to build on top of it. This requires both product APIs and SDK (developer toolkit). Layer 7 supplies both.\n Michels adds that it is less about the IT person than the partnerships between IT and the business groups:\n Enterprise API Management must include the entire Enterprise, not just the techies in IT. The SOA solution, and the other gateways as well, is focused on the IT person and not the business owner of the API program. This is reflected in the UI that they present in their free version as well as their language that includes things like \u201cpolicies\u201d; too much of the business rules are codified in complex policies that require a technical expert to really use.\n The latter point is particularly important. We\u2019ve been selling API management into the enterprise for five years, and we have yet to see a situation where the tech people are not partnered with business people who have a vested interest in \u2013 and will need to manage a big part of \u2013 the API management platform.\n 2. Documentation\n To make APIs accessible, offer documentation and communication tools to\u00a0make it easy to create and manage the applications built on the API itself. Twitter did this very well as a young company but has faltered in its developer communications.\n API Evangelist Kin Lane:\n Communication, communication, communication! Twitter\u00a0started fumbling here in March 2010\u2026.and continued to do so with each release. \u00a0What\u00a0Twitter is doing is speaking to their developer ecosystem, not communicating with. \u00a0We are in the age of social media, we need to have conversations not just an outward broadcast of information.\n Include developers in the API roadmap. \u00a0If your going to do an API, have a process for bringing the heavy consumers into the product development process. \u00a0You don\u2019t have to do everything they say, but make the process inclusive. \u00a0Otherwise they will revolt. \u00a0Twitter\u00a0used to be good about this until Dick took over.\n You can\u2019t reap the benefits of an API ecosystem, and not return value to the community.\n 3. Analytics\n Michels said to think about the collection and processing of all the statistics associated with the use of the API, with an eye toward supporting and encouraging effective usage and discouraging\/limiting usage that is counter to your business or technology goals.\n Apigee has focused significant efforts on its data analytics practice as illustrated in its most recent focus on building APIs for software-defined networks. Here is what I wrote earlier this Fall about the Apigee strategy:\n Apigee\u2019s API platform for multi-vendor SDN is stand-alone software that can integrate network management systems with SDN controllers from multiple vendors through real-time API transformations. This software includes network analytics, enabling dynamic policies on the controller itself, as well as network-based programs that can use trends to trigger a change in network behavior. Apigee\u2019s software reads network traffic into a domain model and publishes network traffic analytics as an API.\n Sam Ramji, vice president of strategy at Apigee, maintains that analytics will help determine how the infrastructure adapts to different data flows. It\u2019s a view that reflects how software is replacing hardware and the role that data plays in the way apps are calibrated.\n  Mashape takes a different view, seeing itself as a Google of APIs, offering analytics as a commodity service. CEO Augusto Marietti:\n For us the analytics, and all the management things are more like a commodity that we give it for free, just to have more and more distribution and more and more consumption. It\u2019s like Google, that gives you Google Analytics for free, because it helps AdWords as side effect. We\u2019re like an object broker for the cloud computing era. We unify the jungle that the API world is. API consumers have one single API key, consumer console and credit card to consume them all, in the same way.\n 4. Universal Access\n Provide seamless and simple support of the various architectures used by the enterprise, whether public cloud, private cloud, on-premise, or a hybrid of several of these.\n 5. Uptime\n High uptime, easy scalability, and redundancy that handles traffic spikes, works around temporary failures in the enterprise backend, and fails gracefully in the event of a backend outage.\u00a0SOA Software\u2019s Ian\u00a0Goldsmith said this for a post I wrote this week about the company\u2019s new enterprise API management platform:\n SOA fits the needs of\u00a0enterprise\u00a0architects and developers \u2013\u00a0people\u00a0who have spent years building extensive infrastructures that can encompass hundreds of software stacks. How to access the data from these stacks is a challenge. Many were architected before APIs emerged as common ways to integrate applications. SOA, as expressed in the name\u00a0itself, comes out of that age when the principles of \u201cservice-oriented architectures\u201d were viewed as the most modern way to integrate multiple on-premise applications into a web environment.\n What do you think? What are some other principles to follow when managing APIs?\n (Feature image courtesy of XPlane \u2013 under Creative Commons.)","item_date":"Nov 12 2012 01:23:11","display_item_date":"11-11-2012","url":"http:\/\/techcrunch.com\/2012\/11\/11\/5-rules-for-api-management\/","source":"techcrunch.com"},{"title":"Sir Tim Berners-Lee: Raw data, now! (Wired UK)","details":"Originally, the acute frustration which led  me to invent the World Wide Web (WWW) in 1989 was all about  documents. The frustration was that all kinds of documents  were sitting in disks on machines. Even at a very advanced place  like the European Organisation for Nuclear Research (CERN),  a networked world in which most computers in my environment  were connected, one couldnt easily browse through all the files.  The WWW design offered a solution, and the world of linked  documents exploded dramatically.\n    However, even at the first web conference in 1994, it was clear  that a rather complex and potentially more profound frustration  (and opportunity) existed when you looked at data rather than  documents. Data, after all, is stuff machines can handle, and  while the web of documents might have seemed intoxicating to early  web surfers, the lure of doing the same thing to the data  was that we could create a world in which it would be programs --  not just people -- that would enjoy the data.  \n    For data, as for documents, the value of any part of the web is  increased by the amount of other stuff out there. For documents it  is the ability to follow links, but for open data it is the  ability to also interconnect and join, to summarise and compare, to  monitor, extrapolate, to infer. \n    The tip of the iceberg\n   We have seen some of the power and acceleration which happens when  governments such as the UK and US have put data on the web. But  this is the tip of the iceberg. The information about spending,  agriculture, health and education that lies behind locked databases  could be used to dramatically improve peoples lives. When  governments begin to release data openly on the web, the growing  movement of hackers and activists and even internal government  agencies and corporations, can begin to use the previously  unconnected and undissected numbers, images and graphs to create  new ways for you to access valuable new information.  \n    Take the example of the UK aid  funded Southern Africa Regional Programme on Access to Medicines  and Diagnostics (SARPAM). This is an organisation that  painstakingly worked with insiders in health ministries and local  health professionals to collect and publish public data on the  price and availability of medicines. They revealed that some  governments were being charged enormously higher rates -- up to 25  times more -- for the same medicines. The findings enabled  governments to put pressure on pharmaceutical companies to reduce  the prices. \n    Imagine how quickly impacts such as these would multiply if  governments were to openly publish this data, not just about the  cost of medicine, but also about student attendance rates or crop  productivity compared to use of pesticides. Scientific data could  help researchers to find new drugs, given genomics and the biology  of individuals, and the massive amount of data needed to understand  and combat climate change would be available to all who work on  it.\n    The benefit of open data is different but also very important in  developing countries. For example, Ghanas government in  collaboration with the Web Foundation, is well on its way to  creating a portal for the release of open, accessible public data.  The   Ghana Open Data Initiative (GODI) just held Ghanas first data  bootcamp, bringing together journalists and developers to find,  extract and analyse public data to tell better informed news  stories.\n    These developments are encouraging, but the simple message to  governments around the world must be consistent and forceful:   raw data, now! Opening up data is fundamentally about more  efficient use of resources and improving service delivery for  citizens.  The effects of that are far reaching: innovation,  transparency, accountability, better governance and economic  growth.\n    Im interested to see the results of the Open Data Research  network, which is bringing together researchers from the global  south to explore the emerging impacts of open data in developing  countries, and to better understand how it is impacting upon  decision making and implementation. The network is being led  by the International  Development Research Centre (IDRC), and the World Wide Web Foundation,  an organisation I founded to keep the Web free (as in freedom) and  open, and to help to bring it within the reach of all.\n    The Web Foundation is leading the charge on other fronts as  well. In September, we launched The Web Index, a decision  making tool for policymakers and others to understand the webs  growth, utility and impact on people and nations. It covers 61  countries, incorporating indicators that assess the political,  economic and social impact of the web, and includes a subset of  indicators that show openness on the web. It includes a  component about open data, and of course the index itself is  available as open data.\n    I commend the leadership and commitment of the UK in the open  data effort. I was honored to be asked by former PM Gordon Brown to  kick-start the open data movement in the UK. PM David Cameron has  continued to put open data at the heart of his agenda and has  been a key supporter of the new Open Data Institute (ODI). I  expect the   Open Up! event taking place in London next week will be a great  meeting place for those interested, a window onto the state of the  art, and a place to set new directions for the future.\n    Sir Tim Berners-Lee, Founder webfoundation.org\n    Open Up!, a conference hosted by the UK Government and  Omidyar Network, will help governments use technology to open up  and enable millions of citizens across the world to hold decision  makers to account and change lives. Join in at www.openup12.org.","item_date":"Nov 11 2012 23:23:42","display_item_date":"11-11-2012","url":"http:\/\/www.wired.co.uk\/news\/archive\/2012-11\/09\/raw-data","source":"www.wired.co.uk"},{"title":"Hackers, Geeks, Activists Invited to Alameda County Hackathon | NBC Bay Area","details":"Alameda County is offering up datasets to anyone wishing to code an app to put that data to good use, according to the Contra Costa Times.\n On Dec. 8, coders will have from 11:30 a.m. to 4:30 p.m. to code a civic app that helps fuel citizenship, according to the newspaper. The hackathon goes on at the Castro Valley library.\n An app must benefit the community, whether that is improving civic engagement or promoting digital education, the newspaper reported.\n Top prize for the most creative and most useful app is $3,000, the newspaper reported. Second place is $1,500, and third place takes home $500.\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Datasets are available at data.acgov.org, the newspaper reported.\u00a0","item_date":"Nov 11 2012 04:41:45","display_item_date":"11-10-2012","url":"http:\/\/www.nbcbayarea.com\/news\/local\/Hackers-Geeks-Activists-Invited-to-Alameda-County-Hackathon--177969691.html","source":"www.nbcbayarea.com"},{"title":"Brooklyn Hacker \u2022 That Moment When RMS Walks Into Your Hackathon","details":"* Editor\u2019s note: This actually happened, but I was not standing super close to the subject during the exchange. \u00a0Conversation paraphrased from eyewitness account.\n There I was at Boston Music Hack Day, rapping with Brendan Baker of public radio fame, when in walks Richard M. Stallman. \u00a0Beard matted into a bizarre pizza crust flapping off the end of his chin, he strolled casually past a small army of Ruby developers unaware of his celebrity while I did a quick double-take. \u00a0He waddled up to a grad student participating in the event who was hanging shyly by the pizza table. \u201cWhat\u2019s going on here?\u201d he asked.\n Student swallowed a hunk of his pizza and replied, \u201cMusic Hack Day.\u201d\n \u201cWhat\u2019s a Music Hack Day?\u201d he quickly replied.\n \u201cWell, it\u2019s a hackathon where a bunch of people hang out and build music stuff on a bunch of APIs.\u201d\n \u201cAPIs?\u201d RMS intoned gravely. \u00a0\u201dIs that like Software-as-a-Service?\u201d\n \u201cWell, yeah,\u201d the student sighed, knowing what to expect next.\n \u201cYou all should be working on a free alternative instead.\u201d\n And before anyone could bring up his Wikipedia page to confirm with the programming legend\u2019s photo, he turned about on a heel and marched out, in search of his own pizza with a grad student in tow.","item_date":"Nov 11 2012 04:34:22","display_item_date":"11-10-2012","url":"http:\/\/brooklynhacker.com\/post\/35445721578\/that-moment-when-rms-walks-into-your-hackathon","source":"brooklynhacker.com"},{"title":"Montrealers log on for anti-corruption \u2018hackathon\u2019","details":"Jacques Duchesneau, former chief of Montreal police, former anti-corruption squad leader and newly elected MNA, applauded Qu\u00e9bec Ouverts efforts Saturday. It\u2019s important to develop software that can detect questionable dealings before the actual deal takes place, he told them.\nPhotograph by: Dave Sidaway \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t, The Gazette\nWhen it comes to preventing corruption, technology may be our best weapon.\nOver the weekend, several dozen tech-savvy citizens gathered with their laptops and giddily massaged such scintillating data as Montreal and Laval city contracts, the registry of Quebec companies, donations to political parties and public calls to tender. Not exactly everyone\u2019s idea of spending a beautiful, crisp sunny day.\nBut what participants in Quebec\u2019s first anti-corruption \u201chackathon\u201d came up with are ways to cross-reference the data, so that citizens can ring the alarm when it appears something fishy is happening at city hall.\nAn app to link donors and businesses to political parties, for example, or a contract vigilance app that would scrape The Gazette database of Montreal and Laval city contracts and join it to the province\u2019s electronic public tender database.\nQu\u00e9bec Ouvert, a group of citizens working toward a more open, transparent and participatory form of democracy through access to public data, organized the event. The Gazette was a sponsor.\nJacques Duchesneau, former chief of Montreal police, former anti-corruption squad leader and newly elected MNA, applauded the group\u2019s efforts Saturday.\nIt\u2019s important to develop software that can detect questionable dealings before the actual deal takes place, he told the group.\nFor years, politicians, engineering and construction companies have been benefiting from inflated prices while citizens have paid with their high taxes, he said.\nThe Charbonneau Commission, which has been hearing explosive testimony for weeks about corruption and collusion in the construction industry, has angered and frustrated many. It\u2019s time to channel that energy into concrete tools that the average citizen can use to police the politicians, Duchesneau said.\n\u201cWe\u2019re on the right path,\u201d Duchesneau said, referring to this week\u2019s resignation of Montreal Mayor G\u00e9rald Tremblay and Laval Mayor Gilles Vaillancourt, both under a cloud of suspicion. \u201cBut the best is yet to come (at Charbonneau).\n\u201cWe\u2019re going to hear horror stories.\u201d\nAny software made in the hackathon will rely on public data, such as city contracts, the Quebec company registry, the registry of lobbyists, Quebec courts judgments and political donations.\nAlthough the data are public, it is not easy to acquire. The company registry, for instance, is an online database that displays only one record at a time. To have the entire data set, the event organizers had to hire a programmer to \u201cscrape\u201d all the records into a single document.\n\u201cThe questions you need to ask are what are we looking for and what are the weaknesses in the system,\u201d Duchesneau said.","item_date":"Nov 11 2012 03:57:53","display_item_date":"11-10-2012","url":"http:\/\/www.montrealgazette.com\/technology\/Montrealers+anti+corruption+hackathon\/7531105\/story.html","source":"www.montrealgazette.com"},{"title":"Director of Web Services at Washtenaw Community College ~ Authentic Jobs","details":"The Director of Web Services is responsible for coordinating and managing the design, development, and maintenance of the official Washtenaw Community College web site. Core responsibilities of this position include overall management of the Web site, directing the centralized Web development team, monitoring technical operations of College Web site, and providing oversight for training and support for Web Services staff.\n Essential Job Duties and Responsibilities: \u2022 Direct and supervise Web technical staff responsible for creating and supporting the College Web presence. \u2022 Maintain consistent graphic and editorial standards; ensure that standards such as code validity, browser version compatibility, and link currency are met; optimize Web site architecture and information design for navigability and performance. \u2022 Hire, supervise, develop, mentor and motivate Web Services staff. \u2022 Coordinate with the marketing department the look, voice, content, and distribution methods & processes for all information. \u2022 Track and analyze website usability and recommend modifications. \u2022 Research and evaluate Internet trends & opportunities and incorporate them in the Web Site as necessary. \u2022 Establish and enforce best online and Web practices. \u2022 Execute marketing initiatives and campaigns. \u2022 Implement interfaces for enabling interoperability with college\u2019s administrative system, Mobile technologies (iPads, iPhones, tablets, etc. ) and APIs for other systems. \u2022 Oversee project systems that will facilitate timely response to customer needs. \u2022 Perform other duties as assigned.\n Minimum Qualifications: \u2022 Bachelor\u2019s degree in technology-related field; Master\u2019s degree preferred. \u2022 A minimum of five years successful experience leading a team of web designers and programmers. \u2022 A minimum of two years successful experience in direct website development and design using a Content Management System such as WordPress, Drupal or Joomla \u2022 Proficient inPHPwith firm understanding of javascript, ajax, html, css, Mysql and Web 2.0 technology. \u2022 Practical experience using and creating web services APIs for mobile devices. \u2022 Demonstrated experience managing complex Web Sites across all phases (design, testing, production) to schedule. \u2022 Ability to triage online problems and apply necessary resources to resolve them. \u2022 Ability to communicate effectively both verbally and in writing. \u2022 Ability to manage priorities, deadlines, and time critical situations for multiple projects and clients, and proven diplomatic, leadership, and public speaking skills. \u2022 Experience in writing and design, marketing campaign management, and public relations content. \u2022 Strong projects and people management skills. \u2022 Strong problem solving skills.\n Position closes for applications on November 16, 2012\n \t\t\t \t\t\t\t\tApply for this job","item_date":"Nov 10 2012 16:57:23","display_item_date":"11-10-2012","url":"http:\/\/www.authenticjobs.com\/jobs\/14950\/director-of-web-services","source":"www.authenticjobs.com"},{"title":"2 Months of Android at Singly","details":"We have been working hard on the Singly Android SDK for the past couple months. Here is some of the progress we have made so far:\n A Singly client that handles authentication and interaction with the Singly APIs.\n Integrated native Facebook authentication if the user has the Facebook Android app installed.\n A drop in authenticated services activity that allows a user to authenticate with multiple services.\u00a0 It shows a list of all Singly services and which services a user currently has authenticated against.\n A drop-in friend picker component ListView that supports any size friends list.\u00a0 It has intelligent caching, image downloading, a table of contents, and other optimizations for performance.\n A remote image downloading and caching utility.\u00a0 It handles intelligent background loading of images, caching those images to disk, and loading them into memory when needed.\n Example applications that demonstrate using the Singly Android SDK.\n \n Here are some screenshots of some of the example components that are already in the SDK.\u00a0 Our goal is to create useful drop in components that enable Android developers to add social data to their applications with very little work.\n  \n Let us know what you think.\u00a0 Tell us if you are using the Android SDK or you would like to build an Android app using the SDK. We would love any feedback to help make the project better and we are always around and happy to help answer questions. As always, you can find us live at support.singly.com, or if you have Android-specific feedback\/questions, ping dennis(at)singly.com.","item_date":"Nov 09 2012 19:21:27","display_item_date":"11-09-2012","url":"http:\/\/blog.singly.com\/2012\/11\/09\/2-months-of-android-at-singly\/","source":"blog.singly.com"},{"title":"Secrets Of Facebook's Legendary Hackathons Revealed","details":"Meet Pedram Keyani, the coder behind the nocturnal events that shake up Facebook. Yes, were talking kegerators, Zucks hammock--and coding.\n\n\n\n\n\n     Revealing snapshots of innovators who grapple with big ideas, solve nagging problems, and upend giant industries.           Read More       \n\n\nPedram Keyani is a manager of engineering on Facebook\u2019s \u201csite integrity\u201d team, meaning he works to keep your account safe from spam and other threats. One of Keyani\u2019s greatest contributions to Facebook, though, may be his unofficial role in organizing and kicking off Facebook\u2019s hackathons, which are held internally about every six weeks. We caught up with Keyani to talk about kegerators, Mark Zuckerberg\u2019s hammock, and how a burst of scrappy nocturnal creativity can change the direction of an Internet behemoth.\n\n\nFAST COMPANY: When was your first Facebook hackathon?\n\n KEYANI: I think in my first month at Facebook. I joined over five years ago. I told my wife, I\u2019m not coming home, I\u2019ll be staying at work, and she kept prodding me: \u201cYou\u2019re staying at work?\u201d It took me 20 minutes to explain to her, we\u2019ll be making cool things, and everyone at the company does this. I went to my first hackathon and fell in love. The next day I was totally beat but couldn\u2019t wait to do the next one. Two weeks later I asked some people, \u201cWhen\u2019s the next one?\u201d and they said it wasn\u2019t planned. I sent out an email saying, \u201cHey, I\u2019m going to get some Chinese food and hack all night.\u201d It was super successful, and most of the company was there. The next day Mark Zuckerberg came to my desk and said, \u201cThat was awesome.\u201d So over time, it became a thing, where every six to eight weeks I asked if people wanted to hack.\n\n The thing around here is, code wins arguments. You could argue something for two days, or you could just make it and prove your point in an hour.\n\n\nThis seems like every company\u2019s dream. I can\u2019t imagine an auto plant getting its workers psyched about a building-cars-a-thon. Do people get overtime for this?\n\n We\u2019re not hourly employees. We\u2019re salaried. Some people come at 4 in the afternoon each day and leave at 4 a.m. No one keeps track of time. The other thing about hackathons is that the most critical rule is you can\u2019t work on the same thing as your day job. It\u2019s a way to experiment with ideas in a low-cost way. Lots don\u2019t make it into products, but every hackathon tends to result in four or five things implemented on the site. A couple have changed the direction of the company.\n\n\nFor example?\n\n Chat. For a long time, there was a lot of negative pressure in the company against building a chat client. The thing around here is, code wins arguments. You could argue something for two days, or you could just make it and prove your point in an hour.\n\n\nHow do people form groups during a hackathon?\n\n We have a group called Hackathon Ideas, and in the week leading up to a hackathon, people post ideas, and groups form organically. One project I worked on two hackathons ago, it was four interns, an infrastructure engineer, someone from our sales team, and me. It worked beautifully, and now a lot of us are friends--people who didn\u2019t talk to each other or know each other before.\n\n\nAs you grew into the hackathon organizer, how\u2019d you decide to hold them every six weeks?\n\n It turns out if you do it more often than every six weeks, people are like, \u201cDude, I have a family. I need to go home. I can\u2019t disrupt my life like this.\u201d But after six or seven weeks, if I wait too long, I\u2019ll start getting two or three emails a day asking about it, and it\u2019s clear we need another hackathon.\n\n\nWhat are some traditions around the hackathons?\n\n We always get Chinese food at the same place, Jing Jings in Palo Alto. Even though now we\u2019re bigger and further away, because of tradition we go to the same place. People pile up food and hack for hours. Usually about 20-30% of people, by 3 in the morning, are asleep or at home. Then steadily every hour after that about 10% of the people drop off until at 6 in the morning I shut the thing down. Having done 30 of these things, I\u2019ve learned an important skill: The Tuesday of a hackathon, I always find time to take a two-hour nap. I\u2019ll find a conference room somewhere. In the previous location, we were four blocks from Mark\u2019s house, and I said, \u201cHey, you have a hammock in your backyard--can I sleep there?\u201d He said, \u201cThat\u2019s silly, just take the key,\u201d and I\u2019d crash on his couch.\n\n We came up with the idea of putting a computer on top, having people swipe an employee badge, and it would take a picture of them with their beer and that would go in their news feed.\n\n\nWhat\u2019s the hardest you\u2019ve laughed at a hackathon?\n\n I built a thing called Keg Presence with my friend George. He had a startup that didn\u2019t do very well, but one thing left over from his startup was a kegerator. We thought, \u201cWe have to figure out a way to bring this to work and not get fired.\u201d We came up with the idea of putting a computer on top, having people swipe an employee badge, and it would take a picture of them with their beer and that would go in their news feed. At the hackathon in August of 2009, we wheeled in this kegerator and went to work as quickly as possible. We tested it, and photos of both of us drinking our beer populated everyone\u2019s news feeds. All of a sudden we saw heads popping up around the office: \u201cHey, where\u2019s the keg?\u201d Within 10 minutes, there was a line of 15 people. We killed the keg that night.\n\n\nBut how do you scale that for a billion users?\n\n That\u2019s a lot of beer. I don\u2019t know if we could scale that. But the basic concept has grown. Basically what we did is make a way for a physical interaction to be the equivalent of entering your user information and password and entering content on the site. The basic idea is, how do you marry Facebook with the physical world?\n\n\nFor other non-coding companies, how can they get the spirit of a hackathon at their workplace?\n\n Lots of other groups--legal, HR, business development--use hackathons to rethink how they do their job, or how they can restructure what they\u2019re doing. I think the core idea is to take ideas you haven\u2019t had a chance to focus on and think about them in a different way. Lots of companies have the notion of a \u201cthink week,\u201d a week to brainstorm other things. It doesn\u2019t have to be in the middle of the night. If a company has hired the right people and trusts its employees to have good ideas, it should trust them to have the free time and autonomy to come up with amazing things the company should explore.\n\n\nThis interview has been condensed and edited. For more from the Fast Talk interview series, click here. Know someone whod be a good Fast Talk subject? Mention it to David Zax.\n\n\n\nShare:","item_date":"Nov 09 2012 17:05:58","display_item_date":"11-09-2012","url":"http:\/\/www.fastcompany.com\/3002845\/secrets-facebooks-legendary-hackathons-revealed","source":"www.fastcompany.com"},{"title":"US Doctor Data to be \u201cOpen Eventually\u201d ","details":"The company behind the project is NotOnly Dev, a Health IT software incubator who describe themselves as a \u201cnot-only-for-profit\u201d company. Earlier this year they released a Doctor Social Graph, which displays the connections between doctors, hospitals and other healthcare organizations in the US. The dataset used is billed as \u201cTHE data set that any academic, scientist, or health policy junkie could ever want to conduct almost any study.\u201d\n  Our goal is to empower the patient, make the system transparent and accountable, and release this data to the people who can use it to revitalize our health system.  They\u2019d acquired the data for that project through a whole load of FOI requests which they gradually stitched together \u2013 but to take the project to the next level they wanted to buy the State Medical Board report data for every state in the US. This data usually includes information on the doctors medical school, information about board certification and information on disciplinary actions against the doctor. In combination with what they\u2019ve already done through FOI requests, this data has massive potential.\n  Yesterday the crowdfunding drive ended, and the project has raised well over its target of $15,000. So now they can get hold of the data, clean it up and release it back into the community openly.\n  But not right away.\n  The company have developed the idea of an \u201cOpen Source Eventually\u201d license. The data, once purchased, will remain exclusive for another six months. Thus the incentive for funders is that they will have exclusive access rights during that period, before the data goes open (under a CC BY-SA 3.0 license).\n  Here\u2019s Fred Trotter talking about the idea:\n    NotOnly Dev reckon the \u201cOpen Source Eventually\u201d license is \u201cthe perfect compromise\u201d for securing the funds necessary to get expensive data out into the public. What do you think?","item_date":"Nov 09 2012 16:58:41","display_item_date":"11-09-2012","url":"http:\/\/blog.okfn.org\/2012\/11\/09\/us-doctor-data-to-be-open-eventually\/","source":"blog.okfn.org"},{"title":"CEO of Open Data Institute: we need to make open data non-threatening (Wired UK)","details":"The disruptive power of open data neednt be seen as a threat to  companies and government agencies, argued Gavin Starks, chief  executive of the Open Data  Institute.\n    Part of the nature of open data is that its transformative,  he told Wired.co.uk. The nature of transformation is that its  also disruptive. There will be certain businesses, certain ways of  doing things that will change.\n    The   Open Data Institute is a government-funded centre of  excellence, based in London, that focuses on incubating and  nurturing new businesses wanting to harness open data, engaging  with the private and public sector with regards to how to make the  most of open data and promoting standards. It was first announced  is George Osbornes Autumn Statement in 2011, but only opened in  October 2012.\n    Starks argues that a big part of the Institutes remit is to  encourage people to engage with the open data movement -- just as  they did with the launch of the web -- rather than ignore it.  Change doesnt mean that the sky falls, although there are lots of  people who use that as an argument, he said.\n    In the four weeks since its launch, the ODI has already made  significant progress. There is a team of six and job ads out for  seven additional roles. The institute has a three-part mission,  which includes unlocking data supplies, empowering people to use  open data and incubating businesses.\n    There are already three startups embedded at the ODI -- Mastodon  C, Placr and Locatable -- who are working on developing  business models around open data. According to Starks, one of the  startups has interrogated the data and found a potential saving in  the health sector of between \u00a3100 and \u00a3200 million. ODI has also  held a number of hacking events, including one using health data,  as well as a schools innovation day. It is also working to create  data-driven artworks to help bring to life what might be perceived  as a dry, inaccessible topic. \n    One of the key challenges for the ODI is maintaining focus: We  are a startup and there is a lot of latent demand, but we need to  focus on doing a few things well.\n    Starks will be speaking about the ODI at Future London,  which takes place on 20 November. For more information and to buy  tickets, visit Future-London.com.\n    Image: Shutterstock.com","item_date":"Nov 08 2012 17:42:31","display_item_date":"11-08-2012","url":"http:\/\/www.wired.co.uk\/news\/archive\/2012-11\/08\/open-data-institute-gavin-starks","source":"www.wired.co.uk"},{"title":"Government to Industry: Let Us Pay You for Government Data","details":"Sometimes the information economy ushers in fantastic opportunities for government to more with less. Efficiencies derived through smart use of IT practices, crowdsourcing,\u00a0transparency and social media are areas that have \u2018injected\u2019 dynamism into government operations. Unfortunately, the opposite holds true: sometimes the information economy ushers in stupid opportunities for private parties to take advantage of government.\n Exhibit A is the Department of Homeland Security\u2019s recent\u00a0solicitation\u00a0for Airline\u00a0Industry\u00a0Flight Data. Seems odd, given that the FAA, which oversees air travel in the US, um, has this information*. But not completely\u2013 all aircraft have to file a flight plan with the FAA before take off, and this is public. However, for a variety of reasons, the plan filed may not be the actual route flown.\n This RFP seeks information that is known by a small group of companies active in the airline ticketing and scheduling industry. GDS acts as a sort of back office for airline ticketing (the history of the GDS alone is a fascinating one, but that for another time), and schedule aggregators (like Innovata) work with IATA to provide schedule data in a standardized format. Getting to the details, the solicitation seeks detailed information relating to US originating or terminating flights in the industry-standard SSIM format. Specifically, it seeks:\n \n b. 502 \u2013 In-Flight service (meals for sale, movies, smoking, non-smoking, etc)\n Seems odd to specifically identify three attributes that you do not want.\u00a0What is weird about this\u00a0solicitation\u00a0(apart from the FAA already having this data) is that DHS already has it\u2013 under the Secure Flight program, the TSA collects traveller information from airlines. This includes a unique ID (PNR) that can be used to associate with the aircraft, and therefore capture info the TSA seeks!\n *Ok, we lied. The FAA actually doesn\u2019t have this data. Of course you would think they create this data, but they don\u2019t\u2013it\u2019s licensed from a private party, giving more opportunities to license public resources to additional government agencies, like DHS!","item_date":"Nov 08 2012 08:00:00","display_item_date":"11-08-2012","url":"http:\/\/blog.urbanmapping.com\/2012\/11\/08\/government-to-industry-let-us-pay-you-for-government-data\/","source":"blog.urbanmapping.com"},{"title":"Developers are the Core of the Mobile Advertising Market","details":"The Seoul Economic Daily has covered the InMobi opinion piece today. Eunjung Son, Developer Channel Manager of InMobi Korea, has explained that Korea market related to mobile advertising is quickly becoming the core market, but needs to have more developer focus with showing some insight.\n With the world\u2019s mobile users reaching 1 billion users, the recent earnings of two IT companies both indicate that \u2018mobile advertising is the general trend.\u2019 As Facebook\u2019s third and fourth quarter\u2019s strong earnings were driven by profit from mobile advertisements, stock price in off-board transactions increased up to 13%. The reason behind Google\u2019s earnings dropping at the same time could also be inferred as the market shift towards mobile. It is thus not an overstatement to say that these changes are the market \u2018reaction\u2019 in the mobile era.\n In Korea, more than half of mobile users are smartphone users. Breaking down the time spent on smartphones, calls only consist of 37% and the remaining time is spent enjoying multi-media and various applications. Such figures give ground to the assertion that the market related to mobile advertising is quickly becoming the core market. Mobile advertisement is no longer a new phenomenon or an unfamiliar medium. Like the banner advertisement which appeared with the internet in the late 1990s, mobile advertisement is settling down as an information element within everyday life.\n However, mobile advertisement possesses not only the freshness of a new medium but also a characteristic that differentiates itself with the pre-existing advertisements. That is, mobile web and app developers serve as core figures in the process of conveying the message to consumers and ultimately inducing consumption. Compared to traditional internet advertisement, mobile advertisement provides more diverse and departmentalized ads. This is why the role of app and service developers is particularly important as they serve the role of media.\n In the mobile advertisement market, app developers are primarily the source of generating simple revenue through advertisement exposure. However, on second thought, apps they make can be a strong \u2018media\u2019 like a broadcasting station or a newspaper in the past. Ultimately, securing killer apps in an advertisement network is the most necessary and best way to satisfying the advertiser. Hence the relationship with developers desperately needs to improve from a simple contractual relationship to a partnership.\n To app developers, mobile advertisement is a blue ocean. Although the public still tends to regard it as a simple means to generate revenue by allocating a part from app interface, it needs to move away from this stereotype. Through informational advertisement that can create a synergy effect with app contents, developers can provide better experience and service to users as well as enjoying the maximum revenue. Recently, many venture companies are starting up based on mobile service and apps. They can target not only the Korean market but also overseas and have the opportunity to introduce their apps and service to a bigger market.","item_date":"Nov 08 2012 08:00:00","display_item_date":"11-08-2012","url":"http:\/\/www.inmobi.com\/inmobiblog\/2012\/11\/08\/korea_developer\/","source":"www.inmobi.com"},{"title":"Developer Spotlight: Scoop.it","details":"Anthony Limjuco, Nov 8, 2012              Author\u2019s note: This is the first in a series of blog posts showcasing developers using our APIs and highlighting best practices.\n Scoop.it is a social media publishing tool that helps you reach interest-based audiences by enabling you to easily curate content on a given topic. With Scoop.it, you can find relevant content faster, edit it easily and publish it in 1-click to engage your followers and acquire new ones via social discovery. Unlike similar tools, Scoop.it is uniquely organized around topics as opposed to the curators themselves.\n Utilizing Sign In with LinkedIn, users can create a Scoop.it account by leveraging their LinkedIn identity. This simplifies the sign-up process and eliminates the need to provide additional data, including e-mail validation. In addition, the user\u2019s profile is automatically populated within the site. Users can also log-in to Scoop.it with their LinkedIn account, whether they\u2019ve created it using the LinkedIn APIs as described above or have previously bound their pre-existing Scoop.it and LinkedIn accounts.\n With the Share API, the Share to LinkedIn option is prominently displayed in the publishing window, making it easy for Scoop.it users to distribute compelling articles with the world\u2019s largest professional network.  \n  \u201cOur integration with LinkedIn is a natural fit and complement to our business. With most of our active users utilizing Scoop.it for professional reasons, LinkedIn is a great platform to publish to and now they can easily sign in and share content with their professional network in an engaging way. And with LinkedIn\u2019s massive scale, even more professionals will be exposed to Scoop.it\u2019s value proposition.\u201d - Guillaume Decugis, CEO and co-founder Scoop.it","item_date":"Nov 08 2012 08:00:00","display_item_date":"11-08-2012","url":"https:\/\/developer.linkedin.com\/blog\/developer-spotlight-scoopit","source":"developer.linkedin.com"},{"title":"AT&T Fined $700K Over Unauthorized Data Charges","details":"The Federal Communications Commission on Tuesday hit AT&T with a $700,000 fine for switching pre-paid customers to a monthly data plan without their consent.\nAT&T will also refund those customers affected by the unauthorized switch, which amounted to $25 to $30 per month, depending on data use.\nThe problems started in 2009 when AT&T started requiring new smartphone customers or those who upgraded their device to enroll in a monthly data plan. Customers with existing pay-as-you-go plans (or disabled data) who kept their phones could remain on pre-paid data plans, but some consumers who replaced their phones under warranty or insurance, or who moved to a new home were switched to the monthly plans anyway.\nAffected customers filed a complaint with the FCC, leading to this weeks consent decree.\nThe move sends a clear signal that wireless carriers cant wrongfully charge consumers, FCC Chairman Julius Genachowski said in a statement. These strong FCC accountability measures will ensure customers are not over-charged. I am pleased that AT&T is taking the appropriate steps to resolve this issue.\nThe $700,000 will be handed over to the Treasury Department.\nAT&T has agreed to a compliance plan that includes: consumer notification, training of customer care representatives, and periodic compliance reports to the FCC. AT&T must also search its records again to make sure it didnt miss any other customers who were improperly added to a monthly plan.\nEarlier today, AT&T announced that it will be investing $14 billion in networks over the next four years, $8 billion of which will go toward wireless improvement and expansion. Genachowski said in a statement that the news is proof positive that the climate for investment and innovation in the U.S. communications sector is healthy.","item_date":"Nov 08 2012 02:04:14","display_item_date":"11-07-2012","url":"http:\/\/www.pcmag.com\/article2\/0,2817,2411850,00.asp?kc=PCRSS03069TX1K0001121","source":"www.pcmag.com"},{"title":"Mobile app development: 94% of software developers betting on HTML5 winning | VentureBeat","details":"A few months ago when Facebook admitted defeat and went native with its iOS app, some thought it was a death-knell for HTML5. But most of the 4,034 developers in a recent survey disagree \u2014 vehemently.\n In fact, according to the\u00a0recent survey by mobile app tools vendor Kendo UI, 94 percent of developers are either using HTML5, or plan to start using it this year, leaving only a minuscule six percent who have no plans to develop with HTML5 before 2013 rolls around in just two short months.\n \n That\u2019s the kind of stat that is sometimes easy to manipulate when there\u2019s a larger percentage in the wishy-washier \u201cplanning\u201d segment, but not in this case, with a full 63 percent of developers using HTML5 today.\n That\u2019s impressive.\n HTML5 is an updated version of the old-school hyper-text markup language that makes up much of the web today. It enables developers to build on their existing knowledge of web technologies such as HTML, Javascript, and cascading style sheets to create mobile apps through frameworks such as Adobe\u2019s PhoneGap rather than having to learn Objective-C to write full-native iPhone\/iPad apps, or Java to write Android apps. Probably even more importantly, by using cross-platform technologies like PhoneGap, HTML5 enables developers to write their apps once and deploy on all major mobile platforms.\n Given the numbers who are already using HTML5, it\u2019s no shock that 82 percent of developers also say that the technology will be important to their jobs in the next year, and a further 12 percent believe it will be become important within the next two years.\n Developers\u2019 rationale for using and preferring HTML5 is no shock to anyone who\u2019s ever developed native apps for multiple mobile platforms. Sixty-two percent said that HTML5\u2032s ability to enable cross-platform support was an important factor in choosing the technology, with another third saying that the availability of tools and code libraries make it appealing.\n \n But the biggest reason developers like HTML5?\n Familiarity. Almost three-quarters of developers said that HTML, Javascript, and CSS were familiar languages which enabled easy access to mobile app markets.\n And what about Facebook\u2019s move away from HTML5. Apparently, that hasn\u2019t shaken developers\u2019 belief in the technology \u2014 half of them weren\u2019t even aware of the move. Of those who did, however, while 17 percent had less faith in HTML5 after the news, 18 percent had more faith.\n The survey is obviously from a company with a vested interest in HTML5 adoption, but it jibes with what I\u2019ve heard from people like Andi Gutmans, key developer of the PHP programming language and current CEO of Zend, who is pushing what he calls cloud-connected mobile apps and just released a version of Zend Studio that enables developers to build native mobile apps with familiar web technologies.\n Which, frankly, just makes sense if you don\u2019t want to build the same app three times for iOS, Android, and Windows Phone \u2026 and if your app is not the most computationally intensive app in the world and absolutely needs to be fully native for performance reasons.\n photo credit: codepo8 via photopin cc","item_date":"Nov 08 2012 01:39:44","display_item_date":"11-07-2012","url":"http:\/\/venturebeat.com\/2012\/11\/07\/mobile-app-development-94-of-software-developers-betting-on-html5-winning\/#0C2YdLr5Emj241MX.99","source":"venturebeat.com"},{"title":"Privacy Icons Legal Hackathon","details":"The Center for Internet and Society at Stanford Law School is a leader in the study of the law and policy around the Internet and other emerging technologies.","item_date":"Nov 07 2012 18:23:00","display_item_date":"11-07-2012","url":"http:\/\/cyberlaw.stanford.edu\/events\/privacy-icons-legal-hackathon","source":"cyberlaw.stanford.edu"},{"title":"Introducing the New Pocket API for Developers and Publishers","details":"It has been an incredible year for Pocket: We\u2019ve grown to more than 6 million users, and we\u2019ve expanded to new platforms including Mac, Chrome and Safari, in addition to our iOS, Android and Kindle Fire apps.\n But we couldn\u2019t have done it without you. We now have 10,000 developers integrating their apps and sites with Pocket, and 45% of Pocket\u2019s more than 1 million daily saves come from third-party apps.\n \n \u201cSave to Pocket\u201d is now a core part of the user experience inside the most popular apps and websites\u2014from Twitter and Flipboard to Tweetbot, Digg,\u00a0BuzzFeed and more. And in content-focused apps like Zite, Pocket is often one of the most-used share services, alongside Twitter and Facebook.\n Today we\u2019re introducing the new\u00a0Pocket API, which makes it even easier for developers and publishers to put Pocket to work in their own apps\u2014and to do so in more powerful ways.\n Start Building with the New Pocket API\n What\u2019s In the Pocket API\n Our new API includes easy integration and access to the great Pocket features available in our native apps. Here\u2019s what\u2019s new:\n \u2022\u00a0Quick authentication: With OAuth 2, users can connect to Pocket with just two taps and without having to enter their username and password.\n\u2022\u00a0Drop-in iOS\/Mac SDK: With just a few lines of code, you can drop Pocket\u2019s all new Objective-C SDK into your app, which manages authentication, performs API requests, and provides access to all of the new API features. (See our video demo below.)\n\u2022\u00a0New API Features: Developers and publishers can access all of Pocket\u2019s most up-to-date features, including favorites, content type filters, more robust tagging, and domain searches.\n\u2022\u00a0New developer portal: We have a new developer site with improved documentation including step-by-step implementation instructions and examples.\n \u201c\u2018Save for later\u2019 has become an essential part of mobile news consumption,\u00a0and I wouldn\u2019t even consider launching a news app without Pocket integration.\u201d -Mike Klaas, co-founder, Zite\n \u201cPocket integration continues to be one of most asked-for features in the new Digg, and the new SDK made implementation super easy!\u201d\u00a0-Jake Levine, general manager, Digg\n \u201cSaving articles to Pocket is a must-have feature for a content-driven application like Readingly. And Pocket users are some of the most engaged users we have.\u201d\u00a0-Alex Kristofcak, founder, Readingly\n \u201cSave-for-later support is an important feature for TweetCaster, which makes integrating with Pocket essential. From a development perspective the new API and iOS SDK couldn\u2019t have been easier to implement and we think our users will appreciate how seamless the integration is.\u201d\u00a0-Evan Conway, president of OneLouder Apps\n Developers can get started immediately with the Pocket API: Just sign up here and start building.\n More Resources: Pocket SDK Demo","item_date":"Nov 07 2012 08:00:00","display_item_date":"11-07-2012","url":"http:\/\/feedproxy.google.com\/~r\/ReadItLaterBlog\/~3\/doA5y_JXvdE\/","source":"feedproxy.google.com"},{"title":"Big Data is the New Black of Tech Trends and It\u2019s Revolutionizing our Healthcare System","details":"Tech industry buzzwords seem to seep into our language with fluid motion. Since 2000 we\u2019ve been http\u2019d, emoticon-d, cookied, virtualized, and iPod\/iPad\/iRevolutionized. Tech buzzwords flow freely in our culture, and crash in a frenzied mashup crescendo every year at SXSW. In 2012 \u201cBig Data\u201d was crowned King of the buzzwords we use but don\u2019t quite understand. How can this be when Big Data is pointed to as the oracle of everything from consumer purchasing to political campaigns, and the revolutionary mastermind of healthcare? McKinsey calls Big Data \u201cthe next frontier for innovation, competition and productivity\u201d \u2013 yet ask 10 people what it is, you\u2019ll get 10 different answers\u2026and few to no business plans for how to harness its power.\n Most people will tell you that Big Data is\u2026big. Conventionally, Big Data is viewed as data sets that are so large and complex for any single, conventional tool to capture and manage. More colorfully, Yahoo CEO Melissa Mayer recently likened Big Data to \u201cWatching the universe develop a nervous system.\u201d The net effect is that systems analysts, developers, healthcare systems and even the US Government are scrambling to develop and adopt a model to harness and analyze the flood of information being produced (the federal government recently set aside $200 million to fund big data initiatives, while the National Science Foundation and the National Institute of Health have jointly awarded $15 million to fund eight big data research projects, focused on the improvement of health).\n Big Data is Social\u2026is it Healthy?\n As shown in the infographic above, Big Data is social \u2013 we are generating and sharing terabytes of information everyday online. Social sharing is becoming an integral fabric of our society (as of February 2012, 66% of online adults use social networking sites according to the Pew Internet & American Life Project).\n Organizations such as Kaiser Permanente and Aetna continue to pump resources (man-hours and dollars) into making electronic personal health data available for its members via secure portals. Couple our online tendencies with the surge in healthcare initiatives in the US, the allocation of funds to make digital health data more transparent (see above), and the mainstreaming of digital health conferences \u2013 from SXSWi Healthcare Track to the Wired Health Conference in NYC last month \u2013 and it would seem natural that health data would become more social. Yet the numbers remain low. According to the Pew Internet & American Life project (May 2011) only 59% of all US adults online have searched for health information, compared to 75%+ who look for car-buying information online, according to JD Powers and Associates.\n Data from the May 2011 Pew Report The Social Life of Health Information reveals that the numbers are lower for our current social networks:\n 23% of social network users follow friends health experiences online\n 24% of users have consulted reviews for medical treatments and 16% read reviews for doctors and healthcare providers (vs 57% who rely on restaurant reviews)\n 15% of social network users have gotten any health information from their social sites (vs. 99% who view cat videos).\n \nAll joking aside, there is a visible gap between the quest for information online, and the proliferation of electronic data we are creating, and the use of these same online tools for healthcare. Despite this gap, Harris Interactive, the McKinsey Global Institute and other pollsters tell us that large numbers, upwards of 65%, of healthcare consumers want their information accessible online \u2013 everything from prescription information to doctor\u2019s notes to their test results. Beyond personal care, what patients and healthcare consumers would do with this information is unclear. Is it a leap to think that the Gen-Xers, Millennials and generations after them would embrace social sharing to the level of swapping health information online.\n The ePatient Movement Setting the Trend\n ePatients, a motivated group of online health consumers who use \u201ce\u201d to mean empowered, engaged, equipped and enabled, will tell you they want their data to use as they see fit to take care of their health (see e-Patient Dave deBronkart\u2019s battle cry \u201cGimme My Damn Data!\u201d). Groups like Partnership with Patients, and Fast Forward Health, the roving TriBeca-meets-Sundance film festival for Healthcare, continue to spotlight patient needs and healthcare consumer demands; both groups use social media to mobilize advocates and build followings for their events. Healthcare conferences have taken over Twitter as well, with the 2012 HIMSS (Healthcare Information and Management Systems Society) Conference set a record number of tweets, generating 33,247 mentions and the #HIMSS12 hashtag was used 28,434 times.\n To me, this looks like the beginning of a long-term, game-changing trend. Analysts follow trends, leaders predict them and business luminaries create them. Consultants, the next time clients come to you looking for the next Facebook or Twitter, or how to launch a healthcare initiative on either, consider creating a bonus section to that proposal to test the waters: the next big platform is out there, waiting to be built. Will your client lead the way? Where are the healthcare luminaries who can set the trend for social health, marrying big data, a growing online presence of ePatients, and a national interest in wellness? That\u2019s the next big thing the dreamers can make a reality \u2013 that is the future of digital and social health.\n About the author: Gigi Peterkin is a writer and digital media consultant. She has worked in healthcare for more than 15 years, launching social media programs for AstraZeneca, GE Healthcare, and Merck Consumer Health among others. You can learn more and hire Gigi via her website, www.gigipeterkin.com\n  \n  If you enjoyed this post, please consider leaving a comment or subscribing to the RSS feed to have future articles delivered to your feed reader.","item_date":"Nov 07 2012 08:00:00","display_item_date":"11-07-2012","url":"http:\/\/blog.viralheat.com\/2012\/11\/07\/big-data-is-the-new-black-of-tech-trends-and-its-revolutionizing-our-healthcare-system\/","source":"blog.viralheat.com"},{"title":"Manipulate the worksheets uploaded at Amazon S3 Storage using Saaspose.Cells REST API","details":"Workbooks are extensively used by a number of people from different walks of life for data analysis and information sharing. The worksheets contain the information such as charts, images, numerical data, hyperlinks, comments, formulas.\u00a0Saaspose.Cells\u00a0is a REST based API that supports\u00a0manipulation of workbooks\u00a0in the cloud. There are a number of operations that can be performed on the data to achieve multiple tasks involved in document manipulation e.g. you may want to get a specific chart or shape from the worksheet. Saaspose storage is the default storage for uploading the files and processing them with\u00a0Saaspose.Cells.\n Saaspose\u00a0offers you to manipulate the worksheets uploaded at\u00a0Amazon S3 storage\u00a0using Saaspose.Cells REST API.\u00a0You can refer to our blog post in which we have mentioned the\u00a0integration of Amazon S3 storage\u00a0with\u00a0Saaspose File Format APIs.\u00a0Saaspose.Cells\u00a0allows you to\u00a0manipulate the worksheets\u00a0uploaded at storage through easy solutions that eliminate the need of manual calculations, analysis and data management.\n You can\u00a0extract a cell\u00a0from a worksheet uploaded at\u00a0Amazon S3 storage. You can also\u00a0get a list of cells\u00a0in a worksheet uploaded at Amazon S3 storage.\u00a0Saaspose.Cells\u00a0allows lets you\u00a0get cell formatting\u00a0in a worksheet uploaded at Amazon S3 storage by using this API in your applications.\n Similarly, you can\u00a0calculate formula\u00a0in a worksheet uploaded at Amazon S3 storage by using\u00a0Saaspose.Cells REST API. You can\u00a0get a specific shape\u00a0from a specific spreadsheet uploaded at\u00a0Amazon S3 storage\u00a0through this feature rich API. You can also\u00a0get a list of auto shapes\u00a0defined in a worksheet uploaded at the storage.\n You can enjoy the new experience of worksheets manipulation with a variety of features on external storage provider. Integration of Amazon S3 storage has added the flexibility to upload your files on\u00a0Amazon S3 storage\u00a0and use\u00a0Saaspose.Cells REST API\u00a0URIs to perform multiple operations on the worksheets. You can utilize our\u00a0SDK\u00a0and\u00a0REST\u00a0examples to work with presentations across any platform. All you need to do is download the required SDK and get started with\u00a0Saaspose.Cells API.\n For more information, please refer to\u00a0Saaspose.Cells documentation. Stay tuned to our\u00a0blog\u00a0for more updates and announcements. You can also get\u00a0free Saaspose account\u00a0for evaluation purposes,\u00a0sign up\u00a0today and start using Saaspose APIs to process your files uploaded at\u00a0Amazon S3 storage.\n \t\t\t\t\t\t\t\t\t\t\t\n      \t\t\t\t\t  \t\t\t\t\t\tThis entry was posted in Saaspose.Cells and tagged calculate formula, Cloud API, get auto shapes, get cell formatting, REST API, spreadsheets, worksheets. Bookmark the permalink.\t\t\t\t\t\t\t\t\t\t\t\n              scroll to Top","item_date":"Nov 05 2012 08:00:00","display_item_date":"11-05-2012","url":"http:\/\/saaspose.com\/blog\/saaspose-cells\/archive\/2012\/11\/06\/manipulate-the-worksheets-uploaded-at-amazon-s3-storage-using-saaspose-cells-rest-api.html?utm_source=rss&utm_medium=rss&utm_campaign=manipulate-the-worksheets-uploaded-at-amazon-s3-storage-using-saaspose-cells-rest-api","source":"saaspose.com"},{"title":"Foursquare Explore now has ratings powered by where people actually like to go, not just star ratings","details":"With today\u2019s\u00a0iOS update, we\u2019ve made finding the best place on-the-go even easier. Now when you search for somewhere to go in Explore, you\u2019ll see a score from 1 to 10 next to the name of a place. It\u2019s our way to give you a quick sense of how much people love it, and it\u2019s a lot different from the other types of ratings you see today.\n \n Instead of other sites where every place gets 3.5 stars, we come up with our scores using the same Foursquare magic that powers Explore. We look at signals like tips, likes, dislikes, popularity, loyalty, local expertise, and nearly 3 billion check-ins from over 25 million people worldwide. And, with every check-in and Explore search, our scores will get smarter and better.\n Download the update\u00a0and find some new places to try! Or, look at scores and search from your computer at Foursquare.com.","item_date":"Nov 05 2012 08:00:00","display_item_date":"11-05-2012","url":"http:\/\/feedproxy.google.com\/~r\/thefoursquareblog\/~3\/udlYuRBl9S8\/","source":"feedproxy.google.com"},{"title":"Do it for the `hack' of it! | Deccan Chronicle","details":"Watching the charming Seth Green nimbly make his way in and out of complicated codes in The Italian Job must have fascinated many a young tech geek. The art of breaking down and improvising codes and software is an asset much sought-after in our city. Hackathons are becoming increasingly popular in IT firms and some are even conducting open hack events annually . With Microsoft announcing its hackathon Wowzapp that kickstarts on November 9, we look at how hackathons have changed the way youngsters ideate in the city . The first reaction when you say hackathon is, \u201cAre you breaking the law?\u201d The software developers beg to differ! \u201cWe aren\u2019t doing anything anti-establishment. What people refer to is cracking, not hacking.\n Hacking is a constructive activity where you create a new idea, or develop an existing one in a way no one else has,\u201d says application developer Nikhil Baliga.\n He won the Open Hack 2012 conducted by Yahoo! a couple of months ago.\n Hackathons are usually on a time crunch (the word is a portmanteau of hack and marathon) Experienced programmers claim that it\u2019s not always possible to create a new code from the word go and finish it in 48 hours. There is a lot of fine tuning one\n can do, and the entire process is exhilerating for the tech lovers. The coding community feels open events like these help dispel myths about the creative process of hacking. Now, companies are opening up to the idea of conducting internal hack weekends to improve the quality of codes and software. Rakesh Menon, user interface lead at a software firm says, \u201cBengaluru is an entrepreneurial hub so the willingness to generate new ideas is very high. Several startups conduct hackathons to see what their employees can produce in a certain period of time.\u201c\n Its definitely a tech-heavy activity but of late, there has been a significant increase in the interest levels amongst young employers. For those who want to make a career out of it, such events are an added \u201cBig honchos are spotted at these events and they follow the programmers very closely. It\u2019s not always about speed, you need to be original with your ideas too.\u201d For those who love their fix of apps on smartphones, this could be your thing. The local chapter of the Android Dev Camp took place earlier this year and had a huge turnout. \u201cOpen Hack Day celebrates the culture of innovation, bringing developers from all over the country to turn ideas into a working prototype, or a hack as it\u2019s called. It provides a chance to work together, and make something cool that can potentially solve a \u201crealworld\u201d problem impacting millions of Internet users,\u201d says a Yahoo! spokesperson.\n If you thought coding is a staid and monotonous job, think again. Hacking can be fun and profitable too! \u2014SB","item_date":"Nov 04 2012 17:09:21","display_item_date":"11-04-2012","url":"http:\/\/www.deccanchronicle.com\/tabloid\/bengaluru\/do-it-hack-it-625","source":"www.deccanchronicle.com"},{"title":"HelloSign Launches the Hassle-Free eSignature API","details":"HelloSign, the easy-to-use and completely free eSignature solution, is opening up its eSignature and Forms platform to developers with the launch today of the hassle-free eSignature API. The HelloSign platform allows developers to enable the same streamlined eSignature solution on their own sites that HelloSign users have come to expect.\nUnlike other eSignature APIs which require expensive developer accounts, unnecessary \u201ctechnology certification\u201d and other complications, the HelloSign API is completely open and free to all developers.\n\u201cWe believe that true innovation comes from developers that are empowered to experiment and build on a transparent platform,\u201d said HelloSign CTO Neal O\u2019Mara, \u201cOur eSignature and Forms API will enable developers to quickly and easily implement eSignatures.\u201d\nThe HelloSign API is REST-based and can be found at: http:\/\/www.hellosign.com\/info\/api\nAbout HelloSign \nCreated by the makers of HelloFax, HelloSign provides always free and easy to use digital signatures. HelloSign combines the simplicity of a consumer product with business grade digital signatures, empowering consumers and businesses alike to securely sign documents online.","item_date":"Nov 03 2012 05:02:45","display_item_date":"11-02-2012","url":"http:\/\/www.prweb.com\/releases\/2012\/11\/prweb10084083.htm","source":"www.prweb.com"},{"title":"Twitter to Add Photo Filters to Compete With Instagram - NYTimes.com","details":"","item_date":"Nov 02 2012 22:04:47","display_item_date":"11-02-2012","url":"http:\/\/bits.blogs.nytimes.com\/2012\/11\/02\/twitter-will-introduce-photo-filters-to-compete-with-instagram\/?smid=tw-nytimes","source":"bits.blogs.nytimes.com"},{"title":"No Copyrights on APIs: Help Us Make The Case | Electronic Frontier Foundation","details":"","item_date":"Nov 02 2012 21:49:39","display_item_date":"11-02-2012","url":"https:\/\/www.eff.org\/deeplinks\/2012\/11\/no-copyrights-apis-help-us-make-case","source":"www.eff.org"},{"title":"Daily Dot  | Open government report card: Grading Obama's transparency","details":"","item_date":"Nov 01 2012 16:25:30","display_item_date":"11-01-2012","url":"http:\/\/www.dailydot.com\/politics\/open-government-obama-report-card\/","source":"www.dailydot.com"},{"title":"Hackathon Contestant HeatData Obsessively Tracks How People Interact With Mobile Apps - The Chronicle: Tech","details":"","item_date":"Sep 10 2012 01:52:10","display_item_date":"09-09-2012","url":"http:\/\/www.chronline.com\/life\/tech\/article_6ac4a509-3bda-5332-bb30-0c09b8cfbecf.html","source":"www.chronline.com"},{"title":"[no title]","details":"Alcatel-Lucent has introduced an open source software engine, apiGrove. This software enables companies to more effectively manage the application programming interfaces (APIs) used to expose the capabilities of cloud platforms, telecommunications networks or enterprise infrastructure. \n     The apiGrove software is based on Alcatel-Lucents Open API Platform. The apiGrove installation package, source code, and documentation are available effective immediately on the open-source repository and collaboration tool external linkGitHub under the business-friendly Apache 2.0 license, said a press note. \n       APIs enable data to flow easily between applications and devices and provide access to the core network assets that are critical for the creation of new services. apiGrove gives companies an easy way to manage the APIs they use to make their networks more programmable, supporting the development of services for both internal operations and for delivery to consumers and businesses. \n     Laura Merling, Senior Vice President, Application Enablement, at Alcatel-Lucent said, Weve reached a point where basic API exposure and management is table stakes for doing business in todays economy. Network exposure, through APIs, is becoming a commodity. The future of the API ecosystem lies in building on an open industry foundation and a holistic approach that spans the network, cloud and enterprise\/IT spaces.\u0094 \n      apiGrove lets businesses manage APIs at scale, while handling access policies and security needed to protect critical assets. apiGrove supports scalability through clustering, load balancing and request routing.  At the base of apiGroves security lies authentication and authorization, injection detection, and certificate management. apiGrove also includes metering features for rate limiting, quota management, and generation of sophisticated transaction records, added the press note. \n  Page(s)\u00a0\u00a0\u00a01\u00a0\u00a0","item_date":"Sep 10 2012 01:51:18","display_item_date":"09-09-2012","url":"http:\/\/voicendata.ciol.com\/content\/news1\/112090710.asp","source":"voicendata.ciol.com"},{"title":"Open or Closed API: Six Guidelines to Help CIOs Make the Call","details":"More enterprises are paying attention to the business-building potential of open APIs, and it\u2019s not hard to figure out why. When companies like Expedia tell ReadWriteWeb that APIs (check out the Expedia API) contribute about 90 percent of its $2 billion a year, you can bet CFOs and CEOs will take notice. The problem is, not every API should be open and, in some situations, an open API might jeopardize your API business strategy.","item_date":"Sep 10 2012 01:34:05","display_item_date":"09-09-2012","url":"http:\/\/blog.programmableweb.com\/2012\/08\/29\/open-or-closed-api-six-guidelines-to-help-cios-make-the-call\/","source":"blog.programmableweb.com"},{"title":"Current status: API v1.1 | Twitter Developers","details":"Today, were excited to release the next version of the Twitter API, version 1.1. A few weeks ago, we outlined many of the v1.1 changes, and with todays release, weve also updated the Developer Rules of the Road (see a summary of the most important policies here) and Developer Display Requirements. To learn about all of the changes, be sure to see the Overview of Version 1.1.\n      \n  Youll notice that the documentation on dev.twitter.com reflects the latest changes and methods available in API v1.1. To better distinguish whether youre viewing docs for version 1 or version 1.1, at the top of each endpoint document, there are new blue pills that indicate the version. Each endpoints resource information box will also indicate the methods API version, its rate limit per window, and more.\n  If its been awhile since youve caught up with the platform, we also recommend reviewing the wide variety of platform offerings now available at our documentation overview.\n  Additionally, based on feedback after the original blog post, we felt it was important to clarify one thing about user tokens and the 100,000 user token limit. The 100,000 user token limit applies only to the small set of clients replicating the core Twitter experience. It does not apply to the majority of other applications in the broader ecosystem.\n  We want and value your feedback, so please feel free to contribute to the thread around API v1.1. And, as always, you can ask questions in the discussion group \u2014 were here to help. If you notice issues with the API, please report them on our Issue Tracker. Were looking forward to working closely with the ecosystem during this transition to 1.1, and are excited to see what people build.\n             \u2190 Previous blog post    \n            Announcements    \n            Next blog post \u2192","item_date":"Sep 10 2012 01:33:49","display_item_date":"09-09-2012","url":"https:\/\/dev.twitter.com\/blog\/current-status-api-v1.1","source":"dev.twitter.com"},{"title":"API Strategy & Practice Conference, 1st-2nd November 2012 | Lanyrd","details":"A conference in New York, United States featuring John Sheehan, Jeremie Miller, Laura Merling and more...","item_date":"Sep 08 2012 20:11:00","display_item_date":"09-08-2012","url":"http:\/\/lanyrd.com\/2012\/api-strategy-conference\/","source":"lanyrd.com"},{"title":"Thread: To TechCrunch hackathoners","details":"If youre looking for something to do, how about picking off one of the ideas on the roadmap for the open Twitter-like ecosystem. \n  \t\tYou could do a nice linkblogging tool. Or a beautiful rendering of river.js in the browser of your choice (or hopefully all of them).\n  \t\tHow about a nice way to manage susbscription lists in a way that can be shared with all applications. One place where users can subscribe or unsubscribe. \n  \t\tRemember to give your users data to other apps, to be part of an ecosystem. Closed silos are for the big guys. Its how we disrupt them, by working together. :-)","item_date":"Sep 08 2012 16:51:14","display_item_date":"09-08-2012","url":"http:\/\/threads2.scripting.com\/2012\/september\/toTechcrunchHackathoners","source":"threads2.scripting.com"},{"title":"Google\u2019s \u201cDevelop for Good\u201d hackathon winners tackle environment, human rights \u2014 Tech News and Analysis","details":"Google held a \u201cDevelop for Good\u201d hackathon for developers surrounding the company\u2019s June I\/O conference, allowing developer groups using Google products to tackle a variety of projects related to human rights, the environment, and politics.\n \t\t\t\t\n \t \t\tAt Google\u2019s I\/O developer conference in June, the company held a \u201cDevelop for Good\u201d hackathon as a collaboration between the I\/O conference and Google\u2019s\u00a0philanthropy\u00a0arm, Google.org. On Friday, Google picked the winners from the hackathon in three main categories: Ideas, Politics and Elections and Green. The selection of winners highlighted some interesting projects where companies are using technology to tackle issues in human rights and the environment.\n Ideas\n Hackers in the Ideas arena were asked to develop a tool that could be used for reporting information from hostile situations with limited internet and repressive government regimes.\n The winning project, Silent Lens, developed software for Android that allows users to capture sensitive information and securely transmit that information to others even with a hostile government and limited connectivity.\n Silent Lens founders explained on their website the importance of getting information from those hostile environments into the world: \u201cInformation about incidents of violence in these situations is tremendously important: it can shame governments into reform, help the international community pressure regimes to cease violence, and increase citizens\u2019 own commitments to affecting change.\u201d\n A Nigerian group created the winning project for Politics and Elections, taking the focus off the US Presidential election and instead bringing democratic participation to Nigeria with a new web platform. The group hails from the Google Developer Group in Lagos, Nigeria.\n The project, a prototype of Assembly Bills, will allow Nigerian citizens to give input on bills through the website, rather than traveling to the host city to participate in the legislative process. \u201cAssembly bills was created to give the ordinary citizen access to give inputs into bills that are to be pass into law by the legislative arms of their country,\u201d the Assembly Bills founders explain on the site.\n The winners of the Green challenge hail from Pakistan, specifically from the the Google   Developer Group in Karachi, where they\u2019re attempting to crowdsource citizen complaints about environmental concerns to bring them to the attention of government officials. The project, called Green It,\u00a0rewards citizens who are particularly active and vocal in the community.\n \u201cWhat we generally concluded from our survey was that people put all the blame on government officials that they never respond to the problems and the area around us remains black where on the other hand the government officials say that we try to resolve every problem reported to us,\u201d the group explained on its site. \u201cWe concluded that there is generally a communication gap between the society and the officials.\u201d\n Through the Google+ platform, Green It will allow citizens to report\u00a0environmental\u00a0concerns to government officials, and other citizens can validate the concerns if they also feel that it\u2019s a problem. If the government doesn\u2019t respond to widespread complaints, the idea is that users can then get media involved in the issue.","item_date":"Sep 08 2012 02:42:48","display_item_date":"09-07-2012","url":"http:\/\/gigaom.com\/2012\/09\/07\/googles-develop-for-good-hackathon-winners-tackle-environment-human-rights\/","source":"gigaom.com"},{"title":"Evan Williams on Twitter and its ecosystem \u2014 Tech News and Analysis","details":"Evan Williams, the co-founder of Twitter and the company\u2019s former CEO during the beginning of its evolution from a side project into a major social-media entity, says that the influence of the network\u2019s ecosystem has been overstated. But is that true?\n \t\t\t\t\n \t \t\tAs we\u2019ve described in a number of recent posts \u2014 including one about my ongoing \u201clove-hate\u201d relationship with the service \u2014 Twitter has been going through a transformation of sorts recently, closing down access to the network by third-party apps and services, controlling more of the content that flows through the system, and generally irritating developers (and in many cases users). One man who knows a lot about this evolution from the inside is former CEO and co-founder Evan Williams, and he took issue on Thursday with a comment I made in one of my posts about how users and third-party apps were responsible for much of the initial growth of the network. \n Some of the comments during a back-and-forth discussion we had on Twitter were interesting, so I thought I would excerpt them here, and also include a Storify collection of the debate as well. I\u2019m hoping to talk more about this and other topics with the former Twitter CEO at GigaOM\u2019s RoadMap conference in November.\n During his time as Twitter\u2019s CEO, Williams had to deal with the beginnings of Twitter\u2019s transformation from a cool project into an actual revenue-generating company, as it started to acquire third-party apps and caused a backlash among developers that is very similar to the one it is facing today. Williams left Twitter in 2010 when he was replaced by Dick Costolo, and formed a startup incubator called Obvious Corp. with former Twitter colleague Biz Stone (which recently launched a new-media publishing platform called Medium) but the former CEO has remained an advisor to the company and a board member.\n While Twitter was trying to figure out in 2010 which external services it wanted to incorporate and which it wanted to leave alone \u2014 a process that angel investor Chris Dixon compared to \u201ca drunk guy with an Uzi\u201d \u2014 Williams admitted that the company had screwed up its relationship with developers, and Twitter held a whole conference for developers called Chirp that was supposed to try and repair that relationship. In reality, however, the tensions between where Twitter wanted the company to go and how that was going to affect third-party apps remained just below the surface, and erupted again recently after moves like the announcement of new API rules and the shutting off of features to services like Tumblr and Instagram.\n What role has the ecosystem played?\n Our Twitter discussion started when Williams mentioned a phrase from my recent post about the company\u2019s ongoing struggle with being open vs. controlling the network. I argued that much of the early power and growth of the network came from being open, since many of the things we associated with Twitter \u2014 such as the @ mention for users, the hashtag, and even the retweet \u2014 were not developed by the company but came from the users themselves, in many cases assisted by third-party apps. But Williams said that this influence is \u201ca common myth but completely overblown\u201d:\n @mathewi virtually all of the network\u2019s power and growth has come from outside the company itself\u2014a common myth but completely overblown\nEvan Williams (@ev) September 06, 2012 At this point, Anil Dash \u2014 who used to work for blogging platform Six Apart and now has a media consulting firm \u2014 agreed with Williams that the focus on how much of a role third-party apps played in Twitter\u2019s success is overstated:\n @mathewi I think the Twitter was made by third party things is mostly nerd triumphalism, not factual. Shaq did more than any indie app.\nAnil Dash (@anildash) September 06, 2012 Dash also noted that some of the elements we associate with Twitter \u2014 even hashtags, which Chris Messina (now of Google) was the first to use on Twitter \u2014 were used in other ways on the internet before Twitter came along, and others noted that the @ symbol was also used on services such as Internet Relay Chat. Williams then pointed out that if it wasn\u2019t for the company\u2019s decision to incorporate and support those features, they would never have become part of Twitter to begin with.\n @mathewi @anildash This is how products evolve. You have 1M ideas\u2014some come from usage, some from inside. You pick and choose carefully.\nEvan Williams (@ev) September 06, 2012 I tried to argue that the point wasn\u2019t to try and determine which played a larger role, the ecosystem or Twitter itself and the decisions it made (some of which irritated users, such as the decision to implement retweets in a different way). The point for me is that the relationship between users \u2014 and third-party services \u2014 and Twitter has always been much more symbiotic than it has a traditional company-user dynamic. And a big part of that was a wide-open API that let tweets flow wherever they wanted to, something Twitter has been busy shutting down.\n Ethan Kaplan, a developer who is a vice-president at Live Nation and used to work for Warner Brothers Records, put it well when he said that all developers really want is for Twitter to admit the relationship is symbiotic, rather than parasitic:\n @mathewi @chrismessina ultimately what we all want is twitter to stop treating their ecosystem as parasitic. It isn\u2019t. It\u2019s symbiotic.\nEthan Kaplan (@ethank) September 06, 2012 And Chris Messina \u2014 who noted that the third-party app Tweetie (which Twitter ultimately acquired and turned into the official iPhone app) was the first to support hashtags \u2014 said that one of the things the company has failed to do is to make it clear who it is making all of its recent changes for. As I\u2019ve pointed out before, it argues that it is doing so for users, but is that really the case? I have to admit that I\u2019m not convinced.\n @ethank @mathewi is it that? It feels like theres not sufficient transparency behind the motivation for the changes. Whore they *for*?\nChris Messina\u2122 (@chrismessina) September 06, 2012 I\u2019ve embedded the full version of the Storify below, with as many of the comments as I could find (apologies to those whose contributions I missed). Interestingly enough, Twitter has said that the new API rules aren\u2019t meant to apply to services like Storify, even though the company seems to fall into the wrong quadrant of customer product lead Michael Sippey\u2019s by-now-infamous chart.","item_date":"Sep 07 2012 20:36:01","display_item_date":"09-07-2012","url":"http:\/\/gigaom.com\/2012\/09\/06\/evan-williams-on-twitter-and-its-ecosystem\/","source":"gigaom.com"},{"title":"Wolfram|Alpha Blog : Making CrunchBase Computable with Wolfram|Alpha","details":"Most of the new features we announce on this blog are large-scale projects where we add a huge chunk of data to Wolfram|Alpha all at once. But there are always dozens of background projects going on at any given time\u2014including a seemingly never-ending effort to expand our database of information on private companies.\n We started out with coverage of only the largest US companies, including giants like\u00a0Bechtel and\u00a0Cargill. But recently we\u2019ve been mining\u00a0CrunchBase to pick up more information on popular tech startups. There\u2019s still a long way to go: as with any database of user-edited content, we have to do a lot of programmatic and manual curation to bring the content up to our standards. But with this year\u2019s TechCrunch Disrupt San Francisco coming up next week, we thought it might be fun to call some attention to this effort.\n We made a special push to try to add Disrupt finalists from the past three years, along with whatever funding history we could glean from CrunchBase. This includes relatively well-funded operations like\u00a0Ark, DataSift, and Firespotter Labs (developers of Disrupt NY 2012 winner Uberconference), as well as smaller companies like\u00a0Babelverse, Incident, and GameCrush.\n Where a history of funding data is available, you can compare several companies\u2019 progress over time. Have they maintained a high level of fundraising? Did they get out of the gate with lots of seed money, and then go quiet? Has finalist status at a Disrupt conference had any impact on fundraising ability?\n \n Well\u2026 maybe we can\u2019t definitively answer that last question, but your speculation can at least be a bit more informed.\n For most companies, note too that Wolfram|Alpha will also display basic web statistics, letting you see if some companies might be winning the war for web viewers, if not venture capitalist dollars.\n Like I said, this is an ongoing project with a lot of data cleanup and alignment, so the startup of your choice might not have made it into the system yet. So let us know if we don\u2019t have someone you\u2019re looking for, or if you can provide any additional (or improved) information on a particular company.\n Good luck to this year\u2019s competitors! If we don\u2019t cover them already, we\u2019ll make an effort to get them into Wolfram|Alpha as soon as possible after the conference.","item_date":"Sep 07 2012 20:17:52","display_item_date":"09-07-2012","url":"http:\/\/blog.wolframalpha.com\/2012\/09\/07\/making-crunchbase-computable-with-wolframalpha\/","source":"blog.wolframalpha.com"},{"title":"Ecosystems","details":"I love Twitter. I\u2019m coming up on six years using the service, and there are few other Internet services I\u2019ve used as much these past six years.\n If you\u2019re in the tech industry, there has been a lot of gnashing of teeth about changes Twitter is making to its API. A lot of folks\u00a0 see this as a bunch of techie-geeky inside baseball, that these are changes that only affect developers (and not real people who use the service), and for the most part they\u2019re probably right.\n But Twitter has thrived in large part because it has built an ecosystem. One where lots of people were encouraged to build cool tools on top of Twitter, tools that allow us to use, and interact with, Twitter in ways that best fit our needs.\n And ecosystems, whether natural or technical, are extremely fragile. One small, seemingly inconsequential change can ripple outward to produce unintended consequences and devastating damage.\n For example, you or I may not care about Tweetbot (an iPhone Twitter client beloved by many); but people I follow on Twitter do care about it. A lot. And, if changes Twitter is making to its API put Tweetbot out of commission, the people I follow on Twitter might not post as much. Because they don\u2019t want to be forced into using a tool they don\u2019t like. And that will make Twitter less valuable, perhaps, for me.\n Today a rumor has swept through Twitter (of course) that the Twitter is killing off its Mac desktop client. It\u2019s just a rumor. But this is the Twitter client I use more than any other. And without it, I\u2019ll use Twitter less. I know this for a fact \u2014 my usage of Twitter has steadily increased with the availability of good (and sometimes great) desktop clients starting with Twitterific.\n In the past, Twitter killing off its Mac client would have been a bummer, but not fatal. Some smart third party developer would have stepped in and filled the gap. And, indeed, this is what happened on the Mac and iPhone until Twitter acquired Tweetie (which in turn became the basis for all of its official iOS and Mac apps). But with the API changes they\u2019ve just announced, they\u2019ve made clear you\u2019d be a fool to step in now to build a Mac client for Twitter.\n And so then what? As Om Malik jokingly tweeted this afternoon, \u201cso what are we supposed to use? their website?\u201d I\u2019ve never particularly liked or used the Twitter web site and I\u2019ll use Twitter much less if I\u2019m someday forced into that.\n I\u2019m just a small, inconsequential part of Twitter, one little user out of hundreds of millions. Twitter doesn\u2019t care about me, and it won\u2019t matter to Twitter if I use the service less.\n But these seemingly small and trivial changes might cause others to use the service less, too. And it all starts to add up.\u00a0 The rots sets in, things start to die off, and before you know it you\u2019re looking at a dry and dusty land.","item_date":"Sep 07 2012 17:59:28","display_item_date":"09-07-2012","url":"http:\/\/mhallville.com\/2012\/09\/07\/ecosystems\/","source":"mhallville.com"},{"title":"Podcast Interview: Harley Manning of Forrester Talks Customer Experience, New Book \u201cOutside In\u201d","details":"Understanding your customers can make or break your business. Companies are realizing that creating outstanding user experiences and making your business customer-focused is the key to success. In his latest book, Forrester Analyst Harley Manning explores how to create outstanding experiences for your users and what we can learn from the customer experience ecosystem.\n Thumbing through Outside In, we were seriously impressed with the data around company growth and customer happiness, plus concepts around innovative UX and how that influences how your users will act.\n We had the chance to interview Harley about his book and why customer experience is the greatest source of untapped ROI for businesses. Listen to the full podcast interview:\n \n Harley will lead the discussion in our upcoming webinar \u201cDelivering Outstanding Customer Experiences Through Proactive Notifications\u201d on September 20th. Register to join us for this webinar now.\n \n \t\t\t\t\t\t\tMeghan is the community manager at Twilio. Find her on Twitter @megmurph and email at murphy [at] twilio [dot] com.\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t\tView all posts by Meghan Murphy \u2192","item_date":"Sep 07 2012 07:00:00","display_item_date":"09-07-2012","url":"http:\/\/feedproxy.google.com\/~r\/twilio\/OBEN\/~3\/KAinIJXAI2Q\/podcast-interview-harley-manning-of-forrester-talks-customer-experience-new-book-outside-in.html","source":"feedproxy.google.com"},{"title":"Instaservice: Customer Service Gets Speedy and Social","details":"With the rise of social media and the transformation to a sound-bite driven information society, instant gratification has become the norm. This also holds true in the world of customer service, as today\u2019s consumers expect ubiquitous real-time customer service that is transparent, authentic, and most of all, fast.\n As we know from our Zendesk Benchmark, speedy response and resolution times are a key driver of customer satisfaction and success. A lightning fast product that keeps support agents super productive is what we strive for here at Zendesk. A big part of this productivity is enabling our clients to engage with customers on their preferred, real-time channel: social media. In this infographic, we take a look at how social media is changing customer expectations and experiences when it comes to customer service.\n To learn more about available social media functions within Zendesk, check out our Twitter and Facebook features!\n \n \t\t\t\t\t \t\t\t\tTo comply with the EU cookie law, we have recently revised our cookies policy. Please review it to learn which cookies we use, how and why we use them, and how you can disable cookies by changing your browser settings. Please be aware that portions of this site will not function properly if you disable cookies.  Unless you elect to disable cookies through your browser, by your continued use of this site you agree to our use of cookies as described in our cookies policy.","item_date":"Sep 07 2012 07:00:00","display_item_date":"09-07-2012","url":"http:\/\/www.zendesk.com\/blog\/instaservice-customer-service-gets-speedy-and-social","source":"www.zendesk.com"},{"title":"REST Fest 2012 in Greenville, SC | Layer 7 - Blogs","details":"Over the weekend of September 13-15, a small band of Web architects and developers will \u2013 for the third year in a row \u2013 descend upon the town of Greenville, SC. They\u2019ll be getting together to catch up on the events of the past year, share stories about recent projects and contemplate the future of Web and mobile applications.\n This may sound like a typical tech conference but REST Fest is hardly that. Taking its cue from OpenSpaces and similar events, REST Fest is organized by attendees, for attendees. For example, one of the days is devoted to everyone hacking on the same general topic. Another is dedicated to short workshops, all presented by selected registrants.\n Similarly, all the general session talks are delivered by the attendees themselves. That\u2019s because one of the \u201crules\u201d of REST Fest is \u201ceveryone talks and everyone listens\u201d. When you sign up to join REST Fest, you are expected to deliver at least a five-minute lightning talk \u2013 and there are no exceptions!\n Notable presenters will include keynote speaker Stu Charlton (former CTO of Elastra), Matt Bishop (Senior Product Architect at Elastic Path), Pat Cappelaere (currently working on NASA\u2019s SensorWeb project), Leonard Richardson (co-author of O\u2019Reilly\u2019s RESTful Web Services), Sam Ramji (Head of Strategy at Apigee) and yours truly.\n I feel privileged to be co-chair of REST Fest and I\u2019m pleased to note that Layer 7 is the event\u2019s Head Sponsor this year. Hope to see you there!","item_date":"Sep 07 2012 00:12:13","display_item_date":"09-06-2012","url":"http:\/\/www.layer7tech.com\/blogs\/index.php\/rest-fest-2012-in-greenville-sc\/","source":"www.layer7tech.com"},{"title":"Catalyst Debrief","details":"Eric Knipp is a Managing Vice President in Gartner Research, where he focuses on Web and cloud application development methodologies and trends. Mr. Knipp is based in Dallas, Texas. Read Full Bio \nCoverage Areas:  \t\t\t\n \t\t\t\n  \t\t\t  \t\t\t\t  \t\t  \t\u2190 So, you want to be an analyst\u2026 \t \n \n  \t\t\t\t\t\t  \t\t\tCatalyst Debrief\n \t\t\t \t\t\t\tI just returned home to Dallas after participating in my first Gartner Catalyst conference. The event was impressive with tons of content and almost 1800 attendees on-site, not including Gartner staff. I\u2019ve presented at a number of Gartner Symposia and Summit events and I was struck by the deep level of attendee engagement at Catalyst. Our attendees are in the midst of mobile, cloud, and big data initiatives and it was great to hear about their experiences and challenges in my conference sessions, roundtables, and of course copious analyst-attendee 1-1 sessions. As I cover Web APIs in depth these days (and to a lesser degree mobile development) I thought I would share my top 3 \u201caha\u201d moments from the conference, based on attendee feedback and questions:\n The mobile bone is connected to the Web API bone. I\u2019ve advised for some time that clients consider a Web API as the first step of a mobile development initiative, in particularly one that depends on connections to existing back-end systems that have not yet been \u2018mobile enabled\u2019. My conversations at Catalyst make clear that practitioners have come to this conclusion as well and are embarking on a variety of Web API initiatives in support of mobile app enablement. I have one further piece of advice beyond establishing a Web API at the outset of a mobile app initiative \u2013 don\u2019t treat it as just a part of or infrastructure for your mobile app, but as a product in its own right. Done well, your Web API will power multiple mobile apps, partner integrations, and Web applications, but that requires a commitment to design that is easy to skip if you think you\u2019re just doing it as a one-shot deal for a single app.\n The majority of Web API initiatives leave versioning for later. I believe this is a dangerous idea for a variety of reasons. First and foremost, if you run just one version of an API you risk introducing a breaking change anytime you extend it. While you can potentially work around this with a very robust set of test cases (which I would strongly encourage), it is easy to paint yourself into a corner when you are unable to fix a bad implementation that seemed like a good idea at the time. Over time your code gets crufty and filled with technical debt and eventually you might be forced to declare bankruptcy \u2013 basically starting over with a new Web API and disenfranchising all of your existing API consumers all at once. On the flipside, versioning isn\u2019t something you should automatically commit to. Running multiple versions has its own costs \u2013 multiple code bases and the overhead that comes with that, multiple groups of API consumers and the service levels you must provide them, and so on. My advice on this one is just to take it seriously \u2013 if you choose not to decide, you still have made a choice.\n Consuming Web APIs from external providers can be a serious challenge. If you deal with a lot of niche players with screwball ideas about Web API design you have your work cut out for you. While the cloud and social leaders like Facebook, Salesforce.com, Twitter and so on get a lot of pub for the quality of their Web API designs, there are myriad small SaaS providers who treat the Web API as an afterthought. Understandably, they instead spend their R&D dollars on slick GUIs and worthy business processes which the business selects without even considering integration. If you are unlucky enough to be asked to integrate some of these niche apps with your existing systems it isn\u2019t going to be a lot of fun, especially if you use the brittle point-to-point style of integration. My advice is that you consider using a gateway to shield your applications from direct exposure to external Web APIs.. this will provide you a single logical layer to deal with and the better API gateways on the market can do some basic transformations and quality of service improvements that allow you to homogenize the interfaces somewhat, which can be very useful if you\u2019re part of a larger team that delegates the business logic implementation to different hands than the actual service consumption\/integration.\n Web APIs are growing in popularity and that makes it a fun time to cover them. Web API gateways, servers, and management technologies are proliferating at the hands of cloud and product vendors alike. If you\u2019re a Gartner for Technical Professionals client I would love to talk to you about it.","item_date":"Sep 06 2012 19:40:07","display_item_date":"09-06-2012","url":"http:\/\/blogs.gartner.com\/eric-knipp\/2012\/08\/27\/catalyst-debrief\/","source":"blogs.gartner.com"},{"title":"New Twitter API Drops Support for RSS, Puts Limits on Third-Party Clients","details":"Twitter announced its new API back in August, which it described as helping create \u201ca consistent Twitter experience\u201d across platforms and devices. \n Although most of the API changes were previously announced, the overview of the final API highlighted some that were previously unknown. The one sure to cause the most immediate frustration: Twitter is ending support for RSS, XML and Atom.\n While it was easy to see the loss of XML coming \u2014 Twitter has slowly dropped support for XML in favor of JSON over the last year and a half \u2014 dropping support for RSS and Atom is a major shift.\n RSS and Atom are the two major formats for serving up web feeds. These feeds can include text, audio, video and other types of media. While most users use RSS as a way to subscribe to web content from a blog or podcast \u2014 the format can also be used as a way to subscribe to tweets.\n Since its inception, Twitter has allowed developers to access Twitter timelines and search queries using RSS. As a result, lots of social aggregators have used RSS as an easy way to pull in tweets alongside messages for other services.\n Many early lifestream services \u2014 including things like FriendFeed (which Facebook purchased in 2009) \u2014 used RSS as a way to pull in and cross post Twitter updates.\n Most aggregation apps can be rewritten using Twitter\u2019s 1.1 APIs, but its important to note another change: Twitter\u2019s 1.1 API now requires authentication via OAuth 1.0a for all endpoints. In other words, if developers want to get data out of Twitter \u2014 they must use OAuth.\n The OAuth requirement has the potential of stopping abusive behavior \u2014 but it has another side effect too \u2014 it means Twitter can control and monitor who is using its data and how that data is being used.\n Apps that use RSS, XML or Atom will need to shift to JSON or other API methods by March 5, 2013.  \n How Will This Impact End Users\n Twitter says that RSS usage of its API was low, though it isn\u2019t clear how many apps and services pull data or feeds in in this way.\n One use of RSS was as a way to display a stream of tweets from a user or a hashtag as a widget on a website. The deprecation of RSS support makes another Twitter announcement \u2014 the ability to build user and hashtag based timelines \u2014 make a lot more sense.\n The new Twitter timeline feature makes it easy for users to create embeddable timelines of events or interactions. This isn\u2019t overtly different from Twitter\u2019s longstanding Twitter update widget for websites \u2014 except that it can now pull in full conversations, as well as search queries for certain hashtags.\n That\u2019s a relatively benign switch \u2013 but the broader ecosystem impact of dropped RSS support will not be apparent until the API changes become mandatory on March 5, 2013.\n More Changes for Twitter Apps\n As we noted last month, some of the changes coming to the Twitter API have rubbed developers the wrong way. Developers were particularly vocal about the limit on user tokens for Twitter apps. In essence, Twitter is capping how many users an app can have \u2014 or at least, that\u2019s how it seemed. \n In its announcement releasing the final 1.1 API changes, Twitter clarified that the 100,000 user token limit \u201capplies only to the small set of clients replicating the core Twitter experience.\u201d (emphasis theirs)\n Essentially this means that if your app isn\u2019t a client \u2014 or you\u2019re a Twitter Certified Product \u2014 the user limit should\u2019t apply.\n Twitter clients will have to deal with more than just a theoretical user limit. Twitter\u2019s updated Rules of the Road Summary for developers and API terms make it clear that that the service intends to crack down on the freedom currently enjoyed by some third-party Twitter clients.\n The biggest change that caught our eye was this:\n \u201cDon\u2019t resyndicate data. If your service consumes Twitter data, don\u2019t take that data and expose it via an API, post it to other cloud services, and so on.\u201d\n In other words, a Twitter client can\u2019t also post Twitter content onto another service. This could be problematic for some of the cross-positing apps on the market \u2014 as well a number of IFITTT recipes.\n Moreover, taken at face value, it appears that information contained within a tweet \u2014 such as a URL \u2014 cannot be sent to another service using a third-party client either.\n I frequently send URLs I run across on Twitter to Pinboard and Instapaper, often using a feature built-in to my favorite Twitter clients. \n This could be problematic for social news aggregators such as paper.li, Postano and RebelMouse. We\u2019ve reached out to Twitter for clarification on its resyndication policies but have not heard back at press time.\n Twitter has spent much of the summer closing off its social graph to apps such as Instagram \u2014 and disallowing services such as LinkedIn from displaying user tweets. \n When Flipboard CEO Mike McCue resigned from Twitter\u2019s board of directors, some speculated that the much-loved app might be losing access to Twitter.\n Twitter\u2019s new API terms could potentially limit how Flipboard is able to pull in data from Twitter streams.\n This is the sort of change that could have a much larger impact on the entire Twitter ecosystem. Sure, it might just be developers and power users complaining now \u2014 but if Twitter is serious about these resyndication rules \u2014 lots of very popular, very mainstream services could be affected.\n What do you think of Twitter\u2019s new API changes and the deprecation of RSS support? Let us know in the comments.","item_date":"Sep 06 2012 03:47:40","display_item_date":"09-05-2012","url":"http:\/\/mashable.com\/2012\/09\/05\/twitter-api-rss\/","source":"mashable.com"},{"title":"Philadelphia Police Department Fights Crime With Twilio SMS","details":"The Philadelphia Police Department is the nation\u2019s oldest municipal agency\u2014and one of the most forward thinking. In March, the department decided to empower residents to \ufb01ght crime by using SMS message. \u201cFor many people, text messages are the preferred method of communication,\u201d said Karima Zedan, director of communications for the Philadelphia Police Department. \u201cWe wanted to make it possible for people to send anonymous tips from their cell phones and potentially help us solve crimes.\u201d\n The Philadelphia Police Department chose to build the tip line using Twilio based on ease of use and price. \u201cBefore Twilio, bridging the phone and Internet was really dif\ufb01cult,\u201d said Dan Steinberg, who co-founded Hyaline Creative, the agency the department uses for web development. \u201cTwilio made it simple, accessible and affordable.\u201d\n Hyaline Creative \ufb01rst tried out Twilio in a project that involved building an online database where businesses and citizens could register their security cameras. To verify the identity of the registrants, the department used a Twilio-powered phone number veri\ufb01cation app.\n Lloyd Emelle, Hyaline Creative\u2019s other co-founder, said it took less than four days to create the tip line. One key requirement was the ability for the department\u2019s Real-Time Crime Center to respond in a secure way to someone who was reporting an emergency situation, while maintaining the anonymity of the caller. The solution, Emelle said, was to share the contents of the message with of\ufb01cers staf\ufb01ng the center, while preserving the privacy of the phone number that sent the text.\n Hyaline Creative initially built the tip line using a regular ten-digit phone number. However, the police department wanted to use a short code, a memorable \ufb01ve or six digit number that is approved by carriers for sending larger volumes of messages, so the developers applied for the short code, PPDTIP, through Twilio. Once it was approved, Hyaline Creative was able to move the app over to the short code in about three days, including the time for testing and quality assurance, Steinberg said.\n Hyaline Creative has already added SMS capabilities to the department\u2019s traditional tip line, and there are discussion to fully integrate the texting app with the department\u2019s existing information systems.\n \u201cOther neat features we\u2019ve built include the ability to check messages for addresses or intersections and keywords, like guns or theft, and to organize messages accordingly,\u201d Emelle said. \u201cThe Twilio API made it super easy for me to get up and running in minutes and to put my ideas into action.\u201d\n Since its launch in April, the people of Philadelphia have embraced PPDTIP and are sending increasing numbers of SMS tips. Texts include everything from reports of suspicious people to sitings of all-terrain vehicles in the streets. Recently, a person witnessed a hit-and-run and sent in the license number and a description of the vehicle via SMS.\n \u201cThere is really nothing as quick and easy as sending a text message,\u201d Steinberg\n said.","item_date":"Aug 30 2012 07:00:00","display_item_date":"08-30-2012","url":"http:\/\/feedproxy.google.com\/~r\/twilio\/OBEN\/~3\/xvw4WGUTFPs\/philidelphia-police-department-fighting-crime-one-text-at-a-time-with-twilio.html","source":"feedproxy.google.com"},{"title":"TrialX: Unlocking Clinical Trials with Twilio API","details":"For patients coming to terms with a new diagnosis or a reoccurrence of their disease, clinical trials represent a rare form of hope. For doctors, clinical trials are a critical stop on the road to a cure.\n But for years, matching patients with clinical trials of new treatments currently under investigation has been an exercise in futility. Trial investigators usually have to rely on \ufb01le cabinets full of paper records to \ufb01nd suitable patients and few tools, if any, exist to make it easier for patients to \ufb01nd clinical trials near them.\n Consequently, four out of \ufb01ve clinical trials are delayed and 50% of the delays are due to participant recruitment challenges. Accrual within clinical trials continues to be a major bottleneck in scienti\ufb01c progress. In the domain of oncology, for example, fewer than 3% of potentially eligible patients enroll in clinical trials. More than 75% of participants are not even aware trials exist, even though surveys have shown that a majority of people would be open to participating in these studies if they knew about them.\n Chintan Patel and Sharib Khan knew there had to be a more ef\ufb01cient way to connect patients and clinical trials. They founded TrialX in 2008, after Patel created a prototype at a Google hackathon.\n \u201cA product manager for Google suggested we develop the prototype as an\n application for the Google Health platform,\u201d Khan said. Microsoft then reached out and asked the team to build a similar application for its HealthVault personal health record platform. Within a few years, TrialX had signed up clients like the Cleveland Clinic, Harvard\u2019s Beth Israel Deaconess Medical Center, the New York-Presbyterian Hospital and the Multiple Myeloma Research Foundation.\n By 2011, TrialX had connected thousands of patients with clinical trials, but the\n team strove to do better. \u201cWe wanted to facilitate a connection that was immediate and effortless,\u201d Patel said. In addition to connecting patients and clinics via email, Patel wanted to add click-to-call functionality to Ask Dory, the company\u2019s redesigned clinical trials search application that makes it even easier for patients to \ufb01nd trials near them.\n \n Patel chose to use the Twilio API based on the recommendation of other developers. A TrialX developer named Aamir Hussain added the click-to-call capabilities in just a few hours. \u201cIntegrating Twilio with Ask Dory was super quick,\u201d Patel said. \u201cWe absolutely love the simplicity of the Twilio technology.\u201d\n In addition to Twilio\u2019s reliability, \ufb02exibility and ease of use, Patel said he appreciates the platform\u2019s reporting capabilities. \u201cSince Twilio provides full logs of the call outcome, its length and even its recording, we can make summary reports with details on the number of people being connected to the trial doctors, which is immensely valuable in assessing the impact of the tool.\u201d\n Patel and Hussain created Ask Dory in response to the U.S. Department of Health & Human Services \u201cUsing Public Data for Cancer Prevention and Control\u201d developer challenge organized by the National Cancer Institute. The Twilio integration was a key factor in the app being judged the winner in this U.S. government-sponsored \u201cinnovation challenge.\u201d\n The team has continued to create value using Twilio. For example, in order to make it easier to communicate with team members in India, Hussain wrote a conference call app that automatically dials everyone who needs to attend a particular meeting.\n In addition, in response to customer feedback, the team built an app named Call-E that makes it very easy for a doctor\u2019s of\ufb01ce to send proactive noti\ufb01cations via voice or SMS to remind a patient about an upcoming appointment or preventative screening. \u201cPreviously, doctor\u2019s of\ufb01ces or hospitals had to rely on manual calling or expensive automated call systems,\u201d Patel said.\n \u201cWe believe Twilio will disrupt communications models in healthcare,\u201d Patel said, noting that Twilio-powered apps are increasing ef\ufb01ciency not only for TrialX customers but also for the company itself.","item_date":"Aug 28 2012 07:00:00","display_item_date":"08-28-2012","url":"http:\/\/feedproxy.google.com\/~r\/twilio\/OBEN\/~3\/_ucOKJPYTs8\/trialx.html","source":"feedproxy.google.com"},{"title":"Self-service for Mobile Consumers","details":"Mobile devices aren\u2019t just for phone calls, SMS, and emails anymore. As our latest infographic illustrates, the percentage of adults who use their mobile devices for online browsing has more than doubled in the last three years. These users are making purchasing decisions through mobile browsing\u2013comparing prices and gathering product info. With customers engaging with your company via the mobile web comes the need to be ready to deliver great, fast customer service.\n Today, we\u2019re rolling out a new mobile customer portal interface to help our customers give their customers a beautifully simple self-service experience through the mobile web. The mobile customer portal is browser-based, so it works on any device, anytime, anywhere, and is optimized for a self-service community base.\n Learn more about Zendesk\u2019s customer self-service \n \n \t\t\t\t\t \t\t\t\tTo comply with the EU cookie law, we have recently revised our cookies policy. Please review it to learn which cookies we use, how and why we use them, and how you can disable cookies by changing your browser settings. Please be aware that portions of this site will not function properly if you disable cookies.  Unless you elect to disable cookies through your browser, by your continued use of this site you agree to our use of cookies as described in our cookies policy.","item_date":"Aug 28 2012 07:00:00","display_item_date":"08-28-2012","url":"http:\/\/www.zendesk.com\/blog\/self-service-for-mobile-consumers","source":"www.zendesk.com"},{"title":"Zendesk on the Google Apps Marketplace","details":"We\u2019re super excited to announce that Zendesk is now a part of the Google Apps Marketplace, mostly because we take full advantage of Google Apps ourselves. Being a global company with offices in San Francisco, London, Copenhagen, and Melbourne, and partners, clients, customers, and friends everywhere in between, it\u2019s necessary that everyone has access to everyone else; 24\/7 and from anywhere in the world.\n That doesn\u2019t just mean Gmail \u2013 which is integral to the way we communicate with one another \u2013 but Calendar (so we can schedule meetings and events), Drive (so we can share and collaborate on documents), and Analytics (so we can track and improve our web performance).\n If you use Google Apps like we do, you can now provision a Zendesk account from Google. For those of you without a Zendesk account, and you\u2019re a Google Apps admin, go set one up now!\n Existing Zendesk customers can access their Zendesk accounts via single sign on (SSO) with just one click from Google Mail, Calendar, Drive, and more. Just visit the Google Apps Marketplace and click \u201cAdd it now\u201d.\n \t\t\t\n \t\t\t\t\t \t\t\t\tTo comply with the EU cookie law, we have recently revised our cookies policy. Please review it to learn which cookies we use, how and why we use them, and how you can disable cookies by changing your browser settings. Please be aware that portions of this site will not function properly if you disable cookies.  Unless you elect to disable cookies through your browser, by your continued use of this site you agree to our use of cookies as described in our cookies policy.","item_date":"Aug 24 2012 07:00:00","display_item_date":"08-24-2012","url":"http:\/\/www.zendesk.com\/blog\/zendesk-on-google-apps-marketplace","source":"www.zendesk.com"},{"title":"Keeping up with Twitter","details":"For many, companies using Twitter to interact with customers might seem like a given; there are more than 500 million users engaging in a public forum. It would seem that most companies should be active, or at the very least, actively monitoring Twitter for Tweets to and about their brand. Many people are trying to use Twitter to engage with brands \u2013 48% of Tweets sent to companies are customer service questions \u2013 many companies are still not taking advantage of this opportunity to build a positive brand. In fact, only 13% of complaints sent to the top 100 retail brands in the US even received a response.\n Our latest infographic illustrates how customers are using Twitter, and the kind of responses they are getting from companies.\n Learn more about customer service software and Twitter\n \n \t\t\t\t \t\t\t\t\tCREATE MY ZENDESK \t\t\t\t\tBy clicking \u201cCreate my Zendesk\u201d you agree to the Zendesk Terms of Service and Privacy Policy.\n \t\t\t\t\n \t\t\t\t\t \t\t\t\tTo comply with the EU cookie law, we have recently revised our cookies policy. Please review it to learn which cookies we use, how and why we use them, and how you can disable cookies by changing your browser settings. Please be aware that portions of this site will not function properly if you disable cookies.  Unless you elect to disable cookies through your browser, by your continued use of this site you agree to our use of cookies as described in our cookies policy.","item_date":"Aug 22 2012 07:00:00","display_item_date":"08-22-2012","url":"http:\/\/www.zendesk.com\/blog\/keeping-up-with-twitter-customer-service-questions","source":"www.zendesk.com"},{"title":"Caremerge Automates Communications for Long Term Care Facilities","details":"Caremerge is disrupting and old industry with new tech. The young company recently won the InnovateLTC and LINKTank award for \u00a0\u201cMost Innovative Company of 2012 in Long Term Care.\u201d Caremerge is bringing mobile apps powered by Twilio to the long term care industry, connecting families, residents, nurses and doctors, all while saving time and money.\n The company, based in Chicago, was founded in 2011 by Fahad Aziz and Asif Kahn. Fahad previously worked at GE Healthcare IT in senior level marketing roles while Asif worked for various Fortune 500 companies in SaaS. The two co-founders sought to solve the problem of miscommunication between hospitals and long term care facilities by creating web and mobile apps that streamline communication and organize vital health data.\n Fahad and Asif spent time in long term care facilities observing how nurses recorded data, communicated with family members and hospitals, and complied with the documentation-heavy requirements of the long term care industry. Staff were using sticky notes, whiteboards, calendars, and checklists to organize and communicate data to residents. Sticky notes got lost, calendars were hard for residents to read, and ultimately hospitals didn\u2019t receive all the information on checklists and whiteboards.\n Caremerge to the Rescue\n Caremerge\u2019s solution was to pipe all that data through applications that would organize and distribute the information efficiently to the proper parties. With Caremerge, staff can use automated calendar updates as reminders instead of sticky notes.\u00a0Nurses can record resident\u2019s vitals and other critical information on a mobile app and have the information automatically pushed to the appropriate parties (hospitals, doctors, families, etc.) instead of faxing paper forms or playing phone tag.\n Caremerge\u00a0also has a variety of apps to help organize staff, reduce paperwork and engage with families of patients. One example is the app, \u201cActivities\u201d, which streamlines the process of getting a resident to an activity for both the resident and staff.\n When a resident is signed up for an activity, Caremerge\u00a0uses Twilio Voice to inform the resident about their activity as a reminder. Another app allows the staff to use Twilio SMS to inform each other of various tasks that need to be performed for residents throughout the day.\n \n Managing Activities for Senior Living Communities from Fahad Aziz on Vimeo.\n Caremerge\u00a0seeks to be the first provider of easy-to-use apps that provide long term care facilities the ability to engage their clients on a real-time basis, while maintaining a clear channel of communication with hospitals, physicians and families. The company is currently in talks with a number of long term care facilities and hopes to bring their apps and expertise to hospitals in the coming year.","item_date":"Aug 20 2012 07:00:00","display_item_date":"08-20-2012","url":"http:\/\/feedproxy.google.com\/~r\/twilio\/OBEN\/~3\/txJqaz-lVTw\/update-long-term-care-communications-with-caremerge.html","source":"feedproxy.google.com"},{"title":"viaCycle Brings Shared Bikes to San Francisco Powered by SMS and Voice","details":"Living in San Francisco, you see a lot of bikers on the road \u2013 it\u2019s one of the most efficient ways to get around the city. But what if you\u2019re not ready to invest in a bike, or maybe friends come to visit and you\u2019re stuck driving across town to find a bike rental?\n viaCycle wants to change that by making bike sharing simple and accessible. Sid Doshi, CTO of viaCycle founded the company with a few of his Mechanical Engineering Grad friends at Georgia Tech in 2009 \u2013 which became their very first customer.\n Quickly realizing the challenge of the logistics and hardware around building a bike sharing program, the team built an electronic lock system powered by Twilio SMS and Twilio Voice, that also includes GPS tracking. Find the bike, call or text in the PIN, and you\u2019ve got a sweet two-wheeled ride to enjoy the day on. We ask Sid a few questions about the story behind viaCycle and what\u2019s next for the company.\n What is viaCycle?\n viaCycle creates bike-sharing systems that allow users to quickly checkout bikes for short trips using their cell-phones. It\u2019s like Zipcar, but for bicycles. We set-up turn-key bike-sharing programs for universities, corporations, mixed use developments, neighborhoods etc.\n What is the backstory of your project?\n viaCycle started as a student project at Georgia Tech. All the founders are former or current Georgia Tech grad students. A couple of us were doing research on urban transportation, and bike-sharing was the latest trend that caught our eye because it was enabled by technology. This was in 2009.\n We wanted to bring bike-sharing to campus, so we applied for a $100k grant from the Ford Fund, which we did not get, but what we found was that even if we had the money, we wouldn\u2019t have been able to actually buy enough bikes and stations to make the program useful. Assuming we actually got all the permissions required to install bike docking stations on campus. So it was pretty clear that existing solutions would not work very well.\n A year later, in 2010, we applied for the same grant again ($50k this time) to develop a new bike-sharing system that would get around most of the issues by completely getting rid of stations.\n Since then, we\u2019ve developed the product and brought it to market. Not having stations cuts down costs per bike to 1\/2 or 1\/3rd \u00a0of existing bike-sharing systems, while at the same time allowing more flexibility in set-up and operations.\n We now operate and manage a program for Georgia Tech as a vendor. We are starting at another school in September, and we are looking to start more programs for companies, universities, resorts etc. in the near future.\n What is your team\u2019s background?\n All our founders are Mechanical Engineers with Master\u2019s degrees. One is actually still completing his PhD. We were part of the Sustainable Design and Manufacturing program where Kyle, our CEO, and I did our graduate research on Multi-Modal Urban Transportation for the Ford Motor Company, so we are familiar with the territory.\n How are you using Twilio?\n \n Our bikes can be unlocked via Voice\/SMS\/App. We use Twilio for the Voice and SMS access. In fact, we didn\u2019t even have smartphone apps when we launched, and they are still in beta.\n Voice\/SMS is important for us, because to develop a system that is truly accessible, we cannot be dependent on smart-phones, and Twilio allows us to bridge that gap without having to build\/support\/scale any telephony infrastructure.\n What other technologies are you using to build\/support it?\n We use a basic web application stack (Django\/Postgres) for back-end stuff. A lot of our technology is actually in electronics, we design our own PCBs, and mechanical design a lot of custom components, though the web stack ties it all together.\n How did you get started developing with Twilio?\n We were looking around for the best way to connect SMS to our web application and thus to our bikes, and researching Asterisk and related options, trying to find something that was as hands-off as possible while still being very cheap.\n We weren\u2019t having much success. We were looking into setting up phone lines, or connecting a cell phone to an on-site server etc, this was in April 2011. Then a friend at another startup mentioned Twilio. We had the entire setup running in 2-3 days from then, and it didn\u2019t cost us anything. It was like a part of the required infrastructure just fell into place.\n What are your next steps with viaCycle? Any new cities you\u2019re targeting next?\n We are staring at another school soon with a 20 bike program. We are also working on the next version of our product. Currently, we are focusing most on bringing viaCycle to more places.\n \n Learn more about viaCycle here.","item_date":"Aug 20 2012 07:00:00","display_item_date":"08-20-2012","url":"http:\/\/feedproxy.google.com\/~r\/twilio\/OBEN\/~3\/icJ92erMuqQ\/viacycle-brings-shared-bikes-to-san-francisco-powered-by-sms-and-voice.html","source":"feedproxy.google.com"},{"title":"Instagram 3.0 \u2013 Photo Maps & More\n\nWe\u2019ve been working hard...","details":"We\u2019ve been working hard on a major update for both Apple and Android devices. We\u2019re excited to announce that today Instagram 3.0 is available to everyone. Download Instagram 3.0 for Apple here or for Android here.\n  With every major release, we pick a theme \u2013 and for this one we\u2019ve focused on the browsing experience. We\u2019ve introduced a new and unique way to browse your photos and others\u2019 photos on a map, which means you\u2019re no longer constrained to browsing through page after page of photos. We\u2019ve also worked hard on updating all the screens in our app to be more visually consistent and polished. You\u2019ll notice new layouts throughout the app that feel fresh, yet familiar. Our engineers have also been hard at work making sure that the browsing experience is as fast as possible. On newer devices, you should notice a marked improvement in speed while browsing. With that, we\u2019ve introduced infinite scrolling in feeds so that you can quickly see more photos while browsing around the app. And finally, we\u2019ve paid special attention to keeping Instagram a safe place. With improved reporting tools, users can now flag both photos and comments for review with ease. We\u2019re very excited by how much has gone into this release \u2013 as always, please provide us with feedback as you explore the new features.\n  \n  Introducing Photo Maps, a new way to browse photos on Instagram. Your Photo Map appears on your profile and lets you showcase your photos on a map. Also, you\u2019ll be able to see other users\u2019 maps from their profiles as well.\n  Important: Before your map is visible to the public, you\u2019ll be prompted to review any photos you previously geotagged. Additionally, we\u2019ve made it easy to remove photos from your map at any time, which will remove any associated geo-data but leave the photos intact on your profile. To read more about how Photo Maps work, read more here.\n  New look for user profiles, the Explore tab, hashtag & location pages\n  \n  Also, we now display bigger grid photos to make it easier to browse large collections of photos and see them too. You\u2019ll find bigger grid photos on user profiles, the Explore tab, as well as on hashtag and location pages.\n  New look for the photo upload screen\n  \n  Larger text box for captions: It\u2019s now easier to write and edit longer captions for your photos.\n Geotag your photos by selecting \u201cAdd to Your Photo Map\u201d: Let people know where you took your photo by turning \u201cAdd to Your Photo Map\u201d on. By enabling this feature on your photo, you will store your current location with the photo and it will show up on your map. You\u2019ll also have the ability to choose a specific place name (e.g. \u201cSouth Park Cafe\u201d) to associate with your photo. Learn more in our help center.\n \n  Now, when you get to the bottom of a page, we\u2019ll automatically load more photos for you \u2014 no need to tap a \u201cload more\u201d button.\n  Many speed improvements to make the app faster & more responsive\n  Instagram v3.0 contains many speed and loading optimizations on newer devices.\n  Users can now report individual comments as abuse or spam\n  \n  To ensure Instagram is a safe and fun place to share your photos, we\u2019ve introduced a way to report comments. Now, just swipe right or tap on a comment and tap the trashcan icon. You can then select \u201cDelete & Report Abuse\u201d to remove the comment and report it in a single action. If you see an abusive or spammy comment on someone else\u2019s photo, you can now easily report it as spam by following the steps above, though you will see a different icon instead of the trash icon. Note: Reporting comments that are not on your own photos is not included in the current Android version, but we do plan to add it in a future release.\n Flagging a user has been simplified to one choice: \u201cReport for Spam.\u201d To report a user, tap the button at the top right corner of their profile page, then \u201cReport for Spam.\u201d If you\u2019d like to report a user for other reasons, please report the abusive or spammy comments or images.\n To report a photo, just tap the \u201c\u2026\u201d button below the photo and then \u201cReport Inappropriate.\u201d\n \nVisit the Instagram Help Center to learn more about version 3.0.","item_date":"Aug 16 2012 07:00:00","display_item_date":"08-16-2012","url":"http:\/\/blog.instagram.com\/post\/29555443184","source":"blog.instagram.com"},{"title":"Four New Integrations for August","details":"Happy August everyone! We\u2019ve got four great new integrations this month that will help you better serve your customers and expand the functionality of your Zendesk.\n Benchmark Email\nBenchmark Email is a powerful email marketing tool that helps you to easily create and send quality, eye-popping emails to your customer base. Get your message to the masses quickly and easily with Benchmark\u2019s email newsletter templates and powerful editor, and track the results from your email campaigns.\n With the Benchmark Email Zendesk integration, you can easily import a list of your Zendesk users into a list in Benchmark for quick and easy mail campaigns to help maximize your email marketing opportunities.\nEasy Insight quickly bolts onto your existing SaaS products such as Zendesk, and much more to provide you with full business intelligence capabilities. They provide a rich library of prebuilt reports and dashboards for each connection and a sophisticated report editor that gives you the capability to take and modify any of those reports, as well as creating your own tabular lists, crosstabs, charts, and diagrams as you desire.\n The integration with Zendesk allows you to create custom reports on tickets or in depth help desk analysis. You can use information from any aspect of a Zendesk to create reports, lists, charts, and more.\nInbenta is one of the leading companies that specialize in Natural Language Processing (NLP) and Semantic search. \u00a0Thanks to the use of their NLP techniques, Inbenta allows its users to present searches as questions or statements instead of limiting them to mere keywords, giving a richer experience to those looking for information on your website.\n This integration allows users to search Zendesk forums with Inbenta\u2019s Natural Language Search. Customers can get relevant answers to their questions immediately by using their own language \u2013 instead of keywords \u2013 pulled directly from your Zendesk content. Inbenta also provides relevant insights into what your customers are searching for and what topics you may need to address better in your help desk.\nUserlike is a SaaS that gives companies that ability to live-chat with customers via websites, iOS apps, and mobile devices.\n With the Userlike Zendesk integration you can easily push your customer data from your Userlike live chat to your Zendesk and keep track of your customer interactions \u2013 create Zendesk tickets out of your chat transcripts and offline messages, from your dashboard or from within the chat.\n \t\t\t\n                      it\u2019s a pleasure and an honor to partner with Zendesk!\n \n \t\t\t\t\t \t\t\t\tTo comply with the EU cookie law, we have recently revised our cookies policy. Please review it to learn which cookies we use, how and why we use them, and how you can disable cookies by changing your browser settings. Please be aware that portions of this site will not function properly if you disable cookies.  Unless you elect to disable cookies through your browser, by your continued use of this site you agree to our use of cookies as described in our cookies policy.","item_date":"Aug 16 2012 07:00:00","display_item_date":"08-16-2012","url":"http:\/\/www.zendesk.com\/blog\/four-new-integrations-for-august","source":"www.zendesk.com"},{"title":"Exploring Our API","details":"At ActiveCampaign we want our users to interact with the software in any way they wish. Although we work hard to make our standard interface as usable and friendly as possible, sometimes there are things that are missing or not as easy to configure. With software development it is difficult to please every user.\n Our API allows developers to craft their own interface, processes, and experience that utilize the power of ActiveCampaign within their own applications and websites. To make this easier, we are creating an \u201cAPI Explorer\u201d that users can access within the software and fully interact with the API using real data and examples.\n Check it out under your settings, API section:\n \n You can simulate API requests through this simple interface which also shows you all relevent information you\u2019ll need to make API requests through your own applications and websites.\n The basic idea is to choose a method and supply the necessary parameters. Then hit the \u201cView API Response\u201d button to see the actual response, or to test it (which only shows the parameters you would include with the request, in case you don\u2019t want to update live data).\n \n Below is some more information and screenshots for each section.\n Request URL: The URL you request\/submit to when getting or posting data.\n \n You can use this URL in any programming language where you can submit HTTP requests (to send or receive data from other servers).\n Method Parameters: All available parameters that can be included with the request.\n \n Parameters are the specific settings or values needed to achieve the request. Think of them as the same fields you fill out in our standard admin interface when creating a campaign. For example: campaign name, type, lists, message used, etc.\n Response: The actual response that comes back from the server after submitting a request.","item_date":"Aug 16 2012 07:00:00","display_item_date":"08-16-2012","url":"http:\/\/feeds.activecampaign.com\/~r\/activecampaign\/~3\/6eKDhFGM3Ew\/","source":"feeds.activecampaign.com"},{"title":"Announcing: The Instagram for Business Blog!\n\nAre you running a...","details":"In the past two years, the Instagram community has grown to over 80 million users \u2014 individuals, brands, celebrities, artists \u2014 around the world. \u00a0As we grow, the role that businesses and organizations are playing continues to grow as well, which is why we\u2019re launching the Instagram for Business blog.\n  We\u2019re excited to see how businesses are using Instagram to create genuine connections with their followers, to share who they are and what their brand stands for, and to engage with the Instagram community. As a business, we want to make sure that you have the knowledge and tools you need to start using Instagram in a meaningful way.\n  We\u2019re launching the Instagram for Business blog as a resource for businesses \u2014 a way to discover what other business accounts are doing on the platform and to learn tips and tricks for how to use Instagram from a business perspective. \u00a0We\u2019ll be posting brand spotlights, API examples, Instagram news and tips for using Instagram for business. If you\u2019re a business savvy Instagrammer, be sure to visit the Business blog for more!","item_date":"Aug 14 2012 07:00:00","display_item_date":"08-14-2012","url":"http:\/\/blog.instagram.com\/post\/29442110637","source":"blog.instagram.com"},{"title":"The LSRC Hackathon is a Wrap! Thanks Austin!","details":"We came, we saw, we hacked overnight \u2013 not only that, but we were welcomed with open arms by Austin\u2019s incredible tech and startup scene, who made it incredibly easy to be a Montreal company organizing and \u00a0hosting an event in a city some of us (me!) had never been to. \u00a0Some seriously impressive hacks were built overnight from Saturday August 11th to Sunday August 12th! Here\u2019s a breakdown of the awesome winning hacks, along with a few of our own honourable mentions.\n \n2nd Place \u2013 NotifHack, created by OtherInbox\u2018s Ben Hamill and Bob Potter, combined Twilio and Context.IO to create a nifty tool that allows users to setup rules to filter email push notifications to your phone. Looks like these two will enjoy shared custody of a sweet Apple Thunderbolt display!\n1st place \u2013 the big winner of the weekend was Jesse Lovelace, who created Deadman.io. Engaging in some risky business? Deadman.io is a digital deadman switch or insurance policy that triggers calls, emails, or texts of important files, photos and\/or data (as defined by the user). Deadman.io combined Twilio, Sendgrid and Context.IO. Jesse also managed to not only get a functional hack in place that leveraged 3 of the 4 participating APIs, but he was able to create a simple website and present his hack with flair. Not bad for 15 hours on no sleep!\nMany thanks to our awesome local judges Mickey Ristroph, Jon Loyens and Joe Boutros. They didn\u2019t have an easy task, and they did an awesome job!\n \n\u00a0Not everyone could be a big winner, but there were a few standouts that we were pretty excited about. One of these was Remail.IO, a service that solves the problem of redundant scheduled emails. Ever scheduled an email only to be in contact with its recipient before the scheduled email goes out, rendering the scheduled email invalid? Kind of\u00a0embarrassing\u00a0when that email goes out anyways because you forgot to remove it from the queue. Remail.IO uses SendGrid to send out emails, and Context.IO to know when you\u2019ve replied to a scheduled contact and takes them out of the scheduling queue. Embarrassment avoided! We know that Taylor Brooks was one of the creators of Remail.IO (thanks twitter!), but unfortunately we didn\u2019t catch your teammate\u2019s name! Let us know what it is so we can send you some t-shirts!\nWe also liked Reply Analytics, which used Context.IO to create a slick interactive dashboard of personal analytics for your email habits. If you\u2019ve ever wondered what your average response time is for email, when you send most of your email and more, this hack will show you. Did you create Reply Analytics? We didn\u2019t catch your name and we want to send you a t-shirt!\nIt was awesome to see Brandon West from Sendgrid again and meet Keith Casey from Twilio and Chris Lamprecht from Searchify. So glad you could join us!\nThanks again go out to Josh Baer, Veronica and Allyson at OtherInbox and Sarah and Matt at Capital Factory. We couldn\u2019t have done this without you!\nThanks everyone who participated for making us start counting down the days to our next Austin event! We can\u2019t wait to come back!\nIf you enjoyed this post, please consider leaving a comment or subscribing to the RSS feed to have future articles delivered to your feed reader.\n{ 2 comments\u2026 read them below or add one } \n  Taylor Brooks August 13, 2012 at 7:31 pm  \nThanks for the mention Sarah-Jane, we had a blast!\nWe\u2019re actually planning on developing Remail.io further and of course deeper integration with Context.io. We\u2019ll keep you in the loop!\nBtw, Nathaniel Jones (@thenthj) was my teammate\u2019s name.","item_date":"Aug 13 2012 07:00:00","display_item_date":"08-13-2012","url":"http:\/\/blog.context.io\/2012\/08\/the-lsrc-hackathon-is-a-wrap-thanks-austin\/","source":"blog.context.io"},{"title":"AlchemyAPI Sponsors HackDenver 2012 August 17\u201318","details":"We are proud to be sponsoring HackDenver 2012, a free 2-day hackathon designed for attendees, both technical and non-technical, to build apps, explore new technologies, hack APIs, compete for prizes, and meet interesting people. The hackathon will be held from August 17th-18th at Uncubed Coworking, located in the River North Art District (RiNo) of Denver.\n The \u201cBest App Using AlchemyAPI\u201d wins $500 in prizes.\u00a0In addition, the \u201cBest Hackathon Overall App\u201d receives $500 in prizes for the team.","item_date":"Aug 13 2012 07:00:00","display_item_date":"08-13-2012","url":"http:\/\/blog.alchemyapi.com\/?p=83","source":"blog.alchemyapi.com"},{"title":"Announcing PhoneGap Library","details":"We are proud to announce the release of the Urban Airship PhoneGap library, which is available as of August 8, 2012. We are shipping the library with support for iOS and Android. This release wraps our existing libraries for both platforms, allowing users to easily embed Urban Airship in their applications. Wrapping our client libraries means that you can include push messaging, location history, and results reporting in your application using just a few lines of code.\nTo make it easier for you to use the library, the same JavaScript API is available for both iOS and Android, so you can use the same JavaScript code on both platforms, with little or no changes. The only differences are in a few functions due to minor quirks in how each platform handles sound and vibration preferences, badging, and initial registration. As a result, some functions are no-op\u2019d as iOS only or Android only calls, but the majority are cross-platform.\nBasic Example\nWe have followed modern JavaScript convention as much as possible in writing the API to take an asynchronous approach using events and callbacks. This style should be familiar to many programmers with JavaScript experience. The short example below illustrates this approach:\nThis is a brief but fully-functioning example that demonstrates the basics of what you can accomplish with the Urban Airship PhoneGap library. Please refer to\u00a0our documentation\u00a0for more information.\nGetting the code\nYou can get the plugin, which includes a sample application for both iOS and Android, from our\u00a0Resources\u00a0page. \u00a0These sample apps have a common code base and will behave in the same manner on both iOS and Android. While both use jQuery to make DOM interaction easier, nothing in our library requires that you have jQuery installed.\u00a0The source code is also available on\u00a0GitHub.\nIf you have questions or need support, please email us at\u00a0support@urbanairship.com.","item_date":"Aug 09 2012 07:00:00","display_item_date":"08-09-2012","url":"http:\/\/urbanairship.com\/blog\/2012\/08\/09\/announcing-phonegap-library\/","source":"urbanairship.com"},{"title":"How To Tell A Story With Code","details":"Whether you are in front of a packed room of hackers, a huddled group on a trade show floor or a sweaty basement filled with gutter punks, every audience just wants you to tell them a story. \u00a0From sprint demos to superhero flicks, telling a good story is the surest way to captivate a human being\u2019s attention, spark imagination and compel absorption of your ideas.\n Narrative is this crazy itch we all want scratched \u2013 if you think back to the last speech, song or standup routine that really grabbed your attention, chances are good that it told a story. \u00a0This very human craving is something we try to feed on our developer evangelism team here at Twilio. \u00a0But, I\u2019ll be the first to admit, constructing a narrative out of code is damn tough.\n As a developer myself, I want the presentations I attend to be full pragmatic utility and technical detail. \u00a0 If you\u2019re telling a story built around technical concepts, the narrative can\u2019t be ornamental or bolted on. \u00a0Pretty language or fairy tale use cases doesn\u2019t impress hackers \u2013 the value of their time is too immense to waste with extraneous metaphor.\n Further, Twilio\u2019s products present a difficult storytelling challenge. \u00a0The broad theme of powering communication binds them all, but threading together Twilio Voice, SMS and Client into a coherent tale that will capture a busy coder\u2019s attention is a problem whose solution is solidly non-obvious. \u00a0Ringing your phone, sending a text message and talking into your browser are very different human experiences that live mostly in isolation. \u00a0Can you immediately think of one contiguous, compelling scenario that would loop in all three?\n Finally, the pure act of telling the story with code is super hard. \u00a0Technical audiences \u2013 to my undying gratitude \u2013 are very discriminating. \u00a0Some joker in a red jacket can\u2019t come along and hack out a mommy class and a daddy argument instantiating a little baby object together and expect to earn a developer\u2019s respect. \u00a0The code written in this story needs to be non-trivial and instructive, do something the developer can\u2019t just squeeze out in five minutes looking at the documentation, and \u2013 above all -\u00a0work.\n Factor in all three of these challenges and storytelling through programming starts to look pretty tough.\n Constructing A Narrative Out Of Code\n But telling that story is our job as developer evangelists and doing that job well is your expectation as our customer. \u00a0And like all the difficult problems we work on at Twilio, the solution involves breaking it down to its constituent components and solving each individually. \u00a0In this case, we need to figure out a framework around which we can build a narrative, find a theme that is compelling, cut some code that works and put that all into one story that would be worth telling.\n In broad strokes, a story is a coherent narrative with a beginning, a middle and an end. \u00a0While extremely basic, such a framework quickly starts to make the solution easier to find. \u00a0For a technical presentation, a beginning could be a problem statement \u2013 a description of some issue that resonates with the audience. \u00a0The middle could be writing some simple solutions to address the discrete parts of the problem. \u00a0Finally, the satisfying conclusion could be stringing that code together into software that works.\n The Karaoke Case Study\n Such a framework makes the narrative coherent, but leaving it at the paragraph above would be a serious snoozefest \u2013 our story needs to be interesting! \u00a0No matter how many lolcats I sneak into the deck, no one is going to leave happy if my beginning, middle and end is boring.\n Using the narrative framework we described above, everything hinges on the problem statement. \u00a0If the problem statement captures the audience\u2019s attention, the solution is likely to sustain it. \u00a0But what is a problem that is likely to resonate with a broad technical audience and be solved by telephones, browsers and SMS?\n In my own experience, the problem statements that grip my attention are the ones with absurd combinations of high-powered technology with completely unrelated problems. \u00a0Jet powered beer coolers, automatic grapefruit segmenters made of Legos, Arduino ping pong cannons \u2013 I love them all. \u00a0Capturing that kind of audacious hacker spirit in our problem statement is sure to be compelling.\n A few months ago, such a problem statement came to mind when looking at the Twilio logo. \u00a0Our tagline is \u201ccloud communications\u201d \u2013 what if we used cloud technology to solve an unexpected problem akin to using a jet engine to supercool Guiness?\n What if we used the\u00a0cloud to sing\u00a0karaoke?\n Building a karaoke machine in the cloud is a catchy theme and perfect for our narrative framework as we have four different pieces to build: the microphone, the sound system, the music and the lyrics. \u00a0Further, they conveniently align with the building blocks we\u2019re trying to make a story out of \u2013 Twilio Voice, SMS and Client. \u00a0Now we just need to cut some Python that\u2019ll do it.\n For the microphone, we can use the phone in the pocket of an intrepid audience member by hacking out a quick TwiML Voice app that puts the audience member in a Conference.\n \n For the sound system, we can use the speakers on my presentation laptop by dialing into the same Conference using Twilio Client and a browser.\n \n The music we can deliver into the Conference by scripting a REST API request to call into the same number as the audience member and execute a <Play> verb.\n \n Finally, the lyrics for the song we can\u00a0send via SMS\u00a0line-by-line to the singer\u2019s phone from a text file.\n \n String it all together with a few bad jokes and we suddenly start to have a quality story we can tell. \u00a0 Not only do we use all three building blocks we wanted to start with, but we also use them fully by demonstrating inbound and outbound Twilio features. With the snippets of code above we have a solid middle to our story that builds to a full karaoke machine \u2013 a suitable conclusion for our code-powered narrative.\n Making It Work\n Now, all that is left is to tell that story. \u00a0Here\u2019s an attempt at such I gave recently at Philly Emerging Tech.\n \n ETE 2012 \u2013 Rob Spectre \u2013 How to Build a Cloud-Powered Karaoke Machine from Chariot Solutions on Vimeo.\n The code for the presentation is up on GitHub, as well as the slides from the deck.\n Storytelling is a crucial skill for all public speakers \u2013 we who ply our trade in tech shouldn\u2019t consider ourselves excluded. \u00a0 A strong narrative is hard to ignore and whether you are slinging slides, bullet points or lines of code, every presentation would do well from having one.\n Just like this blog post, if you build your presentation with a strong beginning, middle and end, a compelling theme, and make them work together, you will capture your audience\u2019s attention and \u2013 hopefully \u2013 their hearts and minds as well.","item_date":"Aug 09 2012 07:00:00","display_item_date":"08-09-2012","url":"http:\/\/feedproxy.google.com\/~r\/twilio\/OBEN\/~3\/FQejDp8Efrk\/how-to-tell-a-story-with-code-2.html","source":"feedproxy.google.com"},{"title":"An Insight to White Labeling of Banckle Chat","details":"Banckle Chat has grown mature and still enhancing every day in terms of live chat features. Because of its super extensive features,  companies and organizations around the globe are becoming our  prospective customers on daily basis. That is why; we keep getting  customers suggestions and queries about certain features at all times.  But after the incredible customer response to White Labeling for Banckle Meeting, the most frequently asked topic around Banckle DOT com is White Labeling service of Banckle Chat.  What does this service include? How do we get it? Who can benefit with  this service? Among many, these are the frequently asked questions from  our customer support.\n So, dont worry! All of your questions will be answered here today as we are taking you now on a little trip of an insight to White Labeling service offers by the Banckle Chat. We always recommend our customers to try out our application with a free Banckle account before they get on board and subscribe it. This act will clear out any confusion which might be there in our customers mind and boost-up their confidence towards Banckle Chat. It also gives them an opportunity to get to know one of the impeccable work done by our developers.\n An Insight to White Labeling of Banckle Chat\n White Labeling service lets you enjoy and play around with the truly insight elements  of the application. These factors are necessities and significant in  terms of usage and reliability. Did you ever consider having your very  own personalized communication with your visitors? Or have you ever  thought of having re-branding environment where you get this feeling of  owning that particular application? If yes, then White Labeling service for Banckle Chat should be your optimum choice as it is highly focused on what our  customers are looking for their enterprises in order to increase growth  and productivity.\n Widget Customization \n Another commendable effort by Banckle Chat team, as we proudly allow our customers to customize the live chat widget. Add face value to your chat widget by replacing the Banckle Chat label with your own companys one. Only White Labeling service allows you to change the overall look of your chat widget. For example, changing the Powered by label into your companys official one. The control of the widget is  delegated in your hands and can even change the overall essence of it  using CSS code. Its an opportunity to inspire your customers with its  outstanding live chat user interface customized by you.\n \nEmail Template Customization \n How about having your own companys email template to send out to your  customers at different occasions? You get full access over the most  professionally designed email templates by our team. For example,  Offline Message Reply, Forward, Auto-reply, Chat History and other  related email templates. Apply your company entities in email templates  and take its customization to a new level.\n \n Why not following up your customers and send all the email notifications from the live chat application with your own email address and SMTP server instead of noreply@banckle.com? You can now configure  your own SMTP server and email address. Isnt that great? Its all  happening just the way you want.\n \nApplication Re-branding \nWhite Labeling service for Banckle Chat allows you to change the applications header  and footer text only. Alter it with your own companys name instead of Banckle Chat; similarly Copyright \u00a9Banckle2010 with your own text. Get your companys logo re-branded as well so that  you can enjoy all the possible options available in the White Labeling package.\n \nFor overall information on White Labeling for Banckle Apps, you can go through our Banckle Apps Wrapped in White Labeling blog. Whether its a customer support system or other features customization e.g. Powered by label etc, we have sealed all for you in one package.  We endeavor our customers suggestions and recommendations in order to  improve as an application. After all, these remarkable efforts towards  its perfection are made for you as aesthetically as possible. If you are  already a Banckle customer and want to subscribe this service, its just a few clicks away. We are always happy to serve you in any way and will assist you all the way through the checkout.\n Share your feedback and if you like my efforts in the blog, hit like! If you are not as yet a part of Banckle Family, Sign Up now and collaborate with your customers anywhere, anytime via Banckle Chat. Stay tuned and get to know about all the latest happenings at Banckle Collaborator Blog or by signing up for our monthly Banckle Collaborator Newsletter. Thank you for reading!\n    Share and Enjoy:","item_date":"Aug 09 2012 07:00:00","display_item_date":"08-09-2012","url":"http:\/\/banckle.com\/blog\/collaborative-and-social-apps\/entry\/an-insight-to-white-labeling-of-banckle-chat.html?utm_source=rss&utm_medium=rss&utm_campaign=an-insight-to-white-labeling-of-banckle-chat","source":"banckle.com"},{"title":"Zendesk API v2","details":"YourTrove, an online service for developers, conducted a survey last year asking what they consider to be the biggest pain points in API development. The responses included out-of-date, unreliable documentation and a lack of guidance. At Zendesk, we\u2019ve built our API to be a developer community and resource for our customers to grow their service and support business.\n David Lorge Parnas, one of the pioneers of software engineering, once wrote about API: \u201cReuse is something that is far easier to say than to do. Doing it requires both good design and very good documentation.\u201d So, in creating and updating our API, we make sure to keep our eyes on both.\n We love seeing just how active our developer community is\u2013with nearly 20 percent of all tickets generated across all Zendesk accounts created through our API. To add to that, our partner ecosystem boasts over 100 third-party integrations.\n Back in March this year, we introduced our next-generation API v2 into public beta. Today, we\u2019re continuing to build on a consistent, well-documented and fully-featured API. Here\u2019s what we\u2019ve been working on:\n Created more API endpoints so our customers can gather more insights, gain more visibility, and further automate admin workflows in their Zendesk\n Developed more documentation that accurately guides our developer community in enhancing their Zendesk, as well as building their own custom apps and integrations\n Standardized our API on the popular JSON format\n Introduced a versioned API so developers can build against a stable, well-documented API as we continue to grow Zendesk\u2019s features and capabilities\n \nCheck out our developer site to learn more!\n \t\t\t\n \t\t\t\t\t \t\t\t\tTo comply with the EU cookie law, we have recently revised our cookies policy. Please review it to learn which cookies we use, how and why we use them, and how you can disable cookies by changing your browser settings. Please be aware that portions of this site will not function properly if you disable cookies.  Unless you elect to disable cookies through your browser, by your continued use of this site you agree to our use of cookies as described in our cookies policy.","item_date":"Aug 08 2012 07:00:00","display_item_date":"08-08-2012","url":"http:\/\/www.zendesk.com\/blog\/zendesk-api","source":"www.zendesk.com"},{"title":"Ooomf Powers App Distribution Using Twilio SMS","details":"Mikael Cho is the Founder of ooomf, a Montreal-based startup building the future of social app discovery. Prior to ooomf, he co-founded 2 startups and graduated from the University of Wisconsin with a degree in Psychology.\n Ooomf is a social app discovery platform. They make it easy for app developers to create beautiful, intuitive landing pages to more easily\u00a0distribute\u00a0their apps and acquire new users. We managed to catch up with one of the founders, Mikael, to ask him about his plans for Ooomf.\n Why did you build ooomf?\n The idea for ooomf came from participating in a Montreal Startup Weekend event this year. We saw so many cool mobile apps being built, but after the weekend, many of the projects started to lose momentum and we didn\u2019t hear from them again. This got us thinking that it would be awesome if anyone with an idea for an iPhone app could benefit from having a place where they could build a following without having to dish out tons of cash on marketing.\n Right now, it\u2019s a massive challenge for mobile developers to get their app noticed with almost 700,000 apps in the App Store. We want to level the playing field so the best mobile apps and developers are showcased, not just ones that have the biggest budgets.\n What have you been focused on following Startup Weekend?\n ooomf is recent graduate of the Spring Founder Fuel Accelerator Program and operates out of Montreal, Canada. Our team came from the agency world, where we\u2019ve had experience designing and marketing apps.\n We spent a lot of time working with and seeing great apps that never got any traction. We think this is wrong and our goal is to create an ecosystem that rewards exceptional mobile apps. There\u2019s so many problems with app discovery today and with thousands of new apps being added into the market each week, the task of making sense of it all is only getting more challenging.\n We think figuring out app discovery comes down to an improved user experience and surfacing the best of what\u2019s out there. In working with our customers, we know the key will be to help them find quality apps and make it dead simple to get these apps on their devices.\n Why did you decide to use Twilio?\n One of the biggest pieces of feedback we got from our customers, is that it sucks to find an app on a desktop or laptop computer or hear about it from a friend and then try to transfer it to a mobile device. The current process is not efficient. It typically involves retyping and potentially misspelling the name in iTunes, or having to download it on a computer first instead of directly on the mobile device.\n We integrated with Twilio to enhance this flow and provide a text-to-download feature so our customers can simply enter their phone number and they\u2019ll receive a link to download the app directly on their mobile device.\n App developers and users love this feature because it just makes it so much easier to download an app directly on a mobile device.\n What\u2019s next for ooomf?\n We have a lot of users located outside of North America, so it was great news when we heard about Twilio\u2019s global SMS launch. The big vision for ooomf is to be the simplest and most beautiful way to discover and share the best apps in the world.\n We\u2019re releasing a public beta version of our product this week, and we\u2019ve got a big vision that we want to achieve in the mobile space. Things are moving really fast, we\u2019re talking with so many great customers, and building like crazy.","item_date":"Aug 08 2012 07:00:00","display_item_date":"08-08-2012","url":"http:\/\/feedproxy.google.com\/~r\/twilio\/OBEN\/~3\/IzTbIZlk-TQ\/ooomf-powers-app-distribution-using-twilio-sms.html","source":"feedproxy.google.com"},{"title":"Google Play \u2013 What Other Android App Markets Are There?","details":"","item_date":"Aug 08 2012 07:00:00","display_item_date":"08-08-2012","url":"http:\/\/feedproxy.google.com\/~r\/AppsgeyserBlog\/~3\/fu_Yul4ejHQ\/","source":"feedproxy.google.com"},{"title":"Heading back to campus this fall? Apply to be a @4sqOnCampus ambassador!","details":"Do you know all the ins and outs of your campus? Are you hip to the happenings within your community? We\u2019re seeking passionate, active foursquare users who are well-connected to student life at their schools to help us grow the foursquare community among colleges and universities as our campus ambassadors.\n \n Sound good? Read the full ambassador description here\u00a0and apply now!\n \n Want to keep up with all of foursquare\u2019s campus happenings, new badges, contests and more? Follow @4sqOnCampus!","item_date":"Aug 07 2012 07:00:00","display_item_date":"08-07-2012","url":"http:\/\/feedproxy.google.com\/~r\/thefoursquareblog\/~3\/8R0vkvGXJN8\/","source":"feedproxy.google.com"},{"title":"Making it easier for you to develop on LinkedIn","details":"Adam Trachtenberg, Aug 6, 2012           Today we announced updates to the LinkedIn Platform. In this post, learn more detailed technical overview of these changes, to help you understand how these changes will impact your code. Additionally, we have a few small new features to help you monitor and manage your application.\n Developers can now request email addresses from users who want to use their LinkedIn credentials to sign-in and register for applications.\n Weve implemented this by introducing Member Permissions, a feature that allows you to specify exactly which information the member shares with your application and actions you can take on their behalf. And our redesigned Authentication screen displays to potential users the Member Permissions youve requested.\n \n With Member Permissions, we have further refined how members control their profile information, along with the profiles of their connections. The profile is now broken into four pieces:\n Profile Overview: A digital business card with name, photo, headline, and current positions\n Email address: The members primary email address on LinkedIn\n Contact Information: Address, phone number, and bound accounts\n Full Profile: Full profile including experience, education, skills, and recommendations\n \nWe encourage you to enable features in your applications that allow members to interact with and share to their network.  The following permissions enable you to bring engagement features to your application:\n Network Updates: Retrieve and post updates to LinkedIn\n Connections: Read 1st and 2nd degree Connections\n Group Discussions: Retrieve and post group discussions\n Invitation and Messages: Send messages and invitations to connect\n \nIn situations where our APIs return profile information of members in your users network, your application will now have access to fields within the Profile Overview Member Permission. We made this change to better balance the experience for all our members, whether theyre using your application themselves or connected to those that are. For more information on profile fields available to your application, review our Profile Fields documentation.\n Resources without member specific information, such as Companies and Jobs are not impacted by Member Permissions. You can continue to use them without needing to explicitly for permission.\n This is available for all new API keys using our REST and JavaScript APIs. If you have an existing API key, we will introduce a migration path shortly, but encourage you to explore the feature in your development environment. If you cannot wait, you can create a new API key for your application.\n On the technical side, weve borrowed the OAuth 2.0 concept of the scope parameter and incorporated it into our OAuth 1.0a and JS Authentication flows.\n For API keys using Member Permissions, we have also standardized the duration of our access grants at 60 days, with the ability for you to seamless extend them in 60 days intervals whenever a member comes to your application.\n To see Member Permissions in action today, check out our Showcase partners Behance, Janrain, The Economist, and Business Insider. \n Full details on Member Permissions and Access Grant Extensions are covered in our Authentication documentation.\n Improving the Share Experience\n With over 1 million publisher sites already enabling the ability to share content to LinkedIn, were now making it even easier for our developer community to use LinkedIn as their main distribution platform for discussing and sharing professional content. \n Our Share API now provides qualified developers with attribution on LinkedIn\u2019s update stream, enabling greater rewards for building customized and integrated LinkedIn sharing experiences in your applications. \n Three developers are already live with Share Attribution: Wordpress, Flipboard, Behance. We will begin rolling out this feature on our homepage over the next few days.\n If you are interested in attribution, visit our Partner Programs page for eligibility criteria and the application process.\n The richer the content shared on LinkedIn, the more engagement it creates among members. As developers using our Share API, you can always specify the exact title, description, and image that you want to be attached the the members share. \n Since some developers cannot incorporate this information into their API calls, weve updated how we process text-only updates. Now, when you only provide a Share without an URL or image, we will  automatically identify embedded URLs in the status, and use that to provide a richer experience. \n Read Share API best practices and technical details in our revised Share API documentation.\n New Throttle Monitoring Dashboard\n To help you understand your throttle limits, we have created a Throttle Monitoring Dashboard. For each resource, you can view your application and member limit, along with knowledge whether your application or members have hit todays quota.\n Remember, we always return a Status Code of 403 whenever you are throttled. It is a best practice to catch and log this result, and have your application behave accordingly.\n However, to simplify debugging, you can now use our dashboard to check the current status of your applications.\n The Throttle Monitoring Dashboard is available in your API Key Management Console by clicking View API Usage.\n We are excited to see how these features will enable your applications to be more effective than ever.","item_date":"Aug 06 2012 07:00:00","display_item_date":"08-06-2012","url":"https:\/\/developer.linkedin.com\/blog\/making-it-easier-you-develop-linkedin","source":"developer.linkedin.com"},{"title":"Updates to our LinkedIn Developer Terms of Service","details":"Adam Trachtenberg, Aug 6, 2012           You have shared a range of feedback describing how we could better serve your business and technical needs as developers on our Platform. \n To better meet our mutual needs, our rewritten API TOS:\n Introduces a Partner Program, designed to give specific categories of applications access to our Platform. Approved partners in these programs often get enhanced use of our APIs and premium support. Learn more about all of our Partner Programs, including the application process.\n Grants the ability to cache data for performance, and enable you to create more responsive applications.\n Permits the creation of test accounts, to make it easier for you to test interactions between members.\n Reorganizes and rewords the overall document to increase clarity.\n Includes a number of other changes.\n \nWe have also updated our Plugin TOS.\n These terms are effective immediately. Please read them and, if necessary, update your application accordingly.","item_date":"Aug 06 2012 07:00:00","display_item_date":"08-06-2012","url":"https:\/\/developer.linkedin.com\/blog\/updates-our-linkedin-developer-terms-service","source":"developer.linkedin.com"},{"title":"Resource identification is not a REST invention","details":"It was so full of bollox that I almost started to hyperventilate \u2013 which is a pun on the abundant use of the word hypermedia in that same post\n Let me just quote one part of the post:\n This subtle shift in the source of application control information makes it possible for the same client code to recognize and execute new features as they appear over time without the need for patches or downloads\n The post contains more nonsense like this. Let me quote yet another brilliant piece, and be done with quoting:\n \n Similar to the problem faced by system architects, developers lack the tooling for implementing hypermedia systems\n Allegedly, what this author calls hypermedia systems hasn\u2019t been conquering every single IT system in the world because of missing tooling? Permission to laugh or cry please\u2026\n The piece praises the browser as a limited version of this ingenious contraption which enables the user to perform diverse tasks like banking and gaming with just the same browser without the need to download. Well excuse me, but all your browser does is continuously download \u2013 all the time\n But let\u2019s leave the post for what it is, and address the issue at hand: changing functionality and function calls within a global IT system (which I think we can call the web, and many Enterprise applications \u2013 not to mention the average SaaS).\nThe basic problem is the evolutionary change aspect. We all evolve, change, grow \u2013 and everything around us. So does IT, and that is good \u2013 and bad. The bad part is where something changes that you don\u2019t want or need to change: in other words, there is added functionality or business value somewhere that you didn\u2019t ask for, yet because it now has changed and you are using it, you have to change your own implementation of it nonetheless \u2013 that is money thrown away, of course\n That own implementation usually is a program that receives responses to requests sent out, and processes those towards or in the back-end. Loosely-coupled as anything might be, it is still coupled: the program you have written to couple with the outside world, is itself coupled to existing IT applications within its own landscape. Technically that might be loosely coupled, but functionally it is very tightly coupled of course\n So that is why backward compatibility was invented. Backward compatibility basically means that whatever new functionality is added, you won\u2019t have to change your program unless you want to use it. Needless\n to say, backward compatibility isn\u2019t a REST invention \u2013 it is a common problem in hich-tech and e.g. the IBM 7080 was introduced in 1961 because its predecessor, the IBM 7070, was not backward compatible at all with the IBM 705\n So, you have to design and distribute new functionality in a smart way, in order to stay backward compatible. In IT, however, there is an additional issue: function calls change too, with added functionality.\nI solved that problem by having each function have exactly one parameter: a by-reference object or array. But let me illustrate the issue first\n OrderCar(Make, Type) is the functionality that let\u2019s you order a car. All you have to do is give make and type to uniquely identify one, and you\u2019ll be granted a brand new car. So, these two parameters (make and type) are mandatory.\n Then, as it\u2019s not a T-Ford, the car salesman upgrades to a new functionality, enabling people to pick a coulour as well. Here is where the problem is at: the function call now has 3 in stead of 2 parameters, so everywhere you use the function, you\u2019ll have to change it from OrderCar(Make, Type) to OrderCar(Make, Type, Colour) \u2013 otherwise it won\u2019t be able to execute, regardless of which functionality you use \u2013 it will miss the third variable and simply cause your program to abort\n However. In compliance with the rules of backward compatibility, the new parameter \u201ccolour\u201d is optional, and in some programming languages you can make a function call parameter optional as well \u2013 in that case, you won\u2019t have to change a thing. The colour will just default to black, you won\u2019t have to change any function call, and there you go.\n But if you\u2019re not the lucky owner of one of those languages, or have the new functionality reflected in a new URL, then you\u2019re toast. Hence why Roy suggested the concept of resource identification:\n This abstract definition of a resource enables key features of the Web architecture. First, it provides generality by encompassing many sources of information without artificially distinguishing them by type or implementation. Second, it allows late binding of the reference to a representation, enabling content negotiation to take place based on characteristics of the request. Finally, it allows an author to reference the concept rather than some singular representation of that concept, thus removing the need to change all existing links whenever the representation changes (assuming the author used the right identifier).\n Roy makes a case for backward compatibility on the web here: just don\u2019t blur the URL with version information or other attributes that (quickly) change over time. He could have just stated \u201cYou know, URLs aren\u2019t backward compatible if you include short-lived info like version numbers and such. So just don\u2019t\u201d but a dissertation needs to be a certain size, and they aren\u2019t famous for being concise\n Of course that doesn\u2019t solve the issue of actually having to change your code and back-end functionality, including front-end maybe, if you want to use that fresh new functionality. After all computer programs get designed and written by people, and are inherently static \u2013 and always tightly coupled to the back-end from a functional point of view. You can soothe some of the pain, but you can\u2019t make it go away\n So, we have yet another person jumping the REST bandwagon with his own interpretation of REST.\n The problem he addressed in the beginning?\n When you have millions of users \u2013 each with an installed version of your app \u2013 every change that requires an updated download can result in lots of \u201cunproductive\u201d time\n The result of the three companies that are trying out what he calls hypermedia?\n These companies and others are adopting hypermedia as a way to improve the flexibility and scalability of both their internal and public APIs\n Strong business case convincingly solved and case closed? Not in the very least\u2026","item_date":"Aug 04 2012 04:27:29","display_item_date":"08-03-2012","url":"http:\/\/www.cloudave.com\/21246\/resource-identification-is-not-a-rest-invention\/","source":"www.cloudave.com"},{"title":"Today In APIs: Yfrog Social, Mobile BaaS, Twitter suspends Mobber and 9 New APIs","details":"Yfrog founder launches new Social network. Mobile Backend as a Service gathering momentum. Twitter suspends online flashmob platform Mobber. Plus: Parsely launches API to empower Publishers websites, Gmail update is Improving Nexus 7 Support\u00a0and 9 New APIs.\n\n\n ImageShack, the providers of popular photo sharing service Yfrog, aims to take on the big players in the Social Networking space via the launch of Yfrog Social, a full-service social networking platform for the web and the iPhone. The site wants to be the best of Facebook, Google+ and LinkedIn. Its selling point: ad-free freemium paid business model and completely open API.\n\n\n The next trend to boost mobile development is the emergence of Mobile Backend as a Service platforms, which do the heavy lifting of providing a mobile developer with a API driven backend that cares of various infrastructural services like database  creation, event tracking, geolocation, hosting, identity, messaging,  notifications, security and social media integration. Mobile BaaS platform providers include Parse.com, Usergrid from Apigee and platforms from Appcelerator and Sencha. A report at SD Times, gives a good summary along with a couple of reference blog posts.\n\n\nTwitter has disabled the use of its API for Mobber, a platform that helps users earn rewards when they distribute promoted content. The Mobber platform allows promoters (e.g. a musician) to create a campaign and ask users to promote their information via their social network accounts, for e.g. a Tweet. If the Tweet hits the target, all of them are released to Twitter in a burst and that is precisely where Twitter saw that as a problem.\n\n\nParsely launches API to empower Publishers websites\n This Is Now: showing the world\u2019s Instagram photos in real time\n Gmail update is Improving Nexus 7 Support And Launches new Labels API\n \n\n\nToday we had 9 new APIs added to our API directory including a face and voice biometrics service, a real-time bug tracking service, a pricing and product promotions platform, a automated trading service, a trading and market data service and an sms service for business applications.\n Below is more details on each of these new APIs. \n\n\n BioID Web Services API: BioID is a webcam based biometrics service for cloud, web, and mobile application developers. The service offers users a secure way to log into their online accounts, authorize transactions and manage online identities, without having to remember passwords or carry tokens. Functionality exposed by the API includes verification and identification of face and voice, quality checking, photo tagging, status checking and more. The API uses REST and SOAP protocols and responses are formatted in XML.\n\n\n Bugsense API: BugSense is a real-time bug tracking service that collects and analyzes crash reports from mobile apps. Reports provide info on wifi status, SDK version, device, carrier, and more. The API allows users to send errors to BugSense from a third party app. The API uses RESTful calls and responses are formatted in JSON.\n\n\n Discfoo API: Discfoo provides a platform for online merchants to create and manage flexible price and product promotions to suit any campaign. Users can set up multiple targeted promotions, automatic promotion redemptions, and flexible pricing.\n\n\nThe API allows developers to integrate the Discfoo promotion engine into applications to calculate discounts based on unique cart scenarios. The API uses RESTful calls and responses are formatted in  JSON.\n\n\n ForexYard API: ForexYard is an online brokerage providing to services to traders around the world. The ForexYard API enables users to build automated trading into third-party systems. Possibly trading actions include: trade requests, stop-loss, take-profit, entry orders, access to past trading, historical data requests, and charting.  The use the API, users must agree to the API disclosure and maintain sufficient funds in trading accounts. \n\n\n LMAX Exchange Trading API: LMAX Exchange is a trading and automated trading platform. The LMAX Exchange Trading API gives users direct market access for all LMAX instruments, fast trade execution(90% of trades are executed in less than 10ms), secure trading,a technical support team, risk-free demo environment, and access to up to 20 levels of current and historical market depth. LMAX Exchange provides Java and .NET client libraries and the FIX library is available on request. \n\n\n Net Oxygen SMS API: Net Oxygen is a Swiss company that offers an SMS service specialized for business applications. It has a number of features, including the ability to track sent messages and delivery status in real time. The Net Oxygen SMS service is not confined to Switzerland; messages can be sent to subscribers worldwide. There are several methods of interfacing with Net Oxygen SMS programmatically, including RESTful, SOAP-based, and XML-RPC APIs. The entire website is provided in French.\n\n\n Online Commodity Broker Certigo API: The Certigo API enables users to perform trades as well as access quotes, alerts, news, account information, margins, and other information from accounts with onlinecommoditybroker.net.  The API is designed to have ultra-low latency to accelerate the rate at which users can trade. Onlinecommoditybroker offers full support and has technical specialists to help optimize integration. \n\n\n SubDB API: SubDB is a free, centralized subtitle database intended for use by opensource and non-commercial software. Users upload subtitles that can be freely download by others. With the SubRank algorithm, the database chooses the best subtitle among all available for a video file making the download process easier. The SubDB API lets users download and upload subtitles from the database. The API uses HTTP calls and responses are formatted as comma separated values.\n\n\n Viddy API: Viddy is a social video sharing service. Its iPhone application lets users shoot 15-second video clips and push them to social networking sites. The clips can be enhanced with filters and other production packages including music, tags, titles, and local information.\n\n\nThe API will allow developers to create applications that could integrate the ability to share videos on Viddy. It uses RESTful calls and responses are formatted in JSON.","item_date":"Aug 04 2012 04:24:11","display_item_date":"08-03-2012","url":"http:\/\/blog.programmableweb.com\/2012\/08\/03\/today-in-apis-yfrog-social-mobile-baas-twitter-suspends-mobber-and-9-new-apis\/","source":"blog.programmableweb.com"},{"title":"API Spotlight: Urbanesia, Brightidea and Fanatix","details":"Of the many APIs we published this week, three were highlighted on the blog by our team of writers. In this post, we\u2019ll shine a spotlight on those three, which included the Urbanesia API. We wrote that Urbanesia, the leading Indonesian lifestyle directory, had released an API that enables third-party developers to incorporate Urbanesia\u2019s location data into apps. The article continues, \u201cAlthough the API is targeted for a focused geography (Indonesia), opening location data to third-party developers should open up opportunities that would not arise without offering an open API.\u201d Read more in the Urbanesia API post or check out the Urbanesia API profile.\n\n\nThe\u00a0Brightidea API allows access to data created using Brightidea On Demand Products. The post explains, \u201cThe Brightidea Innovation Suite is a cloud-based, integrated set of Web 2.0 and social networking tools, designed to give organizations the tools to build an innovation culture amongst employees, partners and customers.\u201d Read more in the Brightidea API post or check out the Brightidea API profile.\n\n\n Fantatix is a \u201c\u2026 social TV platform for sports, but it offers a lot more than simple game viewing.\u201d\n\n\nThe blog post enumerates:\n\n\nFanatix connects your social graph with the global sports television schedule\n Fanatix enables you to connect to\u00a0Facebook to see which of your friends are watching the game\n The private group messenger makes it possible to select groups of college friends or fantasy sports enthusiasts to share the game with\n You can tweet about the game straight from the fanatix app\n \n\n\nTo read more check out the Fanatix API post or take a look at the Fanatix API profile.","item_date":"Aug 04 2012 04:19:54","display_item_date":"08-03-2012","url":"http:\/\/blog.programmableweb.com\/2012\/08\/03\/api-spotlight-urbanesia-brightidea-and-fanatix\/","source":"blog.programmableweb.com"},{"title":"Facebook \"Absolutely Not\" Shutting Down Apps That Mind the ToS","details":"Chances are that you\u2019ve seen the blog post from App.net\u2019s Dalton Caldwell wherein he says that Facebook had taken a push-to-shove method concerning an app that Caldwell and his team were making. In the post, Caldwell directly addresses Facebook\u2019s Mark Zuckerberg:\n \u201cThe meeting took an odd turn when the individuals in the room explained that the product I was building was competitive with your recently-announced Facebook App Center product. Your executives explained to me that they would hate to have to compete with the \u201cinteresting product\u201d I had built, and that since I am a \u201cnice guy with a good reputation\u201d that they wanted to acquire my company to help build App Center.\u201d\n While an acqui-hire from Facebook might be a dream proposition for some people, Caldwell wasn\u2019t interested. In telling the developer relations person that he wasn\u2019t up for working at Facebook, Caldwell apparently set off some alarms within the company. He posits that the message that was being sent his way was that he was to feel honored that he was even being given the chance to have his app acquired, rather than having it be shut down completely.\n But today at the Facebook Ecosystem CrunchUp conference (can you call it a conference?) Director of Project Management Doug Purdy says that he\u2019s never been told to shut an app down as long as it doesn\u2019t violate the ToS. The direct question was \u201cHas Mark Zuckerberg ever asked you to shut down an app that duplicates Facebook functionality, but doesn\u2019t break ToS?\u201d\n Purdy\u2019s answer leaves no room for argument \u2013 \u201cAbsolutely not.\u201d\n Purdy then goes on to say \u201cat the end of the day we\u2019re about building an awesome experience for users. If we don\u2019t have users we don\u2019t have a business. If we don\u2019t have developers we don\u2019t have a business.\u201d\n But what\u2019s probably most intersesting here is that Caldwell never said that Facebook shut the app down. Did he insinuate? Perhaps. But he never directly said that Facebook, be it by Zuck or otherwise, shut down his app. This is how Caldwell sums up the event:\n \u201cStrangely, your \u201cplatform developer relations\u201d executive made no attempt to defend my position. Rather, he explained that he was recently given ownership of App Center, and that because of\u00a0new ad units they were building, he was now responsible for over $1B\/year in ad revenue. The execs in the room made clear that the success of my product would be an impediment to your ad revenue financial goals, and thus\u00a0even offering\u00a0me the chance to be acquired was a noble and kind move on their part.\u201d\n The ultimate question here is what happened next, and we don\u2019t really know. But now we do have at least some confirmation that Facebook won\u2019t be shutting down apps as long as they\u2019re in line with the ToS. That is, unless Facebook suddenly pulls a Twitter in which case the entire game will change once more.\n Image: MIX Event, via Flickr","item_date":"Aug 04 2012 04:13:23","display_item_date":"08-03-2012","url":"http:\/\/thenextweb.com\/facebook\/2012\/08\/04\/facebooks-doug-purdy-absolutely-not-shutting-down-apps-that-abide-by-the-tos\/","source":"thenextweb.com"},{"title":"Monetization Strategies Town Hall Recap","details":"The conversation covered many topics related to monetization, including: business models, utilizing data and privacy concerns, mobile and digital strategy, testing different models of monetization, and finally future monetization models. \nThe conversation kicked off with a discussion to define monetization as well as the purpose. During this discussion, the panel defined monetization as making money from your service and utilizing the defined model to build a business that is sustainable. Matt Sonefeldt suggested aligning the business model with the key value your business creates. \n Data and Privacy: \nAs more data is being captured companies are working to find ways to utilize this data to add value to their customer base as well as find new ways to monetize their crowd. Linkedin provided an example as they discussed their ability to learn which companies are growing and then sell that information to premium customers as likely companies to sell into. The conversation naturally led to privacy and the conclusion here is that companies need good terms of service as well as a solid privacy policy. There is a fine line, but companies need to be upfront about how they are utilizing their customers data.\n Testing Different Models of Monetization: \nEach company seemed to have tried a number of different monetization models. It was interesting to hear from Linkedin and Klout, two companies that experienced a lot of traction early on without defined business models. The takeaway here is that these companies try many different strategies to monetize, they learn from these tests, fail fast, and try out different models. Nathan Hull of Penguin, discussed his efforts to capture the attention of consumers as they are now experiencing and consuming content in different formats which has also changed the competitive landscape.\n Mobile Business: \nAs the future is moving mobile, the companies had a fascinating discussion around this new frontier. Mobile consumption is dramatically impacting businesses, with nearly 20% of traffic from mobile, but very few companies have actually figured out how to capitalize on this new distribution channel. Penguin offered a great example with one of their products,  Monshi Monsters  , where 50% of their 7 to 11 year-olds are paid subscribers and in app purchases represent 80% of their revenue. \n Concluding Thoughts: \nIf companies can get customers who are passionate about their offering they will find a way to make money, it may not be optimized or maximized, but the company will eventually find a way to succeed. Facebook and Twitter are not good examples to draw from because they are such unique companies. The panel recommended starting with the value your customers are getting from your service and then determine how to monetize from there.\n Finally, the audience discussed future business models as well as models we will look back on in 5 years and realize they started during this time, including: integrating technology with pre-tech (such as foursquare) and the growing trend in online to offline, companies like pinterest, google glasses, tablets, privacy, and finally companies that capitalize on concepts related to the sharing economy.\n Gregg Alpert  ( @alpsgm  ) is a  Developer Relations Manager  at  Pearson  ( @PearsonAPI  ) He can be reached at gregg.alpert@pearson.com.","item_date":"Aug 03 2012 05:29:14","display_item_date":"08-02-2012","url":"http:\/\/developer.pearson.com\/blog\/monetization-strategies-town-hall-recap","source":"developer.pearson.com"},{"title":"Xero acquires Spotlight Workpapers","details":"Workpapers has been developed over the past 12 months to work specifically with Xero. It enables year-end compliance to be streamlined through avoiding paper processes and use of spreadsheets.\n Workpapers continues our strategy of providing a compelling platform for Accountants and Bookkeepers. We\u2019ve been very impressed with the feedback the Workpapers team have received and clear signals from our Accounting Partners that we should add Workpapers to our stable\n Workpapers is in beta use by over 100 Accounting and Bookkeeping Partners. We\u2019ll continue to build out the functionality and scalability of the software and release it under the Xero brand in March 2013 for the Australian and New Zealand markets.\n Congratulations to Richard and Julie Francis who will continue to be involved in product development and operation of the software.\n You can read the full market release here.","item_date":"Jul 20 2012 07:00:00","display_item_date":"07-20-2012","url":"http:\/\/feedproxy.google.com\/~r\/xerolive\/~3\/2WGfpyYGHZo\/","source":"feedproxy.google.com"},{"title":"Rethinking Digg: please don\u2019t forget the API!","details":"There\u2019s now an open survey at RethinkDigg (and rethink manifesto) for the planned revision of Digg after the Betaworks sale \u2013 it\u2019s great to see open doors to ideas and thoughts from fans \u2013 especially since it\u2019ll no doubt be a huge product challenge \u2013 also see the ongoing thread on hackernews and (also see coverage on places like Mashable).\n One thing that isn\u2019t mentioned anywhere though is the Digg API \u2013 and it would be sad to loose it (either in it\u2019s current form or in some evolved way) in the mix. Digg launched it\u2019s original API back in 2007 and was one of the early API pioneers \u2013 with a lot of mashups of the V1 API showing what could be done with social news information. The API was then relaunched in August 2010 and and also added a pretty cool streaming API similar to twitter\u2019s. Although it\u2019s not clear how many apps use the API today there were more than 60 mashups using the old API which was put into read-only mode in February 2010 (in the developer zone there are 100+ posts in the forums and the @diggapi Twitter feed has 700+ followers).\n So far, there\u2019s no mention of the API anywhere and Digg API twitter feed is silent on a some intermittent outages.\n Even if traction on the new API isn\u2019t high a Digg API, with the Face.com API recently mothballed it would be sad to see another API go so soon. \n No doubt the betaworks team will have it\u2019s hands full with development but I\u2019m sure Digg developers would appreciate a heads up on where the API might go + an API seems like it would be a valuable asset to the new Digg too. Hopefully we\u2019ll see some good news over time \u2013 if there\u2019s a way we can help out on Digg API front, we\u2019ll do what we can!\n Whatever happens hopefully developers will get kept up to date.","item_date":"Jul 20 2012 07:00:00","display_item_date":"07-20-2012","url":"http:\/\/www.3scale.net\/2012\/07\/rethinking-digg-please-dont-forget-the-api\/","source":"www.3scale.net"},{"title":"New Voice Features","details":"We are happy to announce our several new features for public release. \u00a0These features give you lots of power when it comes to voice technology and phone calls. Now you have access to the following:\n Voice Recording:\u00a0Record the callers voice at selected stages in your IVR menu, to create things such as \u201cvoice tickets\u201d. You can also use this to publish voice recordings to the web. Or why not build your own voicemail together with a web audio player.\n IVR ( Interactive Voice Response) Menus:\u00a0you can now create your own custom phone menus! Let the caller punch digits on his phone and give him requested information, or connect the caller to the right department. You can also use this to retrieve PIN codes or the customer no, before connecting the call to your call center.\n Audio File Playback:\u00a0Play WAV, MP3 and OGG files in your 46elks phone calls. Useful to inform the caller about specific information . You can also combine \u201cplay\u201d with \u201cconnect\u201d and let the caller know where he is being connected, before the call is connected. Also powerful to use with outgoing calls when you need to give the recipient important information.\n We have also added related sample codes and proper\u00a0documentation for these features.\u00a0As we mentioned before there are many features in the backend which are \u00a0being developed and in certain intervals we announce them publicly. If you desperately need a feature that is not in our documentation please always contact us, We might working on it or it might be under beta testing for some of our users. Sky is the limit \n \t\t\t\t\t\t\t\t\t\t\t\n   \t\t\t\t\t \t\t\t\t\t\tThis entry was posted in Announcement. Bookmark the permalink.","item_date":"Jul 20 2012 07:00:00","display_item_date":"07-20-2012","url":"http:\/\/46elks.com\/blog\/2012\/07\/20\/announcement\/new-voice-features","source":"46elks.com"},{"title":"It\u2019s live! To the million businesses on foursquare: you can now use local updates to talk to your loyal customers","details":"It\u2019s live! To the million businesses on foursquare: you can now use local updates to talk to your loyal customers\n  \tfoursquare\u2019s mission is to help people make the most of where they are, and a big part of that is helping to connect people with places they love. With the launch of local updates yesterday, we\u2019ve made it easier than ever for the nearly 1,000,000 businesses on foursquare to connect with their loyal customers (for free).\n \nStarting today, if you\u2019ve claimed your location on foursquare, you\u2019ll see our redesigned merchant dashboard when you log in. As before, you can update your listing information, create specials, and see analytics about your foot traffic. But you\u2019ll notice we\u2019ve also added a ton of new features to make managing your locations easy and intuitive. Share updates with your loyal customers\n Is an author coming to your bookstore to give a reading? Did a shipment of this season\u2019s hottest heels just arrive? Have a delicious new dish on your menu? Let your loyal customers know.\n Updates can be anything \u2013 from news about an upcoming event to photos of the daily specials, and they show up right in your customers\u2019 friends tab when they\u2019re in the same city.\n Talk to your loyal customers. We automatically connect you with people who have liked your business, or who check in there often.\n Your updates are shared everywhere. Updates go to your fans, on your business page (check out Luke\u2019s Lobster\u2019s), appear after a person checks in, and show up in foursquare Explore. It\u2019s like being able to put the chalkboard where you write your daily specials in tons more places.\n Updates are easy to create. You can quickly attach photos, specials, and share to Twitter and Facebook \u2013 it\u2019s as easy as writing a tweet.\n \n Share at all your locations, a few, or just one. Whether you\u2019re Starbucks or you run a couple of local boutiques, managing multiple locations\u00a0just got a million times simpler (we measured it). Create one special for all your locations, or craft a unique identity for each (or anything in between).\n Measure your success. Your dashboard now has more tabs, fancier graphics and better insights into your location\u2019s visitors and performance \u2013 view aggregated stats across all your locations, a handful of places, or just one.\n Run specials with ease. Create and run specials at a group of places or a single location without any extra steps. Use the new toggle button to easily end or reactivate past campaigns.\n \nReady to get going? Learn more about updates and start sharing them today.\u00a0Not on foursquare yet? Claim your location in less than 90 seconds\u00a0and get access to all our free tools to grow your business.","item_date":"Jul 19 2012 07:00:00","display_item_date":"07-19-2012","url":"http:\/\/blog.foursquare.com\/2012\/07\/19\/its-live-to-the-million-businesses-on-foursquare-you-can-now-use-local-updates-to-talk-to-your-loyal-customers\/","source":"blog.foursquare.com"},{"title":"Map data updates for parts of Europe, Africa, and Asia","details":"As part of Googles mission to provide the most complete and authoritative maps to everyone, we\u2019re excited to announce upgraded maps for Croatia, the Czech Republic, Greece, Ireland, Italy, Lesotho, Macau, Portugal, San Marino, Singapore, and the Vatican City.\n Today\u2019s update follows previous updates in North America, Europe, and Australasia, and is part of an ongoing project we call Ground Truth: in which we source data from regional partners, supplement the data with satellite and Street View imagery, and apply a mix of algorithms and human judgement to produce a consistent and reliable geographic dataset which we can update quickly and corresponds as closely as possible to, well, the truth on the ground. In this case you\u2019ll find our maps now include more precise area boundaries, greater coverage of road networks, and more comprehensive local points of interest. To capture and quickly fix errors reported by our users, we\u2019ve also enabled the Report a Problem tool for these countries.\n What does this mean for you, our 800,000 developers? For most of you, these updates will simply roll out over the next 24 hours and your Maps API applications will automatically display the latest imagery; however, for those developers who have cached results from any of our Maps API Web Services, please refresh your cache as soon as possible to ensure your results include the latest information.\n As always, if you have any questions or concerns, please post them to our Google Maps API forums and we\u2019ll be happy to respond.","item_date":"Jul 19 2012 07:00:00","display_item_date":"07-19-2012","url":"http:\/\/googlegeodevelopers.blogspot.com\/2012\/07\/map-data-updates-for-parts-of-europe.html","source":"googlegeodevelopers.blogspot.com"},{"title":"Mobile phone data: the oil of the digital age","details":"Three quarters of the worlds more than six billion mobile phones are located in the developing world, and the ubiquity of these devices in under-served areas provides valuable digital traces of activity that have never existed before.\nIn particular, there is an unprecedented and largely overlooked opportunity to harness this digital data for global development efforts. From tracking the outbreak of diseases to better understanding unrepresented populations, a few promising examples are coming to light, and this mobile phone data is even proving to offer lifesavings insights.\nIn the aftermath of the Haiti earthquake two years ago, the largest mobile phone network in Haiti provided anonymized mobile phone data to help coordinate relief efforts and quantify displaced populations.\nUsing the data, researchers estimated that more than 630,000 people had fled the capital of Port-au-Prince within 20 days of the earthquake. When Cholera struck nine months later, the mobile phone data was again used to focus relief efforts in areas with high-risk of new outbreaks.\n                      \t\t\t\tPost-earthquake distribution of Port-au-Prince (PaP) population, from mobile phone data.  Illustration: Research article \t\t\t              At the end of last year, the United Nations launched a Global Snapshot of Wellbeing. Instead of sending experts around the world to conduct household surveys, the project sent out questions via text messages to more than 5,000 people in 30 countries.\nThe questions ranged from were you sick in the past 7 days? to if you had 15 dollars, what would you spend it on? In one month, the survey collected over 65,000 responses from around the world (see a video tracking the responses here).\n                      \t\t\t\tA snapshot from the UN video, which shows the speed and geographical distribution of responses to a mobile phone survey on wellbeing and interconnectedness Illustration: UN Global Pulse \t\t\t              Researchers are also utilizing mobile phone data to examine human behavior at an unprecedented rate and scale. Over the course of four years, anonymized data was collected from all 1.4 million mobile subscribers within an unidentified small country.\nWhen examining the data on call locations, recipients and purchasing trends, the researchers started to see a unique story unfolding:\nIn the city, social and economic interdependence becomes crucial, more utilitarian social relations develop, and great economic opportunities are available.\nThe researchers go on to conclude:\nWe have found support, and quantified for the first time on a large scale, for arguments for the diversification and growth of personal networks as individuals live or move to large urban areas.\n                      \t\t\t\tChange in mobile phone call frequency for an individual moving from a rural area to the capital Illustration: Report into rural and urban mobile phone usage \t\t\t              Mobile phone data is increasingly being seen as the oil of the digital age. According to estimates by the Mckinsey Global Institute, there is a $600 billion opportunity every year for businesses to use consumers personal location data.\nAs mobile phones become more advanced and widespread, the mining of this data will only become more meaningful. While companies have jumped on the opportunity and established refineries to reap the benefits of the data, global development actors are just beginning to break the surface.\nEric Tyler (@Erict19) is an analyst for the New America Foundation, where he conducts research and advocacy at the intersection of technology, globalization, emerging economies and economic development. You can find more of his work at his personal website.\nMore data\nMore data journalism and data visualisations from the Guardian \nWorld government data\n\u2022 Search the worlds government data with our gateway \nDevelopment and aid data\n\u2022 Search the worlds global development data with our gateway \nCan you do something with this data?\n\u2022 Flickr Please post your visualisations and mash-ups on our Flickr group \n\u2022 Contact us at data@guardian.co.uk \n\u2022 Get the A-Z of data \n\u2022 More at the Datastore directory \n\u2022 Follow us on Twitter \n\u2022 Like us on Facebook","item_date":"Jul 19 2012 07:00:00","display_item_date":"07-19-2012","url":"http:\/\/www.guardian.co.uk\/news\/datablog\/2012\/jul\/19\/mobile-phone-data-development-emerging-markets","source":"www.guardian.co.uk"},{"title":"How Augmented Reality is Shaping the Future of Retail","details":"Google\u2019s recent demo of Google Glass at its I\/O conference shows that a great deal of time and money is being thrown at augmented reality (AR), the use of a virtual layer on top of real-world information.\n However, while Google Glass may be the future of AR, many retailers are already using it today with existing devices. Whether it\u2019s tools that let you try on clothes virtually, apps that help you find your favorite restaurant or new ways for consumers to interact with a brand, AR is already making an impact on retail.\n Here are just a few ways that AR technology is changing retail and altering consumer mobile purchasing decisions.\n Augmented Reality to Localize Businesses\n Modern phones have powerful GPS receivers, compasses and accelerometers that make it possible to know exactly where its located and what they are looking at. AR apps such as Layar take advantage of this to show people information about their surroundings including what businesses are nearby.\n Retailers, by combining augmented reality with localized SEO, are using this technology to reach consumers that, previously, were beyond their grasp. For example, if a pedestrian in an unfamiliar area realizes they need a pair of shoes urgently, they can use an augmented reality app on their phone to not only point them to nearby shoe stores, but browse the stock of those stores before heading to them.\n This makes it easy for the customer to know exactly where they should go for the product they want and keeps businesses from dealing with customers who are a poor match, such as a person looking for running shoes that wanders into a dress shoe store.\n Augmented reality makes it easy for retailers to be matched up with relevant customers. Customers know how to localize businesses that are relevant to them, what businesses have to offer, and read up on other consumers\u2019 product reviews, all without looking further than their cell phones.\n Augmented Reality for Purchasing Accuracy\n However, the power of AR doesn\u2019t end when the customer walks into the shop. AR is also becoming more commonplace inside shops themselves. For example, Lego recently introduced a series of kiosks that, when a shopper would hold up a box, would display the completed model as if it were in their hands.\n Similarly, the United States Postal Service has a virtual box simulator that lets customers see if a package will fit into one of their flat rate boxes, the idea being to eliminate guesswork in choosing which box to use.\n However, even retailers that aren\u2019t building custom AR apps for their service are likely benefiting from the technology. There are countless barcode apps for nearly every mobile phone that lets customers scan codes and read more about the product, including what it does, how other customers review it and how prices for it compare.\n The result of this is that customers are becoming better informed about the products they buy and are more likely to be happy with the purchases they make. Though this does mean increased competition, especially among consumers using AR for price comparisons, customers that make a purchase are more likely to be satisfied.\n Augmented Reality to Enhance Shopping Experience Online\n As important as AR has been to the physical store experience, it has become much more important to the online shopping experience, where it has been used to close the gap between what an online store and a physical store can do.\n For example, De Beers has an AR tool that lets you virtually try on jewelry and SnapShop is an app for your phone that lets you similarly try out any furniture you want in your house.\n The reason for this push is simple, since customers can\u2019t hold or touch goods in an online store, virtual retailers want to give customers as close to that experience as they can get and AR offers a powerful way to do just that.\n While it may not be the same as actually trying on clothes or holding a product in your hands, AR can definitely be the next best thing.\n All in all, AR is going to play an increased role in both retailers and consumers lives. More and more buying decisions are going to be influenced by AR, including both promotional tools and AR information services.\n As such, retailers need to be thinking about their AR strategy today so they can be prepared for this future. Anyone who doesn\u2019t have a plan in place soon may find themselves being forced to play catch-up later and at a serious disadvantage to better-prepared competitors.\n This guest post is written by Lior Levin, a marketing consultant for a psd to html 5 company, and who also consults for a company that offers businesses and individuals with a task management tool.","item_date":"Jul 17 2012 07:00:00","display_item_date":"07-17-2012","url":"http:\/\/www.retailigence.com\/blog\/how-augmented-reality-is-shaping-the-future-of-retail\/","source":"www.retailigence.com"},{"title":"Launched pricing plans","details":"Today were launching the pricing plan for CloudyRec, one step closer to official launch. Basically, using CloudyRec during development and user testing period will be FREE. Only when you launch your app and open access to pubilc, youll switch to use our Pro Plan costing $9.90 per month per app. Well make sure that migration will also be painless and as streamlined as possible. For that $9.90 Pro Plan, youll get up to 2 million API requests per month plus 2 GB multimedia file storage. Therell be additional charges of 8\u00a2 per 10,000 API requests over and 1$ per 5GB over storage space. But not to worry, if you think these additional costs are too much for your great app, we can always sort out a custom plan for your app. Just drop a mail to sales@cloudyrec.com.\n  There are basically two primary factors that we considered behind these pricing. We want to let more and more developers to try CloudyRec essentially risk-free and at the same time to build a sustainable business for our company. Thats why we decided to start charging $9.90 per month from successful developers who have already tested their app with sandbox environment. We believe this is the best deal you can get among our competitors.\n  Right now, were still in beta and still entirely FREE. And not to forget well definitely have rewards for those who try and feedback during this period.\n  So be our early adopters, go ahead and sign up\u00a0now.","item_date":"Jul 17 2012 07:00:00","display_item_date":"07-17-2012","url":"http:\/\/blog.cloudyrec.com\/launched-pricing-plans","source":"blog.cloudyrec.com"},{"title":"The Android App Creation Toolbox","details":"When you first look at AppsGeyser, sometimes it\u2019s hard to see what you\u2019re looking at. It\u2019s not just a platform for developing Android apps. It\u2019s a whole toolbox that can be used to make great apps.\nWhen you first go to the AppsGeyser site, and click Create Now, it looks like you have three options. At the bottom of those three, there is blue text which says \u2018more options,\u2019 giving a total of five options. Let me explain what each one is good for!\nWebsite: This is the simplest option. If you have a website, you enter your URL (the web address that starts with http:\/\/) and POOF you have an app. The downside of this type of app is that the app will only work correctly if the user\u2019s device is connected to the internet.\nHTML Code: There are two ways to use this feature. Either you can use HTML or you can use the text editor. If you\u2019re creating content, you\u2019ll probably prefer the text editor, because it\u2019s just like using Word or another word processing tool. If you got widget code from another site, or if you\u2019re copying a part of your site from the HTML, you can choose the HTML option. This is a great option if you want your app to be available offline, and it doesn\u2019t contain a lot of data.\nVideo Stream \u2013 This one\u2019s easy. Just turn your YouTube Channel or Playlist into an app. Again, this kind of app will only work if the user\u2019s device is connected to the internet.\nZip Archive \u2013 This one is the most complicated option, but it\u2019s also the most powerful one. If you\u2019ve ever used DreamWeaver, FrontPage, or KompoZer, you understand that making a website isn\u2019t making a single webpage. Instead, you make a group of interlinked pages, including separate files for the images. Once you\u2019ve made all of that, you can include all of the relevant files in a single zip archive, and upload it to AppsGeyser. The reason this is such a powerful option is that this option allows you to give your users a full app which is available even if the mobile device is offline. If there are parts of the app which should update more often than every few weeks, such as a blog or a schedule, you can add them to a webpage which is online and link from within the app.\nDocument \u2013 This is a PDF or Document (from Word or similar). Like the Zip Archive option, this allows you to make your app available offline. If you\u2019re making an ebook or a how-to guide, this is a great way to make it. You can always link to a blog in your document to add information regularly.\nWe\u2019d love to hear your thoughts in the comments or on Facebook  or Twitter,\n If you\u2019re interested in making your own Android apps, start creating apps here. \nRelated Articles\nThe Red, White, and Blue of App-Making! \nBecome an App-Making Mentor \nInterview with \u201cAndrew Steroid\u201d About the AppsGeyser Partner Program \n Wed love to hear your thoughts in the comments or on Facebook  or Twitter, If youre interested in making your own Android apps, start creating apps here.","item_date":"Jul 16 2012 07:00:00","display_item_date":"07-16-2012","url":"http:\/\/www.appsgeyser.com\/blog\/2012\/07\/16\/the-android-app-creation-toolbox\/","source":"www.appsgeyser.com"},{"title":"Springpad on Google Play \u2013 They like us! They really like us!","details":"You may have noticed Springpad has been a bit busier than usual recently and, man oh man, you are right! We\u2019ve had a pretty exciting week thanks to the awesome people over at the Google Play app store\u2026 \n We\u2019re Featured on Google Play\n Since the end of last week, we\u2019ve been featured as a Google Play App Store staff pick for Android mobile and Android tablet! We can\u2019t thank the Google team enough for putting us in the spotlight. Since making the list, we\u2019ve seen a record number of new users, fun new notebooks and tons of awesome new springs. Hop on over to our Explore page and scroll on down to the Recent Springs to catch all the new saves and shares!\n We\u2019re part of a live hangout on Google Play\u2019s Google+ page! Don\u2019t forget to set a reminder for our live hangout with Google Play this Tuesday (7\/17) at 3pm EST. Get the full details here:\u00a0http:\/\/goo.gl\/VvyKN","item_date":"Jul 16 2012 07:00:00","display_item_date":"07-16-2012","url":"http:\/\/springpad.com\/blog\/2012\/07\/springpad-on-google-play-they-like-us-they-really-like-us\/","source":"springpad.com"},{"title":"Featured Notebook: Best Android Tablet Apps","details":"I\u2019m a pharmacist by profession, a geek by passion. I\u2019ve been mobile-obsessed since 2006, and currently share my short thoughts on Twitter as\u00a0@khouryrt, and my longer ones on FoneArena, Android.Appstorm, and Windows.Appstorm.\n Having owned an Acer Iconia A100 for 6 months, and struggled to find a repository of high quality tablet-optimized Android apps, I decided to create one where I would only pick the best, \u201cla cr\u00e8me de la cr\u00e8me\u201d. And what better place to do it than Springpad? I already use it and evangelize it as my movie collection catalogue, so creating a Tablet Apps notebook was a click away. Springpad made it dead easy to link to the Market, choose the appropriate screenshots, and add a description. I can even do it directly from my tablet! It also allowed me to pick my super-favorites with a small heart icon, to tag everything into categories, and to share it all with one link whenever someone asks me about my recommended tablet apps. My friends can easily browse the collection and click on an app icon for more details, and if they\u2019re Springpad users, they get to expand to the full notebook view for tags and more options.\n So if you\u2019re an Android tablet owner, or thinking about getting one like the new Nexus 7, head on over to my notebook and check it out. There are well-known fan favorites there like Pocket, Plume and Google Earth, niche apps like RateBeer, iTriage and SeriesGuide, as well as some of my personal favorites like JustReader, N7Player, mVideoPlayer HD and iFood. \u2013 Rita El Khoury\n Some of the highlights of this notebook include:","item_date":"Jul 16 2012 07:00:00","display_item_date":"07-16-2012","url":"http:\/\/springpad.com\/blog\/2012\/07\/featured-notebook-best-android-tablet-apps\/","source":"springpad.com"},{"title":"How FollowMe won the ATT Hackathon","details":"A guy walks into a hackathon with an idea. \u00a0We\u2019ve all heard this story before. \u00a0Startups like Zaarly, LaunchRock and Banjo were all born at hackathon type events and have gone on to launch successful startups. \u00a0The story never grows old, because it holds the promise that we all can bootstrap a dream and create something from nothing. \u00a0The ATT San Francisco Hackathon hosted by StackMob saw Team FollowMe wow the judges with a roadtripping app for iOS that connects friends, maps out trip routes with pit stops along the way. \u00a0Live chat keeps everyone in-sync. \u00a0\u00a0Search for and add restaurants and gas stations stops during your trip. \u00a0The data API was built on StackMob during the hackathon with our iOS SDK.\n Pitches take place Friday night and are a mix of high school speech class and speed dating. \u00a0Developers, designers and self described \u201cidea people\u201d all get one minute to pitch their idea for the weekend. \u00a0Great teams are born from pitches based on original ideas and the speaker\u2019s ability to clearly and concisely explain their vision. \u00a0The right balance of enthusiasm and confidence is crucial for the \u201cidea people\u201d. \u00a0I must admit I was skeptical when Taylor, a self described idea guy, pitched FollowMe. \u00a0I wondered could he attract the talent to bring his idea to life.\n  \n One of the founders of Banjo made an appearance prior to the pitches and implored the audience to execute. \u00a0He said, \u201cThose who sit around talking, won\u2019t have anything to show for it come Sunday\u201d. \u00a0I agree wholeheartedly with this advice. \u00a0Team Voicegram, our second place winners, created this amazing graphic that sums up who attends hackathons vs. who wins.\n  \n  Lo-skill, Lo-effort \u2013 You are a Bystander. \u00a0This is okay. But, you walk away with an empty hand.\n     Lo-skill, Hi-effort \u2013 You are a Dreamer. \u00a0You\u2019ll have to recruit developer talent (recruit = convince developers you\u2019ve got what it takes to lead a team)\n     Hi-skill, Lo-effort \u2013 You could rock, But without blood & sweat, you will not be in a winning team.\n     Hi-skill, Hi-effort \u2013 You are a Hacker! You are the people who make out like bandits with prizes.\n \n  Taylor who pitched FollowMe falls into the Dreamer category, and hustled to bring together a team of Hackers and lead them to victory.\n  \n Demos kicked off at 4pm on Sunday. \u00a0A total of eleven teams showed off their apps. \u00a0\u00a0The criteria for judging are creativity, technical difficulty and presentation. \u00a0FollowMe scored extremely high in all areas and took first place overall and the StackMob prize of 6 months premium package and 2 months of desk space at StackMob HQ (together valued at $20,000). \u00a0Their demo was smooth and showed three phones mapping and chatting on a roadtrip. \u00a0They shared a larger vision which included sharing music on the road trip.\n  We look forward to seeing FollowMe around the office and wish them continued success. \u00a0They are off to a great start.\n \t\t\t\t\t\t\t\t\t\t\t\n By signing up you are agreeing to the terms of use.\n \t\t\t\t\t  StackMob  helps developers build, deploy and scale feature-rich mobile applications easier and faster than ever before. \nLearn More...","item_date":"Jul 16 2012 07:00:00","display_item_date":"07-16-2012","url":"http:\/\/www.stackmob.com\/2012\/07\/how-followme-won-the-att-hackathon\/","source":"www.stackmob.com"},{"title":"Disqus for WordPress Plugin Gets Full Single Sign-On Support","details":"Today we\u2019re happy to announce we\u2019ve released a long-standing update to our WordPress plugin. If you haven\u2019t already, go ahead and download the Disqus WordPress plugin. And a reminder to our awesome developer community: feel free to fork the plugin and help make it even better.\n  So what\u2019s new?\n  Full Single Sign-On (SSO) Support\n  With today\u2019s update, and last month\u2019s silent introduction of a new add-on package \u2014 just SSO, for just $99\/month \u2014 the Disqus WordPress plugin has the most fully-featured, easy-to-integrate Single Sign-On solution available.\n  Our plugin now takes full advantage of both WordPress\u2019 and Disqus\u2019 login systems by allowing sites to set their own custom login button, making the login process incredibly seamless for both existing and new visitors.\n  \n  And for users who set a website link in their WordPress profile, we now show that website link in their Disqus profile too. Even if they\u2019ve logged-in via their WordPress account.\n  Screenshots\n  Similarly to how we added a live demo of Disqus 2012 to our home page, we\u2019ve added a Screenshots tab to the plugin description. Now everyone can know how Disqus will look and feel in their WordPress environment before even installing it.\n  Bug Fixes and Other Improvements\n  Both our team and our greatly helpful community of collaborating developers put together a smorgasbord of bug fixes and improvements. See the plugin change log for a full list. We\u2019ve already got a number of great updates lined up for the next plugin release too, so stay tuned for those.\n  If you\u2019re on WordPress, download the Disqus WordPress plugin today. And if you\u2019re interested in our new $99\/month Single Sign-On package, head over to our For Websites page and fill out the form at the bottom of the VIP Features tab and we\u2019ll be in touch!","item_date":"Jul 16 2012 07:00:00","display_item_date":"07-16-2012","url":"http:\/\/blog.disqus.com\/post\/27346025794","source":"blog.disqus.com"},{"title":"ADP online payroll meets Xero online accounting","details":"Today we are announcing an integration with Automated Data Processing Inc. (ADP). After being in the business for over 60 years, ADP is one of the world\u2019s largest providers of payroll and outsourcing solutions. With roughly 570,000 customers and $10 billion in revenues, they are a formidable player in the payroll space.\n Ever since we\u2019ve been on the ground in the USA we\u2019ve received innumerable requests for a payroll integration. We listened to our customers and have been working hard to make that happen.\n I caught up with ADP at the Sleeter Conference last year and had a brief conversation about the future of payroll and accounting. It was clear from our discussion that we were both focused on making the lives of small businesses simpler. We also discussed that cloud applications are at the nexus of this paradigm. More and more, we are seeing small businesses on the go and wanting the flexibility to manage their business away from the office. And, with today\u2019s announcement, now they can.\n ADP offers three packages for small businesses: Essential, Enhanced, and Complete. To take the payroll compliance headache away from small businesses, ADP calculates federal, state, and local taxes and files taxes using the latest regulations. Small businesses can pay their employees by check, direct deposit, or Visa pre-paid debit card. And submitting payroll data to ADP is a breeze online or through the ADP mobile application.\n The Xero \/ ADP integration allows small businesses to import ADP payroll data into Xero with a few simple clicks. Now small businesses can do their payroll and accounting anywhere, anytime. They can also access real-time payroll management reports and view payroll data at anytime.\n The Xero \/ ADP integration brings together the leaders in online accounting and online payroll. We are very excited about this integration and look forward to helping small businesses and accountants alike manage their finances and payroll \u2013 whenever and wherever they choose.\n You can read the full market release here:\u00a0Xero integrates with leading US Payroll Provider","item_date":"Jul 16 2012 07:00:00","display_item_date":"07-16-2012","url":"http:\/\/blog.xero.com\/2012\/07\/adp-online-payroll-meets-xero-online-accounting\/","source":"blog.xero.com"},{"title":"Taking the Long View on API Ecosystems","details":"Kin Lane has a nice post over at API Evangelist, reflecting on the some of the commentary of Twitter\u2019s ecosystem changes \u2013 don\u2019t forget the Pioneers as he says.\n API skeptics always step up and say they are different, that they have core web products, existing business models, etc. Sure, there are differences, but Salesforce, eBay and Amazon have all managed to achieve success with continued investment in their API ecosystem. \n Whatever Twitter\u2019s particular reasons for their change (and on balance it seems likely to be more harmful than beneficial), building and maintaining an ecosystem is extremely challenging. \n Many companies have been successful however \u2013 and success seems to come almost entirely in those cases where the company\u2019s business goals and it\u2019s partners business goals align with API \u2013 in other words, there needs to be a clear benefit to all parties involved. Examples include:\n  Amazon\u2019s APIs directly drive sales and earn partners affiliate revenue.\n \n  eBay\u2019s APIs add millions of high quality listings and for power sellers the automate the drudgery of maintaining their eBay presence.\n \nexpand to feature sets they would not have been able to cover themselves and create a high degree of stickiness for the service \u2013 for Salesforce user they provide unprecedented flexibility and a wide-range of choices for extensions.\n \n  Twilio and Stripe are \u201cAPI Only\u201d companies \u2013 with their API being their main access channel to and for their customers.\n \n  Skype\u2019s APIs aim to enable the Skype experience on as many devices and platforms as possible and partners benefit from adding the Skype experience.\n \nWe\u2019re certainly seeing many APIs now launching that clearly target the value they want to add and align this with the core business of their company \u2013 it\u2019s frequently no longer \u201cshould we have an API\u201d but \u201chow does an API best reflect our core mission \/ objectives as a company\u201d?\n As such, the well publicized issues around Twitter are arguably due to a larger\/deeper shift in strategy rather than just an API strategy shift \u2013 the API still serves as it\u2019s major distribution channel but there\u2019s seemingly a desire to increasingly control the final access to the consumer. Whether this will pay off remains to be seen. Twitter needs to be ready for the potential fall out from the move \u2013 potentially less willingness amongst third parties to facilitate access to their users, less innovation to reach niche audiences etc.\n [Image from US National Archives: Flickr]","item_date":"Jul 13 2012 07:00:00","display_item_date":"07-13-2012","url":"http:\/\/www.3scale.net\/2012\/07\/taking-the-long-view-on-api-ecosystems\/","source":"www.3scale.net"},{"title":"Release Notes: Springpad iOS App \u2013 v3.0.7","details":"In the latest update (v3.0.7) to the Springpad app for iPhone and iPad we\u2019ve simplified notebook navigation and added single sign-on integration with the Facebook app. Get the update now.\n Redesigned Navigation within a Notebook \n Now you can Filter by type and tag even faster with the new slide-out navigation in your notebooks (iPhone and iPod touch only). When you\u2019re in a notebook, tap the double arrows at the top right to reveal the navigation panel. Here you\u2019ll find collaboration settings, notebook settings, and\u00a0filter options to view by Liked, Type and Tag. This new navigation matches the left-hand navigation menu that was already available on the iPad app.\n You\u2019ll notice that the quick add button is moved to the bottom right and there\u2019s now more space to see your stuff.\n \n Logging in or registering via Facebook is now faster than ever if you have the Facebook app installed on your device. If you choose to log in via Facebook, Springpad will launch the Facebook app to use your existing credentials.\n \n Checklist Fix -\u00a0Hide\/Show completed state for checklists now syncs across all Springpad clients.\n Task Fix -\u00a0Marking tasks as complete will now show as\u00a0complete in the notebook list view immediately.\n Stability Improvements\u00a0- We fixed a crash when editing the message field of a collaboration invitation.\n New Permission Prompt\u00a0- We added a prompt to request access to\u00a0the contacts on your device to look for matches when suggesting people to follow or people to invite to collaborate on notebooks.\u00a0We do a one-time match with that contact data, and never store it on our servers. In previous versions of the app, we were not asking for your permission to access this data.\n Encrypted Transmissions \u2013 We fixed a bug that was introduced with version 3.0 that made transmission between the app and our servers be via HTTP\u00a0protocol. \u00a0All transmissions to and from the iOS app are now via HTTPS.","item_date":"Jul 13 2012 07:00:00","display_item_date":"07-13-2012","url":"http:\/\/springpad.com\/blog\/2012\/07\/release-notes-springpad-ios-app-v3-0-7\/","source":"springpad.com"},{"title":"Big, New API Feature: Taste Profile Similarity ","details":"","item_date":"Jul 12 2012 07:00:00","display_item_date":"07-12-2012","url":"http:\/\/blog.echonest.com\/post\/27047659744","source":"blog.echonest.com"},{"title":"The Echo Nest Announces $17 Million Financing and Welcomes Jeff Crowe","details":"","item_date":"Jul 12 2012 07:00:00","display_item_date":"07-12-2012","url":"http:\/\/blog.echonest.com\/post\/27047619181","source":"blog.echonest.com"},{"title":"Fortumo Mobile Payments Now Available in Saudi Arabia and Kuwait!","details":"Summer holidays are at their peak but this doesn\u2019t stop good news and hard work. In July we added two countries from MENA region - Saudi Arabia and Kuwait.\n Saudi Arabia is a leading mobile market. With an estimated population of 27 million, mobile subscribers have passed 56.1 million, making mobile penetration rate as high as 200%.\u00a0 Another surprising fact is that 60% of the Saudi users already own a smart phone. \u00a0At the same time there is still room for growth in the Internet connections \u2013 the number of Internet users is 12.5 million (representing 44% of the population). This statistics puts Saudi Arabia strongly on the map for mobile services.\n Besides the growing mobile market, e-commerce has seen a significant shift in the past couple of years in the Middle East. Around 39% of the adult internet users in Saudi Arabia buy products and pay for services online through e-commerce services.\u00a0 Also, in the past few years video gaming has exploded in the country. A survey found that gaming accounts for 41% of online purchases, more than any other single category of online shopping, while 87% of gamers are under 25 years old.\n Kuwait, country with population of about 3.5 million, also represents a strong digital market. Already in June 2009, the mobile penetration rate in Kuwait peaked 125% with the total number of mobile subscribers reaching ~3.56 million.\n One of the main reasons for the high penetration rates, both in Kuwait and elsewhere in the region, is thought to be the significant level of multiple SIM ownership. Yet the mobile penetration\u00a0in Kuwait is expected to grow even further.\n Saudi Arabia and Kuwait both show a great potential in the mobile sector. Don\u2019t miss out and create your mobile payment service there today!\n Related posts:","item_date":"Jul 12 2012 07:00:00","display_item_date":"07-12-2012","url":"http:\/\/blog.fortumo.com\/fortumo-mobile-payments-now-available-in-saudi-arabia-and-kuwait\/","source":"blog.fortumo.com"},{"title":"SMS Everywhere: Twilio Now Enables Messaging to over 150 Countries Worldwide","details":"Today we launched Twilio SMS capabilities globally so that developers can send SMS messages to any destination around the world. This was the most widely requested feature ever by the community and we are thrilled to enable this for US and Canadian Twilio numbers. Now developers can enable their Twilio numbers to send and receive SMS message to over 150 countries worldwide, using the exact same API. This means developers can reach anyone in the world with voice and SMS, as simple as using the web.\n Twilio SMS is also now multi-lingual with support for dozens of languages including Arabic, Chinese, Japanese, Greek, and Russian. Now with Unicode (UCS-2) support, developers can deliver the correct characters in their preferred language.\n We\u2019re committed to providing a platform that allows developers to reach their users without geopolitical limitations. We continue to add availablity in new countries and expand Twilio international capabilities.\n To enable global SMS, visit your Twilio account to edit the country permissions dashboard. Here you can turn on specific or all countries to which you\u2019d like to be able to send outbound SMS messages.\n Pricing varies depending on destination, make sure to check all pricing information on our coverage and pricing page. Recipients of SMS will be able to respond at the regular price of their carrier. If you can\u2019t find a certain carrier listed, we are constantly adding new carriers so stay tuned.\n At this time UK is not supported for global SMS but UK Twilio numbers are enabled for global voice and local SMS messaging. For more details visit the FAQ.\n If you\u2019re just getting to know Twilio SMS, start with the resources below. If you have questions or feedback reach out to help@twilio.com, we\u2019re always listening.\n How Twilio SMS Works\n How to Receive Incoming Text Messages\n How to Reply to Incoming Text Messages\n How to Send SMS Messages\n \n \t\t\t\t\t\t\tMeghan is the community manager at Twilio. Find her on Twitter @megmurph and email at murphy [at] twilio [dot] com.\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t\tView all posts by Meghan Murphy \u2192","item_date":"Jul 12 2012 07:00:00","display_item_date":"07-12-2012","url":"http:\/\/feedproxy.google.com\/~r\/twilio\/OBEN\/~3\/vc9lzNWqQNw\/sms-everywhere-twilio-now-enables-messaging-to-over-150-countries-worldwide.html","source":"feedproxy.google.com"},{"title":"Outage Details","details":"Tonight, our planned 15-20 minute service window turned into a full two hour outage. We regret the problems that this may have caused for some of you and wanted to provide a little more technical detail here.\n As we mentioned in our Architectural Overview post last year, the hundreds of terabytes of data within Evernote user accounts are spread over hundreds of \u201cshards,\u201d but we have one redundant pair of servers that run our master \u201cUserStore\u201d database. This only has a small amount of information for each registered Evernote account in order to handle authentication and commerce \u2026 usernames, passwords (salted+hashed), etc. This database has less than 1kB per account, but since Evernote has over 27 million accounts, the overall size is around 25GB.\n This same pair of servers has been running our UserStore database for more than three years, and it\u2019s time to upgrade them. So we set aside two of our new \u201cshard\u201d\u00a0servers with 3x as much RAM, SSDs instead of 15krpm disks, bonded networking, updated kernel, etc. The hardware and OS configuration on the new systems is virtually identical to the last 38 shards we\u2019ve deployed, and our dry-run tests with the new hardware showed good performance with copies of the UserStore database.\n So we planned a scheduled service window tonight to bring the whole service down, copy the current UserStore database to the new hardware, bring it online, and bring the servers back up. In our dry runs, the copy took 8 minutes and the sanity tests and server juggling took another 10-15 minutes, so we aimed for a 20 minute outage.\n When we stopped the service tonight and copied the database, we were able to transfer the data as planned, but then experienced an unexpected \u201ckernel panic\u201d from the new primary server before we could bring it live. The error appeared to be triggered from the TCP stack on the system on the crossover DRBD connection between the pair of servers. We haven\u2019t seen this on any of the (nearly identical) NoteStore shard boxes, and didn\u2019t have a good explanation.\n After a quick review, we decided it wasn\u2019t safe to continue with the upgrade on this new hardware until we could determine the source of this problem. So we needed to fall back on the old original pair of boxes.\n Unfortunately, the old boxes needed a reboot to clear a few lingering issues and also needed to allocate more storage space to the database in order to be safely used without interruption for a few more weeks. We\u2019d deferred this maintenance on the assumption that the upgrade would be successful, but we needed to resolve these issues after the upgrade failed.\n After the reboots and file system adjustments were complete, we brought the old UserStore servers back online. We then brought up the rest of the servers gradually, in order to spread the load on the old hardware (with no DB pages in RAM cache\u2026).\n We apologize again for the inconvenience this may have caused you during the outage, and we\u2019ll be reviewing why our dry-run testing procedures didn\u2019t catch this issue and ensuring that our next upgrade goes smoothly with a less disruptive fallback plan.","item_date":"Jul 11 2012 07:00:00","display_item_date":"07-11-2012","url":"http:\/\/blog.evernote.com\/tech\/2012\/07\/12\/outage-details\/","source":"blog.evernote.com"},{"title":"Devs Unite! Support the Salesforce Stack Exchange Proposal","details":"Stack Overflow is a developer-centric Q&A board that has great features such as syntax highlighting, advanced tagging, user profiles and community voting. Their board format is extremely easy to use and navigate, and the Salesforce technology community has been growing over there.\n There has been a proposal to create a Salesforce-specific Q&A board, called a Stack Exchange that covers the gamut of Force.com platform development and issues all the way up to declarative development; beginner to through expert level questions are all welcome. This new Stack Exchange is entirely community supported, and will be a welcome complement to our current discussion forum\u00a0and other Developer Force technical resources.\n Rock star Matt Lacey has been spearheading the effort and this is what he had to say about this exciting development:\n \u201cIt\u2019s great to see Salesforce supporting a Stack Exchange Q&A Site, it will become in an invaluable community resource with all types of users, developers and administrators in one place. Once the site is live I hope everybody will realise why I\u2019ve been blathering on about it so consistently for the last six months or so, and discover for themselves the power of the Stack Exchange model. \u201c\n If you\u2019re interested in finding out more about this proposal and\/or supporting it, just go to the proposal page, find the \u201cCommit to this proposal!\u201d link at the bottom and join the community. Sign on and be sure to give your feedback to shape the community over there.\n \n  \t\t\t\t\tBookmark the permalink. Trackbacks are closed, but you can post a comment.\nTweet \n  \n                                              One of the resources I used when I first started was Stack Overflow. It was very google friendly with my searches. I would love a Salesforce specific area!\n Supported!\n \n                                              I was one of the first 20 or so people to commit to this so you don\u2019t have to sell me on this but right now the proposal is log-jammed. Until we get at least 100 users from SO with 200 rep or more the proposal won\u2019t go into the beta stage.\n \n                                              I\u2019ll be the contrarian: I\u2019ve always tried to drive questions to the official discussion boards so that there is one core forum. Having questions come in to Developer Force, Stack Exchange, Answers and, god forbid, Linked In, splits the audience of capable responders and diminishes the likelihood of a solution being offered while the issue is still fresh. Salesforce has allowed the capabilities of the official board to fall behind, search and filtering and sorting are primitive, and as a result, people are migrating to other sources.\n \n                                              To echo Greg\u2019s point, we currently need 64 more users with a reputation of 200+ on any other StackExchange site to get this into beta. The StackExchange (the framework StackOverflow lives in) is a wonderfully designed, highly effective, reputation-based forum incubator and hosting service. Unlike most forums\/boards, it incorporates gamification techniques to increase engagement and quality of content, and it\u2019s highly effective at this. For example, try posting an Apex programming question on StackOverflow and see how long it takes to get a good answer. You might be surprised at the response time and quality of answers. \n I agree with Matt Lacey that the very nature of Force.com makes it a unique fit in that StackOverflow is primarily for developers, not administrators, and therefore isn\u2019t always the right place for certain questions and\/or the answers may skew away from clicks-not-code when other options are available. Now that we have salesforce.com behind it, let\u2019s make it happen!!\n \n I think the difference between this and the dev boards though is that when you search for a problem and it\u2019s been asked before, you\u2019ll find only one question asking it \u2014\u00a0and that one question will have a community accepted answer. I think there\u2019ll be considerably less noise than on the developer boards as the community moderation model really does work wonders!\n \n                                              Hey Greg, thanks for supporting the proposal \u2014 Stack Exchange have indicated that they may well fast track this proposal because of the fact that there is such a large community that could benefit from it being live\u2026 hopefully we\u2019ll start seeing some movement soon, but even without we\u2019re up to 36% on that front now and it\u2019s steadily climbing all the time.\n \n                                              I initiated the same questions Luke over on the Google+ post that Matt posted https:\/\/plus.google.com\/u\/0\/111827922894049648255\/posts I would love to get a response from Dana Lee on this as well.","item_date":"Jul 11 2012 07:00:00","display_item_date":"07-11-2012","url":"http:\/\/feedproxy.google.com\/~r\/SforceBlog\/~3\/0SR-9PXbxz0\/devs-unite-support-the-salesforce-stack-exchange-proposal.html","source":"feedproxy.google.com"},{"title":"API Rate Limit Kung-Fu: The Server Side","details":"Adam Green has a great post on programmableweb this morning about Twitter API Rate Limits \u2013 with great tips on how to get the most out of the Twitter API without hitting rate limits. \n Sharing API calls between your server side and the client\u2019s browser is actually critical to making some Twitter integrations work and there\u2019s a great tip on how to use server side caching: \n  You can call the API from the browser, display the results, and then call your server with the data you got from the API. In effect you are using all of the user browsers as a large collection grid. This approach can be used to reduce the amount of API calls you have to do from the server.\n This type of technique highlights how important it is for API Providers to map out the use cases they aim to support with their APIs \u2013 since rate limits can typically completely handicap some usage scenarios \u2013 in particular for APIS which have server-side and end-user scenarios. Here are some of the things we recommend 3scale API Providers think about when planning rate limits: \n  Which audiences does your API Target? Customers? Partners? End Users?\n \n  What are the typical integration cases for each of those groups \u2013 bulk downloads? regular scheduled queries? user driven calls caused by browser activity?\n \n  Which groups of API calls go together and get called in sequence \u2013 e.g. is it possible to do something useful with 1-2 calls or do you need 5 or 6? Often this might result in an API redesign \u2013 but some APIs are inherently call hungry (for example in mapping visualisations you\u2019ll often need to grab offscreen tiles as well as onscreen to create smooth scrolling).\n \n  How are the API call dynamics affected by end user actions? In the case of pure-server side interactions there is often no variance based on usage of the applications calling the API since the server caches responses \u2013 however across the spectrum for mobile apps, the usage volume of a mobile application which goes viral is almost entirely out of the control of the developer that wrote the application.\n \n  Do you allow caching? Often API Providers instinctively disallow response caching on their API due to fear of data misuse. However often caching significantly reduces the volume of API calls which need to be made and hence server loads. Giving developers the right parameters to build efficient applications is good practice where other constraints allow.\n \nAdam Green\u2019s point on caching browser as well as per-user and per-IP limiting highlights that often as an API providers you might need to rate limit multiple dimensions of API usage. \n As a developer, when doing something like browser data collection, you should always ensure that you\u2019re doing this in way which doesn\u2019t violate either user-privacy (by sharing authenticated data across users or overburdening the browser) but it\u2019s a powerful tool and one that benefits the API Provider as well if done right. \n 3scale\u2019s infrastructure supports all of the above \u2013 most providers use application rate limiting to manage access per application key or credentials, others (most often for mobile APIs) add user-rate limiting which allows restrictions on the number of calls per identified user (much as Twitter does) and others also use IP and\/or Referrer domain filtering. Often a combination is the best way to facilitate access without having runaway data usage by a few rogue applications or users.","item_date":"Jul 11 2012 07:00:00","display_item_date":"07-11-2012","url":"http:\/\/www.3scale.net\/2012\/07\/api-rate-limit-kung-fu-the-server-side\/","source":"www.3scale.net"},{"title":"Decibel Chooses 3scale SaaS API Management Solution to Distribute Semantic Music Metadata APIs to Broadcasting, Media and Digital Companies","details":"SAN FRANCISCO, CA\u2013July 9, 2012\u2013 Decibel, the \u2018semantic music metadata company and reportedly the fastest growing European company in the digital music sector, selected 3scale (www.3scale.net), the creator of Out-of-the-Box API management infrastructure, to launch, manage and productize Decibel APIs.\n Decibel\u2019s mission is to power the next generation of digital music services to enhance the consumer relationship with digital music and to become the industry standard for music metadata. In that regard, the Decibel APIs powered by 3scale carry the most in-depth music metadata data you\u2019re likely to find anywhere with more than 3 billions pieces of data on 15 million tracks from 1.1 million albums by 300,000 artists.\n These data-driven assets power the creation of Connected TV, mobile, tablet and web applications; and increase revenues, collaboration and communication through enhanced discovery and business intelligence for consumer electronics manufacturers, digital retailers, record labels and broadcasters.\n 3scale helps companies like Decibel to remove obstacles to adoption and to unleash the power of their API. \u201cBecause Decibel\u2019s model is predicated on revenues as well as market penetration, we needed a provider that could integrate payment services simply, yet have a very simple to use free model,\u201d said Gregory Kris, Decibel\u2019s CEO.  \u201cDecibel chose 3scale because from a technical perspective, it\u2019s faster, leaner and cheaper than the other players in the field. After extensive evaluations of various players 3scale came out the winner.\u201d \n 3scale powers 100+ APIs and has over 60,000 developers writing applications using these APIs. 3scale customers (http:\/\/www.3scale.net\/our-customers) are taking advantage of 3scale API Management platform and infrastructure that includes the following:\n \n\u201cWorld-class innovators like Decibel choose 3scale to open, launch and manage their API because we offer years of experience in productizing our customers\u2019 API capabilities\u201d said Guillaume Balas, 3scale CMO. \u201cWith our years of API management experience 3scale brings the expertise and solution to help companies distribute their APIs, develop new business models and create additional revenue streams.\u201d \n 3scale works closely with customer tech- and business teams to provide optimal solutions for their needs by maximizing the impact and reach of their API among developers and business partners.  For more information about 3scale, please visit sales@3scale.net.\n About Decibel:\n Decibel, a company based in the London digital hub of Shoreditch, supplies business-critical metadata to companies across the media and technology industries. This data can be found in digital retail stores, mobile applications, Connected TVs, streaming sites, recommendation engines and powering record labels and distributors across the digital music ecosystem.\n For more information, please email team@decibel.net \n 3scale is one of the world\u2019s leading cloud-based API infrastructure providers and enables developers, and enterprises to securely open, control, manage and monetize their APIs. 3scale unique suite of infrastructure services brings API providers unprecedented control, visibility on API activity and peace of mind.\n Communications Contact:\n Guillaume Balas","item_date":"Jul 11 2012 07:00:00","display_item_date":"07-11-2012","url":"http:\/\/www.3scale.net\/2012\/07\/decibel-chooses-3scale-saas-api-management-solution-distribute-semantic-music-metadata-apis-broadcasting-media-digital-companies\/","source":"www.3scale.net"},{"title":"Opening Up the Embedly Docs","details":"Last month we announced our newly revamped documentation, with full-text search in the sidebar and thousands of tweaks. Today were putting the docs in your hands.\n  Linus Torvalds famously said that Given enough eyeballs, all bugs are shallow. Thats just as true for documentation as it is for code. Thats why weve made the raw reStructuredText markup behind our docs publicly available at https:\/\/github.com\/embedly\/embedly-docs\/.\n  Want write a new tutorial? Got a library we should be featuring? Found a mistake? Just fork the repo and send us a pull request. Once we accept it, itll go up on https:\/\/embed.ly\/docs\/ within minutes.\n  We build the docs on our server using\u00a0Sphinx, the Python documentation generator, plus\u00a0Pygments\u00a0for syntax highlighting. Consult the\u00a0reStructuredText\u00a0guide for help working with .rst files, then use Sphinx to compile the markup to HTML locally. You can find directions in the embedly-docs README.\n  So dive in! We cant wait to see what the Embedly community has for us.","item_date":"Jul 11 2012 07:00:00","display_item_date":"07-11-2012","url":"http:\/\/blog.embed.ly\/opening-up-the-embedly-docs","source":"blog.embed.ly"},{"title":"GetGlue at Comic-Con 2012!","details":"The wait is over! \u00a0Your favorite time of year is finally here! \u00a0 Comic-Con 2012 is this weekend and GetGlue has once again teamed up with the top networks and studios to bring YOU, the ultimate fans, the most exclusive stickers yet!! \u00a0Comic-Con is THE place to see your favorite actors, writers and directors from both returning and new TV show this fall season and the upcoming blockbuster movies.\n \n -\u00a0ABC rewards fans for attending \u2018The Neighbors\u2019\u00a0booth with an exclusive sticker, and fans of \u2018Once Upon a Time\u2019\u00a0and \u2018666 Park Avenue\u2019\u00a0panels with stickers for each.\n -\u00a0AMC is rewarding fans three exclusive stickers for their hit series \u2018The Walking Dead.\u2019 \u00a0Users can unlock them by checking into the show through the GetGlue website or apps during Comic-Con.\n -\u00a0BBC America will reward fans with an exclusive \u2018Doctor Who\u2019 sticker for those who check-in during Comic-Con\n -\u00a0FOX is rewarding fans with exclusive stickers for their shows appearing at Comic-Con including \u2018American Dad,\u2019 \u2018Bob\u2019s Burgers,\u2019 \u2018Bones,\u2019 \u2018The Cleveland Show,\u2019 \u2018Family Guy,\u2019 \u2018Fringe,\u2019 \u2018Glee,\u2019 \u2018The Simpsons,\u2019 and\u2018The Following.\u2019\n -\u00a0FX to reward users with stickers for their hit shows \u2018Wilfred,\u2019 \u2018Archer\u2019\u00a0and\u00a0\u2018Sons of Anarchy\u2019 to those who check-in during Comic-Con. Fans can also unlock exclusive stickers on site through the QR codes on special larger GetGlue stickers.\n -\u00a0G4 will reward fans for tuning in to their live coverage at Comic-Con. Fans can enjoy \u2018X-Play,\u2019 and \u2018Attack of the Show.\u2019\n -\u00a0HBO will reward fans with an exclusive sticker for\u00a0\u2019True Blood\u2019 and \u2018Game of Thrones\u2019 to\u00a0those who check-in to during Comic-Con\n -\u00a0Lionsgate will reward fans for checking-in to\u00a0The Expendables 2.\u00a0If you\u2019re at SDCC, you can redeem a collectible cling from the Lionsgate booth. Supplies are limited.\n - Marvel rewards fan for checking-in during Comic-Con with 5 different stickers.\n -\u00a0NBC rewards fans for attending the\u00a0\u2019Grimm\u2019\u00a0panel as well as the \u2018Revolution\u2019\u00a0panel with exclusive stickers.\n -\u00a0Showtime  will reward fans with exclusive stickers for their hit shows including \u2018Dexter,\u2019 \u2018\u00a0Episodes,\u2019 \u2018Shameless,\u2019 \u2018\u00a0Homeland,\u2019 and \u2018House of Lies\u2019 when they check-in during Comic-Con.\n -\u00a0SPIKE will award fans for watching their \u2018Comic-Con All Access Live.\u2019\n -\u00a0Warner Bros. Television is offering up an array of sticker rewards for visiting the following shows at Comic-Con 2012:\u00a0\u2019MAD,\u2019 \u2018The Big Bang Theory,\u2019 \u2018Person of Interest,\u2019 \u2018Nikita,\u2019 \u2018CULT,\u2019 \u2018ARROW,\u2019 \u2018The Vampire Diaries,\u2019 \u2018Supernatural,\u2019 \u2018Beauty and the Beast,\u2019 \u2018Children\u2019s Hospital,\u2019 \u2018The DC Nation.\u2019 They will also be rewarding fans for stopping by \u2018The EXTRA Stage\u2019\u00a0with a few surprises along the way!\n To stay in the loop about upcoming stickers and giveaways follow @GetGlue on Twitter or click to\u00a0like us on Facebook.\n  \t\t\t\t\tTagged as: \t\t\t\t\t\tABC,  \t\t\t\t\t\tcomic-con,  \t\t\t\t\t\tNBC,  \t\t\t\t\t\tWarner Bros. Television","item_date":"Jul 11 2012 07:00:00","display_item_date":"07-11-2012","url":"http:\/\/blog.getglue.com\/?p=11133","source":"blog.getglue.com"},{"title":"New Android Beta Build v2.12.0.4","details":"We\u2019ve addressed all of the issues reported in the previous beta build and believe this is a release candidate. Take it for a test drive and let us know how it goes.\n Changes include: :\n -Set fitness goals on the app and track your progress\n -Ability to view progress against goals you\u2019ve created on Runkeeper.com\n -Enter your weight within the app\n -Numerous bug fixes\n If you do find a bug  please provide us as much information as you can  (via the comments section below) about the steps you take to reproduce it. If you are a developer, feel free to use developer tools to generate bug reports and send them over to us at support@runkeeper.com\n Now for the fun part! To install the beta:\n 1) Go to application settings and allow installation from unknown sources\n 2) Click the button below from Android browser to get the new APK.\n \n Don\u2019t be shy! Leave your feedback in the comments section below.\n \t\t\t\t\t\t\t\t\t\t\t\n      \t\t\t\t\t  \t\t\t\t\t\tThis entry was posted in Android Running App, Beta. Bookmark the permalink.","item_date":"Jul 11 2012 07:00:00","display_item_date":"07-11-2012","url":"http:\/\/blog.runkeeper.com\/beta\/android-beta-2-12-0-4","source":"blog.runkeeper.com"},{"title":"Wolfram|Alpha Partners with Samsung to Bring High-Quality Knowledge to Smartphones","details":"We are happy to announce that we have partnered with Samsung to bring Wolfram|Alpha\u2019s computational knowledge to Samsung smartphones. The new GALAXY S III and GALAXY Note will now include the Wolfram|Alpha knowledge base with S Voice and S Note applications.\n Integration in Samsung\u2019s S Voice is yet another application of Wolfram|Alpha technology in a voice command app. The S Voice is available exclusively to users of the new GALAXY S III, which was recently released in the US. By simply tapping the home button and speaking, users will be able to get answers to factual questions by drawing on the expert knowledge of Wolfram|Alpha. Users can ask questions such as \u201cHow high is Mount Everest?\u201c, \u201cWho is Barack Obama?\u201c, or \u201cWhat is the weather like today?\u201c, and Wolfram|Alpha will instantly give the correct answer. \n Samsung GALAXY Note users can also gain access to Wolfram|Alpha using the S Pen optimized S Note application available through the Premium Suite software upgrade. With the GALAXY Note\u2019s S Pen, users can get answers to an equation or perform a knowledge search simply by writing it out. Users can even write formulas, such as y = 2x + 3, and the S Note app will use Wolfram|Alpha to plot and solve the equation. Users can also write words, and S Note will query Wolfram|Alpha to find the answer. For example, if a user writes \u201cWho is the prime minister of India?\u201c, Wolfram|Alpha will answer \u201cManmohan Singh\u201d.","item_date":"Jul 10 2012 07:00:00","display_item_date":"07-10-2012","url":"http:\/\/blog.wolframalpha.com\/2012\/07\/10\/wolframalpha-partners-with-samsung-to-bring-high-quality-knowledge-to-smartphones\/","source":"blog.wolframalpha.com"},{"title":"Making a Better Map: four months of @OpenStreetMap with @MapBox & @foursquare","details":"Earlier this year, we embraced the OpenStreetMap\u00a0movement, changing all the maps on foursquare.com to gorgeous maps powered by MapBox. Today, we\u2019re featuring a guest post from MapBox data lead Alex Barth\u00a0on how MapBox works with the foursquare community and OpenStreetMap to continually make better maps:\n foursquare\u2019s switch to OpenStreetMap-based MapBox Streets on the web a few months ago was a huge incentive for the foursquare community to start contributing to OpenStreetMap. People started to let us know about data problems they saw and actively started mapping themselves. Immediately we saw a big bump in contributor growth, and while the foursquare-motivated user influx is just a small part of an already steep curve (in 2011 the OpenStreetMap community grew by 150%), the foursquare community adds a unique perspective. In short, if people check in at a place that isn\u2019t on the map they want to add it; the trick now is to make adding data to the map easier. In the end, this is about making a better map guided by user feedback and data analysis \u2013 here\u2019s how we\u2019re doing this.\n Analysis: check-ins as priority driver\n Even without foursquare users actively reporting to us, we are leveraging user activities for quality control. Anonymized check-in data lets us run global analysis comparing check-ins with OpenStreetMap locations, and more fine-grained analysis placing the number of check-ins on street level maps. In fact we designed an entire site to help us find where OSM was weak compared to where foursquare users check-in: mapbox.com\/foursquare-checkins.\n Analysis first starts on a very macro level. Here you can see OpenStreetMap data in pink and foursquare check-ins in green on the Indian subcontinent. The overall picture shows solid OpenStreetMap coverage (try the map\u2019s toggle to get a sense of overall coverage).\n \n As we drill into an area, we work with the foursquare API to help quickly focus on which areas on the map are missing data. Here you can see a street level view of Ipanema beach showing foursquare check-in density on MapBox Streets.\n \n All over the globe, users check in on the street level, where data issues are quick to detect. With over 50% of users outside the United States, we are working around the world to identify data gaps on the map that most affect foursquare users while at the same time helping make it as easy as possible for foursquare users to become part of the larger OSM community. In the past four months we have received 138 map-related support requests through foursquare\u2019s channels. As you can see below, in spite of the huge US user base, the majority of issues are reported from threshold countries. This has everything to do with the traditionally strong footing of OpenStreetMap in the United States and Europe. The result is that,in broad terms, foursquare\u2019s switch to OpenStreetMap draws our attention to coverage in countries like Indonesia, Brazil, India and Mexico.\n \n Guided by user feedback and data analysis, we have attempted to respond quickly by working on tracing satellite imagery and fixing street names and label tags. These are some of the cities we have thus improved: Mexico City, Bangkok, Brasilia, Caracas, Fortaleza, Istanbul, Kuwait City, Sao Paulo and Ulaan Bataar \u2013 you can find a full list here. Based on your feedback, we have also looked at a whole series of college campuses.\n It\u2019s key for us to connect with local communities. As our knowledge of remote places is obviously limited, working with locals is essential. Ultimately it is up to local OpenStreetMap contributors to create a rich map of their surroundings, and to add and maintain points of interest, street names, and so forth. Specifically, satellite tracing has helped us to build fruitful relationships, where we can provide large swaths of data to motivate local contributions.\n A great example is Campo Grande, Brazil. Departing from a report by local foursquare user Muzito, we started out improving OpenStreetMap coverage by satellite tracing. A little later we noticed that without our direct interaction, Muzito had figured out how to contribute to OpenStreetMap himself.\n \n Tracing progress after a week. Note how in the meantime Muzito has become active on OpenStreetMap.\n \n Example of street name data, contributed after tracing efforts.\n Another good example is Guadalajara, Mexico, where foursquare has received various reports about insufficient street data. Guadalajara was in a pretty good shape on OpenStreetMap already, but there were pockets in the suburbs that we could quickly fill. Soon thereafter, we saw local user groups quickly following up with added information from the ground.\n \n Progress in Guadalajara between March 1st and March 15th of 2012.\n \n Example of street name data, contributed after tracing efforts.\n Often a support request is not reporting missing data, but a glitch to be repaired. We fix those right away \u2013 for instance this patch of the Tennessee River by Decatur, Alabama.\n Fixed: Tennessee River, Alabama\n Seeing their neighborhoods on foursquare motivates people to add rich level of detail, like this area of Towson, Maryland that foursquare Superuser 3 Talllguy and his friend Phil have worked on.\n Made my first @MapBox map of mine and @PhilR8 \u2018s #OSM contribs in Towson: cl.ly\/0w1l0L3L3o3a2x\u2026 Used @ChadLawlis guide cl.ly\/0M3p0K1z2u1j2n\u2026\n \u2014 Elliott R Plack (@TalllGuy) July 3, 2012\n These examples illustrate how OpenStreetMap grows: in iterations, by individuals contributing significant amounts of data, building off of each other. Rather than a single player going in and creating full coverage, data is added in layers by many. This all improves and expands OpenStreetMap, and its aim to create and maintain the fertile ground for others to build upon.\n If you see areas on the map that could use improvement, please report them through foursquare\u2019s support channel. And, if this post has made you curious about OpenStreetMap, learn about contributing at Learn OSM, or check out our open issue tracker to keep up with developments. Have questions or suggestions? We\u2019d love to hear them \u2013 just tweet us at @MapBox.","item_date":"Jul 10 2012 07:00:00","display_item_date":"07-10-2012","url":"http:\/\/feedproxy.google.com\/~r\/thefoursquareblog\/~3\/25-LtNqayxc\/","source":"feedproxy.google.com"},{"title":"Zendesk Private Messaging on Facebook","details":"Today we\u2019re introducing a great feature addition to our Zendesk for Facebook integration: Facebook Private Messages. Back in December, we launched our original Facebook integration, making it possible to turn a wall post conversation into Zendesk tickets. And from within Zendesk, agents can respond to tickets as Facebook wall posts without having to log into Facebook.\n There are over 900 million people on Facebook creating 3.2 billion interactions per day. Customers are on Facebook and other social channels and businesses need to have a presence on those channels to engage with them. But customers (and companies) don\u2019t always want these conversations to be held publicly. The issue might necessitate the inclusion of personal or sensitive information, or they might not feel comfortable discussing their purchases in a public forum. Now you can take those conversations out of the public eye with Facebook Private Messages.\n With Facebook Private Messages, your customer\u2019s private page message is translated into a Zendesk ticket. The support agent\u2019s response to the Zendesk ticket is relayed back to the customer as a Facebook message. With Facebook Private Messages, you can efficiently engage with customers on the social media platform while helping to ensure the privacy and satisfaction of your customers.\n Here\u2019s what our customer, Gilt Groupe, had to say about the new feature:\n \u201cWith half a million Facebook friends, we want to provide an open and engaging platform for them to talk to Gilt Groupe. But not all conversations should be in the public eye. The ability to turn Facebook Private Messages into Zendesk tickets allows us to seamlessly take our conversations off of a public Wall and still deliver support through the channel our customers prefer. It also makes it easy to monitor both current conversations and have saved references to old interactions in our own environment off Facebook.\u201d\n Give Facebook Private Messages a try on our Facebook page. Message us today about what you love about Zendesk and we\u2019ll surprise you with a sweet Zendesk giveaway! (Giveaway promotion will end at July 11, 11:59pm PT.)\n \t\t\t\n \t\t\t\t\t \t\t\t\tTo comply with the EU cookie law, we have recently revised our cookies policy. Please review it to learn which cookies we use, how and why we use them, and how you can disable cookies by changing your browser settings. Please be aware that portions of this site will not function properly if you disable cookies.  Unless you elect to disable cookies through your browser, by your continued use of this site you agree to our use of cookies as described in our cookies policy.","item_date":"Jul 10 2012 07:00:00","display_item_date":"07-10-2012","url":"http:\/\/www.zendesk.com\/blog\/zendesk-private-messaging-on-facebook","source":"www.zendesk.com"},{"title":"Push Notifications for Mobile Apps with Urban Airship","details":"Would you like 540% more daily app opens, 30% more social sharing, and 20% more mobile transactions? According to AdWeek, push notifications for your mobile apps can increase engagement and adoption dramatically. This post will show you how to send push notifications to iOS apps using Force.com and Urban Airship.\n Push notifications on mobile and other platforms allow your cloud-based applications to send brief alerts and updates to a client application. Apple and Android each offer their own services for this; this will focus on the Apple system for iOS applications, but the Android flow and implementation are very similar and detailed in this blog post.\n We\u2019re going to build part of an iPad app that allows Service Cloud Users to view Cases, attach audio memos, and receive a notification when they close. Here\u2019s what the notification will look like:\n \n To follow the rest of this hands-on tutorial, click through to the full article,\u00a0Push Notifications for Salesforce Mobile Apps with Urban Airship.\n   \t\t\t\t\t\t\t\t\t\t\t\n  \t\t\t\t\t tagged ios, ipad, mobile, push notification, urban airshipBookmark the permalink. Trackbacks are closed, but you can post a comment.\nTweet","item_date":"Jul 10 2012 07:00:00","display_item_date":"07-10-2012","url":"http:\/\/blogs.developerforce.com\/developer-relations\/2012\/07\/push-notifications-for-mobile-apps-with-urban-airship.html","source":"blogs.developerforce.com"},{"title":"Now Live: The Integration Portal","details":"First things first, if you\u2019re a developer, I highly suggest you check out this dev blog post on http:\/\/dev.dwolla.com\/. This post is meant to be more of an introduction of sorts for all of our non-technical business development, product and design friends looking to learn if a Dwolla integration is right for their company, app or project.\n One of the most beautiful and powerful things about Dwolla is its web-driven and open structure, or its ability to play nicely with a host of other Internet technologies.\u00a0Bundled into a series of accessible end points, Dwolla\u2019s network and technologies are available to all in what\u2019s called an API, or Application Programming Interface (great explanatory video of open vs. closed for us laymen). This API allows the network to connect to social platforms, plug-in to existing software and embed itself into new applications. It\u2019s what makes Dwolla compatible with the 21st century and is what\u2019s fueling a lot of our growth.\n Today, our API is getting a new home to lay its head and rest its feet. Introducing, the Integration Portal. Best thing? It\u2019s designed with two kinds of users in mind, the non-technical and the technical.\n \n Why do this? One of the biggest misunderstandings about API portals is that their purpose is only for developers. That\u2019s just not true. Integrations start with a spark of curiosity, \u201cWould this work for my company?\u201d Technical or not, who asks that question is irrelevant at that point; how it\u2019s answered is what determines the outcome.\n It all starts by inferring one simple question: Do you code?\n \u201cI can <code>\u201d takes you to our new and improved developer playground. Clicking \u201cI don\u2019t code\u201d\u00a0 won\u2019t teach you how to code (do checkout treehouse or Code Academy) or show you exactly how to integrate Dwolla into your individual project, but it will be able to provide an easy-to-understand rundown of Dwolla and what an integration could mean for your company.\n We have a lot of cool ideas and surprises planned for this project, but we\u2019d love to hear your feedback in the comments section. What are we missing? How can we make this even better?","item_date":"Jul 10 2012 07:00:00","display_item_date":"07-10-2012","url":"http:\/\/blog.dwolla.com\/now-live-the-integration-portal-a-decision-tree-for-understanding-dwolla\/","source":"blog.dwolla.com"},{"title":"Track Search Api Beta","details":"It uses Apache Solrs eDismax query parser and allows us to provide much more relevant search results than our current SQL Full Text Search based system.\n  It also supports the usage of basic syntax to hone down your search such as wrapping a phrase in \u201cquotes\u201d and + and -.\n  For example:\n  under the bridge -All Saints\n  We\u2019re still tweaking it at the moment, and currently it will not be kept 100% up to date with the latest tracks, but it does have our entire catalogue. We would definitely appreciate some feedback on the tracks that are there!\n  \t\t\t\t\t\t\t\t \t\t\t\t \t\t\t\t\t \t\t\t\t\t\tThis entry was posted \t\t\t\t\t\t\t\t\t\t\t\ton Tuesday, July 10th, 2012 at 9:18 am\t\t\t\t\t\tand is filed under API, Announcements, Search, Solr. \t\t\t\t\t\tYou can follow any responses to this entry through the RSS 2.0 feed.  \t\t\t\t\t\t\t\t\t\t\t\t\tYou can leave a response, or trackback from your own site.","item_date":"Jul 10 2012 07:00:00","display_item_date":"07-10-2012","url":"http:\/\/blogs.7digital.com\/dev\/2012\/07\/10\/track-search-api-beta\/","source":"blogs.7digital.com"},{"title":"Partner\/Reseller API Now Available","details":"For a while now, we have been offering a private label email marketing service to our partners so that they can offer ActiveCampaign email marketing to their clients. Our partners have also requested to have ability to control their domains from their own administration interface and to be able to create (and update!) accounts with ease (or maybe even on client\u2019s demand or action) without having a live person that needs to log into our system to make a change.\n As always, we listened to your feedback and comments.. so\u2026\u00a0The longly awaited API support for our partners\/resellers is finally here!\n As a start we implemented some basic calls that should assist our partners with managing hosted accounts without the need to log into the\u00a0reseller panel. As a reseller, you will be able to create new accounts, modify existing accounts, or cancel them. Also, you can now\u00a0obtain a list of available plans (both default pricing and pricing for a specific account, that includes all discount and addon information, prorated amount if applicable, etc).\n Soon we will be adding more calls to our partner API, most likely starting with calls related to credit-based accounts (as in purchase credits, apply credits to account, etc), followed by calls supporting premium services such as ERJA.\n If you are a reseller already, you can check out the new Reseller API area at https:\/\/www.activecampaign.com\/partner\/api\/\u00a0; it lists all available API calls along with examples for each. Any new API calls will be added to that area.\n If you are not a reseller, but would like to become one, you can do that \u00a0on our ActiveCampaign Partner Program\u00a0page.","item_date":"Jul 10 2012 07:00:00","display_item_date":"07-10-2012","url":"http:\/\/feeds.activecampaign.com\/~r\/activecampaign\/~3\/XbCT_YC41J8\/","source":"feeds.activecampaign.com"},{"title":"Meet us at the International Open Government Data Conference","details":"","item_date":"Jul 10 2012 07:00:00","display_item_date":"07-10-2012","url":"http:\/\/blog.cartodb.com\/post\/26635448785","source":"blog.cartodb.com"},{"title":"Investing in GitHub","details":"In four short years weve done a lot were really proud of. Weve shipped great native apps like GitHub for Mac and GitHub for Windows. Weve made using GitHub from within Eclipse even easier. Weve contributed to killer open source projects like libgit2 and git. Weve thrown hundreds of drinkups, sponsored tons of conferences, drawn a bunch of octocats, and, of course, made building software even better with github.com and GitHub Enterprise.\n  Weve done all this without any outside investment. Our company has been profitable for years, is growing fast, and doesnt need money. So why bother?\n  Because we want to be better. We want to build the best products. We want to solve harder problems. We want to make life easier for more people. The experience and resources of Andreessen Horowitz can help us do that. \n  Why Andreessen Horowitz?\n  We loved it when Marc Andreessen proclaimed, Software is eating the world.\n  Marcs venture firm, Andreessen Horowitz, is younger than GitHub. And like GitHub, theyre trying to do things differently. They believe in software as the future of everything. They want to help founders build great companies. They clearly have no interest in the status quo of venture capital.\n  Over the past few months weve gotten to know Marc and one of his partners, Peter Levine, and we really like them. We wish we could hire them both, but they already have jobs. So instead were going to work with them through their firm.\n  Whats next?\n  We want GitHub to be even easier for beginners and more powerful for experts. We want GitHub everywhere\u2014whether you use Windows or Mac or Linux or some futuristic computer phone that hasnt been invented yet\u2014we want GitHub to be an awesome experience. We want to make it easier to work together than alone. We want to keep changing the way software is developed for the better by making collaborating easier and sharing a no-brainer.\n  We will continue to focus on making software development even better. And were excited to partner with Andreessen Horowitz to help us make it happen.\n   \n       We make sure to read every mention on Twitter. If you find a bug, submit it to support@github.com. Every email is read by a real person.","item_date":"Jul 09 2012 07:00:00","display_item_date":"07-09-2012","url":"https:\/\/github.com\/blog\/1189-investing-in-github","source":"github.com"},{"title":"GitHub Android App Released ","details":"We are extremely pleased to announce the initial release of the GitHub Android App available on Google Play.  The app is free to download and you can also browse the code from the newly open sourced repository.\n    \n  This release includes support for working with Issues and Gists as well as an integrated news feed for keeping up to date with all your organizations, friends, and repositories.\n  The app  features a dashboard for quick access to all your created, watched, and assigned issues so you can always stay connected with the discussion and progress.  You can also view and bookmark any repositorys issue list with configurable filters for labels, milestones, and assignees.\n      \n  Head over to the github\/android repository to see exactly how the app was built, report any feature requests or issues, and stay up to date as development of the app continues.\n  The GitHub Android app was built on some great open source projects that are definitely worth checking out if you are looking to build your own Android apps or want to contribute to the GitHub or Gaug.es apps:\n   JakeWharton\/ActionBarSherlock  Delivers a consistent Action Bar experience in both the latest and older Android versions.\n \n  JakeWharton\/Android-ViewPagerIndicator  Used to support swiping between issues, gists, etc.\n \n  jayway\/maven-android-plugin  Used to build and test the app using Maven.\n \n  alexgorbatchev\/SyntaxHighlighter  Used for syntax highlighting of Gists.\n \n       We make sure to read every mention on Twitter. If you find a bug, submit it to support@github.com. Every email is read by a real person.","item_date":"Jul 09 2012 07:00:00","display_item_date":"07-09-2012","url":"https:\/\/github.com\/blog\/1187-github-android-app-released","source":"github.com"},{"title":"The New Era of Cloud Application Developers","details":"What if I told you that no matter what your educational or professional background is, you could build a cloud app today? Even a couple years ago the notion of this seemed unrealistic; however, just like the shift in technology can change on the drop of a pin, so can the makers who produce it.\n It has been an accepted truth for quite some time that a software developer comes from a computer science background or some related discipline. This notion that they are a highly specialized and formally trained individual is acknowledged and familiar. While this is still the norm in many cases, it doesn\u2019t have to be the only way.\n The new era of software developers is here, and the prerequisites to get started are recognizing the problems you want to solve and having the desire to solve them. \u201cI don\u2019t know how to program software\u201d is no longer an excuse to write off building cloud applications for your business. You can now point and click your way to effective business solutions; all you need to figure out is the path necessary to get you there.\n Point-and-click cloud database building. Configurable security profiles and permissions. Business process automation. Standard user interfaces. The ability make it mobile by checking one box \u2026 All of these complex software solution enhancements and more can be utilized without any code.\n \n Force.com, the core platform for Salesforce.com\u2019s business apps, is designed to help you focus on how to meet the requirements for building out your own business solutions rather than get bogged down on how to execute your ideas and the resources necessary to implement them. With a few configurations set up for your organization, Force.com will handle all of the back-end logistics of how things happen so that you don\u2019t have to.\n Cloud App Development Learning Paths\n There are a few different learning paths you can pursue to start building apps. For those who learn best by doing, we are releasing several Quick Tutorials, in addition to our Force.com workbooks, which act as modular walk through tutorials that you can complete in a matter of minutes to learn bite-sized development concepts on the platform.\n If you learn best by reading and digesting concepts, or would like context into why you do things in the Quick Tutorials, check out this series of wiki articles about understanding cloud application development. Each article dissects an individual piece of the development process and gives a high level description of what tools exist for each step, explains how to use these tools, and links to deeper dives on individual concepts.\n These resources are\u00a0available here, where you can also find other learning tools to help you get started building. There are already some resources there to start building today, but more videos, tutorials, articles, and other learning tools will be added soon in the upcoming weeks to give you the resources you need to succeed.\n Finally, if you would like to see this app building process come to life in less than an hour, please join me and Shannon Hale, the Product Manager of Declarative Apps, in our webinar on building codeless cloud apps. We will work through an example of building an app to solve our business requirements, explain concepts while simultaneously building them out, and open up the discussion at the end to questions from the viewers.\n Go Forth and Develop Cloud Apps\n If you had any doubt about your abilities to be an application developer, reconsider the possibilities. Not only are the tools there to help you execute your ideas, but now there is also a more centralized learning forum to help enable you on your journey. You can be the solution to your business problems, all you need to do is ask the right questions.\n   \t\t\t\t\t\t\t\t\t\t\t\n  \t\t\t\t\t tagged force.com, Getting StartedBookmark the permalink. Trackbacks are closed, but you can post a comment.\nTweet","item_date":"Jul 09 2012 07:00:00","display_item_date":"07-09-2012","url":"http:\/\/feedproxy.google.com\/~r\/SforceBlog\/~3\/O0ul5l_SuTI\/the-new-era-of-cloud-application-developers.html","source":"feedproxy.google.com"},{"title":"Announcing Fluidinfo Tumblr integration","details":"Scanning different services for interesting content is time-consuming. If you and your friends use multiple social networks, each with its own activity stream, it\u2019s even harder. Fluidinfo can show you information aggregated from multiple social networks and help you discover trending hashtags and URLs.\n Today we\u2019ve added Tumblr to the list of services we integrate with. Connect your account and we\u2019ll collect information about every hashtags or URL you\u2019ve ever mentioned on Tumblr. Each hashtag and URL has its own page in Fluidinfo where you can see who has mentioned it and where, follow the hashtag or URL on Fluidinfo, and comment on it directly.\n Linking your Tumblr account is easy. After logging in on Fluidinfo, select Connected services from the menu at the top right of the screen. On the connection page, select Connect Tumblr and approve the request to allow Fluidinfo to read your Tumblr posts.\n Once we\u2019ve scanned your Tumblr posts, you\u2019ll see information about your posts attached as comments to hashtag and URL pages in Fluidinfo.\u00a0Your timeline will show your mentions of hashtags and URLs on both Twitter and Tumblr. If you look at the Fluidinfo page for a hashtag, e.g., #nyc, you\u2019ll see its mentions across both systems.\n After collecting and analyzing metadata from different services, Fluidinfo creates you a dashboard where you\u2019ll see trending hashtags and URLs across your social networks. Just select Dashboard from the top-right menu to see it. You can also\u00a0see what\u2019s trending for others and their recent activity across networks, e.g., http:\/\/fluidinfo.com\/user\/aweissman.\n We\u2019ll connect other services in the coming weeks. We\u2019d love to hear what you\u2019d like integrated, so please email us at info@fluidinfo.com with suggestions. Also, drop us an email if you\u2019d like to find out how we can glue your company\u2019s systems together, or connect them to the social web, to surface trends, patterns of behavior or to enable analysis.\n  \t\t\t\t\t\t\t\t \t\t\t\t \t\t\t\t\t \t\t\t\t\t\t \t\t\t\t\t\tThis entry was posted \t\t\t\t\t\t\t\t\t\t\t\ton Monday, July 9th, 2012 at 10:45 pm\t\t\t\t\t\tand is filed under Product. \t\t\t\t\t\tYou can follow any responses to this entry through the RSS 2.0 feed.  \t\t\t\t\t\t\t\t\t\t\t\t\tYou can leave a response, or trackback from your own site.","item_date":"Jul 09 2012 07:00:00","display_item_date":"07-09-2012","url":"http:\/\/blogs.fluidinfo.com\/fluidinfo\/2012\/07\/09\/announcing-fluidinfo-tumblr-integration\/","source":"blogs.fluidinfo.com"},{"title":"Decibel Chooses 3Scale \u2013 Yahoo Finance","details":"SAN FRANCISCO, CA\u2013Decibel, the semantic music metadata company and reportedly the fastest growing European company in the digital music sector, selected 3scale (www.3scale.net), the creator of Out-of-the-Box API management infrastructure, to launch, manage and productize Decibel APIs.\n Decibel\u2019s mission is to power the next generation of digital music services to enhance the consumer relationship with digital music and to become the industry standard for music metadata. In that regard, the Decibel APIs powered by 3scale carry the most in-depth music metadata data you\u2019re likely to find anywhere with more than 3 billion pieces of data on 15 million tracks from 1.1 million albums by 300,000 artists.\n These data-driven assets power the creation of Connected TV, mobile, tablet and web applications; and increase revenues, collaboration and communication through enhanced discovery and business intelligence for consumer electronics manufacturers, digital retailers, record labels and broadcasters.\n 3scale helps companies like Decibel to remove obstacles to adoption and to unleash the power of their API. \u201cBecause Decibel\u2019s model is predicated on revenues as well as market penetration, we needed a provider that could integrate payment services simply, yet have a very simple to use free model,\u201d said Gregory Kris, Decibel\u2019s CEO. \u201cDecibel chose 3scale because from a technical perspective, it\u2019s faster, leaner and cheaper than the other players in the field. After extensive evaluations of various players, 3scale came out the winner.\u201d\n 3scale powers 100+ APIs and has over 60,000 developers writing applications using these APIs.\n \u201cWorld-class innovators like Decibel choose 3scale to open, launch and manage their API because we offer years of experience in productizing our customers\u2019 API capabilities,\u201d said Guillaume Balas, 3scale CMO. \u201cWith our years of API management experience 3scale brings the expertise and solution to help companies distribute their APIs, develop new business models and create additional revenue streams.\u201d\n 3scale works closely with customer tech- and business teams to provide optimal solutions for their needs by maximizing the impact and reach of their API among developers and business partners. For more information about 3scale, please visit www.3scale.net or contact sales@3scale.net.\n About Decibel:\n Decibel, a company based in the London digital hub of Shoreditch, supplies business-critical metadata to companies across the media and technology industries. This data can be found in digital retail stores, mobile applications, Connected TVs, streaming sites, recommendation engines and powering record labels and distributors across the digital music ecosystem.\n \n  \t \t\t \t\t\tThis entry was posted on Monday, July 9th, 2012 at 19:53.\t\t\tIt is filed under Blog, Featured Slider, Front Page, Metadata, News. \t\t\tYou can follow any responses to this entry through the RSS 2.0 feed.","item_date":"Jul 09 2012 07:00:00","display_item_date":"07-09-2012","url":"http:\/\/decibel.net\/blog\/?p=2572","source":"decibel.net"},{"title":"Streamsend Launches Embed Anything Widget for Email Marketing","details":"","item_date":"Jul 09 2012 07:00:00","display_item_date":"07-09-2012","url":"http:\/\/www.blogger.com\/feeds\/3468042893685990251\/posts\/default\/2318832720034037960?v=2","source":"www.blogger.com"},{"title":"MokiTouch for iPad, now available on App Store","details":"MokiTouch for iPad is now available on the Apple App Store. The iOS companion to the recently released Android app, MokiTouch for iPad is a customizable browser app to turn any iPad into a fully locked and managed kiosk.\n iPads and Android tablets are starting to appear in all kinds of public-facing ways. Tablets are a natural choice for these use cases because they are user-friendly and cost-effective. As some of the early innovators have found, using iPads and Android tablets as kiosks in such a public way has it\u2019s risks. Securing the device and the app so your customers consistently see only the experience you have designed is difficult. Some have tried using Safari and enclosures to lock down the device, but that isn\u2019t enough to provide a safe and secure experience. Just mounting an iPad and opening Safari gives your customers a portal to the entire web and they can simply type in any URL; opening up yourself to the risk of indecent and inappropriate content being left on screen for the next user.\n However, using MokiTouch you can remotely control the whole experience from the app to the hardware. App features include:\n Showing\/hiding the web address\n Administrator password, to keep pranksters out\n Clear private data button \u2013 automatically clear user\u2019s private data like usernames and passwords\n Up to 6 custom buttons to point to any website","item_date":"Jul 09 2012 07:00:00","display_item_date":"07-09-2012","url":"http:\/\/blog.mokimobility.com\/2012\/07\/mokitouch-for-ipad-now-available-on-app-store\/","source":"blog.mokimobility.com"},{"title":"Pinboard Turns Three","details":"Today is the third anniversary of the sites public debut.  Amusingly, I see some icons on the TechCrunch comment thread that launched Pinboard as a for-profit endeavor are already starting to break.  This is why we need archiving!    \nHere are some site stats for this year, compared with one and two years ago:\nbookmarks           3.5 M           27 M           53 M","item_date":"Jul 09 2012 07:00:00","display_item_date":"07-09-2012","url":"http:\/\/blog.pinboard.in\/2012\/07\/pinboard_turns_three\/","source":"blog.pinboard.in"},{"title":"Ecommerce Hackathon","details":"The story of New York\u2019s tech scene is laced with ecommerce success. From Doubleclick serving ads to Seamless serving food to Gilt, Birchbox, Rent the Runway, Fab, Warby Parker\u2026 New York is a transactional town.\n In this grand tradition, on August 4-5 Etsy and Dwolla are organizing an ecommerce hackathon. Ordr.in is sponsoring with some other amazing companies- Twilio, Zappos, Busted Tees, Sincerely, Stripe, Kiip\u2026 you get the idea. \u00a0Awesome tech, awesome people, awesome chance to build something that makes real money.\n Anyone who knows Ordr.in knows we <3 hackathons. But when you combine pure hacker hustle with money, big things can happen. It makes our API sing with joy.\u00a0\n Register and start dreaming of your hack turning into the next Etsy. It can happen- this is New York.","item_date":"Jul 08 2012 07:00:00","display_item_date":"07-08-2012","url":"http:\/\/ordrin.tumblr.com\/post\/26790593659","source":"ordrin.tumblr.com"},{"title":"Unexpected Outage (Resolved)","details":"We have had to take Xero offline due to an issue within the application hosting infrastructure.\n We have identified the issue and our team is working on resolving it. We\u2019ll provide an update shortly on when we expect to have Xero back online.\n Our apologies for any inconvenience this outage is causing.\n (UPDATE) We are still working on fixing the issue, but expect to be back online within the next 30 minutes.\n (UPDATE) We have encountered some further delays in bringing the system back online, which we expect will take around another 30 minutes to resolve. At this stage we now expect to be back online by 8:45pm (GMT) or 8:45am (NZT).\n (UPDATE) The system is back up now and we have the team closely monitoring the system.\u00a0 Apologies for the inconvenience this outage caused.\n (UPDATE) At 6:00am NZT (7:00pm GMT) one of our database servers failed and cut over automatically to its standby server.\u00a0 The standby server failed to start due to issues with one of the database files.\u00a0 The Xero operations team and Rackspace, our hosting provider, were forced to recreate the damaged file in order to recover the database server.\u00a0 There was no loss of data as a result of this issue however the application was offline for approximately two and one half hours.\u00a0 We have a number of levels of redundancy within our hosting environment that typically means our system is not impacted by infrastructure issues.\u00a0 Unfortunately in this situation there was no way for the automated recovery to resolve the issue.\u00a0 We are currently planning an upgrade to the latest version of the database platform that supports more advanced resilience, which should further reduce the risk of a similar outage.","item_date":"Jul 08 2012 07:00:00","display_item_date":"07-08-2012","url":"http:\/\/blog.xero.com\/2012\/07\/unexpected-outage-4\/","source":"blog.xero.com"},{"title":"Are HTML5 WebSockets the Panacea for Real-Time Data Push?","details":"As consumers expect more from their websites and apps, a majority of newer sites and mobile apps are adding features that require \u201cdata push\u201d. Everything from financial apps, business collaboration, real-time social sharing and newsfeeds, to multiplayer games, chat, and voting apps all require \u201cpushing\u201d data to devices with sub-second latencies. WebSockets is a relatively new API supported by some of the latest browsers that attempt to offer a one-size-fits-all solution for pushing data to browsers. WebSockets is new, but it\u2019s not alone. Over the last 15 years, a large number of other APIs, protocols, and methodologies have also been created to support large-scale data push. So, how does WebSockets stack up? What protocol should you use when building your next data push application?\n A Little WebSockets Background\n \n WebSockets exists as one approach to circumvent the original design of HTTP, the standard web protocol. The HTTP protocol, used by websites and many mobile apps today was originally designed to work in a request \/ response model. \u00a0To create the illusion of real-time data push, many websites and apps have used \u201cpolling\u201d, i.e. they make quick, repeated requests back to the server to check for updates. \u00a0A surprising number of websites continue to use this technique, even though this hack doesn\u2019t deliver true \u201creal-time\u201d updates, and at scale, the bandwidth and server costs required to handle the repeated, redundant polling become prohibitive.\n Many other viable techniques exist for data push. \u00a0Large-scale websites have been deployed using lots of other methods, including Comet, Bayeux, BOSH, \u201cLong polling\u201d variations, Pushlets, XMLSockets, and hidden iFrames, to name a few. So, how does WebSockets compare to these approaches, and how do you ensure you\u2019re using the right technique for your next app or website?\n WebSocket: A Deeper Dive\n \n At first glance, WebSockets looks like the obvious choice. \u00a0\u00a0The API is designed to provide a bi-directional communication channel between browser and server over a single TCP socket. \u00a0It\u2019s been standardized by the IETF, and the latest Chrome, Firefox, IE, and Opera browsers support WebSockets. \u00a0It\u2019s designed to minimize bandwidth overhead by reducing HTTP message overhead. \u00a0So, what\u2019s not to like?\n Like any perceived silver bullet, things are not always what they seem. \u00a0Lots of problems exist:\n Browser Support: \u00a0As of June 2012, only 47.64% of browsers currently in use actually support WebSockets (http:\/\/caniuse.com\/websockets). \u00a0\u00a0That means, no matter how good WebSockets appears, you still need a second \u201cfallback\u201d solution to support the majority of Internet users. \u00a0\u00a0And since most \u201cfallback\u201d solutions involve Flash, you\u2019re still out of luck on iOS and other mobile devices.\n Bypassing Proxy servers: \u00a0Many proxy servers do not properly route WebSockets traffic. \u00a0In fact, Peter Lubbers from InfoQ says that WebSockets are \u201c\u2026likely to fail in all but the simplest network topologies.\u201d (http:\/\/www.infoq.com\/articles\/Web-Sockets-Proxy-Servers) This problem alone unfortunately rules out the use of WebSockets within most large corporate networks.\n Detecting Dropped Connections: \u00a0In WebSockets implementations, a common problem is accurately detecting dropped connections. \u00a0WebSocket developers often use a browser API called \u201cwindow.navigator.onLine\u201d to determine connection status, but unfortunately, this API is poorly implemented on many browsers and often doesn\u2019t work at all (especially on some mobile devices). \u00a0This means it\u2019s common to be pushing data to a device that\u2019s no longer listening, while the device doesn\u2019t realize it\u2019s been disconnected.\n Supporting Heterogeneous Environments:\u00a0Desktop and native mobile apps are not hindered by the limitations of HTTP, since they can create TCP and UDP connections. Yet in a WebSockets-only server configuration, these native apps would need to emulate WebSockets. \u00a0Instead, the ideal data push technique in heterogeneous environments (web, native apps, and desktop), is to use the most optimal protocol for each environment, rather than trying to force WebSockets across all devices. \u00a0This magnifies complexity in your server-side code, which now has to support a multitude of environment-specific protocols.\n So, What\u2019s the Right Solution?\n If WebSockets is limited, what are the alternatives? \u00a0Each of the other protocols and techniques mentioned above also have their limitations. \u00a0To further complicate the choice, the landscape of browsers, mobile devices, and protocols is changing rapidly. SPDY and WebRTC are emerging as new protocols, and APNS, C2DM, and GCM muddy the waters for some mobile use cases.\n The answer is to choose a solution that shields the developer from the ever-changing set of protocols, techniques, and environments.\u00a0\n That\u2019s where PubNub comes in. \u00a0PubNub is a protocol-independent data push solution: as a cloud service, we handle the proper protocols automatically depending on your environment. \u00a0As new standards like WebSockets emerge and mature, we implement them into our client libraries, and your apps will get the benefit of always using the latest, most stable standards. \u00a0Because PubNub supports over 30 different programming environments, PubNub always ensures you\u2019re using the most optimal, reliable, and high-performing technique available for your specific deployment.","item_date":"Jul 08 2012 07:00:00","display_item_date":"07-08-2012","url":"http:\/\/blog.pubnub.com\/are-html5-websockets-the-panacea-for-real-time-data-push\/","source":"blog.pubnub.com"},{"title":"HP Cloud Services Partner New Relic Launches New Features","details":"If you\u2019ve not yet tried New Relic, HP Cloud Services customers can sign up  with New Relic today free of charge.\u00a0 All accounts start with 14 days for free Pro.\n So what was announced by New Relic?\u00a0 Bill Hodak, Director of Product Marketing at New Relic shared the following news.\n Think your app is fast? New Relic says that now you can stop guessing and start knowing with their App Speed Index. The App Speed Index leverages New Relic\u2019s Big Data to provide Big Insight. According to New Relic they collect over 55 billion performance metrics and monitor 1.5 billion page loads on behalf of their 25,000 customers and their 450,000 application instances. All of that data equates to 3.5\u00a0terabytes\u00a0of new data collected, stored and analyzed each day.\n \n With their App Speed Index, New Relic customers can classify their application into a Peer Group of similar applications (ex. eCommerce, SaaS, Gaming, and Consumer Internet applications) and benchmark their app with industry peers. The Index is designed to let customers find their percentile rank within a peer group for end user and application response times, error rates, and application availability, to find out how fast they really are.\n Learn more from New Relic about App Speed Index here, or check out this\u00a0blog post. And check their living infographic, which is updated daily to show how the peer groups rank by performance and availability. It even lists the fastest applications monitored by New Relic!\n Custom Dashboards\n Want to see Network I\/O graphs and End User Response Time graphs on the same dashboard?\u00a0 What about some custom business metrics and application response time?\u00a0 According to New Relic their customers can use their Custom Dashboards tool to build any dashboard with any data.\u00a0 \u00a0No coding is required!\u00a0 As a New Relic Pro customer, you click and pick, drag and drop, or instant copy an existing New Relic graph and create a Custom Dashboard.\u00a0 \u00a0For more on the New Relic App Speed Index and their Custom Dashboards, see New Relic\u2019s blog.\u00a0","item_date":"Jul 06 2012 07:00:00","display_item_date":"07-06-2012","url":"http:\/\/h30529.www3.hp.com\/t5\/HP-Scaling-the-Cloud-Blog\/HP-Cloud-Services-Partner-New-Relic-Launches-New-Features\/ba-p\/519","source":"h30529.www3.hp.com"},{"title":"StockTwits Desktop: Product End of Life","details":"Over the last several years we have supported an Adobe AIR desktop application for accessing StockTwits called StockTwits Desktop. While a small group of users use this product, most StockTwits users do not. In order to be able to deliver the best experience on our other products, we are discontinuing support for StockTwits Desktop on July 31, 2012.\n As part of the redesign of our web experience last month, we introduced features that use the latest browser based technology to replicate many of the experiences we had initially built StockTwits Desktop to deliver. While we know that change is always difficult, we hope that the members of our community that use StockTwits Desktop will take advantage of these features and continue to be valued contributors to our platform.\n To aid with the transition we will have been producing content and tutorials on the new functionality that are designed to help get any user get comfortable with the new design and get the most out of the new features we introduced\n You can access general articles on how to use the StockTwits website here.\n To view multiple streams on your screen via the web site you can use the Pop Out Stream feature or resize your browser window and access the complete web functionality.\n Our support team is here to help you with the transition. \u00a0As always, if you need need any help or support please contact us at support@stocktwits.com or at 1-888-785-8948. \n  \nThe information in this blog post represents my own opinions and does not contain a recommendation for any particular security or investment.  I or my affiliates may hold positions or other interests in securities mentioned in the Blog, please see my Disclaimer page for my full disclaimer.","item_date":"Jul 06 2012 07:00:00","display_item_date":"07-06-2012","url":"http:\/\/feedproxy.google.com\/~r\/TheStocktwitsBlog\/~3\/yhfWHEvEp9A\/","source":"feedproxy.google.com"},{"title":"Verelo + PagerDuty: New integration is live!","details":"We are very excited to announce a new integration and partnership with our good friends north of the border at Verelo (they\u2019re based in Toronto, Canada where PagerDuty started as well). \u00a0Verelo is a SaaS-based website monitoring system. \u00a0Unlike many (most?) web monitoring tools, Verelo checks your sites and APIs multiple times a minute. \u00a0That means you can set it to verify your site as frequently as every 5 seconds.\n The new integration allows you to quickly hook up your Verelo account to your PagerDuty account via API. \u00a0You can start receiving phone call, SMS and email based notifications when your websites or APIs are down or are experiencing issues, as monitored by Verelo.\n Integration How-To\n Hooking up your Verelo account to PagerDuty is a piece of cake:\n Log into your Verelo account.\n Navigate to Account Settings, click on the PagerDuty tab and click on the \u201cConnect PagerDuty Service\u201d button.\n You\u2019ll then be taken to the PagerDuty website. \u00a0Log in with your PagerDuty email and password.\n Once logged in, you\u2019ll be asked to create a service to use with Verelo.\n Click \u201cFinish\u201d and you\u2019ll be sent back to Verelo, where you will see the integration in your account.\n Go and edit any Verelo checks that you want to trigger alerts with PagerDuty, check the box, and click update.\n Job done!\n \nIf you have any questions or need any help with the integration, please contact us at support@pagerduty.com.","item_date":"Jul 05 2012 07:00:00","display_item_date":"07-05-2012","url":"http:\/\/blog.pagerduty.com\/2012\/07\/verelo-pagerduty-new-integration-is-live\/","source":"blog.pagerduty.com"},{"title":"Pusher on PhoneGap for Android","details":"Weve a whole host of client and server libraries for various technologies. This means no matter your technology of choice theres an option for you when using Pusher in your app. Whilst having a library and quality documentation are key aspects when using a service sometimes its also nice to have a getting started project that you can just download and run. Using Pusher on Android with PhoneGap wasnt quite as easy as wed like it to be so I decided to create the Pusher PhoneGap Android starter project.\n  The README covers all the details youll need to get started, but Ive also created a screencast.\n     Im excited about this project because it works around the WebView in Android not supporting WebSockets by using a PhoneGap WebSocket wrapper thats then made accessible to the JavaScript runtime in the WebView.\n  Weve already got a few customers trying out the project but if you have any feedback please drop an email to support@pusher.com or give @pusher a tweet.","item_date":"Jul 05 2012 07:00:00","display_item_date":"07-05-2012","url":"http:\/\/blog.pusherapp.com\/2012\/7\/5\/pusher-on-phonegap-for-android","source":"blog.pusherapp.com"},{"title":"Microsoft\u2019s Surface will massively expand the mobile advertising audience","details":"The recent \u2018secret announcement\u2019 (evidently a new PR tactic) of Microsoft\u2019s tablet device, called the Surface, has been causing quite a stir. Not only is this an intriguing new addition to the growing stable of tablet formats, it could more than double the installed base of tablet users \u2013 and the potential audience exposed to mobile advertising.\n There are currently more Windows-powered devices in the world than Android or iOS combined. Not only is this a huge audience, it\u2019s an audience that is already familiar with the Microsoft brand and may, with relief, move into the world of tablets without having to make the binary choice of Google or Apple. And, as they move seamlessly between work and home on their Microsoft devices this could be more consumers reached by advertising, more of the time.\n Paul Childs, our CMO, contributed his ideas to Mobile Marketing Magazine, so visit their site to see how, below the surface, Microsoft\u2019s tablet device could massively expand the app ecosystem \u2013 and with it, the potential for mobile advertising.\n \t\t\t\n                    \t\t  \t\tThis entry was posted in Advertisers, App Developers and tagged android, apple, google, iOS, microsoft, mobilemarketingmagazine, paulchilds, surface, tablet by Brendan. Bookmark the permalink.\t\t  \t\t    \tPost navigation\n  \t\t\t\t\t\t\u2190 Previous  \t\t\t\t\t\tNext \u2192  \t\t\t\t\t \t \t \t \t\t\t\t\t\t\t\t\t \t\t\t\tLeave a Reply \n \t\t\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tYour email address will not be published. Required fields are marked *","item_date":"Jul 05 2012 07:00:00","display_item_date":"07-05-2012","url":"http:\/\/blog.adfonic.com\/app-developers\/microsofts-surface-could-massively-expand-the-mobile-advertising-audience\/","source":"blog.adfonic.com"},{"title":"The Audience for the 2012 Olympics Will be Mobile","details":"As you might have read in the new eMarketer report published July 2,the Olympic audience via mobile devices is projected to reach 1 billion users. \u00a0\u00a0\n Smartphone penetration has gone up\u00a0 from 10% to 50% over the last four years. This increase explains why people will be watching the games on mobile devices. Besides streaming the games, people will use mobile to follow results easily.\u00a0","item_date":"Jul 05 2012 07:00:00","display_item_date":"07-05-2012","url":"http:\/\/blog.pontiflex.com\/2012\/07\/05\/olympic-2012-audience\/","source":"blog.pontiflex.com"},{"title":"New OfficeDrop Android Tablet App","details":"We\u2019ve been talking a ton about tablets, and are huge believers in how the tablet device is changing how people search files and share files. We\u2019ve blogged a ton about the iPad and small businesses and how the iPad is making inroads into corporations. Google has also recently announced a new Android tablet called the Nexus 7 \u2013 a clear indication that Android, as a platform, is taking the tablet seriously.\n The use of the OfficeDrop iPad app is taking off like crazy, and Today we are announcing that we now have an Android tablet optimized version of the OfficeDrop Android Scanner App!\n That is right, the popular OfficeDrop Android app now is designed to run on the new Android tablets \u2013 both 7 inch and 10 inch tablets.\n \n \u201cSmall businesses still use paper for up to 70% of day-to-day business tasks,\u201d said OfficeDrop CEO & Cloud Computing Expert, Prasad Thammineni. \u201cThis update will make it possible for these users to quickly upload paper to the cloud with just a few taps. Our small business customers appreciate the speed and efficiency that OfficeDrop helps them achieve, and now even Android tablet users will be able to realize the benefits of the paperless office.\u201d\n How The New OfficeDrop Tablet Android App Works\n Scan with your device\u2019s camera.\n Upload your scan into your OfficeDrop cloud storage account and it becomes a PDF ready to be searched, shared or organized.\n The PDF is automatically saved onto your Windows PC if you upload it into a folder that you\u2019ve synced using OfficeDrop\u2019s Windows Sync Client (available for free on officedrop.com).\n Then search for text in the scan from your Android device or from the web!\n \nWe want to highlight how easy it is to scan now on a tablet using OfficeDrop\u2019s apps. Additionally, we\u2019ve updated the OfficeDrop iPad app to make it easier to scan. We\u2019ll highlight these changes in a future blog post, but basically we heard your feedback that holding the device to take a multipage PDF scan is pretty hard \u2013 so we\u2019ve optimized the layout and user interface for scanning on the tablet.\n Below is the press release we put out announcing our updated mobile apps\/tablet app.\n July 5, 2012 \u2014 Cambridge, MA \u2014 Google Tablet? OfficeDrop saw it coming. Today, OfficeDrop announces major updates to its iOS and Android cloud scanning and storage apps, beginning with its first Android tablet app! OfficeDrop\u2019s newest mobile apps turn any iOS or Android tablet and phone into a scanner, giving users the ability to scan multiple pages as one document simply by snapping a few photos. With just a few taps, users can instantly turn these photos into searchable PDFs, upload them to the cloud, and share them with others. A super-simple, improved user interface and faster search and sharing functions make the mobile workflow quicker and easier than ever before.\n For example, a small business could use OfficeDrop\u2019s mobile apps to manage their paper on the go. When an employee is on a business trip, she can snap photos of a multi-page contract with her phone, upload the entire PDF as a single document to the cloud and collaborate on it with her coworkers in minutes.\n \u201cSmall businesses still use paper for up to 70% of day-to-day business tasks,\u201d said CEO Prasad Thammineni. \u201cThis update will make it possible for these users to quickly upload paper to the cloud with just a few taps. Our small business customers appreciate the speed and efficiency that OfficeDrop helps them achieve, and now even Android tablet users will be able to realize the benefits of the paperless office.\u201d\n Small businesses are increasingly turning to tablets to speed up their workflows while on the go. OfficeDrop is bringing its scan-to-cloud apps to Android tablets, after seeing massive growth since its iPad app release in September 2011. The new Android Tablet and improved iPad apps will allow small business tablet users to quickly scan documents to the cloud and easily search or browse files within OfficeDrop\u2019s cloud filing cabinet.\n Additionally, OfficeDrop\u2019s iOS and Android updates make its apps much faster for users. \u00a0The new and improved multi-page scanning capability makes getting paper documents to the cloud three times faster than before. Simplified public sharing and the ability to share entire folders the same way users share documents makes for speedier collaboration. Users can also copy public file links to their mobile clipboards, making it easy to distribute files through email, text or mobile browsers. OfficeDrop has also improved the user experience with stylistic and performance upgrades that make using the service easier than ever.\n OfficeDrop is a complete cloud solution for small businesses, from digitizing documents for the cloud to storage accounts that are sharable with teams. To learn how OfficeDrop makes digital life simple with its scan-and-capture apps and services, visit www.officedrop.com.\n About OfficeDrop\n OfficeDrop makes it easy for small businesses to scan paper to the cloud and access files from anywhere, at any time, using any device. OfficeDrop\u2019s cloud filing cabinet lets users sort, search and share documents \u2014 giving paper new life and making documents collaborative. Headquartered in Cambridge, MA, OfficeDrop was founded in 2007. For more information, visit www.officedrop.com\n   \t\t\t\t  \t\t\t\t                    \t\t\t\t\t\tThis entry was posted  \t\t\t\t\t\t\t\t\t\t\t\ton Thursday, July 5th, 2012 at 10:43 am\t\t\t\t\t\tand is filed under android, cloud computing, Digital Office, Document Management, Document Scanners, iPad, iphone, Marketing, mobile, OCR Scanning, OfficeDrop, Online Cloud Storage, Organization, Paperless, Paperless Champions, Product, Product Announcements, Product Features, Product Updates, scanning software, Small Business.  \t\t\t\t\t\tYou can follow any responses to this entry through the RSS 2.0 feed.    \t\t\t\t\t\t\t\t\t\t\t\t\tYou can leave a response, or trackback from your own site.","item_date":"Jul 05 2012 07:00:00","display_item_date":"07-05-2012","url":"http:\/\/blog.officedrop.com\/2012\/07\/05\/new-officedrop-android-tablet-app\/","source":"blog.officedrop.com"},{"title":"Interactive timeline of all major Google Search updates since 1998","details":"Underneath its simple exterior, the Google search algorithm is a complex beast that calculates rankings and returns results at lightening speeds. It has progressed from humble beginnings to something very sophisticated indeed.\n As businesses, we hold our breath with every update and celebrate the success of uplift or pick up the pieces from a penalty.\u00a0These updates commonly have names, which have become infamous recently \u2013 Panda or Penguin anyone?\n How well do you know Dewey or Jagger though?\u00a0Here\u00a0is an interactive timeline (with a humorous edge) that details the entire catalog of Google search updates since its origin in 1998. Enjoy!\n (If you don\u2019t see an interactive timeline just below this text you may have to click through to read this article on our website)\n    This timeline was prepared exclusively for Pingdom by the SEO team at Extreme Creations.","item_date":"Jul 04 2012 07:00:00","display_item_date":"07-04-2012","url":"http:\/\/feedproxy.google.com\/~r\/RoyalPingdom\/~3\/v3Za2_9wyDY\/","source":"feedproxy.google.com"},{"title":"Introducing BigML\u2019s Free Machine Learning Sandbox.","details":"At BigML, we know what it takes to develop code. One of the things we love is having a sandbox \u2013 an environment to play around in, free of charge, while developing our apps. So we hurried to create a free machine learning sandbox. Today we are proud to announce: BigML\u2019s development mode!\n \n Here\u2019s how it works. If you don\u2019t have an account yet, request an invite here. There are no charges involved, no credit card credentials to note and a good portion of promotional credits to get started. Once you have your account set up, go to the account section (the tab labeled with your user id). Simply flip the switch from \u2018production\u2019 to \u2018development\u2019 and you are in \u2018dev mode\u2019. As you\u2019ll see immediately, your credit usage info is dimmed.\n \n As long as you are in development mode, we show a green indicator \u2018DEV\u2019 in the top right hand corner.\n \n If you use BigML.io for access through our API, simply insert \u2018\/dev\u2019 so the URL is https:\/\/bigml.io\/dev\/andromeda.\n What\u2019s the difference between development mode and production mode? The most obvious difference is that in development mode, you won\u2019t be charged a single credit. The flip side is that your datasets are limited to 1 Mb. That naturally also limits your model training data to 1 Mb. Functionally, the production and development environment are identical.\n Many apps can benefit from applying machine learning techniques. We are committed to make that technology as accessible and simple to use as possible. Now you can incorporate machine learning in your applications with minimal effort and at no cost! And if you need a hand, want some advice or have any request, we are here to help you. Usually, you can find us around a campfire here, to chat with you. (Or contact us at twitter, Facebook or at info@bigml.com .)","item_date":"Jul 04 2012 07:00:00","display_item_date":"07-04-2012","url":"http:\/\/blog.bigml.com\/2012\/07\/04\/introducing-bigmls-free-machine-learning-sandbox\/","source":"blog.bigml.com"},{"title":"Huffington Post puts polling power in the hands of developers with new API \u00bb Nieman Journalism Lab","details":"","item_date":"Jul 03 2012 23:13:15","display_item_date":"07-03-2012","url":"http:\/\/www.niemanlab.org\/2012\/07\/huffington-post-puts-polling-power-in-the-hands-of-developers-with-new-api\/","source":"www.niemanlab.org"},{"title":"API Mutiny on Web 3.0","details":"API developers are ready to stage a mutiny. Will the captains of Twitter, LinkedIn, and NetFlix make developers walk the plank?\n\nDuring early market colonization days by Twitter, LinkedIn, NetFlix, Pinterist, and Instagram, the companies prioritize customer land grab over trade and commerce. Web 2.0 companies often choose to build distribution channels first.  The accepted navigation route includes courting third party developers with Open APIs and pseudo Open Data.   The data is freely available, readily accessible, and governed by limited commercial terms of service.   The corporate path to monetization is through advertising and light data API linking rather than building a deep ecosystem platform. However, with smart API developers creating better customer experiences and challenging revenue growth, the company captains have decided to change the terms of service and limit third party API developer participation.  All actions show the companies attempting to monetize their valuable asset and gain more gold coin, while limiting access by privateers.\n\n\n\nTo win in trade and commerce, companies must effectively own the customer relationship, establish a compelling customer destination, and inhibit disintermediation. These goals directly conflict with extending distribution channels through open and free APIs.  In a series of recent actions, Netflix neuters API data, LinkedIn shuts down access, and Twitter modifies a partnership.  Their actions create an environment where end-users will interact with the company through its platform.  The path often alienates third party developers by restricting access to high value content.  For example, NetFlix will be \u201cremoving all metadata for the rental history, recently watched, at home, etc. in all expands for all endpoints\u201d.  If you want to see viewing history, access the information via an approved NetFlix application.   LinkedIn recently eliminated API access to potential competitors Pealk and BeKnown.\n\n\n\nSailing the Open Web is rapidly turning stormy and perilous due to rapidly changing business conditions and terms of service.  According to Kin Lane, \u201cTwitter started as a truly open API, openly accessible by the entire ecosystem, and over the last six years Twitter has actively marketed its API as an open ecosystem, encouraging developers to build.\u201d  Twitter\u2019s history since March 2010 has been to increasingly restrict access by API developers and partners, while at the same time increasing access via user experience interfaces such as Twitter Cards. Twitter is following a strategy first outlined in the \u2018Pinterest and the Money Board\u2019 post.\n\n\n\nAn Ecosystem Platform Recommendation for Twitter\n\n\n\nWith data APIs, users are always one step removed from the platform company. Intermediary applications (between the data provider and end user) have the opportunity to reshape the customer relationship.  Rather than rely solely on data APIs, we have seen forward-thinking organizations create ecosystem platforms revolving around user experience APIs (e.g. Twitter Cards) and domain specific hosting environments (e.g. force.com). cWith user experience APIs, the platform company can create an \u2018Apple experience\u2019; maintaining customer ownership by controlling the \u2018look and feel\u2019 and by authorizing third party extensions.  Add in a 3rd party application hosting environment\u2018 (similar to SalesForce.com), and the ecosystem platform can manage Quality of Service (QoS), share monetization revenue, and enforce compliance regulations (e.g. PCI Security Standards, HIPAA, European Union Directive on Data Protection of 1995).\n\n\n\nInstead of simply publishing data APIs, Twitter could manage and host 3rd party applications within a multi-tenant cloud ecosystem platform.  WSO2 clients are using WSO API Manager and WSO2 AppFactory, a cloud ecosystem platform, to deploy context-aware APIs, rapidly provision 3rd party application projects, automate governance approval tasks, ensure regulatory compliance, monetize user interactions, and host applications that seamlessly extend the user experience.\n\n\n\nThe ecosystem platform environment also provides an opportunity for Twitter partners to deeply embed their business capabilities within Twitter\u2019s application (similar to Force.com, eBay sellers, or Amazon Store environments).  By hosting all business partners as tenant applications within a multi-tenant environment, the ecosystem environment more readily aggregates and shares business information.\n\n\n\nOur recommendation echoes the strategic direction outlined by Twitter\u2019s CEO, Mr. Costolo.  A Wall Street Journal article states Mr. Costolo mentions Twitter\u2019s focus is on evolving its API, or application programming interface, so other companies can build products into Twitter much like how sellers push their wares on Amazon.com.  In announcing Twitter Cards, Mr. Costolo says, \u201cWhat you\u2019ll see us do more and more as a platform is allow third parties to build into Twitter.\u201d This is something we\u2019ve been talking about for a while, and we\u2019re looking forward to adding new ways for developers to do this.\u201d\n\n\n\nTwitter Cards is a step in the right direction. The user experience API enables developers to mash data in a manner similar to LinkedIn group posts while maintaining control over the look and feel.  Twitter Cards and the Twitter platform environment don\u2019t yet deeply embed ecosystem partner components in a manner similar to force.com.  I look forward to seeing Twitter deliver an ecosystem platform enabling third party developers the ability to plug in analytics, marketing programs, and relationship management on top of Twitter\u2019s valuable social graph data.\n\n\n\nBottom Line\n\n\n\nWhile Web 3.0 increases the speed and velocity of business, fundamental business rules still apply.  Business rule number one, create a compelling client experience and solve a client pain point.  Business rule number two, own the customer relationship and establish the corporate brand.  Business rule number three, monetize the customer relationship and extend distribution channels.\n\n\n\nAn ecosystem platform enables your business to build an environment enabling customer relationship monetization, distribution channel expansion, and maintaining customer relationship ownership.  The ecosystem platform goes beyond data APIs.\n\n\n\nWe welcome an opportunity to talk with you about how WSO2 API Manager and WSO2 AppFactory deliver an unrivalled ecosystem platform environment.","item_date":"Jul 03 2012 22:59:43","display_item_date":"07-03-2012","url":"http:\/\/blog.cobia.net\/cobiacomm\/2012\/07\/03\/api-mutiny-on-web-3-0\/","source":"blog.cobia.net"},{"title":"Twitter Won\u2019t Kill the API","details":"Let\u2019s get real about Twitter\u2019s plans for their API. Even though the New York Times claims that Twitter wants to \u201ckill\u201c its API and developer ecosystem, and some competitors demand that Twitter data is too valuable to society and must become part of an open, federated infrastructure, neither of these things are going to happen.\n\nTwitter is not free, and it does not belong to its users. It is a business that belongs to its investors. Turning off the API or giving away Twitter\u2019s competitive advantage over its data would hurt Twitter\u2019s chances of going public eventually, so it is a certainty neither of these \u201cominous\u201c events are part of Twitter\u2019s plans. We should all accept that the people running Twitter aren\u2019t stupid, and turning off the API is a stupid idea.\n\nWhat Twitter\u2019s leadership is guilty of is being extremely clumsy, and consistently failing to understand how its developers earn their living. Announcing that you will announce changes that could affect every Twitter app in \u201cthe coming weeks\u201d is the definition of clumsy communication. It also proves that the people behind this announcement have no idea of the economics behind professional development. I\u2019ve been earning my living from Twitter API development for 3 years, so I can fill Twitter in on the secret they keep missing. Developers earn their living from clients. Sure some devs are just script kiddies trying to learn how to tweet from a PHP script for their own thrills, but the developers who count make money by building apps for clients and tool users who pay them. The last thing paying clients want is uncertainty, and the fear and doubt that the apps they are paying for will not exist in a few weeks is the worst kind of FUD.\n\nAs I just said, that isn\u2019t going to happen, but Twitter\u2019s clumsiness has once again caused the predictable firestorm of worries. The really sad part of this miscommunication is that the clumsy ending obscured the real point of the blog post. If you read it closely, you will see some interesting points that have been completely lost in the current frenzy:\n\n\u201c\u2026 we want developers to be able to build applications that run\nwithin Tweets.\u201d\n\u201cWhat you\u2019ll see us do more and more as a platform is allow third\nparties to build into Twitter.\u201d\n\u201c\u2026 we\u2019re looking forward to adding new ways for developers to do\nthis.\u201d\n\u201c\u2026 we\u2019re hard at work building tools that make it easy for developers\nto build common Twitter features into their own sites\u201d\nDoes this sound like an announcement leading up to Twitter turning off the API or sending out \u201ceviction notices for third-party apps\u201c? Of course not. So why did we get such extremely scary coverage of this post? Because Twitter devs are scared all the time that their work will be turned off arbitrarily, and with no recourse. It is a constant fear, and as anyone who follows the Twitter developer lists knows, a sad reality. Twitter is at war with thousands of malicious apps at all times. They are so busy fighting off the bad actors, that they sometimes reflexively include the good guys in their warnings. They can\u2019t seem to help making threats, and we can\u2019t help being scared of what\u2019s coming next.\n\nThe solution is simple. Don\u2019t try to tell us how much you like us, Twitter, while holding a big stick behind your back. You may not think that \u201dstricter guidelines around how the Twitter API is used\u201d is scary, but the minute I read that, I knew this was going to get ugly. Here\u2019s an idea. If you have good news or want us to support a new feature, ask us nicely, and save the threats for another post. It should be clear that threatening everyone, especially in such a vague way, is going to be all anyone sees or talks about. Even more important, Twitter, if you have bad news coming, don\u2019t announce that you are going to announce it some time in the future. Just yank off that band-aid. Get it over with fast. The reality is never as bad as the anticipation.","item_date":"Jul 03 2012 21:45:54","display_item_date":"07-03-2012","url":"http:\/\/blog.programmableweb.com\/2012\/07\/03\/twitter-wont-kill-the-api\/","source":"blog.programmableweb.com"},{"title":"Twitter cuts off service to LinkedIn, API changes draw ire","details":"(CBS News) Twitter, the micro-blogging site, has cut off tweets on the professional social network LinkedIn, ending a two-year partnership.  Twitter is a website that lets users broadcast 140-character status updates, or tweets, in real time. The micro-blogging service has had a partnership with LinkedIn since 2009.\n If you had previously synced your LinkedIn and Twitter accounts, and selected the option to share Tweets on LinkedIn, those Tweets generated from Twitter will no longer appear on LinkedIn. There will be no other changes to your LinkedIn experience, Ryan Roslansky, LinkedIn head of content, said in a blog post. \n LinkedIn users will still be able to post updates to Twitter from LinkedIn, but not the other way around. \n Cutting off tweets to LinkedIn users is part of a greater initiative at Twitter to create stricter requirements for developers who use the companys application programming interface (API). An API is a set of tools that lets third-party developers write custom programs for a service. \n The new requirements are meant to encourage developers to build apps on Twitters website. The company said it would more thoroughly enforce its Developer Rules of the Road. Twitter wants to ensure its branding is consistent across the Internet, whether tweets are read on the site or a third-party client. \n While the company is cracking down on inconsistency, developers are struggling with the narrowing constraints of integrating with Twitter. \n In a March 2011 note to developers, Twitter platform team member, Ryan Sarver said, developers ask us if they should build client apps that mimic or reproduce the mainstream Twitter consumer client experience. The answer is no. \n The challenges of building a program that doesnt mimic Twitter while ensuring consistency across all platforms has raised the ire of developers - some feeling jilted by the company. Their concern is that they have invested time and resources into developing apps for Twitter, only to have the company change the rules of the game. \n Were building tools for publishers and investing more and more in our own apps to ensure that you have a great experience everywhere you experience Twitter, no matter what device youre using, Twitter product manager Michael Sippy said in a blog post, where he emphasized upgrades, like Twitter Cards. The new addition to Twitter lets users add a few lines of code, or card, to a tweet that will add an expanded view of content on Twitter. \n Twitter faces its own challenges. Much of the companys content is viewed on third-party sites or programs. The micro-blogging service must find the right balance of running a profitable business and maintaining a robust developers community.","item_date":"Jul 03 2012 19:36:49","display_item_date":"07-03-2012","url":"http:\/\/www.cbsnews.com\/8301-501465_162-57464956-501465\/twitter-cuts-off-service-to-linkedin-api-changes-draw-ire\/","source":"www.cbsnews.com"},{"title":"Innovator Spotlight: Samtrafiken","details":"Samtrafiken is a non-profit organization jointly owned by 34 public transport operators and authorities in Sweden. Samtrafiken\u2019s vision is that public transport will be seen as simple, reliable, and convenient and become the number one choice for every journey. Their goal is to bind all of Sweden together by allowing the public to easily travel between destinations using a single ticket for any mode of public transportation.\n Elias Arnestrand, Samtrafiken\u2019s Innovation Manager, talked to us about the unique challenges faced in the public transportation industry and how APIs provided a path to innovation . . .\u00a0\n \nPlease tell us more about Samtrafiken, what is its purpose?\n Simply put, we help people taking public transportation plan their journeys. This can be quite complicated when you have to plan several modes of transportation in a single trip using a combination of local, regional, and national carriers. In Sweden, Samtrafiken is the organizational body creating this system where you plan your whole journey from anywhere to anywhere. That\u2019s one of the elements of our mission. The other part of our mission is to sell tickets combining different providers of public transport. Here in Sweden it\u2019s called Resplus. It allows you to book, purchase, and receive one ticket for an entire journey, including transfers. It is quite unique in Europe to have this kind of cooperation across all of these public transport providers in a single ticketing system. Getting everyone to agree to common rules is very tricky. But we\u2019ve figured out how to deal with these complexities.\n How did Samtrafiken come to the realization that it needed an API strategy?\n Samtrafiken came together with SL (The Public transport authority in Stockholm) and Viktoria Institute (an industrial research institute) to deal with the rapid innovation you see with mobile devices such as the iPhone and Android. We noticed that developers were screen scraping our sites gathering information and timetables to build mobile apps. This was something very new to public transport organizations that traditionally owned this information and kept it as part of their service. These third-party apps turned out to be quite popular with the public. People actually thought that these apps where being provided by the public transport authorities. From this point it became clear that sharing your data and information with third-parties who could deliver good services and innovative apps was the obvious thing to do.\u00a0\n What were some of the initial challenges you were trying to overcome?\n Historically it took us a long time to develop services and when we launched something it was immediately old news with our customers. With the rapid rise of smartphones, we simply could not keep up with the demand from our customers who needed information across these different channels and platforms. \n We also had to deal with the rise of third-party apps and gadgets using screen scraping technology. It started to cause serious traffic problems for our sites. For example, there was one poorly programmed gadget-based app that was trying to get data from our systems. When it didn\u2019t get a data response, it kept trying eventually bringing the system down. You would especially see this when there was bad weather and the buses were running late. Customers were overloading the systems trying to get updated information. So we started to shift our thinking from looking at this as a threat to looking at this as an opportunity to engage third parties from the outside to help drive innovative solutions. This was a significant mindshift in the industry and took several years to fully realize. The idea of losing control was unsettling. So it\u2019s a big success story that we shifted to open APIs.\n Tell us more about your API strategy.\n We started this project around 2009 and created Trafiklab. It was formulated as an initiative for the industry to start to work with open data and open APIs. We wanted to make it simple to access this data and even make it fun for our industry and third party developers to discuss these issues. It was important for us to keep this industry initiative all together on one site instead of each public transport entity creating their own channel, data sources, set of agreements, and different types of APIs. This would have created a huge burden on third parties that wanted to access the complete set of public transportation data and services in Sweden.\n We did our research and looked at the external drivers that motivate developers. What we saw was that developers were driven by finding challenge, the satisfaction of getting their app to work, and the ability to showcase their work to the greater public. These were the drivers that we focused on to help get this initiative successfully off the ground.\n Today, our open APIs are a very important part of our strategy in providing customers with relevant public transport information and services. For example, for public transportation in Stockholm, more than 50% of the requests come from services created by third parties. Apigee is our API gateway for most of these services.\n How are you using Apigee today?\n Apigee was fundamental in the development of Trafiklab as it was essential in helping us open our APIs across all of our public transportation operators and authorities. We use Apigee in a hosted environment and it manages all of our API keys. All of our API requests go through Apigee and it does its magic and protects our back-end systems from bad requests and traffic spikes. So caching and rate limits are very important.\n What benefits have you seen from your API program?\n We have made a big impact in the community of open APIs and open data with developers. It\u2019s a big step that our industry is moving forward with openness and that other parties are using our data in a good way.\n \n Last fall we held a \u201ctravel hack\u201d in Gothenburgh organized by Viktoria Insitute. It was a 24 hour innovation jam. We had about 100 developers broken into 20 development teams developing some amazing prototypes. One cool service that came out of this was an app that allowed you to search for a new apartment and enter your work location where you could see the time it takes to commute to your job utilizing public transportation. It\u2019s fascinating because what looks close on a map could actually be a long commute, yet longer distance work locations could actually take much less time on public transport. We\u2019ve also seen apps that deliver \u201cgamification\u201d of public transport where the more you utilize public transportation services, the more points you are rewarded and shared with your social network.\n Fortunately, we\u2019ve been recognized for our efforts and innovation. We received the 2012 European Public Sector Information Trailblazer award for Trafiklab, our open API initiative for developers. Trafiklab has also been awarded the Golden Link Award 2012 by the Swedish Association of Local Authorities and Regions (SALAR). Trafiklab also won the Good Intentions Award from the Swedish Public Transport Association and the Golden Mobile for public sector services in late 2011.\n What is your vision for your API program?\n I would say that we just started this project but there are numerous data sources that still need to be connected to the external world. It\u2019s hard work but we need to keep opening up the data sets across the public transportation industry. For example, we still sometimes face long and challenging discussions about whether the industry should be charging for information or making it availble for free.. Another dimension that we are dealing with is sale of tickets through third party providers. Transactional related APIs are the natural next step in this program.\n APIs are a marketing and distribution channel for our public transportation information and services. We\u2019ve realized that APIs are the cheapest and fastest way to build applications. And most importantly, APIs let third parties extend our products and services.\n Check out all the Innovator Spotlight interviews!","item_date":"Jul 03 2012 19:24:37","display_item_date":"07-03-2012","url":"http:\/\/blog.apigee.com\/detail\/innovator_spotlight_samtrafiken\/","source":"blog.apigee.com"},{"title":"Five Actions for Maximizing Your API Value","details":"IT Briefcase has posted my article where you may read more about the five actions.\n API Manager infrastructure delivers the tools you need to effectively perform the actions. The short video below explains the features and capabilities.\n More resources (i.e. articles, videos, webinars, getting started guides, and downloadable bits) are available on our WSO2 API Manager product page.","item_date":"Jul 03 2012 19:04:07","display_item_date":"07-03-2012","url":"http:\/\/blog.cobia.net\/cobiacomm\/2012\/07\/03\/five-api-actions\/","source":"blog.cobia.net"},{"title":"Twitter faces the same dilemma as the New York Times","details":"","item_date":"Jul 03 2012 18:54:36","display_item_date":"07-03-2012","url":"http:\/\/gigaom.com\/2012\/07\/03\/twitter-faces-the-same-dilemma-as-the-new-york-times\/","source":"gigaom.com"},{"title":"Ecosystem Milestones","details":"Our recent Big Boulder conference was a milestone in the growth and evolution of the commercial social media industry. Data publishers and data consumers alike came together to discuss what the continued onslaught of public social data means to commerce, and public services. While there is plenty more to do, it was great seeing some consistency in approach, vocabulary, needs, and wants emerge. The ecosystem was nothing but a hodge-podge of interests and firms just a few years ago.\n I just had another moment that felt like another milestone. While evaluating resumes for our Product Manager position, I was telling someone who had submitted their resume that we were looking for someone with more domain expertise. The industry has matured to the point that such a thing actually exists! Domain expertise in commercial social data is actually a tangible thing now.\n Fun moment.","item_date":"Jul 03 2012 07:00:00","display_item_date":"07-03-2012","url":"http:\/\/blog.gnip.com\/ecosystem-milestones\/","source":"blog.gnip.com"},{"title":"Viadeo launches Company Pages in beta!","details":"Viadeo is proud to announce the launch of Company Pages in beta. Thousands of companies will be showcased with a special emphasis on employees who are members of the Viadeo network.\n Discover more about it here, and we invite you to watch this video presenting the Company Pages.\n We hope you like them!","item_date":"Jul 03 2012 07:00:00","display_item_date":"07-03-2012","url":"http:\/\/blog.viadeo.com\/en\/2012\/07\/04\/viadeo-launches-company-pages-in-beta\/","source":"blog.viadeo.com"},{"title":"IBM introduces shopping for the future","details":"The shopaholic and tech nerd in me is excited about IBM\u2019s latest plan \u2013 a new shopping app that makes your life easier. I love shopping just like any other girl however when faced with decisions, labels, etc., it can be a bit overwhelming. I could walk away with something I don\u2019t like and then there\u2019s the hassle of having to return it.\n IBM\u2019s Augmented Reality app will be something you\u2019ll keep handy on your smartphone with all your shopping preferences. Before going out to shop, you\u2019ll spend some time building a complete profile with the things that matter most to you about your shopping experience \u2013 food allergies, green\/earth friendly stores, diet conscious food, sales\/coupons, labels, reviews, ad infinitum. The possibilities are endless. The app will allow you to pan over a selection of items, say for example you\u2019re shopping for a certain cereal.\u00a0 If you\u2019re looking for a high fiber breakfast cereal, the app will locate what you\u2019re looking for, scan the label, let you know if it triggers your food allergies, send you a few reviews, and also alert of any coupons or discounts. That alone saves you a few precious Google minutes in the grocery aisle.\n This app has possibilities for all types of individuals \u2013 a mother with kids who might have finicky taste or food preferences or a Mom who prefers natural baby food vs commercial food products. There\u2019s also the raw\/vegan food group who might be cautious about what they put into their bodies so an app that reads labels and warns them could be extremely useful. However, this extends beyond the grocery store. What if you\u2019re shopping for your next set of wheels? Imagine panning this app over a row of cars and being able to pull up all the details you need for your perfect car? Or imagine now having to provide service for that car and pick a mechanic, a set of tires, etc.\n There are several advantages and IBM plans to collect the data from these experiences and give feedback to the merchants so they can be more conscious of their shopper\u2019s wants, needs, and desires which in turn should make a better experience for you. If all goes well, IBM plans to release the finalized version at the end of this year.\n What do you think of this kind of shopping experience? Would an app like this be useful to you? What other features would you add to it?\n  \n  If you enjoyed this post, please consider leaving a comment or subscribing to the RSS feed to have future articles delivered to your feed reader.","item_date":"Jul 03 2012 07:00:00","display_item_date":"07-03-2012","url":"http:\/\/blog.viralheat.com\/2012\/07\/03\/ibm-introduces-shopping-for-the-future\/","source":"blog.viralheat.com"},{"title":"WWO Google Chrome Web App now available","details":"If you use Google Chrome web browser then please open the Google Chrome web browser and copy and paste the link provided below in the address bar of the chrome browser and press enter.\n Then click on \u201cADD TO CHROME\u201d button on the top right.\n That\u2019s it.\n Now you can quickly and easily visit our website and find the weather forecast for millions of city and town in the world.\n Please do leave your feedback and also rate the web app.","item_date":"Jul 03 2012 07:00:00","display_item_date":"07-03-2012","url":"http:\/\/blog.worldweatheronline.com\/2012\/07\/03\/wwo-google-chrome-web-app-now-available\/","source":"blog.worldweatheronline.com"},{"title":"Google Analytics is a home run \u2013 62% of the top sites use it","details":"When Google bought Urchin Software in 2005 and released its Urchin on Demand service for free to the entire Internet, the company transformed the web analytics industry forever. All of a sudden there was a powerful yet completely free option available for everyone. Webmasters have rallied to Google Analytics ever since, and as of June this year, 62.4% of the top 10,000 websites use the service.\n That kind of market share is quite amazing, and it\u2019s safe to say that Google Analytics is one of Google\u2019s biggest triumphs. Really, the numbers speak for themselves.\n \n Being a both free and powerful analytics package, Google Analytics has become a huge success for Google. It\u2019s popularity is undeniable, and this survey shows that the number of sites that use Google Analytics keeps growing.\n In 2009, 50% of the top 10,000 websites on the Internet used Google Analytics, which as we just showed you has now gone up to 62.4%. Or put another way, Google Analytics usage among the top 10k sites is up 24.8% since 2009.\n And there is more. Back in 2008 we showed that 32.2% of the top 500 websites on the Internet used Google Analytics. That number has now risen to 39.4%. That\u2019s a 22.4% increase in Google Analytics usage among the top 500 sites since 2008.\n New script versus old script\n Interestingly, we noticed that 6.5% of the sites with Google Analytics are still not using the new ga.js script that Google introduced in December of 2007, instead relying on the old urchin.js script. That means these sites are not reaping the full benefit of Google Analytics. We\u2019re not even sure that script works properly anymore, having been deprecated almost five years ago.\n On the other hand, three years ago almost half of the top 10,000 sites were still clinging on to the old script, so the situation is much better now than it used to be. Webmasters have caught on and mostly gotten with the program.\n We also found that a small number of sites actually had both scripts on their page. Just under 0.9% of the sites using Google Analytics fall into this category. We\u2019ll just assume that this is an oversight.\n It\u2019s clear that Google really values the Analytics product, and has launched a number of improvements to the service over the years. Just this past year, Google introduced a new control panel, user flow visualizations, and even a real-time dashboard, perhaps in response to the popularity of services such as Chartbeat.\n Then there is the price. Free is hard to beat. And it\u2019s not like Google is peddling a bad product here, either. Google Analytics has a ton of excellent features, and it\u2019s the kind of product that every webmaster needs.\n There obviously is a market for other paid analytics services since they keep popping up, but that market became much smaller and more niche the day Google introduced Analytics. And each day that passes, Google\u2019s slice of that pie gets a little bit bigger.\n Methodology: We scanned for the inclusion of either \u201c\/ga.js\u201d or \u201c\/urchin.js\u201d in the HTML code of the homepages of the top 10k sites on the Internet (as per Alexa).\n Here is a larger version of the chart, for those of you so inclined.","item_date":"Jul 03 2012 07:00:00","display_item_date":"07-03-2012","url":"http:\/\/feedproxy.google.com\/~r\/RoyalPingdom\/~3\/Sk5DgOU1qQY\/","source":"feedproxy.google.com"},{"title":"New Product Alert \u2013 Custom Wristlets, Purses, and More Bags!","details":"by Patrick Briggs, Zazzle Maniac   \t\t\t \t\t\t\tBOOM!  Fireworks have come early to Zazzle with the launch of custom wristlets, coin purses, accessory bags, and cosmetic bags!  The latest product partner to join the Zazzle platform, Bagettes is a manufacturer that makes custom bags for any occasion (weddings, reunions, a girls\u2019 night out) with high quality materials right here in the USA.  How\u2019s that for a 4th of July celebration!?  Check out some of my favorite new bags (my girlfriend picked them for me) and post your favorites in the comments!\n        \n What do you think of the new bags?  Excited to have a night out with a personalized wristlet?  Share in the comments!","item_date":"Jul 03 2012 07:00:00","display_item_date":"07-03-2012","url":"http:\/\/blog.zazzle.com\/2012\/07\/03\/new-product-alert-custom-wristlets-purses-and-more-bags\/","source":"blog.zazzle.com"},{"title":"Zillow Now Exclusive Provider of Rental Listings to Yahoo! Homes","details":"This is great news for visitors to Yahoo!, who\u00a0can now search Zillow\u2019s hundreds of thousands of rental listings, including those not found on other sites\u00a0\u2014 like single-family homes \u2014 as well as apartment listings.\n And it\u2019s great\u00a0news for landlords, property managers and rental brokers who syndicate their listings to Zillow because they are now getting even more exposure for their listings. That\u2019s right: Not only do your listings reach Zillow\u2019s nearly 34 million monthly unique users on Web and mobile, (6 million of whom are renters, as\u00a0well as those\u00a0who are dual tracking their home search) but the Yahoo! Homes audience as well.\n This news comes on the heels of Zillow\u2019s acquisition of RentJuice, which provides rental relationship management software for landlords, property managers and rental brokers. And it\u2019s yet another example of how Zillow is continually offering new ways to help real estate professionals grow their business.\n If you\u2019re a landlord, property manager or rental broker interested in syndicating your rental listings to Zillow, email propertymanagers@zillow.com or call 866-638-1883.","item_date":"Jul 03 2012 07:00:00","display_item_date":"07-03-2012","url":"http:\/\/feedproxy.google.com\/~r\/ZillowBlog\/~3\/ZtdVnnoi3Ls\/","source":"feedproxy.google.com"},{"title":"Updates to Google Wallet for digital goods including attractive pricing and subscriptions support","details":"","item_date":"Jul 03 2012 07:00:00","display_item_date":"07-03-2012","url":"http:\/\/googlecommerce.blogspot.com\/2012\/07\/updates-to-google-wallet-for-digital.html","source":"googlecommerce.blogspot.com"},{"title":"Introducing the new MOO Lightroom Plugin","details":"3rd July 2012 by Rebeka \t\tIf you\u2019re a photographer, there\u2019s a good chance you use Adobe Photoshop Lightroom. It\u2019s a super professional piece of software perfect for editing all your digital photos.\n And if you use Lightroom to edit the photos you use to create MOO\u00a0Business Cards\u00a0(or\u00a0MiniCards\u00a0or\u00a0Postcards), your life is about to get just a little bit easier.\n With our newly created\u00a0Adobe Photoshop Lightroom Plugin\u00a0you can now export your photos directly to MOO. Yay!\n Here\u2019s how it works:\n Select the photos you wish to export\n Select\u00a0File > Export...\n After the photos have been exported, you will be directed to moo.com, where you can make final adjustments to your cards before adding them to your cart.\n And that\u2019s it! Easy as pie. Mmmm, pie\u2026\n \n(It has been confirmed to work on Lightroom versions 2.7, 3.6, 4.0 and 4.1 on OS X 10.6 and version 3.6 on Vista)\n Check out the plugin for more info and all installation details\n This is currently an Alpha version so we\u2019d love to hear any feedback or ideas for improvements you have in the comments below!","item_date":"Jul 03 2012 07:00:00","display_item_date":"07-03-2012","url":"http:\/\/www.moo.com\/blog\/2012\/07\/03\/introducing-the-new-moo-lightroom-plugin\/","source":"www.moo.com"},{"title":"The future is mobile","details":"Today, Cleartrip turns 6. We couldn\u2019t think of a better way to celebrate than by giving all of the customers who\u2019ve brought us this far a small gift. Today we\u2019re thrilled to announce a whopping big upgrade to Cleartrip Mobile with the addition of international flights and hotels. This upgrade cements Cleartrip Mobile\u2019s lead as the most comprehensive mobile travel solution in India. Our new mobile products are available only on smartphones. As of today, we will no longer be developing new products for basic feature phones.\n  Already the best mobile travel product in the market, Cleartrip Mobile has allowed you to search and book domestic flights and trains, access details for your booked trips and browse destination guides on your mobile phone. With the addition of international flights and hotels, we\u2019ve put all your travel in your pocket.\n  As with all things Cleartrip, we\u2019ve made the international flight search faster, smarter and simpler than anything else out there. A unique two-column layout lets you easily combine inbound and outbound flights without overloading your senses. An intelligent BS Filter (we couldn\u2019t think of a more apt name) removes all the bizarre flight options that no one ever books from search results, leaving only the sensible and logical choices. Each flight we display has a nifty visual indicator to let you compare flight duration and stops at a glance \u2014 no more squinting at your mobile screen to understand what you\u2019re getting.\n  \n  Every traveller needs a place to sleep and Cleartrip Mobile puts over 113,000 hotels worldwide at your fingertips. Whenever and wherever you need a hotel, Cleartrip Mobile will help you find a room. When you need a hotel room at the very last minute, Cleartrip Mobile offers Quickeys \u2014 just tap the \u201cQuickeys\u201d option to see a list of hugely discounted hotels available for immediate check-in. Or you can search for a hotel near you using your current location. \n  \n  Tapping a hotel from the listing brings up hotel details, where you can flip through photos, check out the TripAdvisor traveller ratings, see what amenities the hotel has to offer and a few more details. You can also view the hotel on a map from here.  \n  \n  For a closer look at the hotel, just rotate your phone to see the photos full-size in landscape mode. All our hotel images are optimised for high-resolution displays such as the Retina displays on newer iPhones and the experience is gorgeous.\n  \n  One more thing\u2026 with these new features, Cleartrip Mobile is now available in all of the international markets we serve. Our customers in Bahrain, Kuwait, Oman, Qatar, Saudi Arabia and the United Arab Emirates will also have all their travel in their pockets.\n  So go ahead and visit cleartrip.com on your smartphone. If you\u2019re as addicted to your phone as we are, we\u2019re sure you\u2019ll fall in love with it.","item_date":"Jul 03 2012 07:00:00","display_item_date":"07-03-2012","url":"http:\/\/blog.cleartrip.com\/2012\/07\/04\/the-future-is-mobile\/","source":"blog.cleartrip.com"},{"title":"In Closing Its Platform, Twitter Risks Destroying Its Community","details":"After attaining unlikely success as an open platform, Twitter is demanding that third-party apps show Twitter\u2019s stream the way the company wants them to. \u201cYou need to be able to see expanded Tweets,\u201d Product Manager Michael Sippey wrote in an\u00a0announcement on Friday afternoon. He said those features make Twitter \u201cmore engaging and easier to use.\u201d But if Twitter squeezes too hard on third-party developers, it risks damaging something more important to the company than any set of features: It risks destroying the culture that has grown up around it.\n \t\t\t\t\t\t    \u201cThese are the features that make Twitter Twitter,\u201d Sippey wrote, referring to expanded tweets. For established Twitter users and developers, that statement is hard to swallow. For one thing, expanded tweets are a brand-new product that hasnt been around long enough to have any impact whatsoever. More to the point, nearly all the features that make Twitter what it is today were invented by users dissatisfied with Twitter\u2019s own user interface.\n  The reason users could do this boils down to two words: open platform. Twitter is one of the best examples of what\u2019s possible for a large-scale, real-time tech company that opens its platform to developers. Kin Lane is a keen observer of open platforms who has been\u00a0researching the business of Twitter\u2019s application programming interface, or API. Published by Twitter, the API is a set of commands, available to anyone, that controls the companys servers, allowing outsiders to build services of their own using Twitters inherent\u00a0capabilities.\u00a0\n  As Sippey\u2019s statement sank in on Friday, Lane shared his comprehensive, ongoing timeline of key moments in Twitter\u2019s history.\u00a0Read the timeline and, if youre a long-time Twitter user, recall what it was like. Early on, Twitter bent over backward to accomodate developers. The company even showcased its favorite third-party Twitter apps. The ability to\u00a0build\u00a0on\u00a0Twitter yielded many great apps and successful businesses.\n  And it wasn\u2019t just programmers building software. Twitter users themselves invented social conventions like @ mentions, hashtags and retweets, creating the manners and protocols that define the service today. Twitter took the cue, introducing\u00a0features which built on practices that users invented on their own, including\u00a0clickable hashtags and the\u00a0native retweet button.\n  Cut to the QuickBar\n  Things started to get weird in April 2010 when Twitter acquired Tweetie, the most beloved third-party Twitter client for iPhone at the time. It began to use that first-rate mobile interface to experiment with its business model. That\u2019s where it put the QuickBar, an irritating overlay that showed trending topics above the main timeline screen. It provoked such a backlash that Twitter pulled the feature.\n  Though users defeated the QuickBar, it was a wake-up call. Twitter had taken over a beloved client and started twisting it into something presumably more lucrative but less user-friendly.\n  Over the following year, Twitter gradually built walls around its ecosystem. In March 2011, Ryan Sarver of Twitters platform team warned developers not to \u201cbuild client apps that mimic or reproduce the mainstream Twitter consumer client experience.\u201d That, of course, is what most of the beloved third-party Twitter apps do, more or less. While Twitter only cracked down on apps that operated on the margins of what it allowed, the tone of developing on top of Twitter changed.\n  By December 2011, Twitter had dispensed with Tweetie altogether in favor of its new, more consistent user interface across all its official apps. On Friday, Sippey made clear that this is the experience Twitter wants its users to have. It will release stricter rules of the road for apps \u201cin the coming weeks,\u201d and the company won\u2019t comment on upcoming changes until then.\n  Sacrificed at the Altar of Consistency\n  The first major casualty of Twitter\u2019s new stance has been its relationship with LinkedIn. On Friday, as Sippey delivered the news about \u201cproviding the core Twitter consumption experience,\u201d LinkedIn sounded downright mournful to announce that its users can no longer automatically feed in their tweets. Twitter cut off a major hub of engagement and activity in the name of a consistent experience.\n  As Nick Bilton pointed out in his New York Times blog, even Twitter\u2019s own user experiences are not consistent yet. And third-party Twitter apps like Tweetbot are beloved by users. But unless Twitter\u2019s new rules make it possible for those apps to accomodate the changes without giving up too much of their originality, that era might be drawing to a close.\n  Some observers, like Bottlenose CEO Nova Spivack, think Twitter\u2019s change is shortsighted. Spivack proposes a new business model for Twitter that\u2019s built around the open, extensible nature of the Twitter of old. Twitter is a motherlode of valuable data, and Spivack marvels at the company\u2019s unwillingness to charge developers to compete in extracting that value.\n  In contrast, entrepreneur Anil Dash is skeptical of the idea that outside developers know better than Twitter. Consistency is critical for \u201cnormal people,\u201d he wrote. Dash is right that lots of Twitter apps are spammy and unhelpful. Twitter could probably improve its users\u2019 lives by being more strict with such apps.\n  But Twitter works because people who love the service have built a culture on top of it. They use what makes them happy. In the past, people who weren\u2019t happy with Twitters native offering were able to build something they liked better. If Twitter breaks that ability, the nature of the relationships built on its network will change. Features arent what \u201cmake Twitter Twitter.\u201d People are.","item_date":"Jul 03 2012 04:57:13","display_item_date":"07-02-2012","url":"http:\/\/www.readwriteweb.com\/archives\/in-closing-its-platform-twitter-risks-destroying-its-community.php","source":"www.readwriteweb.com"},{"title":"Can Twitter Change User Expectations Around Monetization? - Forbes","details":"Twitter has a problem. It\u2019s not a very big problem at the moment, but each step that is taken along the road to monetization is a step closer to the problem, and away from the source. Change.\n  The 140 character service continues to add in new tools to the timeline, with URL redirectors, support for long tweets, media links, and more all coming in to the service that, if we\u2019re honest, is trying to break out of the SMS derived length limit that was imposed on it from the very early days of the product. That brevity still is one of the reasons that many of us love Twitter.\n  And love is the exact word to use here. As Twitter continues to change and try and bring in the dollars, many people will wonder what happened to the service they fell in love with. Developers already know the feeling, with Twitter\u2019s changes of the terms and conditions for developer access a goal that tends to move over time, usually to the detriment of the devs, and for the benefit of Twitter\u2019s bank account.\n  Twitter\u2019s problem is simple. Users do not expect change, which makes it very hard to push through anything that will help monetization \u2013 especially as so many people access twitter through third party applications, which rarely support Twitter\u2019s money-making features.\n  Contrast that with Facebook. Being on Facebook means expecting change every single week. Playing \u2018spot the changed feature\u2019 is an ongoing pastime, and while the huge changes such as Timeline also generate a lot of heat, most users will shrug their shoulders.\n  Facebook is\u00a0 in a much better place to tweak content, layout, and style, than Twitter, and in the long run that will be a huge benefit.\n  I\u2019m not suggesting that Twitter should constantly fiddle with the service, but they need to manage the expectations of the users so they will be accepting of change for the benefit of everyone in the long term. Switching off access for networks such as Linked In, continually changing how tweets are displayed on the website, limiting access via the API\u2019s for third party apps and sending ominous notes to developers to follow the news rules\u2026 that\u2019s not the way to do to.","item_date":"Jul 03 2012 00:23:58","display_item_date":"07-02-2012","url":"http:\/\/www.forbes.com\/sites\/ewanspence\/2012\/07\/01\/can-twitter-change-user-expectations-around-monetization\/","source":"www.forbes.com"},{"title":"Weather Channel buys Weather Underground; brand stays","details":"The largest weather news and information provider is about to get bigger. The Weather Channel Companies (TWCC), parent of the cable network and weather.com, is acquiring San Francisco-based Weather Underground with a pledge to keep the oldest internet weather brand going. That includes keeping the staff intact along with wunderground.com and its mobile apps. No details on the price; the deal is expected close this summer.\n In addition to users and traffic, the acquisition brings the larger company deep expertise in weather research and development along with a community-centric approach.  Real estate, too \u2014 The Weather Channel plans to use the SF headquarters as a regional office, according to the  announcement Monday afternoon.\n It\u2019s the latest move for Chairman and CEO David Kenny, who who was hired in January in a surprise leadership change. His charge: to invest more in programming, digital assets and international expansion. Weather Underground fits squarely in that second bucket.  The Weather Channel Companies (TWCC) is a joint venture of NBC Universal and PE firms Bain Capital and The Blackstone Group.\n ComScore pegs the unduplicated number for the sites at 54.6 million uniques based on May 2012 traffic. TWC is by far the larger with 49.8 million uniques compared to Weather Underground\u2019s 9.8 million; TWC also has the largest weather presence online, already more than double Weather Bug, the next largest. (See the chart below for more details.)\n Weather Underground started at the University of Michigan in 1991 as \u201ca menu-based telnet interface.\u201d The name came from, yes, the radical student group that also came from that university and took its name from Bob Dylan\u2019s Subterranean Homesick Blues. The formal company dates back to 1995, when it spun out of UM and launched on the web. \n  Credit: comScoreMediaMetrix","item_date":"Jul 02 2012 21:16:02","display_item_date":"07-02-2012","url":"http:\/\/paidcontent.org\/2012\/07\/02\/weather-channel-buys-weather-underground-brand-stays\/","source":"paidcontent.org"},{"title":"Twitter Blog: Twitter Transparency Report","details":"Wednesday marks Independence Day here in the United States. Beyond the fireworks and barbecue, July 4th serves as an important reminder of the need to hold governments accountable, especially on behalf of those who may not have a chance to do so themselves.\n With that in mind, today we\u2019re unveiling our first Twitter Transparency Report. Inspired by the great work done by our peers @Google, the primary goal of this report is to shed more light on: \ngovernment requests received for user information,\n government requests received to withhold content, and\n DMCA takedown notices received from copyright holders.\n \n The report also provides insight into whether or not we take action on these requests.\n One of our goals is to grow Twitter in a way that makes us proud. This ideal informs many of our policies and guides us in making difficult decisions. One example is our long-standing policy to proactively notify users of requests for their account information unless we\u2019re prohibited by law; another example is transmitting DMCA takedown notices and requests to withhold content to Chilling Effects. These policies help inform people, increase awareness and hold all involved parties\u2013\u2013including ourselves\u2013\u2013more accountable; the release of our first Transparency Report aims to further these ambitions.\n Here\u2019s the data, which dates back to January 1, 2012. You can also find these tables, along with more information about the data, in our Help Center.\n \n We\u2019ve received more government requests in the first half of 2012, as outlined in this initial dataset, than in the entirety of 2011. Moving forward, we\u2019ll be publishing an updated version of this information twice a year.\n Along with publishing our Transparency Report, we\u2019re also partnering with Herdict, which \u201ccollects and disseminates real-time, crowdsourced information about Internet filtering, denial of service attacks, and other blockages.\u201d This new partnership aims to drive more traffic and exposure to Herdict, while also empowering the web community at large to help keep an eye on whether users can access Twitter around the world.\n These two new initiatives\u2014the Twitter Transparency Report and our partnership with Herdict\u2014are an important part of keeping the Tweets flowing.","item_date":"Jul 02 2012 20:38:52","display_item_date":"07-02-2012","url":"http:\/\/blog.twitter.com\/2012\/07\/twitter-transparency-report.html","source":"blog.twitter.com"},{"title":"Public neuroscience data and open API foster creative innovation and new tool development \u00ab Florida Biotechnology News","details":"The Allen Institute for Brain Science opened its doors to a diverse group of programmers and informatics experts for a non-stop week of collaboration, learning and coding based on its public online platform of data, tools and source code. The event brought together more than 30 participants from top universities and institutes ranging from the Baylor College of Medicine in Houston to the Nencki Institute of Experimental Biology in Poland, as well as from start-ups and established technology companies, to develop data analysis strategies and tools based on the newly enhanced Allen Brain Atlas application programming interface (API).\n \u201cThis hackathon stems from our longstanding, open approach to science and our belief that putting our data-rich resources in the hands of the many and varied experts around the globe is the most effective way to drive progress in brain research,\u201d said Chinh Dang, Chief Technology Officer of the Allen Institute for Brain Science. \u201cThe hackathon projects delivered innovative ways of handling data, offering direct contributions to the informatics and programming communities as well as to neuroscience. We hope that this event serves as a springboard for others out in the community to use our API, and we look forward to seeing what can be done with it.\u201d\n The Allen Institute for Brain Science is one of the biggest data producers in neuroscience, with rapidly growing data stores in the petabyte range that it makes publicly available through its Web-based\u00a0Allen Brain Atlas resources. These resources include, among others, anatomically and genomically comprehensive maps of genes at work in the mouse and human brains and receive approximately 50,000 visits each month from researchers around the globe.\n The public API was created as an additional form of data sharing to spur community technology development and further empower scientists to make groundbreaking discoveries about the brain in health and disease\u2014including insights into learning, cognition, development, Alzheimer\u2019s, obesity, schizophrenia, autism, and more\u2014that will deliver better treatment options sooner. The hackathon coincided with the public release of the full Allen Brain Atlas API earlier this month, and a key goal of the event was to ignite community momentum and interest in using it.\n Using the Allen Brain Atlas API, developers can create entirely new software applications, mashups and novel data mining tools for making sense of the large and ever-growing volumes of neuroscience data. The API offers data access across species, ages, disease and control states, providing a powerful means to compare many types of data (e.g., histology images, gene expression, and MRI) among many types of samples (e.g., ages, species or diseases).\n \u201cThe Allen Institute is a leader in large-scale open science, known for providing high-quality data and online tools that advance brain research,\u201d said Sean Hill, Executive Director of the International Neuroinformatics Coordinating Facility (INCF). \u201cWith the Allen Brain Atlas Hackathon and their public API, they are bringing the same collaborative, community-focused approach to technology development and innovation that is at the core of INCF\u2019s mission.\u201d\n The hackathon program was designed to provide scientists and programmers a solid foundation in using the Allen Brain Atlas API for data mining, data analysis and tools development. The event featured a handful of speakers from the Allen Institute, as well as external experts who had leveraged earlier versions of the API in their work. As a hands-on workshop, participants spent most of the time working on projects of their choice. The Allen Institute development team actively participated throughout the week to provide specific examples of API usage, as well as to team up with community participants to develop collaborative projects. Participants\u2019 presentations throughout the week showcased their projects and progress, stimulating new ideas and benefiting from the collective feedback and troubleshooting power of the entire group.\n Projects ranged from practical applications, such as using a list of glioblastoma-related genes to discover biological patterns that could shed new light on the biology of the disease and developing strategies to use gene expression data with functional brain scanning technologies, to purely creative applications, including translating genomic data into music.\n Source code from participants\u2019 projects will be made publicly available on theAllen Brain Atlas data portal was part of the Allen Institute\u2019s next public data release in October, as well as a through the\u00a0INCF website.","item_date":"Jul 02 2012 19:51:31","display_item_date":"07-02-2012","url":"http:\/\/floridabiotechnews.com\/news\/public-neuroscience-data-and-open-api-foster-creative-innovation-and-new-tool-development\/9864\/","source":"floridabiotechnews.com"},{"title":"The Spitcast API: The Science of Surf","details":"For as long as people have been grabbing a board and trying to catch some waves, attempting to predict surf quality has been considered an art. Spitcast is trying to turn the art of prediction into the science of surf. Jack Mullis, the founder and developer of Spitcast, has been surfing for years. In 2005 he decided to combine his knowledge of surfing with his degree in Engineering Physics and create a unique algorithm that can predict future surf conditions by using Java and MySQL to cross reference NOAA regional weather data with observations at specific surf spots. This process has been shown to produce predictions accurate to 1 foot standard deviation. With the Spitcast API, this data can be transformed into custom applications. \n\n\nThe Spitcast API is a RESTful API and provides JSON formatted responses. Spitcast has a wealth of information that would be useful to developers that would like to enable users to view surf related predictions.\n\n\nThe Spitcast API is one of 85 sports APIs and 12 forcast APIs that are listed in the ProgrammableWeb directory.","item_date":"Jul 02 2012 19:24:46","display_item_date":"07-02-2012","url":"http:\/\/blog.programmableweb.com\/2012\/07\/02\/the-spitcast-api-the-science-of-surf\/","source":"blog.programmableweb.com"},{"title":"For Twitter-Owned Apps and Sites, a Cacophony of Confusion ","details":"By NICK BILTON screenshots of Twitter.com At top, the Twitter Web site, which includes the company\u2019s premium Discover tab. At bottom, the Twitter Web site as viewed on the Apple iPad, which highlights direct messages instead.\u00a0 \nDid Twitter just cut off its entire face to spite its nose?\nOn Friday, the company put up eviction notices for third-party apps that mimic tools and services that Twitter provides. But some of those apps are far superior\u00a0to Twitter\u2019s offerings.\nIn a blog post\u00a0with the headline \u201cDelivering a consistent Twitter experience,\u201d the company gave a stern warning to developers who build applications on the Twitter platform.\u00a0\u201dWe\u2019re building tools for publishers and investing more and more in our own apps to ensure that you have a great experience everywhere you experience Twitter,\u201d wrote Michael Sippey, Twitter\u2019s director of product. \u201cNo matter what device you\u2019re using.\u201d\nThere\u2019s one slight problem: Twitter-built products are anything but consistent across different devices.\nTwitter\u2019s mobile site, mobile apps, iPad app, desktop apps and Web site are so divergent that it looks as if they were built by several different companies.\nButtons and icons are in different places across devices and apps. Features, like direct messages, are several layers deep on the iPhone app, yet this features is front-and-center on the iPad. Even the company\u2019s logo is\u00a0inconsistent\u00a0across its products \u2014 some use Larry, the old Twitter bird, while others use the new in-flight version.\nThese inconsistencies go beyond design and user interface issues. Features that are the core to Twitter\u2019s business are\u00a0absent\u00a0too. For example, the Discover tab, a news-like feature that was heavily promoted by the company\u2019s directors late last year, is still\u00a0absent\u00a0from the iPad ecosystem, seven months after the\u00a0company\u00a0said it was coming soon.\nThe \u201cnew Tweet\u201d\u00a0button is not\u00a0consistent across Twitter-built apps and Web sites. \nAlso, Twitter still hasn\u2019t\u00a0adopted what\u00a0developers\u00a0call a consistent \u201cread state.\u201d This means that Tweets and direct messages would be marked as read on different devices, just the way e-mail has worked for years between mobile phones and computers.\n\u201cIt seems that making money without owning the user-experience is hard and Twitter wants to be that go-to Web site and mobile app,\u201d said\u00a0Al Hilwa, an analyst at the research firm IDC.\u00a0\u201dThe problem is they have let the cat out of the bag and many have written better interfaces.\u201d\nUsers are frustrated by these inconsistencies. On the Apple iTunes store, Twitter-built products are tagged with thousands of one-star reviews from reviewers who repeatedly noted how the apps lack features and are slow. There are more uses of the word \u201cterrible\u201d than one can count.\nBetter third-party Twitter products, like the popular app Tweetbot, could be cut off when the Twitter guillotine drops in coming months. Yet in the iTunes store, Tweetbot is flush with four- and five-star glowing reviews.\nThe problem is especially acute on the iPad and other tablets. With tens of millions of tablets in use today, neglecting the tablet experience could harm the company\u2019s overall mobile\u00a0strategy.\nSarah Rotman Epps, a Forrester research analyst who specializes in mobile computing, said in a new report profiling 200\u00a0executives who develop products for tablets that almost all see the tablet as part of the mobile experience, across advertising and features.\n\u201cThe majority think of their tablet strategy as part of their mobile strategy,\u201d Ms. Epps said in an e-mail.\nSo while Twitter gets ready to kill the ecosystem that has helped make Twitter what it is today, and does so by citing a need for consistency, the company might want to pause and take a look at its own products first.\u00a0If it does this, Twitter executives might see what the rest of its users see: that the only thing consistent across Twitter is the lack of consistency among the products the company makes itself.\n \nRelated Articles Also Tagged:\n Developers, mobile, Social, Tablets, Twitter \nPath Tries Again With a Wider Network \nF.C.C. Wants Clarity From Verizon on Proposed Spectrum Deals \nDevelopers Have Mixed Reactions to BlackBerry 10 News \nRIM Offers Programmers Free BlackBerry Tablets for Their Apps","item_date":"Jul 02 2012 19:12:45","display_item_date":"07-02-2012","url":"http:\/\/bits.blogs.nytimes.com\/2012\/07\/02\/for-twitter-owned-apps-and-sites-a-cacophony-of-confusion\/","source":"bits.blogs.nytimes.com"},{"title":"Judge to Twitter: turn over OWS protester's tweets ","details":"But today a Manhattan judge ruled that the micro-blogging company must turn over Occupy Wall Street protester Malcolm Harris tweets to prosecutors, who want to use them as evidence to prove their allegation that Harris knew of and disobeyed police orders to leave the Brooklyn Bridge on October 1, 2011, when he and about 700 other people were arrested. Protesters say the police allowed them onto the bridge only to surround them, trap them with orange netting, and systematically arrest them, one by one -- a process euphemistically called kettling. (Protesters are\u00a0suing the city, alleging that they were not properly notified or given an opportunity to leave the area before they were trapped and cuffed.)\n Twitter notified Harris of a district attorney subpoena for his account information and tweets in January 2012; the activist filed a motion to quash the subpoena and was later joined in the defense by Twitter itself. Todays ruling found that prosecutors have a legitimate reason to seek Harris tweets, but interestingly, require that prosecutors produce a warrant for tweets from the last day of the time period for which they sought information.\u00a0\n  \tThe bigger picture: reform ECPA\n Harris case is another reminder that we must update our electronic communications privacy law, written and passed in 1986, before the internet practically existed. Under the law, the government argues that it get access to our stored communications, even our emails, without warrants. Email is not what it was in 1986, and electronic privacy law needs to take notice.\n We also need to change the law to reflect the fact that our emails are pretty much universally held by third parties like Google, in the same way Twitter holds our logs data and direct messages.\u00a0\n If we cant defend against subpoenas or warrants for our private information, in most cases because we do not even know of such requests for our data, we lose nearly all our privacy rights online.\n If police need a warrant to search my filing cabinet, why dont they need one to search my email? And if I get a chance to quash a subpoena or a warrant for my physical papers in court, why cant I do the same when the state wants to read my email?\n Harris was right and courageous to challenge the subpoena for his account information, and Twitter did the right thing to join him. But we cant rely on the good will of companies like Twitter; after all, it stands alone among major technology companies in the United States in notifying and defending users.\u00a0\n \n The third party issue is not at all an esoteric problem -- but it is a problem of which we likely cant grasp the enormity. Google alone reported receiving over 6,300 requests for user data from the US government between January and June 2011 (see above); the company complied with 93% of them, or about 5,878 in total.\n A federal magistrate judge recently published an article warning us that somewhere in the neighborhood of 30,000 secret surveillance orders are issued every year in the US. In most of those cases, people do not have a chance to challenge the orders because they are submitted to third party content holders like Google or Twitter, not the targets themselves. Often enough the targets of the surveillance never find out that government agents were reading their emails because they are never prosecuted.\n Todays ruling reminds us, yet again, that it is well past time to update electronic communications privacy law. Google knows it. Twitter knows it. Apple knows it. Tea party groups and the CATO Institute know it. AOL knows it. We know it. The list of groups and corporations that have joined together to fight for Digital Due Process is long, diverse and growing.\n So whats Congress waiting for?\n UPDATE: Twitter is now releasing numbers on government requests for user data. See the results of the first report here.","item_date":"Jul 02 2012 19:09:09","display_item_date":"07-02-2012","url":"http:\/\/www.privacysos.org\/node\/730","source":"www.privacysos.org"},{"title":"People v Harris (2012 NY Slip Op 22175)","details":"Twitter, Inc. (Twitter) seeks to quash the January 26, 2012 subpoena issued by the New York County District Attorneys Office and upheld by this courts April 20, 2012 order. That order required Twitter to provide any and all user information, including email addresses, as well as any and all tweets posted for the period of September 15, 2011 to December 31, 2011, from the Twitter account @destructuremal, which was allegedly used by Malcolm Harris. This is a case of first impression, distinctive because it is a criminal case rather than a civil case, and the movant is the corporate entity (Twitter) and not an individual (Harris). It also deals with tweets that were publicly posted rather than an e-mail or text that would be directed to a single person or a select few.  \nOn October 1, 2011, the Defendant, Malcolm Harris, was charged with Disorderly Conduct (Penal Law \u00a7240.20 [5]) after allegedly marching on the roadway of the Brooklyn Bridge. On January 26, 2012, the People sent a subpoena duces tecum to Twitter seeking the defendants account information and tweets for their relevance in the ongoing criminal investigation (CPL 610; Stored Communications Act [18 USC \u00a72703(c)(2)]). On January 30, 2012, Twitter, after conferring with the District Attorneys office, informed the defendant that the Twitter account @destructuremal had been subpoenaed. On January 31, 2012, the defendant notified Twitter of his intention to file a motion to quash the subpoena. Twitter then took the position that it would not comply with the subpoena until the court ruled on the defendants motion to quash the subpoena and intervened.  \nOn April 20, 2012, this court held that the defendant had no proprietary interest in the user information on his Twitter account, as he lacked standing to quash the subpoena (See CPLR 1012 [a], 1013; People v Harris,__NYS2d__, 2012 NY Slip Op 22109 [Crim Ct, NY County 2012]). This court ordered Twitter to provide certain information to the court for in camera review to safeguard the privacy rights of Mr. Harris.  \nOn May 31, 2012 David Rosenblatt, a member of Twitters Board of Directors, was personally served within New York County with a copy of this Courts April 20, 2012 order, a copy of the January 26, 2012 trial subpoena, and a copy of the March 8, 2012 trial subpoena. Twitter subsequently moved to quash the April 20, 2012 court order. To date, Twitter has not complied with this courts order.  \nTwitter is a public, real-time social and information network that enables people to share, communicate, and receive news. Users can create a Twitter profile that contains a profile image, background image, and status updates called tweets, which can be up to 140-characters in length on [*2]the website.[FN1] Twitter provides its services to the public at large. Anyone can sign up to use Twitters services as long as they agree to Twitters terms. Twitter is a Delaware corporation with its principal place of business in California.  \nThe Stored Communications Act (SCA) (18 USC \u00a72701 et seq.) defines and makes distinctions between Electronic Communication Service (ECS) versus Remote Computing Service (RCS), and content information versus non-content information. ECS is defined as any service that provides the user thereof the ability to send or receive wire or electronic communication. (See 18 USC \u00a72510[15]). RCS is defined as the provision to the public of computer storage or processing services by means of an electronic communications system.(see 18 USC \u00a7 2711[2]). The Wire Tap Act (18 USC \u00a72510[8]) defines content information as contents, when used with respect to any wire, oral or electronic communication, includes any information concerning the substance, purport, or meaning of that communication. In contrast, logs of account usage, mailer header information (minus the subject line), list of outgoing e-mail addresses sent from an account, and basic subscriber information are all considered to be non-content information.[FN2] \nWhile Twitter is primarily an ECS (as discussed in Harris,__NYS2d__ ,at 6 ), it also acts as a RCS. It collects and stores both non-content information such as IP addresses, physical locations, browser type, subscriber information, etc. and content information such as tweets. The SCA grants greater privacy protections to content information because actual contents of messages naturally implicate greater privacy concerns than network generated information about those communications.[FN3] \n1.Twitter Users and Standing to Challenge Third-Party Disclosure Request  \nTwitter argues that users have standing to quash the subpoena. The issue is whether Twitter users have standing to challenge third-party disclosure requests under the terms of service that existed during the dates in question. In Harris, (id. at 7) the New York City Criminal Court held that a criminal defendant did not have standing to quash a subpoena issued to a third-party online social networking service because the defendant has no proprietary interest. The courts decision was partially based on Twitters then terms of service agreement. After the April 20, 2012 decision, Twitter changed its terms and policy effective May 17, 2012. The newly added portion states that: You Retain Your Right To Any Content You Submit, Post Or Display On Or Through The Service. (See Twitter, Terms of Service, https:\/\/twitter.com\/tos\/ [accessed June 11, 2012]).  [*3]\nTwitter argues that the courts decision to deny the defendant standing places an undue burden on Twitter. It forces Twitter to choose between either providing user communications and account information in response to all subpoenas or attempting to vindicate its users rights by moving to quash these subpoenas itself.However, that burden is placed on every third-party respondent to a subpoena (see In Re Verizon, 257 F Supp 2d 244, 257-258 [2003]; United States v Kennedy, 81 F Supp 2d 1103, 1110 [2000]) and cannot be used to create standing for a defendant where none exists.  \nThe Stored Communications Act (18 USC \u00a72703 [d]) states:    A court issuing an order pursuant to this section, on a motion made promptly by the service provider, may quash or modify such order, if the information or records requested are unusually voluminous in nature or compliance with such order otherwise would cause an undue burden on such provider. (Emphasis added).In the defense motion they also reference a concurrence by J. Sotomayor who said that it may be necessary for the court to reconsider the premise that an individual has no reasonable expectation of privacy in information voluntarily disclosed to third parties (see United States v Jones, 565 US __, 132 S Ct 957 [2012]).Publication to third parties is the issue. Tweets are not e-mails sent to a single party. At best, the defense may argue that this is more akin to an e-mail that is sent to a party and carbon copied to hundreds of others. There can be no reasonable expectation of privacy in a tweet sent around the world.[FN4] The court order is not unreasonably burdensome to Twitter, as it does not take much to search and provide the data to the court.[FN5] So long as the third party is in possession of the materials, the court may issue an order for the materials from the third party when the materials are relevant and evidentiary (18 USC \u00a72703[d]; People v Carassavas, 103 Misc 2d 562 [Saratoga County Ct 1980]).  \nConsider the following: a man walks to his window, opens the window, and screams down to a young lady, Im sorry I hit you, please come back upstairs. At trial, the People call a person who was walking across the street at the time this occurred. The prosecutor asks, What did the defendant yell? Clearly the answer is relevant and the witness could be compelled to testify. Well today, the street is an online, information superhighway, and the witnesses can be the third party providers like Twitter, Facebook, Instragram, Pinterest, or the next hot social media application.  \n[*4]2. The Court Order, Federal Law and New York State Law  \nThe second issue is whether the court order was a violation of the Fourth Amendment, the Federal Stored Communications Act, or any other New York law.  \nThe Fourth Amendment  \nTo establish a violation of the Fourth Amendment, the defendant must show either (1) a physical intrusion onto defendants personal property; or (2) a violation of a defendants reasonable expectation of privacy. (see United States v Jones (132 S Ct 945, 950 [2012]; Kyllo v United States, 533 US 27, 33 [2001] .) In Jones (id. at 949), the U.S. Supreme Court held that the governments installation of a Global Positioning System (GPS) tracking device on a targets vehicle to obtain information was a physical intrusion on a constitutionally protected area. In People v Weaver (12 NY3d 433 [2009])theNew York Court of Appeals heldthat the placing of a GPS tracking device inside the bumper of the defendants vehicle, by a state police investigator, was a physical intrusion. However, in this case there was no physical intrusion into the defendants Twitter account. The defendant had purposely broadcast to the entire world into a server 3,000 miles away. Therefore, the defendants account is protected by the Fourth Amendment only if the government violated a subjective expectation of privacy that society recognizes as reasonable. (see Kyllo v United States, 533 US 27, 33 [2001], citing Katz v United States, 389 US 347, 361 [1967]).[FN6] \nThe Supreme Court has repeatedly held that the Fourth Amendment does not protect information revealed by third parties. (see United States v Miller, 425 US 435, 443 [1976].) Several courts have applied this rationale and held that internet users do not retain a reasonable expectation of privacy. In Romano v Steelcase Inc., (30 Misc 3d 426 [Sup Ct, NY County 2010])the court held that users would logically lack a legitimate expectation of privacy in materials intended for publication or public posting.[FN7] \nIf you post a tweet, just like if you scream it out the window, there is no reasonable expectation of privacy. There is no proprietary interest in your tweets, which you have now gifted to the world. This is not the same as a private email, a private direct message, a private chat, or any of the other readily available ways to have a private conversation via the internet that now exist. [*5]Those private dialogues would require a warrant based on probable cause in order to access the relevant information.  \nInterestingly, in 2010, Twitter signed an agreement with the Library of Congress providing that every public tweet from Twitters inception and beyond would be archived by the Library of Congress.[FN8] Also, Twitters Privacy Policy states in part:    \nOur Services are primarily designed to help you share information with the world. Most of the information you provide us is information you are asking us to make public. This includes not only the messages you Tweet and the metadata provided with Tweets, such as when you Tweeted, but also the lists you create, the people you follow, the Tweets you mark as favorites or Retweet, and many other bits of information that result from your use of the Services. (see Twitter, Twitter Privacy Policy https:\/\/twitter.com\/privacy [accessed June 11, 2012].)There is no reasonable expectation of privacy for tweets that the user has made public. It is the act of tweeting or disseminating communications to the public that controls. Even when a user deletes his or her tweets there are search engines available such as Untweetable, Tweleted and Politwoops that hold users accountable for everything they had publicly tweeted and later deleted.[FN9] \nTherefore, the defendants Fourth Amendment rights were not violated because there was no physical intrusion of the defendants tweets and the defendant has no reasonable expectation of privacy in the information he intentionally broadcast to the world.  \nStored Communications Act The SCAs requirements for a court order states that:  \nA court order for disclosure under subsection (b) or ( c)....shall be issued only if the government entity offers specific and articulate facts showing that there are reasonable grounds to believe that the contents of a wire or electronic communication, or the records or other information sought, are [*6]relevant and materials to an ongoing criminal investigation. (Emphasis added) (see 18 USC \u00a72703[d]).  \nThe defendants anticipated trial defense is that the police either led or escorted him onto the non-pedestrian part of the Brooklyn Bridge, a defense allegedly contradicted by his publicly posted tweets around the time of the incident. In Harris, (id. at 7-8) the court held that the information sought was relevant. The April 20, 2012 court order was issued to comply with the January 26, 2012 subpoena.  \nThe People are seeking two types of information, non-content information such as subscriber information, e-mail addresses, etc. and content information such as tweets. The SCA protects only private communications[FN10] and allows disclosure of electronic communication when its not overbroad.[FN11] \nIn general, court orders have no limitations on the types of information to be disclosed (18 USC \u00a72703[d]). The SCA mandates different standards that the government must satisfy to compel a provider to disclose various types of information (18 USC \u00a72703). To compel a provider of ECS to disclose contents of communication in its possession that are in temporary electronic storage for 180 days or less, the government must obtain a search warrant (18 USC \u00a72703[a]). A court order must compel a provider of ECS to disclose contents in electronic storage for greater than 180 days or to compel a provider of RCS to disclose its contents (18 USC \u00a72703[a], [b], and [d]). The law governing compelled disclosure also covers the above mentioned non-content records. The rules are the same for providers of ECS and RCS and the government can obtain a \u00a72703(d) order to compel such non-content information (18 USC \u00a72703 [c][1][B]).  \nThe non-content records such as subscriber information, logs maintained by the network server, etc. and the September 15, 2011 to December 30, 2011 tweets are covered by the court order. However, the government must obtain a search warrant for the December 31, 2011 tweets.  \nThe scope of a subpoena duces tecum is sufficiently circumscribed when: (1) the materials are relevant and evidentiary; (2) the request is specific; (3) the materials are not otherwise procurable reasonably in advance of trial by the exercise of due diligence; (4) the party cannot properly prepare for trial without such a production and inspection in advance of trial and the failure to obtain such inspection may tend unreasonably to delay the trial; and (5) the application is made in good faith and is not intended as a general fishing expedition (People v Carassavas, 103 Misc 2d 562 [Saratoga County Ct 1980], citing People v Price, 100 Misc 2d 372, 379 [1979]). The District Attorney seeks the subpoenaed information to refute Harriss anticipated trial defense. In Harris, (id. at 7-8) the court agreed that the subpoena duce tecum was sufficiently circumscribed and a court order was issued on April 20, 2012 to comply with the subpoena.  \nOn May 31, 2012 David Rosenblatt, a member of Twitters Board of Directors, was personally served within New York County with a copy of this courts April 20, 2012 order, a copy of the January 26, 2012 trial subpoena, and a copy of the March 8, 2012 trial subpoena. There are no jurisdictional issues and there are no violations of the New York Constitution.  \nIn dealing with social media issues, judges are asked to make decisions based on statutes that can never keep up with technology.[FN12] In some cases, those same judges have no understanding of the technology themselves (Stephanie Rabiner, Esq., Technologist, Do Judges Really Understand Social Media? http:\/\/blogs.findlaw.com\/technologist\/2012\/05\/do-judges-really-understand-social-media.html [May 9, 2012]). Judges must then do what they have always done - balance the arguments on the scales of justice. They must weigh the interests of society against the inalienable rights of the individual who gave away some rights when entering into the social contract that created our government and the laws that we have agreed to follow. Therefore, while the law regarding social media is clearly still developing, it can neither be said that this court does not understand or appreciate the place that social media has in our society nor that it does not appreciate the importance of this ruling and future rulings of courts that may agree or disagree with this decision. In recent years, social media has become one of the most prominent methods of exercising free speech, particularly in countries that do not have very many freedoms at all.  The world of social media is evolving, as is the law around it. Society struggle with policies, whether they are between student and teacher (NYC Department of Education, NYC Department of Education Social Media Guidelines),[FN13] or the right of a company to examine an applicants Facebook page as part of the interview process (Bill Chappell, State Approves Bill to Ban Employers From Seeking Facebook Login Info, http:\/\/www.npr.org\/blogs\/thetwo-way\/2012\/04\/10\/150354579\/state-[*8]approves-bill-to-ban-employers-from-seeking-facebook-login-info). As the laws, rules and societal norms evolve and change with each new advance in technology, so too will the decisions of our courts. While the U.S. Constitution clearly did not take into consideration any tweets by our founding fathers, it is probably safe to assume that Samuel Adams, Benjamin Franklin, Alexander Hamilton and Thomas Jefferson would have loved to tweet their opinions as much as they loved to write for the newspapers of their day (sometimes under anonymous pseudonyms similar to todays twitter user names). Those men, and countless soldiers in service to this nation, have risked their lives for our right to tweet or to post an article on Facebook; but that is not the same as arguing that those public tweets are protected. The Constitution gives you the right to post, but as numerous people have learned, there are still consequences for your public posts. What you give to the public belongs to the public. What you keep to yourself belongs only to you.  \nAccordingly, the motion to quash is granted in part and denied in part. The court finds in favor of the People for all non-content information and content information in ECS and RCS from September 15, 2011 to December 30, 2011. However, ECS content information less than 180 days old (tweeted on December 31, 2011) may only be disclosed pursuant to a search warrant, and the court decision in People v Harris is so modified. That search warrant should be requested of a judge of competent jurisdiction. However, to avoid any issue of alleged non-impartiality, that warrant should be made to another judge of this court.  \nAccordingly, it is hereby:  \nORDERED, that Twitter disclose all non-content information and content information from September 15, 2011 to December 30, 2011; and it is further  \nORDERED, that the materials be provided to this court for in camera inspection. The relevant portions thereof will be provided to the office of the District Attorney, who will provide copies to the defense counsel as part of discovery; and it is further  \nORDERED, that the clerk of this court notify the Presiding Judge of Jury 2 of the receipt of the materials.  \nThis opinion shall constitute the decision and order of the Court.  \nNew York, New YorkMatthew A. Sciarrino, Jr.  \nJudge of the Criminal Court  \n  \nFootnote 5: The general New York rule is that only the recipient of a subpoena in a criminal case has standing to quash it. (see People v Lomma, 2012 WL 309327 at *5-6 [Sup Ct, NY County 2012], citing People v Doe, 96 AD2d 1018, 1019 [1st Dept 1983] [banking and telephone records]; People v Crispino, 298 AD2d 220, 221 [1st Dept 2002] [defendant, as a customer, has no proprietary interest in the defendants bank account records]).   \nFootnote 6: See also, People v. Suleman, (NYLJ July 13, 2011 at *1 [Crim Ct, NY County] [Decided on 6\/22\/2011]) where the court held that the taxicab owner had no reasonable expectation of the information generated and stored by a GPS device in the cab.  \nFootnote 7:Twitter argues that the court should embrace the holding in United States v Warshak, (631 F3d 266 [6th Cir 2010]). In Warshak, the court found that the defendant had a reasonable expectation of privacy in his e-mails. However, the Warshak case is distinguishable from the case at hand because the former deals with private e-mails as opposed to public postings. Warshak did not address public communications at all; instead the court held only that e-mail requires strong protections under the Fourth Amendment.(Warshak, 631 F3d at 286). If such Fourth Amendment protections were to extend to public postings, it would undermine the very basis of the Warshak holding.    \nFootnote 8: (See Matt Raymond, Library of Congress, How Tweet It Is!: Library Acquires Entire Twitter Archive,  http:\/\/blogs.loc.gov\/loc\/2010\/04\/how-tweet-it-is-library-acquires-entire-twitter-archive\/ [accessed May 30, 2012]). The Twitter community received the initial heads up via their own feed @librarycongress. Twitter has its users consent for disclosure to the Library of Congress by virtue of its Private Policy. The Library of Congress archives is not yet available due to its high volume of composition of billions of tweets, and with an estimate of 140 million new tweets per day. (see Audrey Watters, How the Library of Congress is Building the Twitter Archive, http:\/\/radar.oreilly.com\/2011\/06\/library-of-congress-twitter-archive.html [accessed June 11, 2012].)    \nFootnote 9: See http:\/\/untweetable.com;http:\/\/tweleted.com\/ and http:\/\/mashable.com\/2012\/05\/  \n30\/poliwoops\/.    \nFootnote 10: (See Kaufman v Nest Seekers, LLC, 2006 WL 2807177 at *5 [SDNY 2006] [Only electronic bulletin boards which are not readily accessible to the public are protected under the SCA]; Knop v Hawaiian Airlines Inc., 302 F3d 868, 875 [9th Cir 2002][The legislative history of the Electronic Communications Protection Act suggest that Congress wanted to protect electronic communication that are configured to be private, such as e-mail and private electronic communications.]; Snow v DirecTV, Inc., 450 F3d 1314, 1320-21 [11th Cir 2006] [holding that the SCA does not apply to materials that is readily available to the public.]    \nFootnote 11: Orin Kerr, Comment, A Users Guide to the Sored Communications Act, and the Legislators Guide to Amending It, 72 Geo Wash L Rev 1208 [2004].  \nFootnote 12: The SCA was enacted in 1986 and mainly applied to the start of e-mails. The SCA was enacted long before the creation of Twitter and the concept of blogging which started in 2006.    \nFootnote 13: http:\/\/schools.nyc.gov\/NR\/rdonlyres\/BCF47CED604B-4FDDB752DC2D81504478\/0\/","item_date":"Jul 02 2012 19:09:02","display_item_date":"07-02-2012","url":"http:\/\/www.nycourts.gov\/reporter\/3dseries\/2012\/2012_22175.htm","source":"www.nycourts.gov"},{"title":"Why Your Complaint About Twitter Is Wrong","details":"I know I usually try to be a thoughtful tech writer, but sometimes, holy shit you guys.\n    Twitter, because of their API, actually was a real-time protocol to connect various services in a novel way. I had debates with my other tech-nerd friends about whether Twitter could be one of the fundamental building blocks of the Internet via their powerful API. ... In this scenario, Twitter would have turned into something like a realtime cloud API company. \n    Thats Dalton Caldwell (with my emphasis added), who is a very nice guy, but does nothing to break the pattern that everything I read on the Svbtle network exists solely to infuriate me for no good reason. I even tend to agree with him, and thats why its worth questioning our conventional wisdom.\n  Heres the thing: I love the idea of a realtime cloud API company! Im that dude. I write long, rambly blog posts about it, just like I did about Twitter itself, back when it was young. I love this kind of idealism.\n  But. Nobody wants a realtime cloud API company. I mean, I want one, but speaking from a statistical standpoint, that isnt what any normal person wants. For those who are geeky enough to want something, it ends up looking like Urban Airship or any one of the many other delivery as a service startups. Those realtime delivery thingys are awesome, but nobody would argue that they become the kind of household name brands that one represents entirely with a pictographic bird logo.\n  So why are smart folks like Dalton writing things like this? Why is Nova Spivack talking about a Twitter API problem? Because, in addition to some worthwhile technical requests, theyre lamenting that Twitter isnt just for geeks anymore. This isnt some nefarious plan by the tyrannical cabal that controls Twitter to create a Horrible Commercialized Network For Kardashians; Its a result of the fact that so many normal people showed up to use the service.\n  Geeks are lamenting that they dont dominate and control this network, and expressing it in the only way we know how: Through technological triumphalism. If the culture of a giant network doesnt resemble the culture we prefer, then it must be a problem that can be solved by making the network more technically complicated.\n  What About The Open Web, Maaan?\n  Dont get me wrong; I would love if it made sense for Twitter to be some hippie utopian open protocol that also happened to support a multi-billion dollar company. Thatd be great. But the amount of Kremlinology and hand-wringing over one short blog post from Michael Sippey that Ive seen in the past few days reveals that peoples concerns are not about what Twitter is doing, but rather the core technical communitys own feelings about the fact they dont determine what Twitter is anymore.\n  Now, full disclosure, Michael Sippeys a friend and we worked together for more than half a decade. I havent talked to him about his blog post, but this is a guy who was onstage with Steve Jobs at the original launch of the app store for the iPhone. Hes not some crazy kid who doesnt understand how platforms work!\n  Yet weve got a lot of people using Aaron Whites post as an example of Twitters new clampdown on developers. Ill say this, because its not Aarons day job and he has other projects going on: His app TweetFavor should be shut down. Its an app for prompting others to robo-tweet about a project. It encourages people to repost crappy, spammy tweets, and thats when its working properly. Now, Aaron did it as a quick hack to show off some tech, so I understand he was just scratching an itch, but man am I glad I dont have to read what that app would output in my timeline.\n  The other big example being used to raise alarms about Twitters new direction? The disconnection of tweets from LinkedIn. Okay, show of hands, who loves that LinkedIn tweet integration? Whos gonna say Twitter sucks for taking away that awesome read-tweets-in-LinkedIn experience?\n  It\u2019s inspiring to know Twitter\u2019s pursuing SO many different ways to suck faster.Takes some serious vision to ruin something this awesome.\n\u2014 Merlin Mann (@hotdogsladies) July 1, 2012 Im no expert, but I didnt think Merlin was that big a fan of LinkedIn. Huh.\n  Its about the ecosystem!\n  The most insidious and wrong-headed objections to Twitters not-yet-disclosed future moves is the idea that somehow Twitters moves are affecting the diverse and flourishing ecosystem around Twitters API. Now, to be clear: The company needs to address uncertainty and doubt around their API intentions in order to make developers feel safe.\n  But diversity of the developer community? Lets take a look. Lots of people keep pointing to Tweetbot as an example of the kind of great third-party development that encourages a diverse ecosystem of Twitter developers.\n  Heres geek-beloved Tweetbot developer Paul Haddad on the diversity he wants to see from the developer community:\n  So all the folks pushing the women in tech issue are equally committed and supportive of men in nursing, right? twitter.com\/tapbot_paul\/st\u2026\n\u2014 Paul Haddad (@tapbot_paul) May 25, 2012 Heres Twitters statement on the topic from last week:\n  We are working with Girls Who Code, a new program that will empower high school girls to pursue a career in technology. blog.twitter.com\/2012\/06\/workin\u2026\n\u2014 Twitter (@twitter) June 26, 2012 Yes, why indeed isnt Twitter taking hints from this community about how to encourage more diversity amongst developers? If you want a diverse set of applications in an ecosystem, you have to have a diverse community of developers. Right now, the apps championed as innovators in the narrow, legacy tech community around Twitter are visibly fighting against those new voices entering the community. Is it any wonder why?\n  Sure, Twitters made lots of mistakes with their ecosystem. But their track record of keeping it vibrant and growing is a lot better than most of the critics, and reflects a user focus that few other companies have. They can absolutely do a better job of making their branding consistent, but Id rather have a few dusty corners in some Twitter apps than be cobbling together a hodgepodge of apps from developers who want to close the door behind themselves.","item_date":"Jul 02 2012 19:02:05","display_item_date":"07-02-2012","url":"http:\/\/dashes.com\/anil\/2012\/07\/why-your-complaint-about-twitter-is-wrong.html","source":"dashes.com"},{"title":"Sharing Professional Content on LinkedIn and Twitter","details":"LinkedIn and Twitter have worked together since 2009 to enable the sharing of professional conversations on both platforms. Since this relationship began, some of you chose to sync your LinkedIn and Twitter accounts to share your professional content, knowledge and expertise.\n\nAs Twitter shared earlier today in a blog post from Michael Sippey, they are increasingly focused on \u201cproviding the core Twitter consumption experience through a consistent set of products and tools.\u201d Consistent with Twitter\u2019s evolving platform efforts, Tweets will no longer be displayed on LinkedIn starting later today. We know many of you value Twitter as an additional way to broadcast professional content beyond your LinkedIn connections. Moving forward, you will still be able to share your updates with your Twitter audience by posting them on LinkedIn.\n\nHow can I continue to share updates on both LinkedIn and Twitter?\n\nInitiate the conversation on LinkedIn. Simply compose your update, check the box with the Twitter icon, and click \u201cShare.\u201d This will automatically push your update to both your LinkedIn connections and your Twitter followers just as you\u2019ve been able to do previously.\n\nWhat changes can I expect to see on LinkedIn?\n\nIf you had previously synced your LinkedIn and Twitter accounts, and selected the option to share Tweets on LinkedIn, those Tweets generated from Twitter will no longer appear on LinkedIn.  There will be no other changes to your LinkedIn experience.\n\nProfessional conversations are happening every day on LinkedIn, and we encourage you to take part in these dialogues. If you have any further questions about what this means for your synced LinkedIn and Twitter accounts, please visit our Help Center.","item_date":"Jul 02 2012 17:12:50","display_item_date":"07-02-2012","url":"http:\/\/blog.linkedin.com\/2012\/06\/29\/sharing-on-linkedin-twitter\/","source":"blog.linkedin.com"},{"title":"The US hosts 43% of the world\u2019s top 1 million websites","details":"It should come as no surprise that the United States is the world leader in hosting websites. But just how many of the world\u2019s top sites are hosted in the US?\u00a0Following our study of web hosting in Africa, we now broaden the perspective and can announce that 43% of the world\u2019s top 1 million websites are hosted in the United States.\n Top 20 web hosting countries\n After checking the IP addresses of the top 1 million websites as ranked by Alexa, we put them in order of how many websites are hosted in each country. You can read a bit more about our methodology at the end of the article.\n In total, we identified 191 countries as hosting one or more sites in the top 1 million, with the following 20 in the top:\n \n As you can see, the US is in a clear lead with 431,214 sites, followed by Germany with 82,152, China with 49,730, UK with 37,829, and France with 35,777. In 20th place we find South Korea with 5,659 websites hosted out of the top 1 million.\n This is certainly a long tail if there ever was one \n Europe vs US\n With the US in such a commanding lead, it would, of course, be interesting to see how it compares to other parts of the world. One of the obvious comparisons would be with Europe. Here\u2019s what that comparison looks like:\n \n Even compared to the countries in Europe combined, the US is still considerably larger, with 431,214 sites compared to the 313,252 in Europe.\n You may have noted that Europe and the US together claim around 75% of the top 1 million sites.\n Top 100 countries\n Here are the top 100 countries, which host the most sites in the top 1 million:\n Top 100 web hosting countries  Country Number of sites hosted \n1  United States  431,214  \n2  Germany  82,152  \n3  China  49,730  \n4  United Kingdom  37,829  \n5  France  35,777  \n6  Japan  35,138  \n7  Russia  31,134  \n8  Netherlands  24,214  \n9  Canada  18,218  \n10  Italy  13,293  \n11  Spain  13,047  \n12  Brazil  10,481  \n13  Poland  9,989  \n14  Turkey  7,721  \n15  Australia  6,669  \n16  Ukraine  6,537  \n17  Czech Republic  6,257  \n18  India  6,255  \n19  Hong Kong  5,730  \n20  Korea (South)  5,659  \n21  Sweden  5,243  \n22  Singapore  4,276  \n23  Thailand  4,190  \n24  Ireland  4,122  \n25  Denmark  4,037  \n26  Switzerland  4,022  \n27  Romania  3,746  \n28  Viet Nam  3,434  \n29  Hungary  3,135  \n30  Indonesia  3,002  \n31  Taiwan  2,741  \n Alexa may not be the definitive list to go by, but, at the very least, it should give a good indication of what the real numbers are. We doubt anyone is surprised that the US is in the lead, but it\u2019s interesting to see how the countries seem to jockey for\u00a0positions. Also, we just looked at the top 1 million sites, which is just a small fraction of all websites that exist.\n We will keep an eye on this issue and see how it develops over the coming months and years.\n Do you think that, over time, the US will lose some of its importance in terms of web hosting and we\u2019ll see more top sites hosted outside the US? Let us know in the comments below.\n About the data: The data was collected June 6-8, 2012, with a Pingdom-developed script that successfully scanned 945,472 out of Alexa\u2019s top 1 million sites. The remaining sites failed for various reasons. We should point out that web hosting is a constantly moving target, so where a particular site is hosted may have changed by the time you read this.\n Image (top) via Shutterstock.","item_date":"Jul 02 2012 07:00:00","display_item_date":"07-02-2012","url":"http:\/\/feedproxy.google.com\/~r\/RoyalPingdom\/~3\/i-NnYnOxOm4\/","source":"feedproxy.google.com"},{"title":"3D Printing Plane Parts with Shapeways","details":"Check out this video by BSAV showing the\u00a0excitement\u00a0of receiving his first 3D Printing from Shapeways. \u00a0Ben has 3D Printed his part in White Strong Flexible (Nylon) bit it would also be awesome to see this 3D Printed in polished Alumide for a little more stiffness and less drag with the smoothness.","item_date":"Jul 02 2012 07:00:00","display_item_date":"07-02-2012","url":"http:\/\/www.shapeways.com\/blog\/archives\/1470-3D-Printing-Plane-Parts-with-Shapeways.html","source":"www.shapeways.com"},{"title":"The Age of the App: More Speed, More Simplicity, and More Backends","details":"As I sit here on my first day at Kinvey, on this comfy new chair and with my fancy new computer, I have trouble holding back my excitement for the future of Kinvey. I am surrounded by incredibly talented people that I expect to humble me on a daily basis with their raw talent and knowledge. I hail from Milwaukee, WI, the land of the cheeseheads. Having been a developer for over 5 years in a variety of web technologies, I am excited to continue the path with Node.js at its core. I have worked in full stack development for my entire career, from software architecture to standard css changes.\n              \n As I sit and begin to dig into the code for the developer console, I think about this age we live in. The Age of the App: the age where it is faster to get to the information you want by pulling out your phone and opening the designated app instead of doing so on the computer. Consider a simple example of getting the weather. On my phone the weather is sitting on my lock screen always telling me before I even ask for it. However, if I want it on my computer I need to open up a web browser, go to Google and type in weather boston. This is just for a simple idea like the weather - now try to get your flight information on your airlines website. Instead, you have an app that has all your ticket information and notifies you when there is a delay.\nConsider then, the absurd idea that most apps that are made have no backend. I just recently upgraded my phone and the process of setting up all my apps again was painful. If only the app developers utilized a system like Kinvey to save the settings for my user in the cloud so I could simply sign back in and have all my configuration done already. This is why I work at Kinvey.\n  I work to make it incredibly simple to get your app running with Kinvey and to analyze the information you need. I make your experience clean, direct and above all easy. For example, one of my first tasks is to get data importing from a raw JSON file instead of just a simple CSV. I look forward to taking your backend and putting the shine on it to make it nice and pretty to be displayed to the world. But we can\u2019t do it without your help! I challenge you to point out where we are doing it right and where we could do better. At Kinvey, we got your backend, and Im excited to help make the front of your backend awesome.","item_date":"Jul 02 2012 07:00:00","display_item_date":"07-02-2012","url":"http:\/\/www.kinvey.com\/blog\/item\/181-the-age-of-the-app-more-speed-more-simplicity-and-more-backends","source":"www.kinvey.com"},{"title":"Mobile application provider TapCrowd closes \u20ac1,1 million of funding","details":"Ghent, Belgium \u2013 July 5th, 2012 - TapCrowd, an innovator in mobile application building announced today that it has successfully closed seed funding, totalling \u20ac M1.1.\n \u201cThis funding underlines our commitment to play a pivotal role in the mobile revolution and to build out our beachheads in Sao Paulo for the latin-american market and Istanbul for the asian market. We expect to take our first steps into the US market before the end of 2012\u201d, says Toon Vanparys, serial entrepreneur and chairman of the board at TapCrowd.\n Around the founders Brecht Fourneau and Miqu\u00ebl Vermeulen, we have created a management team with complementary skills, with both knowledge in software development and graphical design as experience in sales and marketing.  The entire management team contributed to this successful capital round. Next to the commercial investments our most important objective is the further expansion of our platform to profile mobile users\u201d, adds Niko Nelissen, board member at TapCrowd.\n Earlier this month TapCrowd signed a strategic agreement with Belgacom. The incumbent Belgian telco will use this platform for the creation of city and event apps under the brand Belgacom MyApp.\n With its recent expansion of the sales and marketing team in its headquarters in Ghent (Belgium) and start-up of points of sales in Sao Paulo (Brazil) and Istanbul (Turkey), TapCrowd has become a leading international player in the app-building platform industry.\n TapCrowd, established in 2009, develops and commercialises a self-service app building platform for mobile marketing of cities, professional- and leisure events, restaurants, car dealers, content publishers, retailers, shops and local businesses.","item_date":"Jul 02 2012 07:00:00","display_item_date":"07-02-2012","url":"http:\/\/www.tapcrowd.com\/news\/mobile-application-provider-tapcrowd-closes-%E2%82%AC11-million-funding","source":"www.tapcrowd.com"},{"title":"WebSpellChecker.net application version 3.8.1 has been released!","details":"","item_date":"Jul 02 2012 07:00:00","display_item_date":"07-02-2012","url":"http:\/\/www.webspellchecker.net\/blog\/?p=984","source":"www.webspellchecker.net"},{"title":"The ThingSpeak API now has a Ruby Gem, code on GitHub too","details":"If you use Ruby to write programs and apps,\u00a0Daniel Treacy\u00a0created a Ruby wrapper for the ThingSpeak API. This Gem makes it easy to access the ThingSpeak API inside of your Ruby code.\n For more information, check out our tutorial and visit GitHub for the full source code. Thanks Daniel!","item_date":"Jul 02 2012 07:00:00","display_item_date":"07-02-2012","url":"http:\/\/feedproxy.google.com\/~r\/internetofthings\/~3\/aUka-SHqIIg\/","source":"feedproxy.google.com"},{"title":"Smart Product API Extends Product Value, Leads To Smart Connected Landscape | Forrester Blogs","details":"In our February 13, 2012, \u201cMobile Is The New Face Of Engagement\u201d report, we talked about the important link between smart products and mobile apps. A key to that link is creating a smart product application programming interface (API) that allows third parties to easily write apps that tap into the data feeds from the connected offerings, extending the value of that product with an \u201capp ecosystem.\u201d\n As a precursor to an upcoming report that will lay out the smart connected product landscape and the unique combination of IT and product development skills required to build them, Forrester interviewed C\u00e9dric Hutchings, the general manager of Withings, a leader in the connected medical device segment.\n The highlights of the discussion with C\u00e9dric included:\n Company vision. The company seeks to improve the value of everyday devices through connectivity and apps.\n Role of API. An API enables different services that could not be built in-house; it makes it easy for third parties to get data flow and integrate it into app. As a result, Withings has an ecosystem of more than 40 third-party apps that integrate with its Wi-Fi-connected bathroom scale.\n Cloud value proposition. A personal wellness data dashboard allows consumers to manage health across a range of devices and inputs\/apps from Withings and other companies.\n Smart product skill requirements. These requirements include a mix of user experience, embedded software\/product development talent, traditional IT web, database and middleware competencies, and partner management liaison capabilities.\n Staff.The spilt is 50% classic product engineering and 50% IT software and development. The IT-centric-staff are key to building the cloud-based engagement platform with its underlying database, web services, and analysis.\n \nIf you are interested in participating in our upcoming research, click participate in report under Developing Smarter Products, or email my colleague Michael Yamnitsky (First initial - last name - at Forrester.com).","item_date":"Jul 02 2012 05:01:53","display_item_date":"07-01-2012","url":"http:\/\/blogs.forrester.com\/john_mccarthy\/12-06-30-smart_product_api_extends_product_value_leads_to_smart_connected_landscape?cm_mmc=RSS-_-IT-_-71-_-blog_122","source":"blogs.forrester.com"},{"title":"A Solution to the Twitter API Problem","details":"Minding The Planet is Nova Spivack\u2019s weblog on emerging technologies and trends. It focuses on ways in which the Web is becoming more present, personalized and precise.   \t\t\t\t\n\n  \t\t\t\tFollow:\n\n  \t\t\t\t\n\n  \t\t\t\tNova Spivack is a technology futurist, entrepreneur, angel investor, and a leading voice on search, collective intelligence and the Semantic Web. More...\n\n  \t\t\t\n\nI am writing this article because I love Twitter, and I\u2019ve built a business around making sense of Twitter data.\n\n I\u2019m concerned by the \u201cominous\u201d signs that they may be heading down a path that betrays the trust and goodwill given to them by millions of users and developers.\n\n I want Twitter to achieve it\u2019s potential, and to be as successful as possible, and I believe those goals are compatible with living up to its promises to be an open ecosystem. I see a better way forward.\n\n In this article I am going to present some observations about Twitter\u2019s policies, what might happen if they abandon them, and what I think would be a better way for Twitter to monetize.\n\n My proposed solution will make Twitter\u2019s advertising people happy and will catalyze tremendous revenue growth for the company \u2014 possibly enabling it to grow orders of magnitude faster and larger than it can by becoming a closed platform.\n\n Rather than closing the APIs down, there is a way for Twitter to benefit massively by making them even more open, but with a built-in monetization strategy.\n\n This will transform Twitter\u2019s vast third-party app ecosystem into a massive ad network for Twitter overnight, enabling those apps to build value around Twitter that ultimately helps Twitter monetize. Twitter will almost instantly become as big as AdSense, Doubleclick and Facebook\u2019s ad network.\n\n It\u2019s a slam dunk for Twitter, and for the ecosystem as a whole. Everyone wins.\n\n Please make sure to read the last sections of this article where I explain this proposal in detail.\n\n If you agree with the points raised below, please make sure that others hear about this article by retweeting it. And in particular, you\u00a0\u00a0should email and or tweet this article to Twitter\u2019s leadership so they can hear directly from you on this issue. It\u2019s important for Twitter to know that (a) we all love Twitter, and (b) we have a proposal for how to make Twitter better that also achieves Twitter\u2019s goals for more ad revenue.\n\n So if you like what I propose below, please share it with @dickc @adambain @jack and @sippey and include your thoughts too.\u00a0I\u2019m sure they would be interested to hear from you on this important issue.\n\n Once again the Web development community is up in arms about the possibility that Twitter might clamp down on the use of their APIs and data. How did we get here, again?\n\n At this point it is still mostly FUD, due to a vague yet alarming blog post by Twitter\u2019s product lead, indicating unspecified coming \u201cchanges\u201d to their API terms. Hopefully Twitter will clarify this post soon.\n\n Developers are understandably skittish and cynical due to a history of Twitter carving away API rights. And there is a widespread and growing feeling of betrayal among third-party app providers.\n\n Dave Winer summarized the current situation well:\n\n @davewiner: The mistake we all made with Twitter, me too, was to think a corporate API could act like an open protocol.\n\n But, it wasn\u2019t just that we all made a mistake. In fact, as many have pointed out, Twitter built their business by promising to be \u201copen.\u201d They still promise that. It says so right in their API terms:\n\n \n\n Note the use of the words \u201copen platform\u201d in this excerpt. What does that actually mean? Twitter doesn\u2019t define it, but there is plenty of precedent for how we should understand that and the intent of the agreement that it indicates.\n\n But Twitter doesn\u2019t only say this in their API terms, they also state it in the terms of use for users who contribute their content, the lifeblood of Twitter.\n\n For example, consider the Twitter user agreement terms, where you grant Twitter a non-exclusive license to your Tweets:\n\n \n\n Here Twitter makes it clear that the intent of granting them access to your intellectual property is so that they can help you make it \u201cavailable to the rest of the world.\u201d\n\n Furthermore, consider this \u201cTip\u201d in the same agreement, where Twitter states that they are creating an \u201copen ecosystem\u201d with \u201cyour rights in mind\u201d \u2014 and \u201cwhat\u2019s yours is yours \u2014 you own your content\u201d\u2026\n\n \n\n It is quite clear that all of us who have been contributing our intellectual property to Twitter, have been doing so under the understanding and explicit agreement that Twitter would provide an \u201copen ecosystem\u201d so that our data could be freely accessible (to us and to others).\n\n I wonder to what degree these promises are legally binding, and whether Twitter can be held to them by users and third-party developers? It\u2019s an interesting question for the legally inclined out there to analyze.\n\n Can Twitter change the terms now, after allowing so many users and developers to proceed under totally different assumptions? Probably they can, but it\u2019s not good corporate citizenship to do that. Furthermore, I think there is a better path for Twitter (see my proposal below).\n\n What If Twitter Changes the API Terms to Be More Restrictive?\n\n Given that Twitter\u2019s API terms and user terms have long promised openness, it does raise a question: What legal rights does Twitter actually have to change those terms retroactively, this late in the game?\n\n And what constrains may exist if they do make changes? What happens to all the content that was contributed under the old terms? What about all the apps that integrated under the old terms?\n\n Twitter reserves the right to modify their terms at any time. However, any content that was contributed under the former license terms may actually still have to be provided under those historical terms, not the new terms. But this is a legal question.\n\n Can Twitter retroactively change the terms under which previous content was licensed to them by users, without the express permission of those content creators?\n\n Of course any new content contributed to Twitter after such a change could be under the new terms, but users would need to be asked to agree to the new terms before they could be held to them. Twitter would at the very least need to ask all users to sign a new terms of use agreement for their content. This would be the proper, transparent, way to proceed.\n\n There\u2019s one more thing to consider: even if Twitter wanted to, it would be difficult in practice to stop all third-party apps and clients from accessing their APIs and data. Restrictions of this nature are hard to define, and hard to enforce. It\u2019s a fuzzy line \u2014 what exactly is \u201creplicating\u201d Twitter\u2019s services, and what is \u201caugmenting\u201d them?\n\n Furthermore, Twitter\u2019s own Website and their own mobile apps use their APIs and it would not be hard to take advantage of that fact to scrape the API and get at their data in various ways. I think numerous workarounds would emerge \u2014 if Twitter forced the ecosystem to find them.\n\n The Power of \u201cWe The Product\u201d\n\n As someone said, if you are giving your data to a service and using that service for free, then \u201cyou are the product.\u201d\n\n That is a fact that most people don\u2019t realize. It\u2019s not \u201cWe The People\u201d anymore, it\u2019s \u201cWe The Product\u201d and if we all banded together, We The Product could exert some influence over those who are monetizing us. I hope that happens soon.\n\n This whole area of what rights and responsibilities Twitter (and for that matter, Facebook and Google+ and LinkedIn and other social networks and data aggregators) have around the content that We the Product have licensed to them is an open question that should be explored further by people with more knowledge of intellectual property law.\n\n It is possible for example that Twitter may not even have the right to restrict access to the content we granted them a non-exclusive license too \u2014 that right to restrict it was not explicitly granted to them. Yes they can restrict the use of their API, but if they hold the only copy of our intellectual property, they may have to at least provide another way to get it (for us, and anyone else).\n\n But it\u2019s unclear, and I hope those of you out there with more knowledge of what the law says about these questions will drill in and comment.\n\n What About Data Portability?\n\n One thing is certain: If Twitter is seriously considering becoming a closed platform that is a failure pattern \u2013 it will not work, in fact it will backfire. It will result in Twitter losing their position instead of gaining more marketshare.\n\n It will also make it even more important to get your data out of Twitter, because Twitter has the only copy of your Tweets. Nobody in their right mind would want their personal lifestream locked in any proprietary silo where they can\u2019t get it out.\n\n Despite promises of being open, and promises that \u201cyou own your data,\u201d in fact, anyone who has contributed any data to Twitter is currently a hostage:\u00a0Twitter has your data and you can\u2019t have it.\n\n Although Twitter\u2019s terms of use only grant Twitter a non-exclusive license to your data, they currently have the only copy of it (for most people) and they do not provide any way to get that data back, except through Twitter. Should they be required to?\n\n It is interesting to note that despite previous promises to change this situation by Twitter reps, today, if you have more than 3200 Tweets in your history you cannot actually see them all, because Twitter does not show more than that number of Tweets from any timeline.\n\n For example, here is what Twitter\u2019s FAQs say about this now:\n\n \n\n Unfortunately there is no way to get your old Tweets out of Twitter. There was a time when Twitter representatives publicly stated that they would address this issue \u201csoon.\u201d But that intention has seemingly fallen by the wayside, or become a victim of Twitter\u2019s changing API strategy.\u00a0After all, if you can\u2019t get your data out of Twitter, you certainly can\u2019t leave the service.\n\n But locking people\u2019s content into a product is not the best way to retain them. Ideally they should stay because they want to not because they have to.\n\n In fact, platform lock-in has failed as a strategy for most big companies that tried it \u2014 and today that\u2019s why we have things like open standards, open protocols, cloud computing, etc.\n\n One solution, at the very least, is that Twitter should be joining the Data Liberation project, like Google did. Or at least providing a way to get your data out, like Facebook does. But they haven\u2019t and there are no signs that they will. That\u2019s a problem that has to be rectified.\n\n But simply giving people their own content back is not enough.\n\n Twitter has an opportunity to be everyone\u2019s personal API \u2014 they could (and should) make it easy for anyone, and any app, to get your data (with your permission), through their API.\u00a0It\u2019s a huge opportunity for leadership in the future.\n\n If they don\u2019t do this, someone else will and Twitter will become just another silo. Silos are the problem that the Internet exists to solve. Please don\u2019t be a silo, Twitter!\n\n Twitter is currently in control of a key piece of human communications infrastructure \u2014 the global short messaging infrastructure.\n\n This infrastructure has become so important to humanity that regime changes now are catalyzed by it. In short this is a critical technology for our civilization today.\n\n Is it safe that one company is in control of this infrastructure? Definitely no. No matter what company it is, it\u2019s risky to have this critical infrastructure go through one single point of control.\n\n First of all, centralizing something so important means that there is a single point of failure. It\u2019s too easy for a regime or hacker to attack it and shut it off in certain areas, or for everyone.\n\n Secondly, by mixing a critical piece of communications infrastructure with commercial interests there are serious risks to free speech \u2014 because governments for example can exert economic and legal pressure against the owner of the infrastructure. It is much safer for everyone for such an infrastructure to be federated and outside of any one party\u2019s control.\n\n But there\u2019s not much we can do about this until a suitable open alternative is provided (and that can\u2019t happen soon enough!). There really needs to be a free, open, federated short messaging infrastructure. There are in fact some contenders for this, but none have caught on yet.\n\n Every app that integrates with Twitter (and Facebook, for that matter) should be integrating with such a network ASAP, as an insurance policy.\u00a0All messages sent to Twitter and Facebook should be mirrored onto an open network now, so that they can be accessed either way. Apps could continue to use Twitter for messaging but could immediately cut over to the alternative network at any time.\n\n What network exists that would be suitable for this, and how could we get everyone to standardize on this now? I\u2019m curious to hear a cogent case for a solution. Why haven\u2019t the existing contenders gotten traction?\n\n In any case, Twitter\u2019s role as our present-day global short messaging network is a special kind of role. It\u2019s a privilege, and it comes with a responsibility to be a good steward.\n\n Twitter is extremely fortunate to have the exclusive first-right to monetize this infrastructure, and I for one am happy for them to have this opportunity, provided that they behave as good stewards of the network and of our data.\n\n If Twitter behaves as a good steward of our network and our data, they can keep that role. But if they abuse that privilege they will ultimately lose it. The world will take it away from them. The Internet will route around them.\n\n For one thing, if Twitter becomes more closed, or abuses this stewardship role, it creates a ripe opportunity for competitors to be more open.\n\n For example, as some have pointed out, if Twitter closes down their APIs or becomes overly restrictive, the first thing that Google+, Facebook, and LinkedIn should do is to be more open \u2014 provide one or several alternative open APIs that make Twitter less necessary.\n\n That possibility alone should be enough to convince Twitter not to become a closed platform. It is their very openness that has brought them to power, and keeps them there.\n\n Furthermore, now that Twitter has cut off LinkedIn, I\u2019m sure that all third-party networks are considering whether or not to continue integrating with Twitter. Imagine what would happen if there was a mass exodus from Twitter by the big networks that connect to it. That would not be good for Twitter.\n\n Twitter has a choice about whether this happens. It\u2019s up to them to decide whether to push the ecosystem into that kind of a response. I don\u2019t think they will do this. It just doesn\u2019t make good business sense in the long run.\n\n Depriving Users of Choice is Not a Good Business Model\n\n Many of Twitter\u2019s users already use, and\/or pay for, third-party apps that require Twitter API access \u2014 These are apps they love and want to continue to use.\n\n Twitter does not provide any suitable alternatives to such apps today. If Twitter simply shut off the pipes to those apps, I think they would enrage 10\u2032s of millions of users, and lose a lot of them. It would also cut off millions of enterprise users and brands that rely on these tools.\u00a0If Twitter cut off those apps, they would lose large amounts of content, users and traffic.\n\n Here\u2019s another interesting fact: Although Twitter states that 90% of users visit Twitter.com and Twitter\u2019s own mobile apps, that doesn\u2019t mean they ONLY use them.\n\n In fact most Twitter users access Twitter in many different ways via different apps and tools in different contexts. Sometimes they use multiple tools that provide different special features they want.\n\n Twitter would be making a mistake if they assumed that by cutting off those other apps, people would use Twitter more.\u00a0Without such apps people might actually use Twitter less (because Twitter.com is just not suitable for all those other use-cases and never will be).\n\n Some people access Twitter via the Twitter.com website, while others access it primarily via Twitter\u2019s mobile apps. But many also use third-party mobile apps. Some access Twitter through\u00a0Flipboard, or on widgets embedded in Web pages, or via third-party apps like\u00a0Hootsuite,\u00a0Radian6, and my own company\u2019s\u00a0Bottlenose.\n\n These third-party apps provide layers of functionality that go well-beyond merely replicating the experience provided on Twitter.com. They provide browsing, search, filtering, analytics, alerting, team collaboration and more.\n\n They don\u2019t compete with Twitter.com presently. Twitter.com doesn\u2019t provide anything comparable. Preventing people from using Twitter, the way they want to, and need to, is not a good business model.\n\n When people want your product, give it to them the way they want it. Listen to them and respond to their needs. That\u2019s how to win them and keep them.\n\n A Much Better Proposal for Twitter\u2019s Revenue Model\n\n So from all of the above it should be clear that there are many good reasons for Twitter to NOT restrict access to it\u2019s APIs or clamp down on third-party sites and apps that integrate it.\n\n But at the same time, we all understand and sympathize with Twitter\u2019s legitimate for-profit needs to monetize their network. Advertising is certainly one way to do this, but it is not mutually exclusive to having an open API.\n\n What\u2019s the solution then? Is there anything Twitter can do to satisfy both their own needs for more eyeballs on their ads, and the needs of their enormous ecosystem of users and developers?\n\n Yes there is. But they for some reason are not seeing it. Here it is\u2026\n\n Instead of trying to get all eyeballs to go to Twitter.com and Twitter\u2019s apps, it would make much more sense to monetize all the eyeballs that are presently looking at Twitter data in other apps and sites.\n\n This enormous network of apps and sites that consume and display Tweets today could be used to drive traffic to Twitter advertisers. It\u2019s a huge surface area \u2014 much larger than any website or set of proprietary aps can ever achieve. And all of it could be carrying Twitter\u2019s ads.\n\n The problem here is that Twitter\u2019s ad people are thinking in early 2000\u2032s paradigms \u2014 \u00a0they are thinking about display ads on websites and apps. But they should be thinking of ads that appear INSIDE streams of content in Twitter.\n\n We\u2019re in a new era of the Internet. In fact, the Net has moved beyond the Web, and even beyond apps. It\u2019s about data now. If you want to advertise, make sure your ads travel with your data, and then set your data free.\n\n Twitter is a network, not an app. They should be monetizing the network by embedding ads in the content that flows through it, or charging to get the content without the ads. This gets the ads into every site, every UI, every app, that carries Twitter. It\u2019s incredibly viral.\n\n The best way to do this would be for Twitter to provide everyone with two options for how to consume their APIs:\n\n OPTION A- Free API Access: Use the APIs for free, but every 25th (or nth) Tweet is an ad, from Twitter\u2019s Ad network. Apps must carry these ads if they use the free API.\n\n A large variety of apps and Websites would instantly be carrying Twitter ads.\n\n \n\nOPTION B \u2013 Premium API Access: Apps can opt-out of showing Twitter\u2019s ads, using a premium version of the APIs, but they have to pay a very low fee, per 1000 Tweets they receive.\n\n This fee should be set low enough that they can sell their own ads and still have a good profit margin. But these small payments will rapidly add up for Twitter into a huge revenue stream (it might even be larger than the revenues from ads).\n\n If this option existed many apps could charge their users to upgrade to a pro version without the ads and they could pay Twitter from those subscription revenues. Or apps could sell their own ads and still pay Twitter. Twitter effectively makes money either way.\n\n \n\nThe beauty of this proposal is that everyone wins. Twitter gets paid in either case \u2014 they make money from all Tweets and all third-party apps that use their API.\u00a0It\u2019s in fact far better than cutting off the API, or just running ads on Twitter\u2019s own site and apps.\n\n If Twitter did this, the ad people at Twitter would be happy. Furthermore, Twitter would almost instantly be the largest ad network on the Web, or at least equal to AdSense, Doubleclick and Facebook. Twitter would have ad space in thousands of apps and websites overnight. It would be a brilliant, lucrative, slam dunk of a move.\n\n The more apps and sites that integrate with Twitter, the bigger and more profitable the company would be. And it would be unstoppable. This is really what Twitter should do. It\u2019s the best move on the board by far.\n\n Once again I want to reiterate that I personally am a huge fan of Twitter. I love what they have achieved and I love the potential that still remains in what they have built.\n\n I don\u2019t want to see it ruined by short-term thinking.\n\n Billions of dollars of market value have been created by third-party companies that have built on the Twitter APIs. Numerous businesses have been formed, ventured funded, and even acquired, around these APIs. Careers and have been bet on access to these APIs.\n\n Thousands of businesses and other organizations, including many of the Fortune 1000, now depend on sophisticated third-party apps to analyze and engage with Twitter data in ways that are not possible using Twitter\u2019s own tools.\n\n Tens of millions of people consumer Twitter data through third-party apps that target specific needs (higher-end professional and enterprise apps, mobile apps for certain devices, etc.).\n\n Twitter is extremely fortunate that all of this has happened, and they should view this as a good problem to have, rather than a bad one.\n\n If Twitter starts shutting down their APIs or being overly restrictive they will ultimately be harming their own users and developers, and that just is not a good way to build a business. They will be selling out their own future for a short-term gain. That is a good way to destroy one\u2019s own empire. I am pretty sure the leadership team at Twitter realizes this.\n\n But most importantly, Twitter has a much better option \u2014 what I\u2019ve proposed above.\n\n Provide both a free and premium API, and require free API users to carry Twitter ads in the stream. It\u2019s a great move and would put an end to all this FUD. Twitter would instantly have a business model that makes sense, and massive goodwill from developers. There is just no reason NOT to do this.\n\n Please help us make Twitter\u2019s management team aware of this proposal, just in case they have somehow missed it.\n\n Tweet this article to them. Email it to them if you know them. Or Tweet about it to Twitter\u2019s leadership:\u00a0@dickc\u00a0@adambain\u00a0 @jack @sippey \u2014 I\u2019m sure they would be interested to hear from you, their valued users and developers.\n\n Twitter: Please clarify your dev post by @sippey - http:\/\/t.co\/fwqLmgtZ -- there is a lot of FUD. Your developers deserve a straight answer. 1\u00a0day\u00a0ago","item_date":"Jul 02 2012 04:25:00","display_item_date":"07-01-2012","url":"http:\/\/www.novaspivack.com\/uncategorized\/the-twitter-api-problem#.T_ED4A0mUUM.twitter","source":"www.novaspivack.com"},{"title":"Twitter developers dismayed by promise of stricter API rules","details":"(Credit: CBS\/Twitter) \n  Twitters announcement that it will tighten the rules governing its APIs sent a shock wave through the developer community, leaving many feeling jilted by the microblogging service and worried about the direction the company is heading. \n Ending a policy that had been in effect for the past two-and-a-half-years, Twitter announced Friday it planned to institute stricter rules for its application programming interface to ensure that the core Twitter consumption experience includes a consistent set of products and tools. \n In a company blog post announcing the forthcoming policy, consumer product manager Michael Sippey discussed broadening its expanded tweets  program, which allows users to preview stories, images, and videos linked to within tweets. \n [W]eve already begun to more thoroughly enforce our Developer Rules of the Road with partners, for example with branding, and in the coming weeks, we will be introducing stricter guidelines around how the Twitter API is used, Sippey wrote. \n Twitter simultaneously canceled a cross-posting partnership with LinkedIn. In essence, LinkedIn users will still be able to post updates to Twitter from the business network, but not the other way around. \n That string of events shocked many in the developer community. One developer on Hacker News said Twitter has a valuable set of Internet resources, but that the company has never been particularly great at thinking of innovative uses for it or developing great clients. \n There are a lot of really interesting and different ways to use those, regrettably they are trying to kill most of them and inflict a homogonized, boring, monoculture on their user base they can monotize, which will make the experience progressively worse, the poster said. \n In a tweet, software pioneer Dave Winer suggested that corporate greed was responsible in the apparent change in direction: \n \nThe mistake we all made with Twitter, me too, was to think a corporate API could act like an open protocol.\n\u2014 Dave Winer ? (@davewiner) July 1, 2012  In another tweet, Mike Loukides, the VP Content Strategy at OReilly Media, accused Twitter of turning its back on the developers who helped build up the service: \n \nVery sad. If ever a company owed its value to healthy 3rd party clients, it was Twitter. Too bad they dont get it. tnw.co\/Mz3o02\n\u2014 Mike Loukides (@mikeloukides) July 1, 2012  The promise of new policies has also spawned a coordinated tweet campaign to share third-party developers concerns with Twitter executives. Nova Spivack, the CEO of real-time personalization startup Bottlenose, offers some suggestions in a blog post on how to improve Twitters API. It is Spivacks hope that his readers will tweet the article to Twitter executives to influence policy thinking. \n  Related stories\n LinkedIn bows to Twitter over tweet traffic direction\n MetroTwit app hits Windows 8 store\n Your one chance to reply to aliens\n \n\n  I want Twitter to achieve its potential, and to live up to its promises to be an open ecosystem. And Im concerned by the ominous signs that they may be heading down a path that betrays the trust and goodwill given to them by millions of users and developers, Spivack wrote. \n If Twitter starts shutting down their APIs or being overly restrictive they will ultimately be harming their own users and developers, and that just is not a good way to build a business. They will be selling out their own future for a short-term gain. That is a good way to destroy ones own empire. I am pretty sure the leadership team at Twitter realizes this. \n CNET has contacted Twitter for comment and will update this report when we learn more.","item_date":"Jul 02 2012 03:42:15","display_item_date":"07-01-2012","url":"http:\/\/news.cnet.com\/8301-1023_3-57464614-93\/twitter-developers-dismayed-by-promise-of-stricter-api-rules\/","source":"news.cnet.com"},{"title":"What Twitter could have been by Dalton Caldwell","details":"I remember when you could go to Twitter.com and see the global firehose on the front page. They had no traffic. The global feed was mostly employees and their friends talking to each other.\n  When Twitter started to get traction, a year or two into their existence, I decided that Twitter was the Best Thing Ever. I realized that Twitter, because of their API, actually was a real-time protocol to connect various services in a novel way. I had debates with my other tech-nerd friends about whether Twitter could be one of the fundamental building blocks of the Internet via their powerful API. When reporters or investors asked me what I thought the most exciting company in the valley was, I would invariably answer \u201cTwitter\u201d.\n  As I understand, a hugely divisive internal debate occurred among Twitter employees around this time. One camp wanted to build the entire business around their realtime API. In this scenario, Twitter would have turned into something like a realtime cloud API company. The other camp looked at Google\u2019s advertising model for inspiration, and decided that building their own version of AdWords would be the right way to go.  \n  As you likely already know, the advertising group won that battle, and many of the open API people left the company. While I can understand why the latter camp wanted to build an ad-based business, the futurist in me thinks this was a tragic mistake. If you are building an advertising\/media business, it would then follow that you need to own all of the screen real-estate that users see. The next logical step would be to kill all 3rd-party clients, and lock down the data in the global firehose in order to control the \u201ccontent\u201d. \n  Perhaps you think that Twitter today is a really cool and powerful company. Well, it is. But that doesn\u2019t mean that it couldn\u2019t have been much, much more. I believe an API-centric Twitter could have enabled an ecosystem far more powerful than what Facebook is today. Perhaps you think that the API-centric model would have never worked, and that if the ad guys wouldn\u2019t have won, Twitter would not be alive today. Maybe. But is the service we think of as Twitter today really the Twitter from a few years ago living up to its full potential? Did all of the man-hours of brilliant engineers, product people and designers, and hundreds of millions of VC dollars really turn into, well, this?\n  Nowadays, every time I get a K-Mart ad in my feed, or see wonky behavior in the official clients, or see Twitter drop another bomb on their developer ecosystem, I think back and wish the pro-API guys won that internal battle.","item_date":"Jul 02 2012 01:15:44","display_item_date":"07-01-2012","url":"http:\/\/daltoncaldwell.com\/what-twitter-could-have-been","source":"daltoncaldwell.com"},{"title":"Careful, Twitter \u2014 remember what happened to MySpace and Digg","details":"Twitter sent some shock waves through the technology community with a blog post on Friday that talked about its plans for the future, and suggested that those plans don\u2019t necessarily involve third-party services and apps. Although the company phrased its statement as a move designed to standardize the experience for Twitter users, developers and others in the broader Twitter ecosystem clearly took the post as a warning shot across the bow \u2014 especially since the company simultaneously shut down a cross-posting partnership it had with LinkedIn\u00a0.\u00a0It seems clear that Twitter wants to control the network as tightly as possible so that it can monetize it more easily, but doing so also comes with substantial risks.\n\nIn his blog post, consumer product manager Michael Sippey talked a lot about the introduction of features such as \u201cexpanded tweets,\u201d which show more information from providers like GigaOM and the New York Times when a link is included in a tweet. He said the company wants to broaden that program to more publishers, as well as giving them tools to display expanded tweets and other features on their sites \u2014 but he also made it obvious that developers who stray outside of the lines are taking a big risk:\n\nThese comments set off warning bells for a number of developers, who said they were concerned that Twitter was going to crack down on any third-party app or service. One developer on Hacker News said that in his view, Twitter was trying to shut down third-party services so that they could \u201cinflict a homogenized, boring, monoculture on their user base [that] they can monetize, which will make the experience progressively worse.\u201d Said Turntable.fm developer Jonathan Kupferman:\n\nThis isn\u2019t the first time that Twitter has upset the developer community by throwing its weight around. In 2011, there was widespread criticism of the service for the way it issued new rules around use of the Twitter API \u2014 and also the way it behaved towards those who crossed the line by shutting off their access without even a warning, as it did in the case of entrepreneur Bill Gross and his Ubermedia network. At the time, one critic accused the company of \u201cnuking\u201d the Twitter ecosystem.\n\nThe company also came under fire in 2010 for the way it handled relations with third-party developers after it bought an app called Tweetie. Hunch founder Chris Dixon said Twitter was \u201cacting like a drunk guy with an Uzi\u201d by telling developers not to bother developing Twitter apps, and a number of companies and investors that had been putting money and time into the Twitter ecosystem stopped doing so. So some of the negative reaction to Sippey\u2019s post stems from being burned twice already.\n\nSome observers have argued that Twitter is just doing what it has to do in order to control its network and build a sustainable business, and that third-party developers don\u2019t have any right to expect favorable treatment, since they are piggybacking on its API and resources. Longtime Twitter users, however, say the service\u2019s behavior is a betrayal of all of the other services and apps that helped generate most of the goodwill it is now busy monetizing. As John Abell of Reuters pointed out on Friday, much of the value that users find in Twitter comes from the way it connects to other services.\n\nAnd there is a very real risk to this kind of aggressive focus on control and monetization, as a commenter on Hacker News pointed out: restricting the ways that users can access and display their tweets, whether through strict API rules or moves like the LinkedIn shutdown, could irritate the user base that Twitter is relying on to click ads and do all the other things it is planning around monetization. Ultimately, the company could ruin the experience that made Twitter so compelling in the first place, in the same way that MySpace and Digg did.\n\n\n\n\n\nThere are plenty of reasons why MySpace failed, including the conflicting desires of a giant corporate owner like News Corp., but it also started to hemorrhage users because it focused more on monetization through ads and other elements than it did on maintaining a good experience for users. Digg did something similar \u2014 in an attempt to build a bigger company and leverage its user base for profit, it added a whole range of \u201cservices\u201d and features that were designed mainly to appeal to corporate customers and advertisers. The end result was a wholesale desertion of Digg for other communities like Reddit.\n\n\n\n\n\nTwitter has a tiger by the tail \u2014 it has an active user base in the hundreds of millions, it has become an almost indispensable tool for both news junkies and the media (although this carries risks as well) and it is starting to see some favorable responses to its ad model. But it is also a community, where the users provide the vast majority of the content that is being monetized, and while screwing around with that relationship may appear to make short-term financial sense, it could end in disaster.\n\n\n\n\n\nPost and thumbnail images courtesy of Flickr users Rosaura Ochoa and See-ming Lee","item_date":"Jul 01 2012 05:52:32","display_item_date":"06-30-2012","url":"http:\/\/gigaom.com\/2012\/06\/30\/careful-twitter-remember-what-happened-to-myspace-and-digg\/","source":"gigaom.com"},{"title":"Twitter issues warning to developers: Now, we do it our way","details":"If you\u2019re a third-party developer using the Twitter API, the company is giving you advance warning today: Get ready to do it Twitter\u2019s way.\n\n\nToday Twitter published a blog post\u00a0outlining a new direction for the company, which will involve more restrictions on third-party use of the API and expanding functionality and usability of information within Twitter.com and approved clients.\n\n\nAs Twitter grows, the desire to have users experience tweets the Twitter way grows as well.\n\n\nThe announcement wasn\u2019t incredibly specific, and the only clear impact of this move right now is a change at LinkedIn, where users will no longer be able to sync their accounts and have their Tweets displayed next to their professional profile on the LinkedIn site.\n\n\nUsers will still be able to send updates to Twitter via LinkedIn, much as they send updates through apps like Instagram or Nike+ GPS to their feed, but if they want to read those tweets, they\u2019ll have to visit Twitter.\n\n\nWhile the change only comes for LinkedIn right now, it\u2019s easy to imagine that other third-party apps that use the Twitter API could be affected in the near future as well, apart from the embedded tweets and Twitter widgets the company provides.\n\n\nIn the beginning of Twitter\u2019s lifecycle, third-party apps and clients proliferated, and it was only in March of last year that the company clamped down on those companies and people who used them.\n\n\nThe current move builds on this, demonstrating the company\u2019s desire to more closely control how users experience the Twitter brand across different platforms and draw users away from clients and apps. At the same time, Twitter is working to improve its own channels, having recently added expanded tweets that partner with big-name brands for a better in-app experience.\n\n\nThere\u2019s a business angle to these changes, as Twitter expands from a scrappy startup into a global brand that could bring in as much as $1 billion in revenue by 2014. Many companies are struggling to succeed with mobile advertising, but as  my colleague Mathew wrote, Twitter has seen more success in this area than others like Facebook. It seems natural that keeping Twitter users on Twitter would play an integral part in that growth.","item_date":"Jul 01 2012 03:45:55","display_item_date":"06-30-2012","url":"http:\/\/gigaom.com\/2012\/06\/29\/twitter-issues-warning-to-developers-now-we-do-it-our-way\/","source":"gigaom.com"},{"title":"Zynga unleashes API, gaming social network","details":"Zynga, one of the world\u2019s most prominent online gaming companies, announced that it will release a public API that will allow third-party developers to take advantage of its technology, as well as the release of \u201cZynga With Friends,\u201d a social network that will attempt to build connections around online gaming.\n\n\nZynga has some of the world\u2019s most enthusiastic and prolific online gamers, boasting 290 million monthly active users and more than 22 million mobile players per day. In an average minute, players of Zynga games send over 1 million messages to each other, play 64,000 words in \u201cWords With Friends,\u201d and create 43,000 drawings in \u201cDraw Something,\u201d CEO Mark Pincus said during an event at the company\u2019s headquarters in San Francisco.\n\n\nThe Zynga API will allow third-party game developers to take advantage of Zynga\u2019s technology and servers, which the company said are strong enough to allow the company to release 1,000 new features to its games in a given week. The Zynga API will allow gaming companies to build their own games on top of Zynga\u2019s technology, enhancing online gaming opportunities for smaller startups.\n\n\n\u201cNow third parties will be able to focus on what they do best. Enjoy and create beautiful games,\u201d said\u00a0Kostadis Roussos, chief engineer at Zynga.\n\n\nZynga built much of its success through social games connected through Facebook, such as Farmville, so its release of a new social network called \u201cZynga With Friends\u201d puts it in direct competition with the world\u2019s most popular social network. Facebook, too, has benefitted from Zynga\u2019s success, with a good portion of ad revenue and users coming to Facebook specifically to interact with Zynga products, which GigaOM has written about previously.\n\n\n\u201cZynga With Friends\u201d will allow users to build a centralized profile for all their Zynga gaming experiences, aggregating their top performances across multiple games on both web and mobile platforms, which is a growing part of Zynga\u2019s business.\n\n\nThe social network will allow multi-player gaming, chat features, and suggestions of both virtual and real-life Zynga-playing friends.\n\n\nIn addition to the \u00a0new social network and API release, Zynga also released several new games and features at Tuesday\u2019s event. They include:","item_date":"Jul 01 2012 03:40:24","display_item_date":"06-30-2012","url":"http:\/\/gigaom.com\/2012\/06\/26\/zynga-unleashes-api-gaming-social-network\/?goback=%2Egde_2637634_member_128588850","source":"gigaom.com"},{"title":"Hack Your Mind: Allen Brain Atlas API Launched","details":"The Allen Institute for Brain Science, a non-profit medical research organization in Seattle, Washington, worked for several years to build a map of gene expression in the human brain. It released a Mouse Brain Atlas in 2007, then used similar techniques to complete its Human Brain Atlas in 2010. Last week, at its first hackathon, the Institute launched an API for the Human Brain Atlas, allowing researchers all over the world direct access to that information.\n\n\n\n\n\nOf course, the Allen Institute has always made its data freely available, but the API offers new ways to use Brain Atlas information without having to download petabyte-sized archive files. Resources include a RESTful model access, an image download service, a 3-D expression summary download service, and more. As detailed in a recent press release:\n\n\nDuring last week\u2019s hackathon, participants created \u201c[p]rojects rang[ing] from practical applications, such as using a list of glioblastoma-related genes to discover biological patterns that could shed new light on the biology of the disease and developing strategies to use gene expression data with functional brain scanning technologies, to purely creative applications, including translating genomic data into music.\u201d\n\n\nThe Institute plans to make source code from all the hackathon projects available as part of its next public data release in October.\n\n\nYou can get extensive Allen Brain Atlas API documentation online, and also see some code examples.\n\n\n(Hat tip:  Medgadget )","item_date":"Jul 01 2012 03:40:12","display_item_date":"06-30-2012","url":"http:\/\/blog.programmableweb.com\/2012\/06\/29\/hack-your-mind-allen-brain-atlas-api-launched\/","source":"blog.programmableweb.com"},{"title":"What Can We Learn About #ISTE12 Through Twitter?","details":"This website is deliberately advertising-free. But the research and writing that I do here is my full-time work -- again, deliberately so.  If you find my writing interesting or insightful, please consider a donation.","item_date":"Jun 30 2012 06:49:50","display_item_date":"06-29-2012","url":"http:\/\/www.hackeducation.com\/2012\/06\/29\/twitter-iste12\/","source":"www.hackeducation.com"},{"title":"Twitter Ends LinkedIn Partnership, Who's Next?","details":"Throughout Twitter\u2019s infancy, the company had a loose philosophy toward its APIs. In the quest for a user base, developers were welcome to do much anything they wanted in integrating with Twitter, which often mean creating spin-offs that muddied Twitter\u2019s original intended experience. \n Six years later, Twitter has grown up. And as the 140-million-plus user service continues to expand to reach main stream audiences, Twitter knows it needs to assert and define itself more than ever.\n Which is why when it was announced on Friday that Twitter had ended its tweet syndication partnership with LinkedIn, it signaled Twitter\u2019s continued shift towards controlling the way users experience their tweets. Previously, LinkedIn had a deal with Twitter which allowed for syndication of users tweets inside of LinkedIn\u2019s flowing user activity stream. As a result of that syndication, much of the recently added tweet features \u2014 expandable tweets, threaded conversations, and the like \u2014 weren\u2019t showing up on users\u2019 LinkedIn pages.\n \u201cUltimately, we want to make sure that the Twitter experience is straightforward and easy to understand,\u201d wrote Twitter consumer product lead Michael Sippey in a company blog post, \u201cwhether you\u2019re on Twitter.com or elsewhere on the web.\u201d\n Losing Twitter could hurt LinkedIn\u2019s user engagement, as much of the content flowing through users\u2019 feeds were integrated tweets. LinkedIn assures me, however, that its social news service LinkedIn Today \u2014 which was originally powered in part by Twitter \u2014 won\u2019t be affected. \n \u201cTweets have not been powering LinkedIn Today for some time,\u201d a LinkedIn spokeswoman told me.\n While ending the LinkedIn deal was big, I\u2019ve heard from several sources that we should expect more of the same in the not-too-distant future. Just give a close read to Product Manager Sippey\u2019s blog post, which went up just minutes before LinkedIn\u2019s post. Sippy\u2019s missive contains some especially strong wording, a harbinger of what\u2019s to come for other developers: \n \u201c\u2026we\u2019ve already begun to more thoroughly enforce our Developer Rules of the Road with partners, for example with branding, and in the coming weeks, we will be introducing stricter guidelines around how the Twitter API is used,\u201d Sippey wrote.\n As Twitter matures, tightening the reigns makes sense. LinkedIn\u2019s deal, and other syndication deals like it, essentially work against the entire aim of what Twitter has been trying to accomplish over the past eight or so months. Recall the launch of the \u201cnew, new\u201d Twitter, a major visual and conceptual overhaul of the service which aimed to make it more accessible to the uninitiated. It cut through the jargon that has sprung up naturally in the Twitter ecosystem, instead relabeling different sections as \u201cConnect, Discover and Home.\u201d Viewing decontextualized tweets from inside a partner site, then, loses much of the value Twitter has tried to add. \n Not to mention the potential consequences on mobile monetization, an area where Twitter seems to have excelled where many others have failed. If Twitter experiences on third-party mobile clients lack the same monetization potential as that of Twitter.com and its native apps, it could be a huge pain point for the company\u2019s business.\n So who\u2019s next?\n Flipboard, for one, comes to mind. After a user authenticates their social profiles with Flipboard, the app pulls content from disparate sources across the web \u2014 Facebook, RSS, Google+, and yes, Twitter \u2014 in the form of URLs, and re-paginates the information in Flipboard\u2019s own custom layout. While the result is a rather attractive social magazine, it displays third party content in a very distinctive, very Flipboard way. And it looks nothing like tweets which show up on Twitter.com.\n Last month, Kara Swisher reported that Flipboard CEO Mike McCue, who also holds a seat on Twitter\u2019s board, was likely to give up that seat due to Flipboard and Twitter possibly being set on a future \u201ccollision course.\u201d\n \u201cHow users consume and use Twitter is a key part of its future, and that is what Flipboard does well already,\u201d one person with knowledge of the situation told Swisher. \u201cThere is going to be an inevitable crossroads for the two companies,\u201d this person said.\n Speculation at the time \u2014 including my own \u2014 involved Twitter\u2019s potential acquisition of Flipboard in order to bolster the microblogging platform\u2019s discovery capabilities. But considering Twitter\u2019s recent actions towards LinkedIn, this may not be the case. \n For what it\u2019s worth, Flipboard tells me this hasn\u2019t been a discussion the two companies have had thus far. As a third-party developer, Flipboard tells me its relationship with Twitter is strong, and it expects the two will continue working together.\n Flipboard isn\u2019t the only one that could be affected. As Sippey\u2019s post notes, it seems that copycat clients \u2014 or third party apps that really don\u2019t add much value outside of what Twitter.com offers \u2014 may be on the chopping block: \u201cBack in March of 2011,\u201d Sippey writes, \u201cmy colleague Ryan Sarver said that developers should not \u2018build client apps that mimic or reproduce the mainstream Twitter consumer client experience.\u2019\u201d \n Take Tweetdeck, the service Twitter purchased a little more than a year ago for $50 million, which basically offers a power-user version of Twitter. Historically, its interface has been drastically different than that of Twitter.com, though in recent months, Twitter has shown a willingness to change that: The company redesigned the app last year, removing a number of features and streamlining others. Twitter didn\u2019t kill or handicap the service immediately, most likely because the company didn\u2019t want to upset TweetDeck\u2019s vibrant community of very vocal users. But it could very well get another throttling or makeover in the coming weeks.\n Other clients that aren\u2019t as popular \u2014 Tweetbot, HootSuite and Seesmic \u2014 may not be as safe. HootSuite may even already be preparing for a change; The company just launched a series of new app integrations on Thursday.\n I doubt, however, that other services which rely on Twitter firehose access for more analytics and enterprise oriented needs \u2014 like Topsy, Gnip, DataSift and Klout \u2014 don\u2019t have much to worry about. They\u2019re working with vast amounts of data, completely independent of the user experience. Indeed, Topsy and Gnip told AllThingsD that there was no indication either company\u2019s relationships with Twitter would change at all.\n But Twitter-mimicking developers take heed: There is a sea change coming. And Twitter, not you, will soon be in fuller control of its user experience.","item_date":"Jun 30 2012 06:46:46","display_item_date":"06-29-2012","url":"http:\/\/allthingsd.com\/20120629\/twitter-cuts-off-linkedin-whos-next\/","source":"allthingsd.com"},{"title":"Delivering a consistent Twitter experience","details":"Just over two weeks ago, I talked about more interactive experiences within expanded Tweets and how easy it is for users to discover even more great content on Twitter. The technology behind expanded Tweets -- Twitter cards -- gives developers and publishers a way to tell richer stories on Twitter, directly within Tweets and drive traffic back to their sites. Since launch, weve seen great engagement and more than doubled the number of partners that are part of expanded Tweets, and we continue to add more.   \n         Twitter cards are an important step toward where we are heading with our platform, which involves creating new opportunities to build engaging experiences into Twitter. That is, we want developers to be able to build applications that run within Tweets. Just a few days ago, our CEO Dick Costolo said, \u201cWhat you\u2019ll see us do more and more as a platform is allow third parties to build into Twitter.\u201d This is something we\u2019ve been talking about for a while, and we\u2019re looking forward to adding new ways for developers to do this.   \n         As always, we\u2019re hard at work building tools that make it easy for developers to build common Twitter features into their own sites in a simple and consistent way. Some examples of these tools include the Tweet Button, Follow Button, embeddable Tweets and the search widget. Ultimately, we want to make sure that the Twitter experience is straightforward and easy to understand -- whether you\u2019re on Twitter.com or elsewhere on the web.   \n         These efforts highlight the increasing importance of us providing the core Twitter consumption experience through a consistent set of products and tools. Back in March of 2011, my colleague Ryan Sarver said that developers should not \u201cbuild client apps that mimic or reproduce the mainstream Twitter consumer client experience.\u201d That guidance continues to apply as much as ever today. Related to that, we\u2019ve already begun to more thoroughly enforce our Developer Rules of the Road with partners, for example with branding, and in the coming weeks, we will be introducing stricter guidelines around how the Twitter API is used.   \n         We\u2019re building tools for publishers and investing more and more in our own apps to ensure that you have a great experience everywhere you experience Twitter, no matter what device you\u2019re using. You need to be able to see expanded Tweets and other features that make Twitter more engaging and easier to use. These are the features that bring people closer to the things they care about. These are the features that make Twitter Twitter. Were looking forward to working with you to make Twitter even better.","item_date":"Jun 29 2012 21:08:34","display_item_date":"06-29-2012","url":"https:\/\/dev.twitter.com\/blog\/delivering-consistent-twitter-experience","source":"dev.twitter.com"},{"title":"I'm Done Developing for Twitter - Aaron White","details":"There\u2019s a reason my side projects have favored the Twitter platform: I\u2019m a fan of the service, and earnestly want to improve the experience of her users. No more: Twitter is a hostile partner to developers and has bluntly terminated my projects without so much as a reason.\n First it was Proxlet\u00a0(now defunct). Chris Ricca and I built Proxlet because we wanted to improve our Twitter experience. It\u2019s too easy to resent your tweet-stream when it\u2019s overflowing with #FollowFridays, Paper.li and other assorted tweet-spam. So we did something about it: we built the ultimate Twitter API Proxy that would allow us to mute undesirable applications and hashtags in a way that could work w\/ nearly every important native or web client. It was a huge hit, well covered by major tech blogs, gaining a large following and even dedicated support in native clients.\u00a0\n One day, Twitter decided to take issue with the implementation. Instead of reaching out to us to warn us of any potential action, they unceremoniously shut our application down, leaving every single one of our users in a broken state: suddenly their clients just stopped working; they had no way of knowing how to recover, and we had no time to prepare to tell them.\u00a0\n Flash forward half a year, I built a small app called TweetFavor. It was built for two reasons, the first to help my friends and I coordinate tweeting. Instead of receiving an email request saying \u201chey tweet this!\u201d you receive an email and can pledge a single tweet of your choice the requester can redeem in the future. The second reason I built it was to teach students at Intelligent.ly\u00a0how to build Google App Engine, Twitter Bootstrap apps. Imagine my surprise when mid-class, Twitter shutdowns down TweetFavor without any reason given.\u00a0As far as I can tell, it in no way violates their terms of service, but even more \u00a0insulting, it looks like they never even logged into to investigate it.\n All my developer goodwill towards Twitter has been exhausted. They\u2019ve demonstrated not once to me, but twice, that they have no desire to work with developers, but rather\u00a0antagonize\u00a0them as they see fit.\u00a0\n Moral of the story: be-wary of developing applications with dependency to a platform. And if you\u2019re doing so for hobby projects, let this serve as a guide about where you should steer your efforts.","item_date":"Jun 29 2012 20:24:57","display_item_date":"06-29-2012","url":"http:\/\/restrictionisexpression.com\/post\/26144987502\/im-done-developing-for-twitter","source":"restrictionisexpression.com"},{"title":"Path and Nike FuelBand Pair Up, Flaunting the Benefits of a Private API","details":"Facebook and Twitter rose to prominence in part because they played so nicely with others. Yes, the sites were great products first. But they became even better platforms, opening their application programming interfaces to let third-party developers integrate their services. And as the platform era matured, open APIs practically became en vogue.\n But there is still value in being exclusive.\n Path, for instance, is perhaps a prime example of this. It has an API, though it currently only has a single announced partner: Nike. On Friday, the company announced a small update in the partnership, adding integration between Nike\u2019s FuelBand fitness tracking bracelet and the private social network. Path publishes a summary of your FuelBand activity in the form of an interactive graph at the end of each day, complete with included data points on where and when you\u2019ve checked in using Path. \n Ultimately, the aim is a mix of motivation and geeking out on data \u2014 you can see how close you\u2019ve come to your daily exercise goal, while simultaneously tracking all the places, activities and reasons why you achieved it (or why you didn\u2019t). It\u2019s incremental, building on the previously announced partnership between the two companies when Path first started publishing Nike+ running route updates.\n However small an update, it\u2019s also an indicator of Path\u2019s API partnership strategy as a whole. When Path first spoke of its API, the plan wasn\u2019t to open it up to the masses \u2014 it was, and remains, a private API, with partners selected in terms of which ones make sense for the company to work with, and which ones make for a better user experience.\n \u201cWe want to improve the experience for the collective users of both apps,\u201d Path CEO Dave Morin told me in an interview on Thursday, \u201cwhich is why we\u2019re selective about who we\u2019re partnering with.\u201d\n That selectivity means more time is spent on the integration, making the content in a user\u2019s stream actually more attractive. The new FuelBand graphs are sleek, offering users granular information about their fitness activity in a polished interactive graphic. That\u2019s more than can be said for some apps that show up in Facebook\u2019s News Feed, even after Open Graph integration.\n \u201cWe\u2019re driven by quality,\u201d Morin said. \u201cWe\u2019re driven by design.\u201d\n And ultimately, the better and prettier the content is that shows up in the stream, the more users will actually want to engage with the product on a regular basis. Quality begets stickiness, and stickiness begets a company with a better chance at surviving.\n We\u2019ll no doubt see more companies partnering with Path in the future. My hunch says it\u2019ll be a slow, steady ramp up over time, with Path being selective about who it wants to work with.","item_date":"Jun 29 2012 20:05:59","display_item_date":"06-29-2012","url":"http:\/\/allthingsd.com\/20120629\/path-and-nike-fuelband-pair-up-flaunting-the-benefits-of-a-private-api\/","source":"allthingsd.com"},{"title":"How Web APIs Unlock Value in the Cloud","details":"","item_date":"Jun 29 2012 20:03:59","display_item_date":"06-29-2012","url":"http:\/\/mashable.com\/2012\/06\/29\/web-api-cloud\/","source":"mashable.com"},{"title":"DocuSign Speeds Integration with New eSignature REST API","details":"DocuSign today launched a new eSignature REST API for its industry leading DocuSign Global Network. This innovative, visually driven API makes connecting with DocuSign\u2019s eSignature Transaction Management platform faster and easier for customers and partners. Organizations leveraging the API can offer DocuSign eSignature convenience from existing or new business processes in just hours of development time instead of weeks \u2013 accelerating speed to results, reducing costs, and delighting customers. \n \u201cMore developers are using DocuSign\u2019s open eSignature integration framework to integrate into their existing business solutions and apps than any other eSignature solution,\u201d said Grant Peterson, chief technology officer, DocuSign. \u201cThe release of our new eSignature REST API standard and toolkit dramatically shortens the time it takes to integrate and deploy DocuSign \u2013 no matter what systems or devices customers use to run their business.\u201d\n   \n  \u201cThe eSignature REST API is visually stunning and ready for enterprise use,\u201d said Dave Messinger, community architect, Appirio. \u201cWe were encouraged with how quickly we were able to integrate DocuSign into our CloudSpokes architecture \u2013 taking us days to prototype solutions instead of weeks or months. It just works. We\u2019re excited and encourage CloudSpoke\u2019s developers to take a look.\u201d\n \n DocuSign\u2019s new eSignature REST API standard is built to be open and extensible, and was developed in collaboration with a number of the world\u2019s leading cloud-based companies, including Appirio, Box, Mashery, salesforce.com, and others. This new industry standard will continue to be enhanced based on developer, partner, and customer feedback.\n \u201cDocuSign\u2019s new eSignature REST API will enable salesforce.com customers \u2013 as well as developers in our partner ecosystem \u2013 to quickly and easily integrate with DocuSign,\u201d said Mike Rosenbaum, senior vice president, AppExchange & Force.com Operations, salesforce.com. \u201cEnterprises are accelerating their move to the cloud with architectures built around open APIs, instead of dealing with servers and software.\u201d\n As an integral component of the DocuSign Cloud Partner Program, the DocuSign Developer Program provides resources to facilitate integrations of all kinds, including technical evangelists, a complete API, online community, and a developer program described by ProgrammableWeb as \u201cthe strongest of all the [electronic signature] providers researched.\u201d\n DocuSign\u2019s new eSignature REST API Toolkit includes:\n visualization of the API call flows, function code examples, and\n detailed explanations of every step.\n \n\u201cOffering developers an API Explorer to get started quickly is a key element of DocuSign\u2019s full-dimensional API program,\u201d said Neil Mansilla, director of platform evangelism & partnerships, Mashery. \u201cDocuSign did an extensive integration using Mashery I\/O Docs. DocuSign\u2019s impressive code contributions back to the project will benefit developers and the open source community.\u201d\n \u201cWe already have more than 5,000 active developer accounts using our existing APIs today,\u201d said Roger Erickson, vice president for customer success, DocuSign. \u201cThe speed-to-value equation of our new standard is truly amazing, and we view our eSignature REST API as a \u2018crossing the chasm\u2019 moment for embedding DocuSign eSignature and workflow management into any business process or system.\u201d\n Customers and partners are already using DocuSign\u2019s eSignature REST API to help accelerate transaction cycle times to increase speed to results, reduce costs, and enhance customer satisfaction with the easiest, fastest, most secure way to send, sign, track, and store documents in the cloud. \u201cDocuSign has really made a step forward in helping developers with the new eSignature REST API,\u201d said Martin Davey, executive vice president of Industry Solutions, Thunderhead.com. \u201cThe API walkthrough area provides a quick and simple introduction to using the services and the REST API allows us to send out documents for signature, get real-time status updates, and embed DocuSign within our apps.\u201d\n Companies and developers interested in learning more about the new eSignature REST API standard and toolkit, upcoming webcasts, and other API resources should visit the DocuSign Developer Center at www.docusign.com\/developer-center.","item_date":"Jun 29 2012 19:59:59","display_item_date":"06-29-2012","url":"http:\/\/cloudnewsdaily.com\/2012\/06\/docusign-speeds-integration-with-new-esignature-rest-api\/","source":"cloudnewsdaily.com"},{"title":"Going Mobile with AppDynamics REST API ","details":"Its always great when customers want to build their own applications on top of your data and platform. A few weeks back one of our customers in Europe decided to build their own mobile application, so they could monitor the performance of their mission-critical business transactions from any smart phone or mobile device.\u00a0Here is the unedited story we\u00a0received\u00a0of how this customer went mobile with AppDynamics:\n I looked into AppDynamics\u2019 REST API and was very keen to use that data but was unsure of how I could visualize the it. Since all data per monitoring point was available in either XML or JSON format, it seemed the ideal choice to go with a Javascript user interface. After searching around I found the Dojo Gauges and started to code up a simple webapp using AppDynamics\u2019 REST API and data we collect everyday.\n Technology Stack\n Here is a quick overview of the architecture and technologies used in the mobile application.\u00a0\n The Mobile Application\n Our application is hosted within Django so I can use django\u2019s powerful dynamic admin\u00a0interface for a backend. Though this is not implemented yet so for now I am just using it to serve\u00a0some Static content and proxy AJAX calls to AppDynamics.\n The core application consists of:\n \nFunctionally, the webapp serves the basic static page layout to the browser on the client device and then instantiates the Dojo GlossyGauges which I have extended to include their own embedded self-update timers. The update timers update the gauge values by making REST calls to AppDynamics through the proxy module. The proxy module is necessary as the browser will block cross domain ajax requests. You probably don\u2019t need it if you host the application on the same server as AppDynamics and use apache + mod_proxy.\n Here is a screenshot of our mobile application:\n \n The gauges update themselves in real-time so there is no need to refresh the page.\n Adding new gauges is as simple as exporting the Information Point REST-URL from AppDynamics and adding it to my settings.js file and then creating a view for it in views.js. This manual process will be replaced by simply adding it to the Django admin interface at a later stage and then dynamically generating views.js and settings.js via a django view.\n It was also simple enough to extend the interface to get current open cases from JIRA and retrieve current events from our CMS systems.\n It is also possible to show our performance data by location on a map layout as shown below:\n \n If you would like to share any applications, plugins or custom reports that utilize AppDynamics data then drop me an email at: appman @ appdynamics (.) com.","item_date":"Jun 29 2012 19:55:32","display_item_date":"06-29-2012","url":"http:\/\/www.sys-con.com\/node\/2304696","source":"www.sys-con.com"},{"title":"New Ordr.in Developer Tools- Woot!","details":"Developers rejoice! Ordr.in just released v2.0 of our API wrappers, making it easier than ever to build an Ordr.in food ordering app. Brand new Python, Ruby and NodeJS packages plus a PHP library. All available for your coding pleasure.\n Changes include a lot of debugging, enhanced documentation, and new schema to make them more easily interoperable. Angels weep! Special shout our to interns Michael Lumish and Jason Teplitz for their great work.\n And please send us your feedback. As always we\u2019d love to hear from you.","item_date":"Jun 29 2012 07:00:00","display_item_date":"06-29-2012","url":"http:\/\/ordrin.tumblr.com\/post\/26144795302","source":"ordrin.tumblr.com"},{"title":"How Web APIs Unlock Value in the Cloud","details":"The cloud is where it\u2019s at. It\u2019s where business data resides. It\u2019s where social user-generated content sits, where forward-thinking creators place their tools. Unfortunately, the cloud is also the place all that good stuff stays, unused and unloved. That is, unless you offer smart ways to access it. That is where a web API comes in.\n Just as the power of crowds has populated the social content repositories of Web 2.0 \u2014 YouTube videos, Facebook updates, tweets, and more \u2014 the web API enables designers and developers to re-purpose the body of knowledge that is the cloud. Here are some examples of how companies have used a web API to create more value.","item_date":"Jun 29 2012 07:00:00","display_item_date":"06-29-2012","url":"http:\/\/blog.tiggzi.com\/2012\/06\/how-web-apis-unlock-value-in-the-cloud\/?utm_source=rss&utm_medium=rss&utm_campaign=how-web-apis-unlock-value-in-the-cloud","source":"blog.tiggzi.com"},{"title":"New Help Center","details":"Today we are proud to announce the opening of Pogoplug\u2019s Help Center. \u00a0The Help Center contains our knowledge base, instructional videos, and customer support, and is the new home for our community forum, formerly hosted on Pogoplugged. The new Pogoplug Help Center is the perfect place to learn more about Pogoplug, get answers to your FAQs, submit a support ticket, or chat with other Pogoplug users.\n \n Find answers to your most frequently asked questions, tips on troubleshooting and how-to guides in our updated\u00a0Knowledge Base.\n Watch video walkthroughs of Pogoplug features in our brand new Videos section.\n Talk with other Pogoplug users, share tips, tricks and hacks, and chat about Pogoplug (or whatever else) in our new User Forums.\n Create a support ticket with our dedicated customer service team, who can provide solutions to your problems 24\/7, in the Ask Support tab.\n \n If you have any questions about the migration or suggestions how we can improve the Help Center, let us know in the Help Center Discussion forum thread.\u00a0 We\u2019re confident these changes will improve your Pogoplug experience. Enjoy!","item_date":"Jun 29 2012 07:00:00","display_item_date":"06-29-2012","url":"http:\/\/blog.pogoplug.com\/new-help-center-2515\/","source":"blog.pogoplug.com"},{"title":"Our First Field Trip to ISTE","details":"While students across the nation were clearing out lockers and emptying backpacks in preparation for summer, we were already considering our summer homework: project: How do we take Quizlet and make it even better for the fall so that more students across the globe have access to amazing study tools that they can use anywhere?\n Like any good student, we know that finding the answer means asking the right questions and doing some research.\n Hello, San Diego!\n We took a field trip this week to sunny San Diego\u2019s ISTE (International Society for Technology in Education) conference. We wanted to hear from teachers about how they implement technology in their classrooms and how they want to support their students. We didn\u2019t have to look or listen very hard to find what we were looking for: teacher enthusiasm for student-centered technology flowed through sessions, waxed eloquent on couches and cubes, and even bubbled over into spontaneous sidewalk conversations.\n What we learned surprised us. It\u2019s not about the \u201cone app to rule them all\u201d; it\u2019s about what you can create with technology, and how that process of creation can promote collaboration and true engagement. Teachers are less interested in devices and more interested in what it is about technology that engages students: the fact that technology enables students to take ownership of their own learning, collaborate with each other, and create something they can be truly proud of and present to the world.\n Lasting Impressions\n We are so inspired by this group of amazingly passionate, caring, thoughtful teachers that hailed from all corners of the globe\u2013from Australia to Alabama to Argentina\u2014to learn more so their students can learn more. So from everyone here at Quizlet, we want to give you a huge shout out for your commitment, passion, and innovation on behalf of your students!\n  \t\t\t\t \t\t\t\t \t\t\t\t\t \t\t\t\t\t\tThis entry was posted  \t\t\t\t\t\ton June 29th, 2012 at 3:21 pm by sophia\t\t\t\t\t\tand is filed under General.","item_date":"Jun 29 2012 07:00:00","display_item_date":"06-29-2012","url":"http:\/\/quizlet.com\/blog\/our-first-field-trip-to-iste\/","source":"quizlet.com"},{"title":"Zynga Releases a New API, Which Makes Creating Games Easier","details":"Today at an \u201cUnleashed\u201d event in its San Francisco headquarters, Zynga announced the new \u201cZynga API\u201d.\n Backend services don\u2019t require backend servers, and the company is opening the service up to the world.\n The company announced that 1M actions happen on its services per day, which means that the company can withstand some serious activity.  \n This is a developing story, please refresh for updates.","item_date":"Jun 29 2012 00:50:42","display_item_date":"06-28-2012","url":"http:\/\/thenextweb.com\/insider\/2012\/06\/26\/zynga-announces-the-new-zynga-api-so-that-people-can-enjoy-and-create-beautiful-games\/?awesm=tnw.to_o0Wh","source":"thenextweb.com"},{"title":"Google Compute Engine\u2013Day One Analsysis","details":"At GoogleIO today, Google announced the limited launch of its Infrastructure as a Service product, Google Compute Engine. This comes as no surprise, the move has been rumored for months and Google does undeniably have some of the largest scale infrastructure on earth alongside the smarts to run it amazingly effectively \u2013 a potent combination that shoud result in them having the ability to deliver IaaS as well as anyone else. Delivering IaaS however is not just about the technology, it\u2019s also about convincing customers as to the credibility of the service and building an ecosystem of providers around the core. Here\u2019s an analysis of where GCE is at day one (with the caveat of course that these things change very rapidly and I\u2019d expect Google to innovate quickly on the offering).\n Ten second summary \u2013 if Google wants to really broaden where Google Compute Engine goes, and its broader success, they need to build out the product, clarify the way it\u2019s built, increase the ecosystem and most importantly prove to the marketplace that they\u2019re serious about this project.\n Lack of detail\n Google has been a bit limited on details at launch. The product is in limited trials. There is no word on what kind of VM format the product uses and beyond the message that there is a \u201cvery clean REST API\u201d (in fairness the developer page does have a degree of information about the API), we don\u2019t know what that looks like or how broad the API is. That said, Google does have a very extensive history of building robust and broad APIs so this should not be too much of a concern. The chart below shows the options that are available at launch;\n \n It will however limit short term adoption as enterprises require a little more certainty before jumping on board. Perhaps for this reason Google is pushing the launch as ideally suited for short-term compute intensive workloads where portability and long term robustness are less important than brute strength. According to Google;\n The initial version of Google Compute Engine is designed to run high performance, computationally intensive virtual compute clusters in the Google Cloud. It can also be used as a generalized computing resource in combination with other Google Cloud technologies such as Google App Engine or Google Storage\n In terms of the details we have the information that GCE has multiple availability zones, but none in the EU or Asia Pacific. In terms of how Google is pitching the offering, they say that GCE will offer users the ability to;\n Maintain and store data in persistent block storage From a VM image, mount persistent block storage (persistent disk) that maintains state beyond the life cycle of the VM instance. Data on persistent disks are retained even if your virtual machine instance suffers a failure or is taken offline. Persistent disk data is also replicated for additional redundancy.\n \n Manage network access to your virtual machines Use your virtual machines alone or connected together to form a compute cluster\n Connect your machines to the Internet with a flexible networking solution that offers static and ephemeral IPv4 addresses for your instances.\n Use an easily configurable firewall to set up network access to your instances.\n Create an internal network of virtual machines or set up access to external traffic by setting up customizable firewall rules.\n Connect your VM instances to each other and to the Internet with our fully encapsulated layer 3 network. Our network offers strong isolation to help protect your instances from undesired access.\n Locate other instances in your project using DNS lookup of VM names.\n \n Use a variety of tools and OAuth2.0 authentication to manage your virtual machines Access your virtual machine instances through the Compute Engine console, RESTful API, or through a simple command line tool.\n Take advantage of OAuth 2.0 to authenticate to the RESTful API to create and delete virtual machine instances, disks, and other resources. Also, leverage OAuth 2.0 to seamlessly integrate with other Google Cloud services such as Google Cloud Storage.\n Use service account identities to authenticate your instances to other services, and remove the need to push keys into VM instances.\n \nInterestingly, according to RightScale, with GCE customers can encrypt data at rest both for local ephemeral drives as well as for network attached drives. In the case of the network attached disks the encryption happens on the host before it is put on the network, so it\u2019s also encrypted in transit. This will be a boon for security conscious organization who need encryption both at rest and transfer.\n Build the Ecosystem\n Google has built a reasonably strong ecosystem for initial launch \u2013 from management and deployment partners (RightScale, OpsCode and Puppet Labs)\n According to the RightScale post about the launch (RightScale are a launch integration partner) the offering is not API compatible with any other cloud. While many in the community are talking about the war between the API standards (with the AWS defacto standard on one side and OpenStack on the other) the existence of management providers such as RightScale sitting over infrastructure makes this something of a non-issue. Interestingly enough another management vendor, enSTratus, was not involved in this launch at all \u2013 that\u2019s certainly interesting for those of us who hold enStratus up as an indicator. MapR is partnered with GCE to offer its Hadoop distribution for GCE users, which in itself is kind of poetic since Google themselves pioneered MapReduce for their own internal search. WIth GCE, Hadoop is coming back to Google infrastructure.\n The Google situation is slightly more interesting however since Google themselves offer a number of disparate offerings that are synergistic with GCE. As Thorsten Von EIcken from RightScale said in his post;\n In the big picture, the most significant and perhaps most profound reason for Google Compute Engine\u2019s existence is the synergy with other Google cloud services. Currently Google offers Cloud Storage, Big Query, App Engine, and Cloud SQL as independent services. But if you look under the covers of App Engine there are services for message queues, push channels to browsers and mobile devices,\u00a0 NoSQL data stores, email, and more. All this starts to add up to as impressive a portfolio of cloud services as anyone can offer and up until today it was missing just one thing: a compute engine to tie it all together.\n Price, SLA and Support\n Google is promising pricing around 50% of other IaaS vendors \u2013 that\u2019s eminently compelling, assuming the service is robust and fully featured (early days yet). However cost isn\u2019t everything \u2013 the key thing for enterprise customers is whether or not a service has support attached and is backed by an SLA. In the case of GCE, Google asserts that GCE has an SLA behind it but in an ironic twist, the link to the SLA detail page is dead. As for support \u2013 Google is going to offer paid support for GCE customers but only via a discussion with the Google sales team \u2013 that will gives prospective customer cause for concern \u2013 Google needs to be very clear about the cost, availability and the breadth of the support offering.\n Pricing of the various GCE components is detailed in the diagram below;\n \n First a caveat \u2013 this is day one and no cloud product can be expected to be 100% sorted from day one. Google has an amazing economy of scale when it comes to infrastructure and those economies should be able to deliver IaaS on a very price competitive basis. That said, delivering enterprise grade solutions backed by clear SLAs and excellent service is probably something that Google struggles with \u2013 hence their focus on project based workloads that are all about bashing massive amounts of compute at a problem. In fact at their launch Google told of the Institute for Systems Biology who went from a 1,000-node internal cluster to a 10,000-node Compute Engine deployment, reducing the time to find associations within a genome from 10 minutes to \u201ca few seconds.\u201d That\u2019s compelling for that particular use case, but if Google wants to really broaden where GCE goes, and its broader success, they need to build out the product, clarify the way it\u2019s built, increase the ecosystem and most importantly prove to the marketplace that they\u2019re serious about this project.\n \n                                                                                                                              This entry was posted in Business and tagged Application programming interface, cloud computing, enStratus, google, google app engine, google compute engine, Google I\/O, Representational State Transfer, RightScale. Bookmark the permalink. Follow any comments here with the RSS feed for this post.                                                  Post a comment or leave a trackback: Trackback URL.","item_date":"Jun 28 2012 23:55:42","display_item_date":"06-28-2012","url":"http:\/\/www.diversity.net.nz\/google-compute-engineday-one-analsysis\/2012\/06\/28\/","source":"www.diversity.net.nz"},{"title":"Mobile Application Development with Appcelerator Titanium and Force.com","details":"Please check out our new hands-on article and tutorial, Developing Mobile Applications with Force.com and Appcelerator Titanium, that teaches you how to build cross-device mobile apps for Force.com and Database.com with Appcelerator Titanium. In this article, you\u2019ll learn how to use Titanium Studio, a powerful IDE that simplifies mobile application development and lets you rapidly build, test, package, and publish mobile apps. There are some really cool features in this tool, including Git integration, which the tutorial teaches you how to use. Thanks to Kevin Whinnery at Appcelerator for his contribution. Happy coding!\n   \t\t\t\t\t\t\t\t\t\t\t\n  \t\t\t\t\t tagged mobile, mobile application developmentBookmark the permalink. Trackbacks are closed, but you can post a comment.\nTweet","item_date":"Jun 28 2012 07:00:00","display_item_date":"06-28-2012","url":"http:\/\/feedproxy.google.com\/~r\/SforceBlog\/~3\/wSitZkt11LA\/mobile-application-development-with-appcelerator-titanium-and-force-com.html","source":"feedproxy.google.com"},{"title":"Kinvey and Urban Airship Integration Streamlines Data-Driven Mobile App Engagement","details":"Were thrilled to announce that we have expanded our partnership with Urban Airship, the leading platform provider for high-performance push messaging. This expansion unveils a seamless integration that enables developers to easily build a new class of push-enabled applications for native operating systems and the mobile web. At Kinvey, we make it easy for developers to set up, operate and manage scalable cloud backends for mobile, tablet and web applications. We have integrated our API and libraries with Urban Airship\u2019s to enable sending push notifications to devices running on iOS and Android through a single interface and SDK.\u00a0\u00a0\n              \n                                                                                                 \u00a0We have added to Urban Airships Helium messaging delivery platform, which offers very high throughput and enterprise-class SLAs, and reaches the broadest range of Android devices, including Nook and Kindle Fire.\n  This expansion ties in our private beta solution that melds Kinveys best-of-breed mobile backend and high-performance push messaging services with a variety of external cloud and legacy data sources. Through our open \u201cService Links,\u201d mobile apps can connect to third-party data sources\u00a0and use business logic to trigger push notifications based on data changing in any backend. An example use case of this new capability is tying into inventory and fulfillment systems and mapping APIs to notify drivers of new pickups, changing schedules or required rest periods.\n  To learn more about this integration, see the full press release here.\n  To provide your app users with a full-service cloud backend and an engaging experience through Urban Airships push notifications, be sure to sign up for Kinvey.","item_date":"Jun 28 2012 07:00:00","display_item_date":"06-28-2012","url":"http:\/\/www.kinvey.com\/blog\/item\/180-kinvey-and-urban-airship-integration-streamlines-data-driven-mobile-app-engagement","source":"www.kinvey.com"},{"title":"Our Awesome Collaboration with jclouds","details":"Remember, a little while ago,\u00a0when my boss asked \u201cWhat do developers want?\u201d\u00a0 Among the answers to that question is: they want to collaborate with other developers.\u00a0 It\u2019s way cooler and more fun to impress your friends with the work you\u2019re doing, right?\u00a0 It\u2019s even better when they join in and help you because they think what you\u2019re doing is important.\n That\u2019s the story I\u2019m going to tell today.\u00a0 When we looked around on how to best serve the Java community with a language binding, an obvious choice was to collaborate with\u00a0jclouds, which is an easy to use, open source library that enables Java developers to write against once and function against a wide variety of cloud providers.\u00a0 We wanted to develop code alongside the jclouds community and to contribute code back so anyone could extend the code. Our team spent time with jclouds founder Adrian Cole while working on early code that enabled jclouds to interact with our HP Cloud Identity Service, Object Storage, Compute, and CDN and a great friendship with them was formed that would ultimately enable us to showcase bursting from HP\u2019s private cloud offerings to our public cloud at HP Discover 2012.\n In the early days of our relationship with jclouds, HP Cloud Services\u2019 Jeremy Daggett and Mike Arnold worked directly with Adrian to come up to speed on jclouds and the intricate interaction of Google Guice dependency injection and Java annotations used behind the scenes that makes jclouds so easy to use for the end developer.\u00a0 As time went on, HP CloudSystem\u2019s Dirk Hogan and Siva Ghantasala joined as well and eventually our team\u2019s code was integrated back into the jclouds code submission flow so that it was part of their 1.4-alpha release.\u00a0 HP CloudSystem now enables live bursting of compute instances to HP Cloud Services.\u00a0 Awesome all around.\n Then, I had another idea.\u00a0 Enabling HP Cloud Compute to work with jclouds went so well, might Adrian and others in the jclouds community be interested in building for Block Storage as well if I could get them into\u00a0the Block Storage Private Beta?\u00a0 Yes, they were, and Block Storage support is now part of the jclouds 1.5-alpha release currently under testing.\n This collaboration with jclouds has been great for both sides.\u00a0 Adrian and team get early access to our newest services, providing cutting edge access to their constituents.\u00a0 We get a richer developer ecosystem and candid feedback earlier in our processes than would be otherwise possible.\n While it\u2019s the latest example, this collaboration with jclouds isn\u2019t the only one we\u2019ve worked on.\u00a0\u00a0Ruby Fog\u00a0is a similar multi-cloud provider library where we have contributed code.\u00a0 We created an early\u00a0PHP binding\u00a0and\u00a0built a Drupal module\u00a0on top of it for use on our own web sites, both of which are open for anybody to use.\u00a0 Our Windows Powershell CLI is based on a.NET binding whose source we\u2019ve made available\u00a0too.\u00a0 This is why we\u2019re called \u201cDeveloper Experience\u201d, not \u201cUser Experience\u201d.\u00a0 We are developers ourselves and we want the best experience possible for our like-minded peeps in all the communities they interact with.\n Want in on all the fun?\u00a0 There are plenty of other languages and platforms out there we\u2019re interested in collaborating with.\u00a0 Interested?\u00a0 Find me on Twitter @nerdguru and let me know!","item_date":"Jun 28 2012 07:00:00","display_item_date":"06-28-2012","url":"http:\/\/h30529.www3.hp.com\/t5\/HP-Scaling-the-Cloud-Blog\/Our-Awesome-Collaboration-with-jclouds\/ba-p\/503","source":"h30529.www3.hp.com"},{"title":"Developer preview! Build your app directly into foursquare with our new Connected Apps platform","details":"Over 25,000 developers use the foursquare API to build apps based around people\u2019s experiences in the real world. Until today, though, those apps, whether they help keep people healthy, plan trips, or even give dogs a treat, have been stand-alone. Today, we\u2019re announcing the developer preview of the next evolution: Connected Apps \u2013 developer-crafted experiences that exist within foursquare.\n This opens up a ton of possibilities, all around interacting with people\u2019s experiences while they\u2019re having them. For instance, Eat This, Not That\u00a0can suggest healthy dishes the moment someone checks in at a restaurant. Or The Weather Channel can tell people the forecast when they check in to a new city.\u00a0Whatever you build, anyone in foursquare\u2019s 20,000,000-strong community can connect it to their account and see your messages in their post-check-in screens. We\u2019ll attribute all your app\u2019s contributions, so other people can connect your app or click through to your site.\n \n Developers, you can start testing this out today with the new developer preview endpoints in our API (the documentation is here). It\u2019s a developer preview because we want to see what you do, what our users are looking for, and where we should take the platform.\n In working towards this preview, we partnered some companies to help bang on the endpoints and build apps that flex the power and potential of the apps platform. Some of them (Foodspotting, Sonar) are enhancing existing integrations, some of them (Eat This, Not That,\u00a0The Weather Channel) are new to the foursquare platform and are experimenting around how to better engage their users.\n Our preview partners below can be found in the app gallery, which we\u2019ll update again with our full launch:\n Eat This, Not That \u2013 From Men\u2019s Health Mag, Eat This, Not That tells you what dishes to order and what to avoid at restaurants when you check in\n Untappd\u00a0- Shows recommendations from friends and others on which beer to order, and easily check-in to beers from within foursquare\n Sonar \u2013 Informs you of your connections to interesting people nearby\n Soundtracking \u2013 Share and discover songs your friends have shared at a place\n Snoball \u2013 Trigger charity donations when you check in, and tells you about friends\u2019 donations\n Foodspotting \u2013 Share and get recommendations on which dishes to order\n GroupMe\u00a0- Makes it easier to share your check-in with a group of friends on GroupMe\n Blue Legends\u00a0- Notifies you when you earn extra rewards for checking into Lufthansa venues\n Instagram \u2013 Share your Instagram photos on foursquare, which friends can see and tap through to the Instagram app\n Path \u2013 Share photos and text from Path, which tap through to the Path app\n The Weather Channel \u2013 Get the weather forecast with your first check-in of the day and when you check in to a new city\n \nYou can read all the details over at our developer blog\u00a0and overview. We\u2019re rolling this out to iPhone and Android today, and BlackBerry soon to follow. We\u2019re still in the testing and development phase, so please send feedback here, sign up for the foursquare-api mailing list for updates, and stay tuned for a larger launch later this year!","item_date":"Jun 28 2012 07:00:00","display_item_date":"06-28-2012","url":"http:\/\/feedproxy.google.com\/~r\/thefoursquareblog\/~3\/3ewxEJLuwBE\/","source":"feedproxy.google.com"},{"title":"Use Springpad with Facebook to Share Your Ideas","details":"Our goal with Springpad is to help you make better decisions by connecting you with useful information from people you trust. And now, with our new Facebook integration, we\u2019re making it even easier to share ideas with your real-life friends! \n \n Many of us use Facebook to like books, movies and music or to check-in to restaurants and businesses that we visit. But there\u2019s no simple way to share a searchable, savable list of those things with your friends \u2013 until now! With our new Facebook integration, you can automatically import all of the things you like and the places you check-in to on Facebook into Springpad, creating easy, organized notebooks of the things you care about. By importing your likes, we\u2019ll give you a running start to creating a list of your favorite things and places that you can share with your friends!\n And of course, the things that are imported into Springpad will automatically be enhanced with useful links, media and information from places like foursquare, OpenTable, Yelp, Rotten Tomatoes and more!\n \n By adding Springpad to your timeline, you can effortlessly share your public springs with your network of friends. On your Facebook profile, you\u2019ll see a new section with your recent Springpad activity. And, your friends will see the most interesting things that your do on Springpad in their news feeds. \u00a0Of course, you can still choose to manually share individual notebooks or springs to Facebook, Twitter or email at any time.\n When your friends see the things you\u2019re sharing to Facebook, they can sign up for Springpad to follow any of your public notebooks. Based on the interests that you have in common, your friends can decide which of your notebooks to follow \u2014 allowing them to see the shares from you that they really care about!\n \n To link an existing account, log in at Springpad.com and visit your Settings. If you\u2019re a new user, you can register for a new Springpad account by signing up with your Facebook credentials. More details about linking with Facebook.\n Once you link your Springpad account to your Facebook account, you will automatically be following your Facebook friends who are already using Springpad, so you can benefit from their likes, springs and shares!\n \nDo you have questions or concerns related to the new Facebook integration? We\u2019ve got your back! Check out the Springpad and Facebook FAQ post, or get in touch with us at feedback@springpad.com.","item_date":"Jun 28 2012 07:00:00","display_item_date":"06-28-2012","url":"http:\/\/springpad.com\/blog\/2012\/06\/use-springpad-with-facebook-to-share-your-ideas\/","source":"springpad.com"},{"title":"Livefyre launches on FOXSports.com","details":"Sports and social media were made for each other. You can watch the game and chat with your fellow fans and foes simultaneously on Twitter, you can keep the friendly childhood rivalries burning on Facebook, and blogs let you publish your views any topic from what LeBron was really thinking when he moved to Miami to your brother\u2019s regrettable trade in fantasy football.\n Well sports fans, now you can express your unique perspective on every aspect of sporting news on FOX Sports sites in real-time conversations powered by Livefyre. Starting today, you can immerse yourself in sports conversations from around the world on FOXSports.com on MSN, and SPEED.com\u00a0with other FOX Sports sites coming soon.\n Share your opinion and watch the reactions in real-time without refreshing your page.\n Easily call out your friends and followers from Facebook and Twitter and include them in the conversation.\n Keep track of the related banter from Facebook and Twitter directly on the page.\n \nTrust us, you won\u2019t be able to pull your eyes off the stream this weekend during the final of Euro 2012,\u00a0or when speculating on the outcome of Friday\u2019s showdown between\u00a0Michael Phelps and Ryan Lochte in the 200 meter individual medley. We look forward to watching the conversations unfold on FOXSports.com (and yes, we\u2019ll be there to add our two cents too \u2013 Viva Espa\u00f1a!).","item_date":"Jun 28 2012 07:00:00","display_item_date":"06-28-2012","url":"http:\/\/blog.livefyre.com\/livefyre-launches-on-foxsports-com\/","source":"blog.livefyre.com"},{"title":"APIS.io :: The open-source API directory","details":"APIS.io is an open source and free API registry service that allows    developers to publish and discover REST APIs and interact with them online.","item_date":"Jun 28 2012 05:12:51","display_item_date":"06-27-2012","url":"http:\/\/apis.io\/","source":"apis.io"},{"title":"The ISTE 2012 Exhibit Hall & the State of Data Portability in Education","details":"Typically, discussions about this involve student information systems \u2013 students\u2019 names and grades \u2013 and the inability for other applications that schools use to pull from or feed into them. But \u201cstudent data\u201d is more than that. Now it includes everything that a student does digitally. So when there is no portability, it isn\u2019t simply that thee data about enrollment or test scores is trapped in these systems; it\u2019s that students can\u2019t control, access or keep their own information.\n \nThis past weekend, I spoke with a mom who told me about the new 1:1 iPad program at her daughter\u2019s middle school. At the beginning of the year, the school offered parents the chance to buy the iPad. But this particular mom said she couldn\u2019t afford it, nor was she particularly interested in doing so \u2013 she wasn\u2019t convinced the iPad was useful or necessary, let alone \u201cworth it.\u201d But as the school year progressed, and she saw all the things her daughter was using the iPad for, she changed her mind. So at the end of the year, when it was time to return the iPad to the school, she asked if she could purchase it. \u201cNo,\u201d the school said. She\u2019d missed her one and only chance at the beginning of the year. The school took the iPad back.\n But it wasn\u2019t just the device that the school took back. There was data on that device \u2013 all the student\u2019s assignments, all her drawings, all her creative writing, all her photos, and yes all the free games she\u2019d played and levels she\u2019d achieved. In most cases, that data was trapped in those apps. No iPad for this family now means no record of much of what this young girl did in sixth grade.\n We can cheer for the end of the paper-oriented classroom all we want; and much of the reams of papers that kids bring home at the end of the year probably just gets tossed away. But at least with paper we have that option. In an increasingly digital world \u2013 and a world without data portability \u2013 we don\u2019t have that option. Students and their families have very little control. And we aren\u2019t asking enough hard questions when we adopt technology tools in the classroom about this: how do students get their data out? Is data stored in the cloud, and is it accessible via a Web interface?\n \n This story weighed heavily on me as I ventured in to the ISTE 2012 Exhibit Hall. I had my boyfriend Kin Lane who works as an API evangelist in tow. We planned to walk up and down the aisles and ask every company whether or not they have an API, an application programming interface. As I\u2019ve written about here previously, an API is a way for applications to communicate with one another, to request and to share information. An API isn\u2019t a guarantee that you won\u2019t end up with data silos, but having one is certainly a sign that the application is built to be extensible -- and extensible without having to have special downloads or special partners or secret handshakes to make things happen.\n But with hundreds and hundreds (and hundreds and hundreds) of booths, there was no way we were going to talk to everybody. Frankly, two steps into the exhibit hall, I was ready to turn on my heels and leave.\n So I can\u2019t offer a complete listing of who on the showroom floor had APIs. But based on our small sample, most do not. Most of the folks working the booths didn\u2019t know what APIs are. (And, sorry, this is a tech conference, so no excuses for that.) \u201cThis is the first time anyone has asked ever asked me this question,\u201d one person said. That means it isn\u2019t just the tech companies\u2019 problem here; it\u2019s educators\u2019 too.","item_date":"Jun 28 2012 05:07:18","display_item_date":"06-27-2012","url":"http:\/\/hackeducation.com\/2012\/06\/27\/iste-2012-exhibits-student-data-portability\/","source":"hackeducation.com"},{"title":"First Ever Allen Brain Atlas Hackathon Unleashes Big Data API to Push Neuroscience Forward - MarketWatch","details":"SEATTLE, Jun 25, 2012 (BUSINESS WIRE) -- The Allen Institute for Brain Science convened the first ever Allen        Brain Atlas Hackathon last week, opening its doors to a diverse group of        programmers and informatics experts for a non-stop week of        collaboration, learning and coding based on its public online platform        of data, tools and source code. The event brought together more than 30        participants from top universities and institutes ranging from the        Baylor College of Medicine in Houston to the Nencki Institute of        Experimental Biology in Poland, as well as from start-ups and        established technology companies, to develop data analysis strategies        and tools based on the newly enhanced Allen Brain Atlas application        programming interface (API).                                     \n                                          This hackathon stems from our longstanding, open approach to science        and our belief that putting our data-rich resources in the hands of the        many and varied experts around the globe is the most effective way to        drive progress in brain research, said Chinh Dang, Chief Technology        Officer of the Allen Institute for Brain Science. The hackathon        projects delivered innovative ways of handling data, offering direct        contributions to the informatics and programming communities as well as        to neuroscience. We hope that this event serves as a springboard for        others out in the community to use our API, and we look forward to        seeing what can be done with it.                                     \n                                          The Allen Institute for Brain Science is one of the biggest data        producers in neuroscience, with rapidly growing data stores in the        petabyte range that it makes publicly available through its Web-based        Allen Brain Atlas resources (   www.brain-map.org    ).        These resources include, among others, anatomically and genomically        comprehensive maps of genes at work in the mouse and human brains and        receive approximately 50,000 visits each month from researchers around        the globe.                                     \n                                          The public API was created as an additional form of data sharing to spur        community technology development and further empower scientists to make        groundbreaking discoveries about the brain in health and        disease--including insights into learning, cognition, development,        Alzheimers, obesity, schizophrenia, autism, and more--that will deliver        better treatment options sooner. The hackathon coincided with the public        release of the full Allen Brain Atlas API earlier this month, and a key        goal of the event was to ignite community momentum and interest in using        it.                                     \n                                          Using the Allen Brain Atlas API, developers can create entirely new        software applications, mashups and novel data mining tools for making        sense of the large and ever-growing volumes of neuroscience data. The        API offers data access across species, ages, disease and control states,        providing a powerful means to compare many types of data (e.g.,        histology images, gene expression, and MRI) among many types of samples        (e.g., ages, species or diseases).                                     \n                                          The Allen Institute is a leader in large-scale open science, known for        providing high-quality data and online tools that advance brain        research, said Sean Hill, Executive Director of the International        Neuroinformatics Coordinating Facility (INCF). With the Allen Brain        Atlas Hackathon and their public API, they are bringing the same        collaborative, community-focused approach to technology development and        innovation that is at the core of INCFs mission.                                     \n                                          The hackathon program was designed to provide scientists and programmers        a solid foundation in using the Allen Brain Atlas API for data mining,        data analysis and tools development. The event featured a handful of        speakers from the Allen Institute, as well as external experts who had        leveraged earlier versions of the API in their work. As a hands-on        workshop, participants spent most of the time working on projects of        their choice. The Allen Institute development team actively participated        throughout the week to provide specific examples of API usage, as well        as to team up with community participants to develop collaborative        projects. Participants presentations throughout the week showcased        their projects and progress, stimulating new ideas and benefiting from        the collective feedback and troubleshooting power of the entire group.                                     \n                                          Projects ranged from practical applications, such as using a list of        glioblastoma-related genes to discover biological patterns that could        shed new light on the biology of the disease and developing strategies        to use gene expression data with functional brain scanning technologies,        to purely creative applications, including translating genomic data into        music.                                     \n                                          Source code from participants projects will be made publicly available        on the Allen Brain Atlas data portal (   www.brain-map.org    )        as part of the Allen Institutes next public data release in October, as        well as through the INCF website (   www.incf.org    ).                                     \n                                          The Allen Brain Atlas Hackathon was hosted by the Allen Institute for        Brain Science and funded jointly with the International Neuroinformatics        Coordinating Facility (INCF).                                     \n                                          About the Allen Institute for Brain Science                                     \n                                          The Allen Institute for Brain Science (   www.alleninstitute.org    )        is an independent, 501(c)(3) nonprofit medical research organization        dedicated to accelerating the understanding of how the human brain works        in health and disease. Using a team science approach, the Allen        Institute generates useful public resources used by researchers and        organizations around the globe, drives technological and analytical        advances, and discovers fundamental brain properties through integration        of experiments, modeling and theory. Launched in 2003 with a seed        contribution from founder and philanthropist Paul G. Allen, the Allen        Institute is supported by a diversity of government, foundation and        private funds to enable its projects. Given the Institutes        achievements, Mr. Allen committed an additional $300 million in 2012 for        the first four years of a ten-year plan to further propel and expand the        Institutes scientific programs, bringing his total commitment to date        to $500 million. The Allen Institutes data and tools are publicly        available online at    www.brain-map.org    .                                     \n                                          SOURCE: Allen Institute for Brain Science","item_date":"Jun 28 2012 02:39:36","display_item_date":"06-27-2012","url":"http:\/\/www.marketwatch.com\/story\/first-ever-allen-brain-atlas-hackathon-unleashes-big-data-api-to-push-neuroscience-forward-2012-06-25","source":"www.marketwatch.com"},{"title":"Google+ History API will bring in your past updates from around the web | VentureBeat","details":"Called Google+ History, this new API will let you add past statuses, updates, purchases, pictures, and more to you personal Google+ timeline from a variety of social and mobile services. These objects are displayed in pretty tiles called \u201cmoments.\u201d\n This feature wasn\u2019t included in the Google+ update announcements at Google I\/O, the company\u2019s developer conference happening this week in San Francisco. But it ties Google+ to existing social networks in some interesting ways, and it may soon have some ties to the just-announced Google+ Events as well.\n With History API-enabled apps, you\u2019ll be able to browse through your Google+-linked accounts on a range of social services and apps \u2014 Gray mentioned Foursquare and Twitter as examples. You can then pick and choose from past events, images, updates, and checkins, pulling those items into your Google+ profile.\n Basically, for users it\u2019s a more or less convenient way for you to beef up your shiny, (relatively) new Google+ profile with rich content from your past and to give your profile a more Facebook-like, comprehensive snapshot of who you are. It\u2019s comparable to similar Facebook Timeline features that allow you to affix earlier dates and location tags to statuses and photos.\n For developers, it\u2019s a great way to push your app\u2019s content into the Google+ stream \u2014 and with any luck, create a Facebook Timeline-like bump in usage similar to how you\u2019d use Facebook Actions for your app\u2019s structured data.\n And while other-network updates can\u2019t yet be tied to Google+ updates and events or grouped together themselves (think of linking a collection of Flickr pictures with a Foursquare checkin, or grouping all updates from an app \u00e0 la Timeline\u2019s structured-data groupings), don\u2019t be surprised if features like that pop up soon.\n You can check out the new API and documentation for yourself. For now, this is API is in developer preview mode. \u201cTake that, puny users, mwahaha,\u201d said a fictional Google rep.\n Stay tuned for lots more from Google I\/O today \u2014 the fun and news are all just getting started.","item_date":"Jun 28 2012 00:34:19","display_item_date":"06-27-2012","url":"http:\/\/venturebeat.com\/2012\/06\/27\/google-plus-history\/#.T-uaeOvwbqU.twitter","source":"venturebeat.com"},{"title":"Flexible and Collaborative API Design and Documentation with Apiary.io","details":"A big part of APIs success is the agility and flexibility they introduce into development and business processes by providing self-service, valuable, programmatic interfaces that can be used to quickly deliver new resources or data, allowing collaboration between 3rd party groups.  With all of this flexibility and collaboration, often times many APIs are designed and developed in isolation, without much interaction with the consumers who will actually be using the interface.\u00a0apiary.io, a new API service provider is looking to change all of this with their new collaborative, REST API documentation platform.\n\n\nApiary.io starts with what they call the API Blueprint, which is a custom DSL (domain-specific language) allowing you to quickly describe your API, and from this blueprint apiary.io will then generate API documentation, a debugging proxy and bug reports.\n\n\n\n\n\nOnce you\u2019ve designed the initial blueprint for your API, the auto generated testing interfaces and documentation provides an interactive way for your intended API consumers to begin making calls and playing with the API interface, while providing you with an inspector for you to monitor interactions with the API, while debugging and providing feedback in real-time.\n\n\napiary.io is a fast way to statically mockup your API and deliver a prototype that allows for real-time interaction with your target API consumers, allowing you to efficiently build exactly the API they need before investing too many resources into development.  The apiary.io platform also provides GitHub integration allowing you to store your API Blueprint in your repositories, providing automatic updates as soon as you commit a new version\u2013extending the collaboration to the social coding environment where you are already developing.\n\n\n\n\n\nOther API documentation tools like Swagger, provide a way to generate documentation and code samples once your API is done and defined using a Web Application Discovery Language(WADL), but apiary.io delivers documentation in real-time as your designing and mocking up your API.  This process stands to vastly improve API design, while also allowing you to much more efficiently design, develop and evolve APIs around your business data and resources.\n\n\napiary.io is poised to change the way we think about API design and development, radically increasing the pace in which APIs can be deployed.  If you want to get involved in this new way of approaching APIs you can register on the site, but since it\u2019s a closed beta, it could be some time before you will get access.  Don\u2019t worry though, the current beta users have the ability to invite other users, so if you look around in the API community you might just be able to find someone you can get an invite from.","item_date":"Jun 27 2012 21:36:20","display_item_date":"06-27-2012","url":"http:\/\/blog.programmableweb.com\/2012\/06\/27\/flexible-and-collaborative-api-design-and-documentation-with-apiary-io\/","source":"blog.programmableweb.com"},{"title":"Android Push Notifications are LIVE!","details":"Producteev                  Producteev.com is a leading Cross-Platform Task Management Application for Teams and Individuals. We have apps for  IM, E-mail, iPhone, Android, Mac, Windows, Web, Google Cal, and more... The Task Manager that works the way YOU do. Come check us out at http:\/\/www.producteev.com\n                  Twitter FacebookPage Tumblr","item_date":"Jun 27 2012 07:00:00","display_item_date":"06-27-2012","url":"http:\/\/blog.producteev.com\/141686933","source":"blog.producteev.com"},{"title":"Introducing API for Screenshots.com","details":"Exciting news! We now have an API for Screenshots.com, and at least for now, it\u2019s completely free. We\u2019re letting anyone in the world use it (within reason!), and we\u2019re giving away both small and large screenshots plus the complete historical record for a domain.\n You can learn more about access, api documentation and the terms of service at Screenshots.com\/api\/\n Let us know what you think!","item_date":"Jun 27 2012 07:00:00","display_item_date":"06-27-2012","url":"http:\/\/blog.domaintools.com\/2012\/06\/introducing-api-for-screenshots-com\/","source":"blog.domaintools.com"},{"title":"Public transit routing and layer now available in the Google Maps API","details":"Google Maps API now enables developers to add Transit data, including public transit directions, to their maps and apps.\n Whether youre planning a trip from your computer or on the spur of the moment from your mobile device, Google Maps helps you find directions in more than 475 regions around the world. Today were pleased to announce that public transit directions are now available in the Google Maps API.\n Public transit has been one of the most requested features by Maps API developers, and you can now use it in both the Google Maps Javascript v3 and the Directions Web Service. Its simple for you to update your apps to also offer routing by public transit in addition to driving, bicycling, and walking. The transit route responses include the number of stops, direction of travel and more. It will also tell you what type of vehicle you will be travelling on. Everything from a typical subway train to a funicular! \n To support the launch of routing by public transit were also adding the Transit Layer to the Maps API. For example if you are a retail chain, the Transit Layer allows you to show all the bus major transit lines that run past each store. The Transit Layer can be displayed by enabling the TransitLayer(), it\u2019s as simple as that!\n If youre using a Google Map, you can now use the Directions API web service to add this useful and helpful transit data to your map. As always, if you have any questions about public transit in the Google Maps API, we recommend posting to our sponsored tag on Stack Overflow, or if you\u2019re at Google I\/O swing by at the Google Maps Developer Sandbox. Happy commuting!","item_date":"Jun 27 2012 07:00:00","display_item_date":"06-27-2012","url":"http:\/\/feedproxy.google.com\/~r\/blogspot\/Gkaos\/~3\/sXAxTwdWE-4\/public-transit-routing-and-layer-now.html","source":"feedproxy.google.com"},{"title":"Now delivering location-based offers tied to mobile payments: Placecast launches the first-ever white-label HTML5 Mobile Wallet ","details":"Placecast announces the launch of its latest product: an innovation in the mobile space that combines location-based mobile marketing with mobile commerce - the ShopAlerts Mobile Wallet.\n Built in HTML5, the Wallet works on all smartphone platforms and offers customers a first-ever turnkey solution for launching personalized location-based mobile offers tied to mobile payments. There\u2019s no app to build for marketers, and no app for consumers to download. \n Unlike other mobile wallet offerings, Placecast will white-label the product specifically to credit card companies, mobile operators and retailers through a robust set of API\u2019s, or through the reskinnable HTML5 Wallet.\n Check out the video in our next post for a closer look at some of the Wallets\u2019 features and user-experience.","item_date":"Jun 27 2012 07:00:00","display_item_date":"06-27-2012","url":"http:\/\/blog.placecast.net\/post\/26004996965","source":"blog.placecast.net"},{"title":"Google Doubles Down on Messaging","details":"Today at the I\/O conference Google unveiled the next iteration of its Android Cloud to Device Messaging (C2DM) service, now called Google Cloud Messaging (GCM). \u00a0There are some key differences as well as enhancements that we are sure will please both developers and app users:\nMulticast messages: in the same request you can now broadcast messages up to 1000 registration IDs at the same time\nMultiple senders: now multiple parties can send messages to the same app with one common registration ID\nTime-to-live messages: you can set values for your messages between 0 to 4 weeks helping to ensure messages about events that expire don\u2019t reach your users after the fact\nC2DM will be maintained for the short term but will accept no new users nor grant new quotas. We\u2019re evaluating all the new changes to GCM and intend to add full support for it soon. In addition, we expect our Helium message delivery platform for Android devices will continue to offer advantages to developers including very high throughput and enterprise-class SLAs.\nAlso, initial details are coming out today about major enhancements Android 4.1 (Jelly Bean) brings to the Android notification framework. Apps can now display larger, richer notifications to users that can be expanded and collapsed with a pinch. Users can take actions directly within notifications, such as clicking buttons to send messages like \u201cbusy\u201d or \u201ccall soon\u201d to a missed call notification. And notifications are listed to show the most urgent things first, not just the most recent. In addition, users can long-press notifications for more options including getting information about the sender and having the option to disable further notifications from the app.\nA slide shown at Google I\/O summarized these major enhancements with a title of \u201cThe user\u2019s attention is precious\u201d\u2014we couldn\u2019t agree more. Following Good Push best practices can ensure that your push messages deliver a tremendous engagement and retention uplift, and both serve and delight your users.","item_date":"Jun 27 2012 07:00:00","display_item_date":"06-27-2012","url":"http:\/\/urbanairship.com\/blog\/2012\/06\/27\/google-doubles-down-on-messaging\/","source":"urbanairship.com"},{"title":"Enter the Sandbox \u2013 Zazzle at Google I\/O","details":"If you\u2019re headed to Google I\/O today or tomorrow, be sure to stop by the Zazzle pod.  Google I\/O is an event focused on the tech world\u2019s latest web, mobile and social breakthroughs and bringing together thousands of developers who are turning them into tomorrow\u2019s startups.  Zazzle will be in the \u201cDeveloper Sandbox,\u201d to showcase the technologies we\u2019ve pioneered (RSS Feeds, Store Builder, Zazzle\u2019s API) and how those can be used with Google products.  Of course we\u2019re also going to encourage developers to go to the Zazzle Google Doodle store to get their exclusive Google Doodle Shirts, Google Doodle Posters, Google Doodle mugs, and more.  What developer can live without this Google shirt?","item_date":"Jun 27 2012 07:00:00","display_item_date":"06-27-2012","url":"http:\/\/blog.zazzle.com\/2012\/06\/27\/enter-the-sandbox-zazzle-at-google-io\/","source":"blog.zazzle.com"},{"title":"The University of Dayton \u201cAdmits\u201d Team Matchbox","details":"In a press release that was put out today, Dayton\u2019s VP of Enrollment, Sundar Kumarasamy talked about why he decided to use Matchbox as his admissions weapon of choice.\u00a0\u201dFor us, using Matchbox was a no brainer,\u201d said Sundar Kumarasamy, vice president for enrollment management and marketing at the University of Dayton. \u201cMatchbox provides us with an intuitive solution that empowers our readers to give each applicant the critical attention they deserve while saving us countless hours in the process. This transition represents a significant leap forward for our admissions reviews process. By providing a more personal, holistic, clutter-free view of the applicant, we can effectively match students to our school.\u201d\n The mobility of Matchbox provides schools with a platform that matches the mobility of admissions professionals. An admissions officer can sit down with a prospective student and have them walk the officer through their application without sifting through papers or awkwardly balancing a laptop. We believe the admissions process should be about experiencing the applicant\u2019s story, and we are thrilled to have another customer who values the portability of the iPad and the power of the data that the Matchbox toolkit provides.\n Our CEO and Founder, Stephen Marcus, weighs in on the Dayton-Matchbox partnership: \u201cThey have forward-thinking admissions professionals who are elevating their competitive efforts with Matchbox. The University of Dayton has a proven track record of embracing innovative admissions and student retention methods. They are constantly sharpening their tools by refining best practices and adopting the latest technology available to identify their best applicants faster.\u201d\n Welcome to the Team, Dayton!","item_date":"Jun 27 2012 07:00:00","display_item_date":"06-27-2012","url":"http:\/\/feedproxy.google.com\/~r\/getmatchbox\/~3\/f886sbh6OkU\/","source":"feedproxy.google.com"},{"title":"Developer Plugin v1.0: Helping WordPress developers develop","details":"One of the great things about developing for WordPress is the number of tools available for developers. WordPress core ships with a  bunch of useful features (e.g. WP_DEBUG\n) with many more built by the community (like our own Rewrite Rules Inspector and VIP Scanner) that make development and debugging a breeze. The hardest part is getting your environment set up just right: knowing what constants to set, what plugins to install, and so on.","item_date":"Jun 27 2012 07:00:00","display_item_date":"06-27-2012","url":"http:\/\/developer.wordpress.com\/2012\/06\/27\/449\/","source":"developer.wordpress.com"},{"title":"Integrating Facebook and Twitter with your Buddy-powered apps","details":"Lately we\u2019ve been getting a lot of questions on how to integrate Facebook and Twitter with Buddy on iOS, so we decided to build to samples to illustrate how this is done.\n Currently with iOS 5 it\u2019s easier to talk to Twitter then Facebook because of the deep Twitter integration with the OS, however both samples should be straightforward. At the most basic level both apps try to get your Facebook\/Twitter information and then check for a Buddy username with the same name as the account on Facebook or Twitter. If the name is not found they create a new user with the same account name and the password set to the hash of the account ID.\n To configure the samples, take a look at the README file in the top-level directory of each sample. There are a few steps you will need to follow to get them running, especially for the Facebook samples.\n As before the samples can be found in the Samples section of the documentation: http:\/\/buddy.com\/documentation.aspx#SDKs_and_Samples or more specifically you can find:\n The Facebook sample here: http:\/\/buddy.com\/sampledownload\/ios\/BuddyFacebookSample.zip\n The Twitter sample here: http:\/\/buddy.com\/sampledownload\/ios\/BuddyTwitterSample.zip\n \nIf you need more help with the samples, or you find issues with them, don\u2019t hesitate to send us an email at: support@buddy.com.\n \t\t\t\n  \t\t\t\t \t\t\t\t\t\t \t\t\t\t\t\t \t\t\t\t\t\t \t\t\t\t\t   \t \t\t \t\t\t\t\t\t \t \t\t\t \t\t\t\t \t\t\t\t\t \t\t\t\t\t\tThis entry was posted \t\t\t\t\t\t  \t\t\t\t\t\ton Wednesday, June 27th, 2012 at 3:21 pm\t\t\t\t\t\tand is filed under Technical Updates. \t\t\t\t\t\tYou can follow any responses to this entry through the RSS 2.0 feed.  \t\t\t\t\t\t \t\t\t\t\t\t\t\t\t\t\t\t\tYou can leave a response, or trackback from your own site.","item_date":"Jun 27 2012 07:00:00","display_item_date":"06-27-2012","url":"https:\/\/blog.buddy.com\/2012\/06\/integrating-facebook-and-twitter-with-your-buddy-powered-apps\/","source":"blog.buddy.com"},{"title":"Eclipse Git Plugin 2.0 Released","details":"We are proud to also be launching eclipse.github.com to highlight all the new features and improvements.\n   \n  Eclipse Juno includes the 2.0 release of EGit, the Eclipse Git plugin, which has changed a lot since 1.0 with newly added support for submodules and stashing as well as tons of performance improvements.\n  These new features combined with improved support for merge, rebase, synchronize, and blame make EGit 2.0 the biggest release to date and definitely worth upgrading\/installing today.\n  The 2.0 GitHub plugin has also come a long way with full support for working with Issues, Pull Requests, and Gists.  You can also now search for and clone repositories from GitHub.com from within Eclipse.\n  Head on over to the new http:\/\/eclipse.github.com to download and read all about the improved Git and GitHub plugins available for Eclipse Juno.\n   \n       We make sure to read every mention on Twitter. If you find a bug, submit it to support@github.com. Every email is read by a real person.","item_date":"Jun 27 2012 07:00:00","display_item_date":"06-27-2012","url":"https:\/\/github.com\/blog\/1181-eclipse-git-plugin-2-0-released","source":"github.com"},{"title":"Monkey Mace - Craigslist, LinkedIn, Netflix, and others, don't owe us anything.","details":"Articles have been cropping up that lambast Craiglist (1, 2), Netflix (1) , and LinkedIn (1) for policing their APIs and criticize them for stifling innovation.\n API access is not a charitable offering to the start-up community for these obviously for-profit companies. The goal of any company who offers an API to their data is to build a eco-system that promotes lock-in and added benefits for their service, not to help other companies profit and take advantage of their hard work.\n I agree with some of the criticisms that point out the inconsistency of these companies enforcement of their terms of service. Regardless, we should not be shocked that they will exercise tight control and act quick to stamp out anything that threatens their business.\n In the case of the recent controversy surrounding Pealk, a start-up that was using LinkedIn\u2019s API to provide premium features to users at a discount, should we really be shocked or up in arms that LinkedIn shut them down?\n Start-ups should stop feeling entitled to other companies\u2019 data in the name of innovation, and either:\n Man up and build a start-up where you collect all the necessary data you need yourself.\n Build services around APIs that help make the API offering company so much money and benefit that they don\u2019t mind if you take a little coin for yourself in the process.\n \nReally, please just stop acting like spoiled brats. Nobody owes you anything.","item_date":"Jun 25 2012 20:09:40","display_item_date":"06-25-2012","url":"http:\/\/monkeymace.com\/post\/25740159275\/craigslist-linkedin-netflix-and-others-dont-owe-us-anyth","source":"monkeymace.com"},{"title":"Amazon Web Services Blog: The AWS Partner Network (APN) is Now Live","details":"The AWS Partner Network (APN) is now live, with three tiers and a wide variety of benefits at each tier. This is a global program, accessible to customers and partners world-wide.\n  In addition to the official launch of APN, we are also enhancing our AWS Channel Reseller Program and setting up a new Authorized Government Partner designation.\n  If you are an AWS user, APN gives you the power to find high quality partners to help you get the most from the AWS cloud. You can get started by searching the AWS Partner Directory:\n  \n  If you are part of the AWS partner ecosystem, APN provides you with the technical, sales, and marketing support that you need to build a successful cloud business. The program offers benefits to Technology Partners who develop applications and developer tools on top of AWS and Consulting Partners who help our customers to get up and running on AWS.\n  As an APN partner, you qualify for one of three tiers - Registered, Standard, or Advanced, with specific benefits at each level. Registered Partners have access to the following items:\n  Partner training\n  Partner webinars\n  Partner events\n  Partner newsletter\n  \n  We are also expanding the AWS Channel Reseller Program. This program is designed for Systems Integrators and Managed Service Providers who provide professional services on the AWS platform and wish to resell and manage AWS services for their customers. Advanced tier partners can qualify as Channel Reseller Partners to gain eligibility for discounts on published AWS pricing and public designation as an AWS Channel Partner. These partners must own and manage AWS accounts on behalf of customers, provide help desk support, and meet several other criteria.\n  Finally, we are also launching a new Authorized Government Partner designation. This designation is reserved for AWS Partners with a public sector AWS practice and a desire to target public sector customers at the Federal, State, and Local levels. Standard and Advanced tier partners can qualify for this designation if they have at least two government (Federal, State, or Local) contracts at the prime level that are eligible AWS customers. The partner must execute a Government addendum to the AWS Reseller Agreement, and meet other requirements.","item_date":"Jun 25 2012 19:45:19","display_item_date":"06-25-2012","url":"http:\/\/aws.typepad.com\/aws\/2012\/06\/the-aws-partner-network-apn-is-now-live.html","source":"aws.typepad.com"},{"title":"Schoology Releases an Open API, Developer Platform and Public Content Feature to Make Learning an Integrated Experience - MarketWatch","details":"SAN DIEGO, CA and NEW YORK, NY, Jun 25, 2012 (MARKETWIRE via COMTEX) -- Schoology, the leading cloud-based, collaborative learning platform for K-20 classrooms, today announced the release of its open Application Programming Interface (API), developer platform and public content network at the International Society for Technology in Education (ISTE) Annual Conference taking place June 24-27 in San Diego. The Schoology API and developer platform allows third-parties to interact with, and extend, the features and functionality found in Schoology, including the ability to build applications that run directly on the Schoology platform -- resulting in a seamless and integrated experience for the educators, students and administrators that use Schoology to connect and collaborate every day.                                     \n                                          For the more than 20,000 schools and districts that deploy Schoology as its preferred learning management system (LMS), the new public API and developer toolkit extends the value and functionality of the platform, allowing districts to scale and customize Schoology to meet their individual needs. The API appeals to individual developers and leading independent software vendors (ISVs) servicing the education market. Popular third-party applications like Blackboard Collaborate(TM) and Turnitin(R) are now applications that live and run directly within the Schoology platform -- providing a totally seamless user experience.                                     \n                                          We recognized early on that it would be impossible to develop every possible feature or learning resource ourselves, said Jeremy Friedman, Founder and CEO of Schoology. We believe in the efficiency and quality of community driven content and industry collaboration and are dedicated to providing others with the tools and environment that makes learning together easy and rewarding.                                     \n                                          The Schoology application platform will be generally available at the start of the new school year with applications from more than 20 preferred partners, including Turnitin, Blackboard Collaborate, Desmos, LearningStation, Remind101, SchoolTube, Symbaloo, VirtualNerd, iMeet, GoAnimate and others. Custom-built applications using the Schoology API will be featured and promoted in the Schoology App Marketplace and among its highly engaged user-base of more than one million educators, students and administrators.                                     \n                                          Schoology is also announcing a new collaborative platform for creating and sharing content. The cornerstone of Schoologys mission is collaboration, providing a medium for educators to communicate and collaborate beyond the classroom. The new public content feature underscores this goal and extends the functionality of the platform, enabling educators around the world to easily find and share classroom resources while simultaneously building the worlds best repository of educational content. Now educators can share and collaborate by topic, grade-level, common core, state standards, etc. Best-in-class, third-party content such as Khan Academy is now directly integrated into Schoology. In addition to its user-friendly search capabilities and category filters, the new public content feature lets users rate the material and offer useful feedback to the community. Educators can also save content in their private resource area for future use, or embed content directly into a class with one simple click.                                     \n                                          Educators around the world have been developing world-class content using Schoology, continued Friedman. Now we are allowing them to share the content they have created with others to help build a more efficient teaching experience. With the release of our public content network, teachers can join a large and growing community that is focused on helping students succeed.                                     \n                                          Visit Schoology at the ISTE 2012 Conference in booth #4736. To learn more about the Schoology developer platform go to    http:\/\/developers.schoology.com    .                                     \n                                          About Schoology  New York City-based Schoology is the developer of a K-20 collaborative learning platform that incorporates online learning, classroom management and social networking in one platform. The cloud-based solution is available as both a free, stand-alone product and as a fee-based, integrated enterprise-class solution for schools and districts. Schoology integrates seamlessly with existing school\/district technologies to create a single, scalable interface where educators can teach, assess, administer and analyze student progress. Schoology has transformed how educators communicate and collaborate with students, parents and peers. The platform expands the educators support and resource network by enabling meaningful academic collaboration through the sharing of best practices and digital content via school-to-school networking and an easy-to-access centralized resource library. For more information visit schoology.com or follow the company on Twitter @schoology.","item_date":"Jun 25 2012 19:45:04","display_item_date":"06-25-2012","url":"http:\/\/www.marketwatch.com\/story\/schoology-releases-an-open-api-developer-platform-and-public-content-feature-to-make-learning-an-integrated-experience-2012-06-25","source":"www.marketwatch.com"},{"title":"Firefox Command Line Hackathon - Incompleteness","details":"Im in the process of finishing off bug 724055 which will make it as easy as is possible to create new commands, just set a pref to point to a directory where your commands are stored in JSON (ish) files, and youre done.  \nIn the next few days Ill update this post  with a set of resources for creating commands and ideas for commands. Well hang out in IRC and crank away and see how many commands we can get written in a day.  \nIf you can spare some time, wed love to have some help.  \nUpdate: Originally this post had a date typo and said the 27th. The real date is the 26th.","item_date":"Jun 25 2012 19:41:24","display_item_date":"06-25-2012","url":"http:\/\/incompleteness.me\/blog\/2012\/06\/20\/command-line-hackathon\/","source":"incompleteness.me"},{"title":"Twitter API Best Practices: Single vs. Multiple User OAuth","details":"This guest post comes from Adam Green, a Twitter API consultant who builds custom Twitter apps for clients, and blogs about Twitter programming at 140dev.com. Follow him at @140dev.\n\n\nTwitter\u2019s OAuth system is an essential part of using the API, but there are two distinct models for OAuth and good reasons for using one over the other in certain situations. The simplest way to perform an API call with OAuth is called single-user, and it is done by creating a new app at dev.twitter.com, and requesting a set of user access keys for that app. With these keys the single user will be the Twitter account that owns the app. A more complex method is to allow multiple users to authorize your app to work with their account, and then store a unique set of OAuth keys for each user. You will then be able perform API calls using each set of user keys. Choosing the best of these two programming models depends on a lot of factors:\n\n\nThis is one of the major reasons for going with multi-user OAuth. Each Twitter account is allowed to make 350 API calls per hour, so if you do single-user OAuth that is your application limit. With multi-user, you can make 350 API calls per hour per user. If just 100 users\u00a0authorize your app, you will be able to make 35,000 calls per hour!\u00a0These calls can be used for any API function that reads data from any\u00a0account. You are not limited to just the account that gave you the\u00a0OAuth keys. For example, you can do 35,000 requests for user profiles with users\/lookup, which would give you 350,000 profiles. This is not taking away anything from the authorizing user. The 350 calls per hour limit is available to each app a user authorizes. Twitter keeps separate rate limit counts for each app\/user combination.\n\n\nIf you need to perform API operations that change an account, such as\u00a0sending tweets or adding followers, mutli-user OAuth is required. You\u00a0can only modify a Twitter account with keys for that account. This\u00a0means that need to stay within the 350 calls per hour limit.\n\n\nBuilding a working system that does the multi-user \u201cOAuth Dance\u201d is\u00a0not trivial. This isn\u2019t helped by Twitter\u2019s documentation on this\u00a0subject, which is only useful after you already understand how it\u00a0works. The other complication is Twitter\u2019s lack of internally\u00a0developed client libraries. You have no choice but to use a third\u00a0party OAuth library, and since these libraries are free, the authors\u00a0have no motivation to provide support. Like lots of programming tasks,\u00a0multi-user OAuth is not hard to do, but it sure is hard to learn.\u00a0Single-user Oauth clearly wins in this category. You just copy a set\u00a0of user keys for the app into your code and make the API request.\n\n\nThere is no user interface needed for single-user OAuth, everything is\u00a0done within a server script, so this is another win on complexity.\u00a0Adding multi-user OAuth to a website means that you have to find a way\u00a0to ask for authorization by users that doesn\u2019t scare them. This is\u00a0usually done by making this your account creation and login system.\u00a0Rather than asking explicitly for a user to authorize you to modify\u00a0their account, you typically offer to let the user \u201clog in with\u00a0Twitter.\u201d As part of that login you also get a copy of the user OAuth\u00a0keys. The simplest approach is to treat each user who logs in as an\u00a0account on your site. This eliminates the need for a separate\u00a0registration process, but this model must be integrated into your site\u00a0design.\n\n\nOnce you get a set of OAuth keys for multiple users, you need to be\u00a0able to store them long-term and retrieve them quickly. Oauth keys are\u00a0permanent, unless the user revokes your app\u2019s authorization, so you\u00a0will want a reliable storage model. While a flat text file might for a\u00a0few sets of keys, you will probably want to have a database available\u00a0for this, which raises the memory and server load requirement for your\u00a0app. When doing single-user OAuth, you can just paste the keys into a\u00a0script and include that in any code that needs to call the API.\n\n\nError handling is always an interesting part of Twitter API\u00a0programming, especially when their servers start accidentally\u00a0returning the wrong error codes. Multi-user Oauth coding requires you\u00a0to test for a much large set of possible errors. The user\u2019s keys will\u00a0fail if their account is deleted, suspended, and in some cases when it\u00a0is made into a protected account. I wish I could provide a definitive\u00a0set of codes for each of these conditions, but they change too often.\u00a0You can be fairly sure they will be in the 400 range, except when the\u00a0API gets confused and returns error codes in the 500 range.\u00a0Single-user is more reliable in this case, since you are controlling\u00a0the underlying account.\n\n\nAccepting OAuth keys from users means that you are taking on a heavy\u00a0responsibility. With their keys you can basically do whatever you want\u00a0in their account: tweeting, DMing, following, changing their avatar\u00a0and bio, etc. You do not want to get hacked when doing multi-user\u00a0OAuth. There are many possible security models, all of which are\u00a0beyond the scope of this article. A brief description of the model I\u00a0prefer is to assign cookies to each user with a unique, hash generated\u00a0key. I never pass the actual keys back to the browser or store them in\u00a0user cookies. I always confirm that the user making a request that\u00a0will modify their account is a holder of valid cookies that allow me\u00a0to look up their OAuth keys on a secure server.\n\n\nI just love the programmer term \u201cdeprecated.\u201d It is so Nixonian. Kids,\u00a0ask your parents about Nixon. The Twitter API is \u201cevolving\u201d, another\u00a0presidential idiom. Anyway. Be aware that if you build a site and app\u00a0around user authorization there is a chance that Twitter will\u00a0deprecate this system. They have already split the authorization model\u00a0once to allow more control on the part of the user, and that could\u00a0certainly happen again. If you do have to go through this type of\u00a0transition, all of your users will have to authorize your app again.\u00a0With single-user OAuth, you will be able to perform this change\u00a0yourself.\n\n\nIt\u2019s clear that complexity is the major disadvantage of multi-user\u00a0OAuth, but if you need high rate limits or the ability to modify an\u00a0account, this is your only path. I try to always start an app using\u00a0single-user OAuth when building a proof of concept version, and then\u00a0only migrate to multi-user if it is obvious that the production system\u00a0will require it.\n\n\nPhotos by Hil and Tarik Browne.","item_date":"Jun 25 2012 19:40:14","display_item_date":"06-25-2012","url":"http:\/\/blog.programmableweb.com\/2012\/06\/25\/twitter-api-best-practices-single-vs-multiple-user-oauth\/","source":"blog.programmableweb.com"},{"title":"Converseon Launches New API to Infuse Industry\u2019s Most Accurate Social Intelligence into Big Data","details":"The mission of ConveyAPI is to become the engine that powers an ever-expanding ecosystem of third party socially-intelligent business applications.","item_date":"Jun 25 2012 19:39:17","display_item_date":"06-25-2012","url":"http:\/\/www.prweb.com\/releases\/2012\/6\/prweb9636192.htm","source":"www.prweb.com"},{"title":"Simpli.fi API Simpli.fi Search Retargeting Platform","details":"As part of our ongoing commitment to provide clients with innovative tools that deliver optimal results, Simpli.fi has developed an entire suite of RESTful Application Programming Interfaces (APIs) that allow you to seamlessly interact and build on top of Simpli.fi\u2019s global cloud architecture. These APIs are resilient \u2014 enabling you to build an entire system over them.\u00a0\n We offer full reporting and workflow API\u2019s to allow your media buying and optimizing as seamless as possible.\n \n No need to\u00a0remember a separate log in. With our workflow API, you can bring the power of Simpli.fi into your own platform.\n Working with our API offers distinct advantages, such as:\n Feature Complete: Simpli.fi\u2019s APIs contain all the same features available through the UI, allowing for a seamless transition from one platform to the other.\n Programmatic Interaction: APIs are particularly useful when wanting to pull information programmatically.\n Customizing Reports: Reports can be accessed real time, scheduled or modified, this is particularly useful when pulling together information that needs to be merged into 3rd party systems.\n Keyword Bulk Uploads\/Modifications:Changing the keywords associated with a campaign is made easy through APIs. Additionally, APIs help facilitate large pricing changes for individual bidding. This is especially useful if you are using your own optimization engine and need to make modifications to campaigns based on new optimizations.\n Audit Logs: Our APIs allow you to track how your campaigns are being managed \u2014 by whom and when. They also enable detailed tracking of your campaigns in order to alert, identify, and correct any campaign modifications.\n RESTful \u00a0API: Allows all functionality to be automatically self-documenting, discoverable, and browsable\n \n Agencies\/advertisers who want to integrate \u2013Simpli.fi\u2019s reporting with their dashboards. Ad Networks who want to use Simpli.fi to increase their reach and enable them to sell search retargeting.\n \n The APIs are self-documenting REST APIs that are automatically discoverable and browsable.\n The APIs provide campaign setup, management, reporting, ad creation\/editing, and optimization.","item_date":"Jun 25 2012 19:39:02","display_item_date":"06-25-2012","url":"http:\/\/www.simpli.fi\/our-solutions\/api\/","source":"www.simpli.fi"},{"title":"SITA Lab invites airlines to trial new API platform | SITA.aero","details":"In a move that could see a new wave of innovative travel applications being developed, SITAs research technology team, SITA Lab, today invited airlines and developers to join trials of developer.aero. This is the API (application programming interface) developers portal which SITA Lab is now moving into its next phase of development.\nThe platform is being designed to enable airlines, airports and other industry players to extend existing IT processes by harnessing the application developer community to provide new apps for the industry and world travelers.\nJim Peters, CTO, SITA, said: API-client architecture is the key enabler that is driving new-gen app development and allowing apps to be developed for virtually any platform. Exposing data or processes through APIs allows developers to create features that will benefit everybody. It opens the possibility of solutions being created that the companies might not have the resources to develop or for which they might not have even anticipated a consumer need.\nIn the airline industry, for example, allowing developers access to data on checked bags as they pass through various checkpoints on their journey, can power a useful API for self-service baggage tracing. This is just one area on which our team at SITA Lab is focusing.\nIf successful, this platform will open the door, in a secure and controlled manner, for application developers to access vast amounts of industry data enabling them to create applications for use on multiple devices, including mobiles and tablets.\nPeters continued: We expect that our research with developer.aero will promote innovation across the industry. Relationships with the developer community, both within and outside of the air transport industry, will be expanded and strengthened while the design of the portal will maintain control of data and infrastructure.\nSITA plans to have several APIs available on developer.aero. The first of these which SITA wants to trial with airlines is BagTrac. This API exposes data on the location of individual checked bags as they are scanned during the journey. As the leading IT provider to the air transport industry, SITA manages BagMessage which handles 1.2 billion bag messages every year. These include more than 400 million bag processing messages which give details of the exact location of a piece of luggage, for example, on a carousel at a particular airport, on the plane, in the baggage sortation area, etc. By exposing this data, SITA will be allowing developers to quickly and easily create niche solutions.\nFor those airlines interested with working with SITA Lab on BagTrac, developer.aero will enable the relevant data and business processes to be exposed so that developers can produce solutions that can track a bag like a parcel around the world. These solutions could also allow passengers to file a report from their mobile if something were to go wrong, providing updated routing instructions - a passenger self-service feature with potential huge cost savings for airlines.\nAnother API which SITA has made available is the Mobile Boarding Pass. No departure control or check-in system today provides a complete 2D, IATA-compliant, mobile boarding pass image creation and customizable message delivery process, to any mobile device. SITA is now exposing its solution so that airlines dont have to create their own. This is a community offering which will work with any check-in system and is compatible with upcoming technologies such as Apples Passbook for iOS 6.\nToday, 73% of passengers1 would like to use 2D bar-coded boarding passes, yet to date only the major airlines with considerable IT resources have been able to deploy mobile boarding passes; other airlines are left struggling on how to deploy this in a cost-effective manner. This API gives airlines the opportunity to offer their passengers the convenience of these passes.\nThe APIs are only available to trial by airlines directly or developers who are authorized by that airline. They can access the platform and register to use it from today.\n1. Passenger Self-Service Survey 2011 \n \t\t\t \t\t\t\t\t\t \t\t\t \t\t\t\t\t\t \t\t\t                           \t\t\t Share or bookmark with:\n \t\t\t \t\t \t\t\t \t\t\tFacebook \t\t \t\t \t\t\t \t\t\tTwitter \t\t \t\t \t\t\t \t\t\tDigg \t\t \t\t \t\t\t \t\t\tDelicious \t\t \t\t         \t\t \t\t         \t         \tTechnorati","item_date":"Jun 25 2012 19:38:51","display_item_date":"06-25-2012","url":"http:\/\/www.sita.aero\/content\/sita-lab-invites-airlines-trial-new-api-platform","source":"www.sita.aero"},{"title":"User innovation with API, showcase mite - Democratizing APIs","details":"With this post we are opening our API Providers Interview series. In our previous blog post we were asking a question: how API providers promote and sell their APIs. So to better answer this question we went to multiple API providers and asked them.\n In our blog we will do a multiple-posts API Providers Interview series where we will highlight some of the API providers and their experiences in providing APIs to their consumers.\n This blog post will be about APIs provided by mite. mite is a SaaS product created by Yolk and it is a time tracking application. That\u2019s what stated on the mite website:\n  Time tracking is vital for invoicing, accounting, and scheduling.\n Yes, it\u2019s been an annoying necessity for all too long. Let\u2019s strike back.\n With an advanced, yet simple tool to help you get things done: mite.\n  You might have heard about this German startup. They won Enable2Start startup competition sponsored by Financial Times Germany.\n There are many other time tracking web and mobile applications, however what\u2019s unique for mite it has a large number of 3rd party Add-Ons or Plugins. There are many mobile applications (iPhone, Android and webOS), desktop applications (Mac, Windows) and integrations with some other SaaS and Desktop billing systems.\n To be honest I was very surprised to see how many 3rd party applications and tools were integrated with mite, and some of them are build for mite. Service provided by mite application have become a platform, and a key role for it were Mite APIs.\n Here are some facts and numbers from mites CTO\u00a0Sebastian Munz:\n API is REST-based serving both JSON and XML.\n Back-end created in Ruby.\n Exists since 4 Years and was created in parallel with product.\n Used by both external\/3rd party tools as well as by the mite website.\n Serving around 3,000,000 API calls a day.\n Extensive use of caching based on memcache and HTTP caching headers (Cache-control and ETag). See our other blog post about HTTP caching headers.\n API and Website statistics and monitoring is done with New Relic.\n Simple HTTP Basic authentication over HTTPS is used. Users are can be authenticated with username\/password or security token. With HTTPS all communication is encrypted.\n \nFrom my side I can add that mite\u2019s API is well documented\u00a0(in English and in German) and what is also very nice it\u2019s used by mite internally. Just like for Twitter, mite\u2019s website is using the same API as 3rd party tools and AddOns. It is definitely a challenge to evolve APIs in parallel to\u00a0evolving\u00a0a product, but\u00a0benefits\u00a0from \u2018eating your own dog food\u2019 are paying off.\n mite\u2019s API business model is similar to Netflix. Companies and individuals attracted by service are primary users of APIs. With APIs mite can significantly enhance applicability of it\u2019s service by opening the door for long-tail use-cases. For example when users are coming with specific reporting or integration requirements, which are too expensive or unique for mite to implement, APIs could be a self-service point where users could implement their requirements on their own. So on mite\u2019s example we see that:\n  Open API is a key for user innovation\n  And once that happened you can be sure that your user base is your best business multiplier, you just need to support it and keep you APIs up and running.\n In future we will post more blogs about API providers and how they develop their businesses with APIs, you can subscribe to our blog or follow us on twitter @elasticio.","item_date":"Jun 25 2012 19:38:43","display_item_date":"06-25-2012","url":"http:\/\/blog.elastic.io\/post\/25666524546\/user-innovation-with-api-showcase-mite","source":"blog.elastic.io"},{"title":"Is the Google Maps API price reduction too little, too late? ","details":"It was not a happy day for developers and companies running popular services on the Google Maps API. The reason: Googles jaw-dropping pricing, which pegged the cost of every 1,000 map loads above 25,000 at $4.\n     Numerous large Google Maps API users dropped Google Maps like a bad habit in the wake of the Googles pricing announcement. This shouldnt have come as a total surprise. After all, even the best-monetizing companies on the consumer internet would probably have a difficult time finding a way to make the economics of $4 per every 1,000 map loads work.\nFast forward to today. Apple has its own mapping product. Google is naturally concerned, and it shows. So what is the company to do? Slashing prices on the Google Maps API would probably be a good start, and thats just what the search giant did on Friday as it announced that the price for 1,000 map loads above the free limit has been reduced to 50 cents. That represents a discount of more than 87% off the original $4 price tag.\nWhats more, Google also decided to treat all map types the same. Previously, excess loads of Styled Maps were charged at a different rate.\nThor Mitchell, the Google product manager who delivered the news, wrote We hope the changes we\u2019re announcing today will help you continue to deliver the most innovative maps experience to your users. But that appears to be little more than PR spin.\nThe reality is that Google isnt trying to retain anybody. In its announcement, the Mountain View-based company noted that just 0.35% of sites using the Maps API exceed the limit of 25,000 loads per day, so this clearly isnt about pleasing existing users. What this is about: all the users that abandoned the Google Maps API for greener (read: more affordable) pastures.\nThose users include companies like Foursquare, which turned to OpenStreetMap when faced with paying $4 for 1,000 map loads. While its always possible that some companies will give Google Maps a second look, it seems more likely that former users will remain former users.\nCertainly, from a financial standpoint, the Google Maps API wont kill Google. But there are more strategic implications here. This isnt the first time Google has upset developers with pricing and with the company reportedly set to launch a cloud services platform to compete with Amazon at this weeks I\/O developers conference, the search giant had better remind itself that in many markets, you only get one real opportunity to set your prices.\n                 Patricio Robles is a tech reporter at Econsultancy. Follow him on Twitter.","item_date":"Jun 25 2012 19:02:45","display_item_date":"06-25-2012","url":"http:\/\/econsultancy.com\/us\/blog\/10184-is-the-google-maps-api-price-reduction-too-little-too-late","source":"econsultancy.com"},{"title":"The FreightCenter API Delivers Savings and Transparency","details":"The FreightCenter API makes it possible for e-commerce retailers and software developers to integrate freight rates, and book shipping as part of the check out process. Shipments can be tracked without ever leaving the retailer\u2019s website. The customer can choose from 25 of the leading freight providers in the U.S.\n\n\nThe API uses XML web service architecture and is platform independent.\n\n\n\n\n\nFounded in 1988 and based in the Tampa Bay area, \u00a0FreightCenter.com notes in a June 2012 press release that it \u201chas doubled the annual revenue it generates by helping more than a million shippers to secure volume-discounted freight pricing from reliable nationwide carriers. FreightCenter.com operates in Pasco County and generates all of its business through secure online freight shipping and logistics service transactions.\u201d\n\n\n\u201cOur business model is unique in that we hire in-house developers to create the freight software we use internally to process shipments, as well as license externally to businesses that need a method of managing inbound and outbound freight shipments,\u201d said Doug Walls, chief information officer for FreightCenter.com.","item_date":"Jun 25 2012 18:52:48","display_item_date":"06-25-2012","url":"http:\/\/blog.programmableweb.com\/2012\/06\/25\/the-freightcenter-api-delivers-savings-and-transparency\/","source":"blog.programmableweb.com"},{"title":"Travel Biz Monitor :: SITA requests airlines to trial new API platform","details":"SITAs research technology team, SITA Lab, invited airlines and developers to join trials of developer.aero. This is the API (application programming interface) developers portal which SITA Lab is now moving into its next phase of development.According to a release, the platform is being designed to enable airlines, airports and other industry players to extend existing IT processes by harnessing the application developer community to provide new apps for the industry and world travellers. \n\u00a0Jim Peters, CTO, SITA, said, \u201cAPI-client architecture is the key enabler that is driving new-gen app development and allowing apps to be developed for virtually any platform. Exposing data or processes through APIs allows developers to create features that will benefit everybody. It opens the possibility of solutions being created that the companies might not have the resources to develop or for which they might not have even anticipated a consumer need.\n \n(You need to login first to read complete story).\u00a0 New User?\u00a0Register \u00a0for\u00a0FREE!","item_date":"Jun 24 2012 22:09:11","display_item_date":"06-24-2012","url":"http:\/\/www.travelbizmonitor.com\/sita-requests-airlines-to-trial-new-api-platform-16784","source":"www.travelbizmonitor.com"},{"title":"Exploitation by STL Hackathon? | philip i thomas","details":"On the surface, the STL Hackathon by Teralogix sounds like a fun event \u2013 code for 24 hours, eat free food, build a new product, and get a chance at $20k if you win. Regardless of the outcome, you receive a free iPad 3 at the end of the event.\nHowever, dig a little deeper and you find out some alarming details:\nWhy do I have to sign a terms and condition waiver?\nAll participants will have to sign a terms and conditions waiver because the winning team(s) idea and intellectual property (IP) will become property of Teralogix. Only the winner or winning team will assign IP rights to new company. Teralogix reserves the right to select one or more winners.\n(Source, bolding by me)\nSo, if Teralogix likes your idea, they can claim it and everything you produced as their own. I cannot find their \u2018terms and conditions waiver,\u2019 so it is unknown what stake, if any, you retain in your idea. The $20k goes into the company they found, so you are left with an iPad, a new job, and no money. Because Teralogix may select multiple winners, they can essentially claim ownership over every idea presented at the conference, thus depriving all participants of their hard work.\nThis goes against the entrepreneurial spirit in St. Louis and indicates depravity by Teralogix. Do not be tricked into going to this conference \u2013 Teralogix is not attempting to incubate your idea into a company, they are trying to steal your work. There is no incentive to submit a good idea, so go for the free food and the iPad, but go elsewhere to cultivate your idea.\nThanks to Dylan Hassinger for finding this egregious problem with the hackathon.","item_date":"Jun 24 2012 22:06:41","display_item_date":"06-24-2012","url":"http:\/\/philipithomas.com\/2012\/06\/exploitation-by-stl-hackathon\/","source":"philipithomas.com"},{"title":"Google slashes price 88% for using Google Maps API","details":"Earlier this month, Google revealed that it hopes to improve its Google Maps 3D feature by equipping planes with its own hardware and having them take oblique image photos.\n (Credit: Josh Lowensohn) \n  Google has announced an 88 percent price cut for those using Google Maps on high-traffic Web sites and services. \n The move, which Google Maps API product manager Thor Mitchell announced yesterday, comes a few days before the developer-oriented Google I\/O show and two weeks after Apple ditched Google Maps for the upcoming iOS 6. \n  \n  Google lets others embed Google Maps on their own sites and services through the Google Maps API, or application programming interface. When Google announced new limits to Google Maps usage last October, Mitchell said at the time, We need to secure its long-term future by ensuring that even when used by the highest volume for-profit sites, the service remains viable. \n But other factors entered into the viability equation. Apple is the highest-profile defection, but there have been others, too, as Web sites dumped Google Maps because of high prices and put their weight behind the OpenStreetMap project instead, Google evidently took note. \n Weve been listening carefully to feedback, and today were happy to announce that were lowering API usage fees and simplifying limits, Mitchell said. While the Maps API remains free for the vast majority of sites, some developers were worried about the potential costs. In response, we have lowered the online price from US $4 per 1,000 map loads to 50 cents per 1,000 map loads. \n Yes, thats a factor of eight cheaper. Mitchell added: \n Were beginning to monitor Maps API usage starting today, and, based on current usage, fees will only apply to the top 0.35 percent of sites regularly exceeding the published limits of 25,000 map loads every day for 90 consecutive days. We arent automating the application of these limits, so if your site consistently uses more than the free maps allowance well contact you to discuss your options. Please rest assured that your map will not stop working due to a sudden surge in popularity. \n Mitchell also pointed to another service that may have changed Googles thinking: map-based ads that turn its mapping service into a money maker. \n You can generate revenue from your Maps API application using AdSense for Maps, which enables you to display relevant ads on or alongside your map. As with AdSenses text-based ads, the publisher gets some of the resulting ad revenue -- and Google keeps the rest. \n Via Daring Fireball.","item_date":"Jun 23 2012 22:45:08","display_item_date":"06-23-2012","url":"http:\/\/news.cnet.com\/8301-1023_3-57459328-93\/google-slashes-price-88-for-using-google-maps-api","source":"news.cnet.com"},{"title":"IP Agreements at Hackathons","details":"This is a list of books currently on my To Read shelf... literally. I do not suggest or anti-suggest any of them at this time as I havent read them yet.\n Agile Project Management with Scrum\n Beautiful Code\n \n   This is not the home of dotProject or web2project.  It is the home of CaseySoftware, LLC.  Any dotProject support questions should be referred to their support forums.","item_date":"Jun 23 2012 22:31:14","display_item_date":"06-23-2012","url":"http:\/\/caseysoftware.com\/blog\/ip-agreements-hackathons","source":"caseysoftware.com"},{"title":"TwitLonger \u2014 When you talk too much for Twitter","details":"RT @jschox: An API for the bloodstream. New Patch Will Constantly Monitor Your Blood http:\/\/bit.ly\/MCT59m @sano_int in @FastCoExist cc @Rock_Health \t\t\t\t\t\t \t\t\t\t\t\t\thttp:\/\/tl.gd\/i0411m \t\t\t\t\t\t\t\u00b7 Reply","item_date":"Jun 23 2012 17:38:17","display_item_date":"06-23-2012","url":"http:\/\/www.twitlonger.com\/show\/i0411m","source":"www.twitlonger.com"},{"title":"Google to launch Amazon, Microsoft cloud rival at Google I\/O ","details":"Google is very likely to launch a cloud services platform at its annual developer conference, Google I\/O next week in San Francisco. It was one of the topics of discussion in the hallways of our Structure 2012 conference. We have since confirmed with multiple sources who are familiar with Google\u2019s plans which include a more comprehensive offering that its current app engine and storage offerings. The Mountain View, Calif-based Internet giant declined to comment.\n\n\nIn early May, my colleague Derrick Harris broke the news that both Google and Microsoft were launching their competitors to Amazon later this year.\n\n\nWhile Amazon seems to be a target for all cloud service providers, my sources say that the real target for Google seems to be Microsoft and its developer community. While Amazon has achieved amazing traction with startups and new cloud companies, experts believe that there is a wider opportunity to tap into the corporate markets. Amazon too is trying to move into the enterprise market. The enterprise developer community is also one of Microsoft\u2019s biggest strengths, and Google wants to go after them.\n\n\nIn order to lure these enterprise developers, the company has focused heavily on making it easier to write, deploy and manage applications on its platform. It is doing so by partnering with third parties. Two companies I have heard who are in cahoots with Google are Rightscale and Opscode.","item_date":"Jun 23 2012 17:34:29","display_item_date":"06-23-2012","url":"http:\/\/gigaom.com\/2012\/06\/22\/google-to-launch-amazon-microsoft-cloud-competitor-at-google-io-2012\/","source":"gigaom.com"},{"title":"How LinkedIn betrayed a 5-man startup","details":"I think one of the messages here is that if youre a start-up and your product requires a third-party API that you have no control over to function then youre taking a massive risk.This isnt specific to Linked.In: API access isnt a right. If your product requires a third-party data source and you dont have any contractual agreement with that third-party you have to accept that the rug could be yanked from under your feet at any moment.\nAnd maybe that risk is acceptable to you and your investors - but to not have a backup plan is foolish.","item_date":"Jun 23 2012 02:13:47","display_item_date":"06-22-2012","url":"http:\/\/news.ycombinator.com\/item?id=4148915","source":"news.ycombinator.com"},{"title":"Developers Should Worry That LinkedIn Betrayed This French Startup","details":"Earlier today we looked at an interesting incident which saw LinkedIn revoke access to its API for Pealk, a France-based startup which developed a headhunting application that hooks into its data, concluding that there were a number of questions left unanswered.\n LinkedIn says it is cutting access next week because the app threatened its members\u2019 data, despite prior warnings. That led us to\u00a0wonder why a company that relies on firehouse access knowingly refused to alter its service to comply with the terms and conditions required of it.\n The short answer is that, as we suspected, 5-man startup\u00a0Pealk did try to adapt itself to support LinkedIn\u2019s terms; but the longer answer is a story of how the global professional social network toyed with, and then betrayed, a small team, in a case that should concern any developers that work with the company.\n From partnership talk to shut down\n The story began back one year ago when Pealk\u2019s Boris Golden (pictured third from left below) and his three fellow co-founders began to put the idea for an app into motion. Through work with a previous startup, which saw him use it daily, the serial entrepreneur had built a comprehensive list of strengths and weaknesses of LinkedIn.\n Having identified the need for a \u201cproductive and a cool tool to explore the data\u201d, the concept of Pealk was born. The Web app was released in beta two and a half months ago but, Golden told me, the team had essentially spent a year working on the concept \u2013 clearly time that the four founders don\u2019t want to put to waste.\n \n The app \u2014 which today has some 2,500 registered users \u2014 made promising strides filling a niche need for headhunting on LinkedIn, offering recruiters a range of features that included a quick browser that allows for looking at multiple user profiles at one time,\u00a0eliminating the need for multiple tabs sessions.\n Early on, the app attracted the attention of LinkedIn\u2019s own staff who began to engage with positive feedback. Even on the call that informed them of the shutdown, LinkedIn told the now 6-man Pealk team that they \u201creally love the features, its marketing\u201d and the app in general.\n LinkedIn began to talk more specifically with Pealk about collaborations and, in particular, held a number of conference calls to discuss a possible \u2018partnership\u2019. While the nature of how the two companies might collaborate was always left open ended, at no point did LinkedIn execs ever reveal that it was breaching its term and conditions.\n The revelation came out of the blue. Last week, following a new round of partnership talks, the company was contacted by LinkedIn with a request for a conference call to discuss closing down its access to the API with immediate effect.\n Golden, who is head of product at the startup, managed to negotiate a two week extension but admits that he is baffled by the u-turn.\n \u201cOne minute we were set to do a new conference call and then suddenly we were told\u00a0we\u2019d be shut down, instantly.\u00a0They\u2019d never mentioned a specific problem before and when we were told \u2018you\u2019ll need to be shut down\u2019 we began making and proposing changes in order to comply.\u201d\n As email exchanges between the two firms passed to The Next Web indicate, Golden was open with his determination and offers to change the Pealk app in order to pass LinkedIn\u2019s requirements and keep it open. Yet the company declined to make this possible.\n Questionable motives\n A couple of additional details began to paint a concerning picture around the way LinkedIn conducted itself in its exchanges with Pealk.\n After news of the API cut off, LinkedIn got in touch to suggest a possible trip to its Mountain View HQ and potential projects for the firm, while asking more about Pealk\u2019s structure. In turn,\u00a0Golden revealed that the firm was self-funded with one angel backer, numbered 5 employees and other details.\n LinkedIn asked if the startup had VC backers (it doesn\u2019t), prompting the suggestion that an official collaboration, or perhaps even acquisition\/hiring, could be on the cards.\n Also, notably, the guys at Pealk noticed that a member of LinkedIn\u2019s design team had spent \u201csignificant time testing all of the app\u2019s features\u201d, Golden recalls, to the point that he generated an obscure bug which \u2014 when looked at in more detail \u2014 alerted Pealk to exactly who he was.\n While it is true that these two incidents \u2014 LinkedIn checking into Pealk\u2019s background and a designer giving the app a full run out \u2014 can be viewed as positives which saw LinkedIn dig into more details about a prospective partner, they can also be interpreted as the company sussing out the possibility that it launch a similar app without causing controversy by cutting into a VC\u2019s investment or prompting a legal case.\n Given the u-turn that occurred days after both incidents, certainly the latter, more sinister explanation has additional credence.\n Golden believes that, with hindsight, LinkedIn was eyeing up the clout of his company.\n \u201cThey made a big mistake,\u201d he says,\u201din thinking that we were just students or freshly graduated kids who had made something cool and would just move on. We know we have killer features, and have received a tonne of positive feedback, even from LinkedIn staff themselves.\u201d\n Pealk has been in contact with lawyers in the event that LinkedIn iterates its own version of the app or reaches out to members of the small team with offers of employment.\n Partnering with LinkedIn is a \u201crisk\u201d\n Despite that, Golden says, the experience hasn\u2019t changed his admiration for LinkedIn or the fact that the startup would like to restart the relationship and get back to discussing collaborations with the social network.\n \u201cLinkedIn is great, and it has amazing opportunities, we still want to work with them.\u201d\n However, with LinkedIn looking to increase its\u00a0commitment to third parties and open its own app store soon, Golden has advice for would-be developers wanting to take advantage of its API.\n \u201cThis is their platform and their data so you take a risk if you work with them. Even if they are interested in your product, there is no guarantee that you\u2019ll have a good relationship with them. You need to balance the considerable opportunities of LinkedIn\u2019s data with the really awkward way that the company can handle its relationships.\u201d\n Exact violation remains unclear\n LinkedIn has been criticised in the past for switching off API access in questionable circumstances, and last June it shut out\u00a0five services that appeared to compete with its own offering: Facebook-focused career network\u00a0BranchOut, brand management app Visible.me, Monster\u2019s social recruiting program BeKnown, professional reputation service Mixtent and the resume service Daxtra.\n In all five cases LinkedIn provide details behind the API shutoff, as Social Barrel explains:\n LinkedIn said BeKnown, Mixtent and Visible.me were blocked from LinkedIn\u2019s API because they were using it to send promotional messages, which means profiting from access to its API, a violation of TOS. And LinkedIn said since BeKnown was also charging for enterprise services that relied on the API, it, liked BranchOut was in effect charging for access to LinkedIn content.\n The company has been less specific about Pealk and a further request for comment drew the following response, which still does not explain how the app violates its T&Cs:\n We are not going to provide any further comment on this issue. As shared in the original statement. This developer is violating our terms of use. They have been notified of their shut off, and have still not complied with our terms of service.\n As we suggested in our initial post, Pealk has similarities to LinkedIn\u2019s own premium services. A number of the app\u2019s most glittering reviews have come from sales, HR and recruitment professionals, all of whom LinkedIn would prefer use its services for recruiters\u00a0(ranging from $49.95 to $499.95 per month) and services for sales professionals (from $19.95 to $99.95 per month).\n \n \u201cThe truth is that we strongly believe we were finally a threat to them. Some people have said that they have \u2018LinkedIn Recruiter\u2019 accounts and that they like Pealk better,\u201d Golden says.\n \u201cWe think that we\u2019re really supplementary to their solutions and we have been a great fit together. That\u2019s a pity as\u00a0LinkedIn is focused on data and we\u2019re focused on productivity and user experience.\u201d\n \n Golden\u2019s comments are particularly interesting as they embody the rationale behind opening APIs and other data up to third party developers, the idea is to tap fresh brains and creativity to help innovate a service\u2019s user experience.\n Apps like Pealk bring a new dimension to LinkedIn for many of its users. While the threat of rivals like BranchOut, who were monetising its service for their own good, is perhaps understandable, it is hard to see how a promising, niche app \u2014 whose 2,500 users is a fraction of 150 million plus membership base \u2014 can warrant the shoddy treatment that LinkedIn has given Pealk.\n Why developers should be concerned\n Working with company APIs is all about trust, from both sides. For the company, the main issue is that third-parties use data responsibly, while developers need to know that their innovation and ideas will be respected.\n The fact that Pealk has produced an app that innovates on what LinkedIn already offers, should mean that the company treats the company well and respects it for injecting fresh ideas.\n The company showed this side of its relationship early on, when it provided positive feedback, reached out to the team and discussed possible collaborations. That is the kind of treatment that makes developers feel valued and want to work with companies.\n However, flipping 180 degrees, refusing to help and then revoking access goes against that principle. While equally, Golden is frustrated that LinkedIn\u2019s press statements have painted Pealk as a bad guy, after the company said the app is being shut down to \u201cprotect members\u2019 data\u201d.\n At fast glance, that rationale seems feasible as sales and headhunting is often a spammy business \u2014 both on and offline \u2014 but in reality, LinkedIn\u2019s move can, at best,\u00a0be interpreted as cutting out a rival service or, at worst, that is is working to copy the innovation and work of a \u2018small guy\u2019 with zero credit.\n Pealk is a fledgling firm that has invested time and money in helping improve the LinkedIn ecosystem, and addressing issues it saw in the service. While swiping away API access for BranchOut is one thing, snuffling the grassroots startups that innovate the service is entirely another, and it serves as a warning for other LinkedIn-focused developers.\n As we mentioned, the company has plans to open an app store and it is proud of its relationship with developers, ironically it chose today to announce an extension of its female hackday initiative \u2013 but Pealk\u2019s story raises serious question marks over its attitude towards the development community.\n In the course of researching this article, we spoke with other companies and independent developers who have used the LinkedIn API and none expressed their surprise that LinkedIn might be blocking out a competitor.\n Their feedback was clear, they\u2019ve never had a bad relationship with LinkedIn because they have always tried to avoid rivalling it in any way. While that\u2019s a reasonable and risk-averse bet, we can\u2019t help but feel that LinkedIn should be welcoming all innovation if it wants to improve its service.\n What should LinkedIn have done?\n It seems clear that LinkedIn was internally very divided about Pealk. On the one hand, a number of its staff took an interest and used the app \u2014 with its developer team reaching out positively \u2014 but on the\u00a0other side\u00a0of the coin, Golden believes that members within its premium teams wanted the possible rival shut down.\n Given 5-man Pealk\u2019s relatively modest size, LinkedIn could (and should) have looked into hiring the firm and its staff.\n If it wasn\u2019t that sure on the viability of an acquisition\/aquihire, it could have continued to plot a partnership to explore possibilities. Internal conflict aside, the app remained a niche with limited threat. It could have been a chance to see how certain features would be received by a wider audience in preparation for the introduction of a more official product at later date.\n Assuming that the rivalry is what closed it down, LinkedIn could have been transparent with its reasoning and come clean on exactly why Pealk is having its API access cut, as it did with Branchout and others.\n As it stands, the whole episode is unsavoury and paints the company in a particularly bad light. With its generalist press statements and refusal to go into specific details, LinkedIn looks like it is trying to bury the news, and provide zero credit to a development team that has taken its service and improved it.\n We\u2019re quite sure that \u2014 given\u00a0the issues it has had of late, which now include a $5 million lawsuit \u2014 this isn\u2019t an image that the company wants and, as passionate Golden has said, the door is not closed and Pealk would welcome the chance to end this mess and get working with LinkedIn again.\n LinkedIn rivals like fellow French service Viadeo would do well to get in touch with Pealk, its API access lapses next Tuesday (June 26) when its innovative brains will be set on finding areas of opportunity that lie elsewhere.\n For the record, here\u2019s LinkedIn\u2019s original statement on the matter, which it is not providing any further comment on:\n We have an ecosystem of 60,000 developers who access the LinkedIn platform everyday as part of our open developer program. We regularly monitor the developers of this program to ensure they are following our terms of use. In this case, this developer is in clear violation of LinkedIn\u2019s terms of use and is abusing the guidelines we\u2019ve put in place to protect our members\u2019 data. They were notified in advance of this shut off, but have still not complied with our terms of service.\n Pealk itself has released a fresh update today, which reads:\n Pealk noted that LinkedIn recently stated in an answer to questions from journalist\u00a0that: \u201cIn this case, [Pealk] is in clear violation of LinkedIn\u2019s terms of use and is\u00a0abusing the guidelines we\u2019ve put in place to protect our members\u2019 data\u201d and \u201cthe\u00a0important point is that [Pealk] were and continue to abuse the guidelines we have in\u00a0place to protect member data\u201d.\n Pealk considers that these statements are entirely false: we have been entertaining\u00a0constructive and almost permanent discussions with LinkedIn for more than eight\u00a0weeks and LinkedIn never reported to us a single violation of their terms of use.\n It\u00a0goes without saying that Pealk would have been fully available to cooperate with\u00a0LinkedIn to adapt the app to LinkedIn\u2019s terms of use to the extent necessary;\u00a0however, we were once again at no single point informed of such necessity by\u00a0LinkedIn.\n In this context, statements from LinkedIn such as those quoted above are prejudicial\u00a0to Pealk, which therefore reserves its rights to initiate any necessary actions to\u00a0preserve its interests.\n \u27a4\u00a0Pealk\n \n Feature image via Flickr \/ Mario Sundar","item_date":"Jun 23 2012 02:13:33","display_item_date":"06-22-2012","url":"http:\/\/thenextweb.com\/insider\/2012\/06\/22\/how-linkedin-betrayed-5-man-startup-pealk-and-why-developers-should-be-concerned\/","source":"thenextweb.com"},{"title":"How AT&amp;T Adopted APIs as an Innovation Paradigm - Forbes","details":"At first blush, the letters API may bring to mind a maze of technical complexity, but more accurate associations connect the concept of an API to increased revenue, business transformation, rapidly forming partnerships, thriving communities of developers, and open innovation.\n  When looking at a technology phenomenon like APIs, the application programming interfaces that allow direct access to resources so that applications can be built, it is easy to ignore the larger forest of the business practices instead focus on the technology trees. But, by taking a step back and looking at not just the technology mechanisms of APIs, but also the patterns they have created for meeting modern business challenges, it is possible to take a massive leap forward.\n  In this article I will argue that there is a huge opportunity for large companies to adopt APIs not just as a way of programming but as a new paradigm for accelerating achievement of a variety of business goals. By creating its own API economy, a large company like AT&T can both provide remedies to common problems facing large organizations and at the same time open up and accelerate innovation and product development. A close look at what AT&T has done with its API program illustrates this point. In essence, by adopting APIs, AT&T has leveraged a massive amount of learning and best practices to both spur innovation and increase revenue.\n  The Ups and Downs of Being Really Big\n  AT&T is the world\u2019s largest communications holding company by revenue. It has a massive, global network with advanced capabilities and tens of millions of customers. The battle for dominance among telecom companies takes place on many fronts, but one of the most important is the challenge of creating applications that take advantage of the wireless network.\n  In the modern wireless world, it is not only vital to offer your subscribers the best phones and the best plans, but also the best apps. When customers find an app useful, they use more of the network, which generally increases revenue. The companies who create the apps also act as de facto marketing partners for AT&T.\n  AT&T has a collection of assets that are quite valuable to developers. Its network backbone and services such as short message service (SMS) and multimedia messaging service (MMS) support millions of consumers and businesses with high-speed communications.\n  The calculus is quite simple. If AT&T can accelerate the creation of popular apps that make use of its network, it will increase both the number of customers it has and the amount of its products that those customers use.\n  Here\u2019s the problem. AT&T is a massive company, and it works and moves like a battleship. The company\u2019s processes were designed to sell sizeable, long-term deals to large customers. In other words, the processes were thorough, but far from fast.\n  For example, in a recently published case study from Apigee, Jon Summers, AT&T\u2019s Senior Vice President of Applications and Services Infrastructure, reported that before adopting APIs and the practices they enable, onboarding applications to the company\u2019s messaging network took three to six months. Note that this is not the process of building the application, but the task of getting it working in production. The on-boarding process involves creating an agreement, doing network design, integrating with the network, certifying applications, and integrating with settlement processes so that financial transactions can be supported. The end-to-end process through which a developer of an app worked with AT&T to launch a new product could take up to 18 months. Many developers simply did not have the resources, time, or money to support a project of that length and as a result did not work with AT&T.\n  AT&T realized that in order to accelerate app development, the company had to become a lot more like large Internet companies like Google, Twitter, Facebook, and Amazon, all of which offered APIs developers could use after a creating a relationship in a few clicks using a web interface. Developers are used to a streamlined experience. They want to get apps on the market fast. To attract developers, AT&T had to find a way to become much easier to work with.","item_date":"Jun 22 2012 22:31:01","display_item_date":"06-22-2012","url":"http:\/\/www.forbes.com\/sites\/danwoods\/2012\/06\/21\/how-att-adopted-apis-as-an-innovation-paradigm\/?goback=.gde_2637634_member_126769582","source":"www.forbes.com"},{"title":"The Amazon API battle for the cloud rages on","details":"The battle over whether it\u2019s productive for cloud providers to clone Amazon\u2019s APIs will rage on this week. Amazon rival\u00a0Rackspace built its new cloud platform atop OpenStack and will not support the popular Amazon APIs while Eucalyptus, famously, will. Rackspace President Lew Moorman is not sure what that support buys Eucalyptus.\n\n\n\u201cAPI cloning does not get you to interoperability. The cloud is not a protocol. It\u2019s a deep piece of technology that has to be replicated. If Amazon decided to open source its technology that would be a different story,\u201d Moorman said in an interview going into today\u2019s GigaOM Structure event where he will speak.\n\n\n\u201cThere are big important proprietary stacks in this world, Microsoft and Amazon and maybe one or two one others but there has to be an open alternative. It is a technology stack, not just a set of tools,\u201d he added. Rackspace, IBM, HP and others are backing OpenStack as that open cloud foundation and, many say, a hedge against Amazon\u2019s growing cloud dominance.\n\n\n\u201cThe Microsoft [Azure] platform is the same thing. What people need to call them on is if Microsoft says it\u2019s open because it runs Linux, misses the point of cloud. Spinning up servers is just one thing. The cloud is technology that has to be integrated into applications. Azure is as proprietary as Amazon.\u201d\n\n\nAmazon and its adherents say that by supporting its various EC2, EBS and other APIs, third parties get the interoperability they need with the gigantic Amazon public cloud. The discussion will continue at the GigaOM Structure\u00a0event kicking off Wednesday where Moorman, Amazon CTO and VP Werner Vogels and Microsoft Server and Tools president Satya Nadella will all speak.","item_date":"Jun 22 2012 22:30:09","display_item_date":"06-22-2012","url":"http:\/\/gigaom.com\/cloud\/the-amazon-api-battle-for-the-cloud-rages-on\/?goback=%2Egde_2637634_member_126572243","source":"gigaom.com"},{"title":" Doug Williams, Twitter: We're so focused on the consumer market...we want the ecosystem to serve the enterprise market","details":" Doug Williams, Twitter","item_date":"Jun 22 2012 22:17:01","display_item_date":"06-22-2012","url":"https:\/\/twitter.com\/crimsonhexagon\/status\/215833034707058688","source":"twitter.com"},{"title":"LinkedIn Shuts Down Headhunting App: How Open is Open?","details":"The business social network LinkedIn is shutting off access to its LinkedIn API for one company over what LinkedIn calls a  terms of service violation. Headhunting app Pealk released a notice that it \u201cwill be no more\u201d by next week. It raises the importance of reading the legal details, as well as the different definitions of open.\n\n\n\n\n\nThe team of at least four at Paris-based Pealk emailed a press release, as well as posted a major notice on the company website. The message claims LinkedIn is not being open enough:\n\n\nLinkedIn\u2019s statement makes clear the service is being shut off for violations:\n\n\nThere is no further information about exactly what terms the Pealk is violating. \u201cI won\u2019t be going into the details,\u201d LinkedIn\u2019s Julie Inouye said. \u201cThe important point is that they were and continue to abuse the guidelines we have in place to protect member data.\u201d\n\n\nBased on previous discussions with LinkedIn and the nature of Pealk\u2019s service, I\u2019d guess the application may harvest more data than is allowed, perhaps caching responses, another no-no. There does seem to be a disconnect between many developers and API providers on exactly what open means. Most terms make it clear that downloading the whole database is not the purpose of an API. However, where a company sits along that continuum is one that has more to do with business model and infrastructure scaling than the ideal of a completely open web.\n\n\nWhen I spoke with LinkedIn\u2019s Adam Trachtenberg about LinkedIn\u2019s big year he said the company takes a member-centric approach to its rate limiting and other terms of service. \u201cOur goal is to enable the most successful integrations possible while making sure that all members are having a fantastic experience,\u201d Trachtenberg said.\n\n\nIt\u2019s unlikely that this recent issue is about rate limits, as LinkedIn gave at least a week\u2019s notice to the developer.\n\n\nWhen the LinkedIn platform launched we wrote it was \u201copening wide.\u201d Yet, last year several apps were shut down for violations. Earlier in 2011, the developers of CubeDuel ran into LinkedIn usage limits, though the company extended those limits twice.\n\n\nLinkedIn is one of 735 social APIs in our directory, which also lists 47 LinkedIn mashups.","item_date":"Jun 22 2012 18:55:41","display_item_date":"06-22-2012","url":"http:\/\/blog.programmableweb.com\/2012\/06\/21\/linkedin-shuts-down-headhunting-app-how-open-is-open\/","source":"blog.programmableweb.com"},{"title":"Google Lat Long: Introducing Google Maps Coordinate: Organize teams on the move","details":"Imagine you are a call center operator at an electric utility company. \u00a0A call comes in reporting a downed powerline in one of the northern suburbs of your city, and an entire neighborhood is without power. \u00a0You need to quickly dispatch one of your line repairers to the site, which is almost an hour away. \u00a0To save time and get the power back up quickly, you want to know which line repairers are already in the area and send them the relevant information about the job.\n  \n  That\u2019s where Google Maps Coordinate comes in, a new tool designed to improve communication between businesses and their employees in the field. \u00a0As the number of mobile employees continues to grow, so does the need for a location sharing solution that works in real-time. \u00a0Research firm IDC estimates that there will be over 1.3 billion mobile workers by 2015 (37.2% of the total workforce) *  . Google Maps Coordinate combines the power of Google\u2019s mapping technologies with modern smartphones to help organizations assign jobs and deploy staff more efficiently.\u00a0\n  \n  Share real-time location. Google Maps Coordinate is built on Google\u2019s mapping and geolocation infrastructure so the app will send an accurate location, even if you\u2019re indoors (Google Maps Coordinate integrates with  Google Indoor Maps ).\u00a0\n Record data. \u00a0Mobile teams often need to collect information while out in the field. Google Maps Coordinate allows the admin to customize the fields that the mobile team needs to capture and collect \u2013 from measurements to client contact details \u2013 directly in the app.\n \n  Meanwhile, a designated dispatcher back in the office can:\u00a0\n  Create teams. Add team members to a Google Maps Coordinate team and see their locations in the Google Maps Coordinate web and mobile app. \u00a0For example, our electric utility company might create a special team for home electricians and another for line repairers.\n Manage jobs. Easily create jobs, precisely locate the job, assign the job to the nearest team member and notify them instantly. \u00a0The next time there is a downed powerline, the operator at the utility company will have no problem identifying the closest team member and assigning them to investigate the issue. \u00a0\n View past jobs and locations. Get the hard data needed to make strong business decisions. With Google Maps Coordinate, businesses can easily visualize the locations of all their jobs and teams, including current and past jobs. \u00a0Businesses can assess where they should be assigning or hiring more workers and how to optimally place their teams.\n \n  Any business can sign up for Google Maps Coordinate. \u00a0Google Maps Coordinate is built to work seamlessly with the entire Google Enterprise Maps and Earth experience, and it comes with an API that can integrate with any of your existing systems. \u00a0\n  \n  Contact our sales team or a Google Enterprise Maps and Earth reseller if you\u2019re interested in signing up for \u00a0Google Maps Coordinate. Share your Google Maps Coordinate use cases and feedback on our Enterprise G+ page.","item_date":"Jun 22 2012 00:33:03","display_item_date":"06-21-2012","url":"http:\/\/google-latlong.blogspot.com\/2012\/06\/introducing-google-maps-coordinate.html","source":"google-latlong.blogspot.com"},{"title":"Twitter\u2019s Fail Whale Of A Day: Mostly Down Since 12:35 EDT | TechCrunch","details":"In what\u2019s sure to piss off most news readers and general web-perusing enthusiasts, Twitter has gone down. We\u2019ve heard reports that it\u2019s been down for the last fifteen minutes, and without access to Twitter, it\u2019s tough to say whether or not the company has given any updates. \n\n\nA quick visit to DownForEveryoneOrJustMe.com shows that the site is down across the boards, and of course visiting Twitter.com offers up the same results. \n\n\nWhat\u2019s strange is that the usual Fail Whale is missing. \n\n\nWe\u2019ve checked in on Twitter\u2019s blog site and found that they\u2019ve made no mention of the matter yet, but we did see this:\n\n\n\n\n\nWe\u2019re reaching out now to see what exactly the problem is. \n\n\nThe good news is that Twitter\u2019s Mobile site seems to be up and running, so if you\u2019re really desperate to get your Tweet on, get it done on your phone. On the other hand, the iOS app is offering up no joy at the moment. \n\n\nWe\u2019ll keep you updated as we go. \n\n\nUpdate: 1:00 EDT \u2014 And she\u2019s back.\n\n\nUpdate 2: 1:49 EDT \u2014 Back down. \n\n\nUpdate 3: 3:00 EDT \u2014 She\u2019s up again, but there\u2019s no telling how long it\u2019ll last. We\u2019ve been on a roller coaster ride.  \n\n\nUpdate 4: 3:20 EDT \u2014 Twitter has made the following statement regarding what is going on with this massive service disruption. Check it out:\n\n\nUpdate 5: 4:15 EDT \u2014 Twitter has \u201cclarified\u201d what a cascaded bug is, and done so in 140 characters or less:\n\n\nIt\u2019s been about three hours since the site formerly known as Twitter went down completely. This is a full-scale outage, people. No fail whale, no cutesy message about how awesome Twitter\u2019s traffic is at the moment, just an error page. \n\n\nI\u2019ve been refreshing Twitter\u2019s developer site, and it looks like Status and Timeline updates have been going back and forth between \u201cPerformance Issues\u201d status (yellow) and \u201cService Disruption\u201d status (red). \n\n\n\n\n\nTweeters seem to be going through a bit of withdrawal, with many wanting desperately to tweet about how Twitter is down. Irony is fun. \n\n\n\n\n\nIn other news, Facebook is probably having a great day.","item_date":"Jun 21 2012 19:58:33","display_item_date":"06-21-2012","url":"http:\/\/techcrunch.com\/2012\/06\/21\/twitter-is-down-but-where-is-the-fail-whale\/","source":"techcrunch.com"},{"title":"Technology News: Developers: Netflix Circles the Wagons Around Its API","details":"Online video provider Netflix has changed the terms of its application programming interface, which third-party developers use to create Netflix-related apps. By locking down user data, Netflix is creating a disincentive for developers to continue efforts the company deems inappropriate, Pund-ITs Charles King said. So it helps Netflix. Developers, not so much.\n \n  Netflix (Nasdaq: NFLX) has announced changes to its public API (application programming interface) that will prevent developers from accessing information about its customers.\n As of September 15, users rental history and information about what they viewed will no longer be included in the API, putting that data out of reach for developers creating third-party apps for interacting with the video provider.\n Netflix will also change the API base URL domain from api.netflix.com to api-public.netflix.com. The new domain is active now, and all requests must be directed there by Sept. 15.\n Netflix will also retire all existing versions of the catalog index files by Sept. 15 and replace them with new ones that will be divided into separate files for streaming and for DVDs.\n Further, Netflix has retired the AppGallery pages. They were outdated and seldom used because people go to various app stores and other websites to find applications, the company said.\n Finally, Netflix has changed its terms of use as well as the email addresses for Netflix API support. \n  The companys decisions regarding its API sparked accusations that Netflix is trying to throttle independent developers, preventing them from making money by wrapping code around Netflixs API and offering the bundle as their own paid service.\n The new terms of use essentially mean that third-party devs wont be able to charge users for any significant value they added on top of the Netflix service, according to Goodfil.ms.\n Other reports quoted Netflix officials as saying the company only wants to prevent anyone using Netflix content, such as titles and descriptions, from advertising a competing service. It also wont permit resale of its information to other businesses.\n By locking down user data, Netflix is creating a disincentive for developers to continue efforts the company deems inappropriate, Charles King, principal analyst at Pund-IT, told TechNewsWorld. So it helps Netflix. Developers, not so much. \n  Stepping in It Again \n  Netflix seems to have grown a knack for attracting controversy over the last year or so. Last July, it stirred up a storm of protest when it announced that it would separate its DVD and streaming businesses, and change its pricing.\n The resulting flap led to an abject apology from company CEO Reed Hastings and the abandonment of its plan to split its businesses. The pricing changes stayed.\n However, this latest announcement could be laying the groundwork for a separation of Netflixs DVD and streaming businesses once again, by cataloging them separately.\n This time, though, the blowback from the companys customer  base may not be nearly as uproarious.\n I doubt the effect of this decision will be so severe, mainly because the effects will be felt only by developers and Netflix customers who use third-party apps and services, Pund-ITs King contended. Unless they feel pain directly in their pocketbooks, consumers tend to ignore the red lights and sirens and move on.\n Netflix did not respond to our request for further details.","item_date":"Jun 21 2012 06:01:59","display_item_date":"06-20-2012","url":"http:\/\/www.technewsworld.com\/story\/75424.html?wlc=1340258250","source":"www.technewsworld.com"},{"title":"API Evangelist job - BodyMedia - San Francisco, CA | Indeed.com","details":"Do you love building apps from new technologies? Are you a \u201cdo-er\u201d who is passionate about learning new technologies and sharing them with others? Do aspire to work on a product that has social impact by improving people\u2019s health? If so, we\u2019re looking for you to become our API Evangelist. BodyMedia is the pioneer in developing and marketing wearable body monitors that equip consumers with information they can use to make sweeping changes to their own health and wellness. We are looking for a ...      \ngithub\uff1ajobs  - 2 days ago                      -                       save job             -                               \t          block \t                                                 \u00bb                                      View or apply to job\n          \t                              API Evangelist jobs in San Francisco, CA","item_date":"Jun 21 2012 05:13:29","display_item_date":"06-20-2012","url":"http:\/\/www.indeed.com\/job\/API-Evangelist-at-BodyMedia-in-San-Francisco,-CA-e84257bb3d31ac15","source":"www.indeed.com"},{"title":"Facebook Adds Subscription Billing For App Developers, Backs Away From Credits As Primary Currency","details":"Why Microsofts Surface Tablet Should Shame the PC Industry\u00a0 \u2014\u00a0 On June 18, Microsoft (MSFT) beckoned 200 or so members of the media to an industrial, grimy part of Hollywood for what it described as a cant miss affair.\u00a0 The dutiful reporters met at the appointed hour\u20143:30 p.m. \u2026 \n  \nMicrosoft Surface Just Made the MacBook Air and the iPad Look Obsolete\u00a0 \u2014\u00a0 Microsoft has guts.\u00a0 Its what you get when youre the underdog; either that or you curl into a RIM and die.\u00a0 Microsoft is the underdog because no matter how many hundreds of millions of people use its software, the cool and the future belong to Apple.\n  \nNo Price, No Date, No Apps, No Problem.\u00a0 No Wait \u2014 Problem.\u00a0 \u2014\u00a0 Im just now catching up on the news about the new Microsoft Surface.\u00a0 Reactions seem mixed, tilting slightly positive or at least hopeful.\u00a0 People seem to want to believe Microsoft can pull this off and thats understandable \u2026 \n  \n Microsoft Surface tablets: Reading the fine print\u00a0 \u2014\u00a0 Summary: Here are a few more hidden tidbits about Microsofts new Surface tablets, launched in Los Angeles on June 18.\u00a0 \u2014\u00a0 Yesterday Microsoft announced it would be making two Windows tablets of its own \u2014 one Windows8-based and another Windows RT-based.\n   More: istartedsomething, MSDN Blogs, Bits, Ben.geek.nz, Benzinga and Business Insider\n  \n Memo: Ballmer rallies troops around Surface, hints at Office, Windows Phone news\u00a0 \u2014\u00a0 Yesterday was a big moment in Microsofts history, as the company unveiled its own tablet computer, Surface, moving into the PC hardware business for the first time.\u00a0 \u2014\u00a0 After unveiling the tablet on stage in Hollywood \u2026 \n   More: CNET, The Verge, Business Insider, MSDN Blogs and WMPoweruser.\u00a0 Thanks: @johnhcook\n  \n With the Surface, Microsoft just started writing its next chapter\u00a0 \u2014\u00a0 I first met Steven Bathiche when I took a trip to Microsofts campus to check out what the company had been doing in the research and development department.\u00a0 Microsoft PR had sworn to me that I would be blown away by the \u2026 \n \n A new way to experience profiles: with or without replies\u00a0 \u2014\u00a0 When you visit a Twitter users profile, you may want to see his or her original Tweets rather than their conversations.\u00a0 Then again, you may enjoy seeing the back-and-forth exchanges they have with other Twitter users.\n   More: PC Magazine, Mashable!, Marketing Land, TechCrunch, The Verge, The Next Web, AllTwitter and Pulse2 Technology \u2026\n  \n Google Threatens To Sue Huge YouTube MP3 Conversion Site\u00a0 \u2014\u00a0 YouTube is without doubt the biggest free resource of videos and music available online today.\u00a0 It can be enjoyed on the site itself, embedded in any webpage, or accessed via YouTubes Application Programming Interface.\n  \n U.S., Israel developed Flame computer virus to slow Iranian nuclear efforts, officials say\u00a0 \u2014\u00a0 View Photo Gallery \u2014 Stuxnet and other big worms and viruses:\u2009The computer virus is growing in popularity as the weapon-of-choice in the Middle East.\u00a0 Heres a look at some of the more notable viruses and worms.\n   More: The Verge, digitaltrends.com, Ars Technica, Telegraph, Hillicon Valley, Wired, Mashable! and Betabeat","item_date":"Jun 20 2012 06:48:13","display_item_date":"06-19-2012","url":"http:\/\/www.techmeme.com\/120619\/p55#a120619p55","source":"www.techmeme.com"},{"title":"Facebook, Please Don\u2019t Kill More APIs","details":"It\u2019s time we talk, Facebook, before you do something you\u2019ll really regret. You see, I noticed you with that look in your eye again. The sort of look that marks the end of a promising young company and along with it a well-loved API. You say you\u2019re changing, but I\u2019m not sure I can ever trust you. Let me tell you why.\n\n\nWe all remember when you acquired Instagram and said it would be an independent product. We thought that was a good sign for the Instagram API. But it sure looked fishy when you launched a potential competitor to Instagram. Why does this put me on edge?\n\n\nYour track record includes talent acquisitions. In fact, you\u2019ve stated publicly that it is the only reason for you to acquire companies. The NextStop API had a good group of ex-Googlers behind it and was just getting started when you snatched them up. Their CEO went on to launch Timeline, but NextStop is nowhere to be found. I admit this one is a bit personal, as I won second place in a NextStop contest for unrut. The site is not nearly as useful as it could be once the NextStop code was stripped out.\n\n\nGranted Gowalla wasn\u2019t on top of the world when you bought its team, but there were still avid users. And there were developers like Ben Dodson pouring themselves into building on top of the Gowalla API.\n\n\nFriendFeed was your first acquisition, Facebook, and it might be the saddest of all. You spared the site and the FriendFeed API, but the you left it to languish, slow and wholly unsupported. And why, so you could count the inventor of GMail as an employee until his shares vested?\n\n\nI realize it\u2019s a bit unfair for me to complain about killing APIs at the same time as lamenting dying services remaining on life support. But I don\u2019t think we should get in a discussion about fairness, because it wouldn\u2019t make you look very good.\n\n\nAnd now you\u2019ve acquired Face.com, the face detection company. That\u2019s a great domain name, but the Face.com API is also a very popular. I think it\u2019s been used in at least one hack at every Hackathon I\u2019ve attended. I understand why you\u2019d want to keep the technology to yourself, but it\u2019s been set up as infrastructure for many apps. You\u2019d be killing much more than one service.\n\n\nAnd I think it would be useful for you to think of APIs from this infrastructure point of view. If you continue to succeed to get the world\u2019s people using your service, you\u2019ll become something of a public utility. Already many sites count on you for authentication. You sit at an important place for developers. So far, your whims are troubling.\n\n\nI\u2019m watching carefully. Already the Face.com announcement is vague: \u201cOur mission is and has always been to find new and exciting ways to make face recognition a fun, engaging part of people\u2019s lives\u2026 At Facebook, we\u2019ll get to continue to pursue that mission.\u201d Let\u2019s hope the mission is continued in an open sort of way, without having to watch you kill another API, Facebook.","item_date":"Jun 20 2012 05:49:46","display_item_date":"06-19-2012","url":"http:\/\/blog.programmableweb.com\/2012\/06\/19\/facebook-please-dont-kill-more-apis\/","source":"blog.programmableweb.com"},{"title":"Google Developers Live, all year round - Google Developers Blog","details":"We think of Google I\/O as a place where our technologies come to life. Over the course of our three-day conference next week, you will see new technologies demoed for the first time, learn from the Googlers behind the code and meet with Sandbox partners who\u2019ve built businesses using our APIs. And while I\/O Live makes Google I\/O accessible to every single developer around the world, we want to bring you knowledge, tools, and people from the Google Developers community, all year round\u2013not just three days in June.\n We think connecting you with Googles experts and your fellow developers can inspire incredible app creation. So today, were introducing Google Developers Live, a destination for developers around the world that will feature live, interactive broadcasts ranging from developer-focused game shows to Office Hours where you can connect with the engineers who created and work on your favorite Google product.  \n Google Developers Live allows us to bring you the excitement of Google I\/O year-round, beginning today with the release of starter-level sessions on everything from Android to YouTube, to help prepare you for the more advanced content that will be presented next week. And, if you have any questions after watching these tutorials, weve set up Office Hours with each of our presenters on Google Developers Live so you can ask them directly. \n Tune in to Google Developers Live at http:\/\/developers.google.com\/live and start filling your calendar with Office Hours, App Reviews and more from Googles many products, along with original programming. Whether youre looking for Android, Chrome, Google+, Drive, Cloud Platform, Google Maps, YouTube or something else, we\u2019ve got a hangout for you - all year round.\nLouis Gray is a Program Manager on Googles Developer Relations Team, running Google Developers Live. He eats, sleeps and breathes Google products and APIs.","item_date":"Jun 20 2012 05:36:06","display_item_date":"06-19-2012","url":"http:\/\/googledevelopers.blogspot.com\/2012\/06\/google-developers-live-all-year-round.html","source":"googledevelopers.blogspot.com"},{"title":"Intel expert warns of open API security issues","details":"By  Editor\u00a0Posted on  June 15,12 \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tThe volume of open APIs \u2013 application programming interfaces \u2013 is rising and with it, the number of gateways is also soaring, creating a number of security issues along the way, says Peter Logan, an application engineer with Intel.\n Speaking at the Cloud World Forum event in London this week, Logan explained that in 2007 the number of open APIs numbered around the 500 mark and by the end of last year it had numbered more than 4,000 \u2013 a figure he expects to have risen significantly as we reach the mid-point of 2012.\n \u201cThe problem with this rise is the similar rise in the number of gateways that this situation creates \u2013 where SOAP and other technologies come to the fore,\u201d he explained.\n ITSP notes that SOAP \u2013 the Simple Object Access Protocol \u2013 is a protocol specification for exchanging structured information in the implementation of Web Services in computer networks.\n The protocol relies on XML \u2013 extensible mark-up language \u2013 for its message format, and also usually relies on other Application Layer protocols, most notably HTTP (hypertext transfer protocol and SMTP (simple mail transfer protocol) for message negotiation and transmission.\n And it is this `opening up\u2018 of IT platforms to so many different protocols that introduces security problems to the technology mix, Logan told his audience, with a classic example being the API on the Transport for London (TfL) Web site.\n The APIs on the TfL Web site, he says, can be used for a very wide range of things, all the way from working out whether your tube train is going to arrive on time, through to booking a `Boris Bike\u2019 for the afternoon.\n But the security issue with open APIs goes further than this, he argues, as we are now at the stage where people\u2019s personal tax data is going via an open API on HMRC\u2019s computers and Web site.\n When you factor in the issue that a lot of healthcare data is also flowing across these open APIs, he says, this means it has become very important to secure all of these gateways.\n \u201cAs a result, we are now also seeing the arrival of service gateways and allied services such as Software-as-a-Service portals like Salesforce and Google\u2019s many portals,\u201d he explained.\n The problem facing IT security professionals, he says, is that the technologies that are needed to tackle the problem of open APIs effectively are becoming more complex and diverse than ever before.\n For example, he told his audience, you have ID management, application security, legacy applications and databases to secure and \u2013 just to make life interesting \u2013 IT security professionals also need to bridge multiple APIs across domain middleware and identities.","item_date":"Jun 20 2012 05:34:58","display_item_date":"06-19-2012","url":"http:\/\/itsecuritypro.co.uk\/2012\/06\/15\/intel-expert-warns-of-open-api-security-issues\/","source":"itsecuritypro.co.uk"},{"title":"New Hotel Data From Factual","details":"We\u2019re pleased to announce that Factual has released the first version of our Extended Attributes for Hotels. This dataset contains an additional 38 attributes for over 115,000 hotels, motels, B&Bs, and other lodgings in the United States, released in beta.\n While Factual Places Data contains the core attributes such as name, location, address and phone number, these rich vertical datasets hold data tailored to a specific vertical. Our Hotels data contains information on:\n \n This table can be queried using your existing API key (get one here if needed) and the existing API. \u00a0Browse our data preview and have a look at the hotels documentation to learn more.","item_date":"Jun 19 2012 17:32:33","display_item_date":"06-19-2012","url":"http:\/\/blog.factual.com\/factual-hotel-data","source":"blog.factual.com"},{"title":"Razorfish 5, Vol 3","details":"As open application programming interfaces (APIs) become the standard mechanism for interacting with software platforms on the Internet, businesses are now offering them as another way for their partners and customers to interact with their brands. We take a closer look at how to build platforms and APIs that are right for your business, and outline the fundamentals of what constitutes an effective API for a platform.","item_date":"Jun 19 2012 17:19:31","display_item_date":"06-19-2012","url":"http:\/\/www.razorfish5.com\/articles\/apis.aspx#01","source":"www.razorfish5.com"},{"title":"Sunlight Foundation and Twilio, A Winning Ticket","details":"Election season is upon us! As we inch ever closer to the voting booth this November\u00a0we\u2019ve already begun to see a large number of civic-focused apps, to help inform, educate and empower citizens. Just a couple of days ago, at the Personal Democracy Forum in NYC, members of the Sunlight Foundation took the stage to announce two new, very exciting, Twilio powered apps to the public and I was lucky enough to have a front row seat for the unveiling.\n The Sunlight Foundation is a non-profit, nonpartisan organization that uses the power of the Internet to catalyze greater government openness and transparency, and provides new tools and resources for media and citizens, alike. Two new tools they\u2019ve debuted at PDF are Scout and Call on Congress. Scout makes it easy to stay on top of what\u2019s happening in government. The app provides a search and alert service to notify you whenever issues you care about are included in legislative or regulatory actions. Sign in, subscribe to things you care about, give it your email address or phone number and you\u2019ll get an SMS any time there is an update.\n Call on Congress makes connecting to Capitol Hill as easy as ordering pizza. By dialing into 1-888-907-6886 you can access a wealth of data, including finding out how your representatives are voting on bills and raising campaign money. This is especially signifcant, since certain portions of the population may not have or be comfortable with accessing information on the internet. This allows them to access information and connect to lawmakers using a comfortable phone interface.\n How did they build it? We caught up with the developers of the Call on Congress app, Sunlight Foundation\u2019s Jeremy Carbaugh and Dan Drinkard to talk about the tech behind the Call on Congress app.\n  Sunlight Foundations Jeremy Carbaugh\n What is Call on Congress?\n Call on Congress is the Sunlight Foundation\u2019s new free service to help anyone with a telephone connect with and learn about their lawmakers. Simply dial 1-888-907-6886 to find out how your representatives are voting on bills and raising campaign money and learn about upcoming bills in Congress.\n To date, a computer has been required to access Sunlight\u2019s web sites and data offerings. And while computer use and access to the internet is ever increasing, there is still a significant portion of the public that don\u2019t have access to these resources.\n What role does Twilio play in the Call on Congress app?\n The service is built on top of the Twilio API that enables us to create interactive phone and SMS applications. Users can navigate through a series of menus, be connected directly to the office of their representatives, and even leave us voice messages.\n I can\u2019t express how awesome it is to work with Twilio. It is incredibly easy to do things that would otherwise be quite difficult to create. We can create dynamic responses that are spoken back to the user using Twilio\u2019s text-to-speech feature. We can have recordings of a spoken script play for static content or even play music, if we had the need. We can even forward phone calls so that users can be connected directly to the office of their representative without having to write down the phone number and dial the call.\n While Call on Congress primarily uses Twilio\u2019s voice features, we used SMS on our other new service, Scout.\n I hear that there\u2019s also some other cool technology behind Call on Congress?\n Yes! Call on Congress is also one of Sunlight\u2019s first multilingual projects. The service is available in English, Spanish, and Esperanto\u2026 yes, Esperanto. This posed some unique challenges as we had to deal with both a static spoken script and dynamic text that would be read using Twilio\u2019s text-to-speech feature.\n Dan created an amazing, multitiered solution for this problem. Each bit of text used in the application is run through a series of steps to determine the proper response. All of the text is written in English is the code, but is internationalized by:\n Creating an MD5 hash of the text\n Determining the response language\n Checking for a cached translated copy of the text if it is not in English\n Generating a translated copy of the text using the Google Translate API\n Generating an audio file name based on the MD5 hash and a slugged chunk of the text\n Doing a HEAD request to S3 to see if an audio file exists for the chunk of text\n Responding either with a Twilio command using the translated chunk of text or a Play command with the S3 URL of the audio file, if one exists\n \nThis method allows us to basically drop in new languages as we create the audio for them. We can launch a new language and have Twilio\u2019s text-to-speech render the Google translated text until we drop language audio files into the application.\n Dan is smart.\n Your use of APIs is truly awesome, and you guys also have APIs of your own, right? Can you tell us some more about the Sunlight APIs?\n One of the most exciting aspects about this project, at least for us internally, is that almost all of the features of this service are powered by our existing APIs:\n Member lookup by ZIP code and contact information is powered by our Congress API.\n Bill information comes from the Real Time Congress API.\n Campaign contributions and member biographies are provided by the Influence Explorer API.\n \nOur friends at TurboVote are responsible for the API that allows us to provide information about local election offices.\n Call on Congress is a great example of what can be done using our API offerings. We were able to create this application without having to do any scraping or collecting of data of our own. In fact, it\u2019s a great example of what can be done entirely with APIs. Call on Congress is nothing more than a cache and the logic that combines the Twilio, Google Translate, and Sunlight APIs into a single application.","item_date":"Jun 19 2012 04:35:25","display_item_date":"06-18-2012","url":"http:\/\/www.twilio.com\/blog\/2012\/06\/sunlight-foundation-twilio-winning-ticket.html","source":"www.twilio.com"},{"title":"Much Ado About Nothing: The Truth Behind Netflix\u2019s API Changes | TechCrunch","details":"Last week, Netflix made some changes to its API program and Terms of Use for connecting with it. Since then, there\u2019s been some confusion about what the changes actually mean for developers. That confusion was highlighted in a blog post by Goodfil.ms Monday morning, which claimed that Netflix was \u201cquietly smothering its third-party ecosystem.\u201d (Update: Goodfil.ms has updated its blog post after getting some clarification from Netflix.) \n\n\nWhile there are some significant changes, there\u2019s nothing that should stop third-party developers from adding value for subscribers looking to access the service through mobile, web, or other connected device applications.\n\n\nThe biggest change to the API is that Netflix will no longer share its rental history with third-party developers. That includes movies that Netflix users have streamed or received through its DVD-by-mail service. According to the blog post, the company will continue to support third-party application developers, but \u201cdo so in a way that is aligned with our broader objectives.\u201d\n\n\nBigger questions have arisen around the new Terms of Use, which some fear will stifle third-party applications by limiting how developers can make money off the apps they build, and their ability to share Netflix data alongside other competing services. With the new Terms of Use, there are two main behaviors that Netflix is trying to prevent: First of all, it\u2019s hoping to stop developers from reselling technology and information from the Netflix API to third parties. And secondly, it\u2019s trying to stop developers from scraping its metadata and using it to advertise competing services.\n\n\nIn the Goodfil.ms blog post, the company suggests that if you build a Netflix app, you won\u2019t be able to charge for it. But a Netflix spokesperson says there\u2019s a key distinction between how the Netflix API is being used in applications that are sold directly to consumers versus those that attempt to resell that technology to others. Netflix is not trying to stop companies like InstantWatcher or Fanhattan from creating commercially viable businesses based on surfacing its titles through their services and charging consumers for them.\n\n\nIt\u2019s mainly attempting to stop developers from becoming middlemen that resell its API technology to other companies. So a Fanhattan iPad app sold to consumers is OK, but licensing out its white-label search and discovery platform to a consumer electronics manufacturer is not. That is, unless it doesn\u2019t want to include Netflix titles in the search.\n\n\nThe Goodfil.ms blog also claims that Netflix wants developers to \u201cclose their eyes and pretend that other services don\u2019t exist.\u201d But that\u2019s not entirely true, either. The new terms of use aren\u2019t meant to stop developers from creating apps that provide universal search across multiple competing video services. But they are saying that developers can\u2019t rely on or scrape Netflix\u2019s metadata as the main source of information about film titles if they\u2019re going to showcase other services as well.\n\n\nThe big trend here is that providing access to Netflix\u2019s streaming service is OK, but the company is pulling some proprietary data back behind the curtain. Want to show users which movies their friends have liked by hooking up with Facebook Connect, and forward them to Netflix? That\u2019s fine. But you won\u2019t be able to repurpose Netflix\u2019s own viewership or recommendation data for use in your own app. For most developers that hook into Netflix\u2019s API, that shouldn\u2019t be a problem.","item_date":"Jun 18 2012 19:48:47","display_item_date":"06-18-2012","url":"http:\/\/techcrunch.com\/2012\/06\/18\/netflix-api-changes\/","source":"techcrunch.com"},{"title":"Netflix API : Upcoming Changes to the Netflix API Program","details":"Over the past years the Netflix business has evolved to focus on delivering a great streaming experience on a wide variety of consumer electronics devices. Similarly, we have been evolving our API program to focus on servicing the rapidly growing universe of these devices used by our more than 26 million streaming members globally. As the API program evolves with the business, we now need to make some changes. While we will continue to support third parties as they develop and offer Web sites and applications that interact with Netflix, these changes are designed to do so in a way that is aligned with our broader objectives.\n Here is a detailed description of the changes and the timing for each.\n Functional Changes\n We have already modified the value of the <available_until> element for all titles to be 1\/1\/2100 unless the title is to become unavailable within two weeks of the requesting date.\n We will be removing the following endpoints, effective on September 15, 2012:    \/users\/userID\/rental_history\n \n We will be removing the following elements from the title_states endpoint, effective on September 15, 2012    <watched_date>\n \n We will be removing the following RSS feeds, effective on September 15, 2012:    Most Recent Rental Activity\n Movies At Home\n \n We will be removing all metadata for the rental history, recently watched, at home, etc. in all expands for all endpoints.\n We will be changing the API base URL domain from api.netflix.com to api-public.netflix.com.\u00a0 This new domain is active now.\u00a0 All requests will need to point to it by September 15, 2012, when the api.netflix.com domain will be retired.\n We will be retiring all existing versions of the catalog index files, to be replaced with new catalog index files.\u00a0 The new files will be structurally the same as the 2.0 version but there will be two files, one for streaming and one for DVD.\u00a0 The new files will be available in the coming weeks (we will post here with the new endpoints) and we plan to retire all other existing endpoints on September 15, 2012.\u00a0\n We will be retiring the AppGallery pages found at http:\/\/www.netflix.com\/AppGallery.\u00a0 This is effective immediately. These pages were outdated and seldom used as people tend to go to various App stores and other Web sites to find applications.\n \nTo be clear, none of these changes will affect the Queue-related resources or data.\n The Terms of Use has been updated and will be effective today.\u00a0 That said, for developers that have applications that need to be changed, they will be grandfathered in until September 15, 2012.\u00a0 To view the revised Terms of Use, go to <a href=\u201dhttp:\/\/developer.netflix.com\/page\/Api_terms_of_use\u201d>http:\/\/developer.netflix.com\/page\/Api_terms_of_use<\/a>.\n Support\n From now on, we ask that you now contact Netflix API Support at publicapi@netflix.com as we will be retiring apisupport@netflix.com.\u00a0 This change is effective immediately.\u00a0\n If you have questions about these changes, please address them to this new email address or bring them up on the forums.\u00a0\n - Daniel Jacobson,\u00a0Director of Engineering for the Netflix API","item_date":"Jun 18 2012 18:25:26","display_item_date":"06-18-2012","url":"http:\/\/developer.netflix.com\/blog\/read\/Upcoming_Changes_to_the_Netflix_API_Program","source":"developer.netflix.com"},{"title":"SpeakLike\u2019s API to Language Barriers: Hasta lavista, baby!","details":"According to a BBC news story, SpeakLike \u201ccrowd sources work to its pool of 3,000 translators, They can translate your blog into 35 languages and, incredibly, can even do tweets and live chats.\u201d Plus, they have the SpeakLike API.\n\n\nSpeed is an advantage. A tourist can type in a road sign, for example, and get a translation in seconds.\n\n\nBased in New York, SpeakLike can also handle huge jobs, recently translating 240,000 words into 11 languages for a European product manager in a week.\n\n\nUsers can project the speed of their translation, which can vary by language, \u00a0by checking for the number of translators online at a given time.\n\n\nPricing starts at 6 cents a word.\n\n\nSpeakLike offers a RESTful API. A second method for automating translation in an application comes via Transkit SDK.\n\n\nBoth access methods automate the use of human translation in the developer\u2019s application, \u00a0as well as using translation memory to reduce duplicate translation. \u00a0What sets the RESTful API apart is that leverages speakLike tools such as Terminology Manager and Style Guides, critical for business translation.\n\n\nThe Transkit SDK, by contrast, uses both machine and human translation.\n\n\n\n\n\nAccording to SpeakLike, \u201cIntegration is a breeze. Depending on the functionality desired, you can call SpeakLike from within your system in as little as 24 hours. One of our clients was able to build a custom internal solution to automatically translate their website content from English into nine languages in less than one week, with only one developer.\u201d","item_date":"Jun 18 2012 18:22:34","display_item_date":"06-18-2012","url":"http:\/\/blog.programmableweb.com\/2012\/06\/18\/speaklikes-api-to-language-barriers-hasta-lavista-baby\/","source":"blog.programmableweb.com"},{"title":"Netflix Quietly Smothers 3rd Party App Ecosystem","details":"Update: We\u2019ve received word that the new API Terms of Use aren\u2019t as sinister as on first glance. From VentureBeat:\n  We are not prohibiting sites from showing competing services, however we do not want anyone to use Netflix content such as titles and descriptions to advertise a competing service.\n  We\u2019re not prohibiting developers from monetizing their applications by selling them directly to consumers. We will not, however, permit resale of our information in a business-to-business fashion.\n  This definitely goes a long way to remove the uncertainty around the new API Terms. We feel that the inability for users to retrieve their viewing history and ratings remains an issue, but we\u2019re glad to learn that existing, mutually-beneficial uses of the API (such as ours) appear to be still valid.\n   Update 2:  TechCrunch and TheNextWeb have now confirmed this. I wouldn\u2019t be surprised if we see an updated Terms of Use from Netflix in the near future to put the issue beyond doubt. We\u2019ll update here again if that occurs.\n  \nOn Friday afternoon, Netflix published a blog post announcing a breaking change to their API, and dedicated a small paragraph to the fact that their API terms of use had been updated. On a technical level, these changes will cripple many apps currently integrating with Netflix, but the legal changes may be even more significant. Netflix customers should be aware of not only the upcoming changes to any 3rd party apps they might use, but what this says about Netflix as a business.\n     You can find the announcement on their developer blog. What they are announcing is that from September 15th you will no longer be able to export your viewing history and associated ratings through their API. All API endpoints that expose rental history, and other parts of the API that tangentially supply that information will be removed. All they\u2019ve said regarding motivation for that change is that they are changing their API so that it \u201cis aligned with our broader objectives\u201d. It appears Netflix considers what films you\u2019ve watched and what you thought of them as their data, not yours.\n  More significant are the changes to the API terms of use, softly mentioned in the post as: \u201cThe Terms of Use has been updated [\u2026].  To view the revised Terms of Use, go to http:\/\/developer.netflix.com\/page\/Api_terms_of_use.\u201d Checking the latest version against the google cache, we noticed several significant additions under section 1.9\n  Under the new terms, the following are no longer allowed (Update: Netflix clarification indicates that these are not targeting consumer-facing applications - see update at top of post):\n  distribute Content to any third party other than directly to end users through your own Application;\n charge, directly or indirectly, any fee (including any unique, specific, or premium charges) for access to the Content or your integration of the APIs in your Application, or use the APIs to build an enterprise application (e.g., that you distribute to other companies);\n use or display Titles in an Application for search and discovery of content linking to competing services;\n use or display Title Metadata in an Application unless it is used solely to facilitate or enable the search and discovery of Netflix services. For example, if your Application enables users to search for the availability of a movie or TV show from Netflix as well as from other services, you may only display Title Metadata in association with the availability of the movie or TV show from Netflix, not the other services;\n \nThe first implication of these additions: if you decide you just want to create a \u201cNetflix\u201d app, and add significant value on top of the Netflix service, you cannot charge your users for that value. You can do something positive for Netflix, but not for yourself. There is no incentive for you to build something useful for Netflix customers, and if you\u2019ve already built your app, you have three months left.\n  The second implication of these additions: if you want to play with Netflix, you must close your eyes and pretend that no other online streaming services exist, or might have films your users want to see. We know that a lot of our users are Netflix customers, but we also understand that they don\u2019t only use Netflix. So we integrated with Netflix to help people find the best films to watch in whichever way best suits them. We think that\u2019s of mutual benefit to Netflix and our users.\n  This looks to us like a play to keep users from moving their viewing history onto competing services, and to stop their users from seeing gaps in their content library that may be available for purchase elsewhere. While this might be in their short term interest, in the longer term, making it harder for people who want to spend money watching films legally online cannot be a good business decision.\n  In an industry as fragmented and combative as online streaming, the consumer\u2019s experience is, sadly, often the first to be compromised. Please let us know your thoughts in the comments.","item_date":"Jun 18 2012 17:56:18","display_item_date":"06-18-2012","url":"http:\/\/goodfil.ms\/blog\/posts\/2012\/06\/18\/netflix-quietly-smothers-3rd-party-app-ecosystem\/","source":"goodfil.ms"},{"title":"API - Our Impacts - Ecometrica","details":"Our Impacts and EmissionFactors.com are powered using Ecometrica\u2019s extensive emission factors database, available for integration with your internal or web-based systems through the use of our Application Programming Interface (API). The database is maintained and updated by Ecometrica\u2019s senior analyst team and contains over 40,000 data points, including emission factors, conversion factors, and assumptions.\n The API is capable of providing responses in JSON, XML and HTML. Up-to-date documentation is available online, and is presented clearly and concisely, to support you throughout the integration process and during maintenance and updates to your solutions. The API comes with its own pricing structure for commercial use \u2013 however, Ecometrica also makes free API keys available to developers who are building their own not-for-profit applications.\n Time, Location, Activity\n In order for an assessment to be accurate, emission factors need to be specific to the time and location of your data, and need to correspond to the activity for which you are calculating emissions. The flexibility of our API allows you to choose the specific time, location and activity for which emission factors and assumptions are needed, ensuring that the most accurate and relevant factor is applied to your data every time.\n Extensive Database\n The database offers emission factors and assumptions for a wide range of emission activities. Assumptions automatically convert the data you have on hand to the unit used by the best emission factor available, while emission factors are applied to your data to determine the emissions resulting from a specific activity.\n Currently, the database contains:\n Over 9,000 emission factors, including the most up-to-date factors published by the IPCC, Defra\/DECC, the EPA, and Environment Canada\n Emissions factors for a wide selection of activities, including electricity consumption, natural gas consumption, other fuel use, waste, refrigerant gas loss, vehicle travel, air travel, travel by public transit and taxi, hotel night stays and freight transportation by ground, sea, rail and air\n Over 30,500 assumptions, including assumptions related to fuel efficiency, electricity intensity and price, natural gas intensity and price, stationary fuel price and density, vehicle fuel price and density, fuel calorific values, energy and heat content of fuels, waste density and intensity and average journey length for air travel\n A complete selection of standard conversion factors relevant to typical assessment data units\n Geographically specific emission factors and assumptions for most countries, regions, counties, provinces, states and cities (where available)\n \nFollow Protocol\n An accurate and complete assessment accounts for all six Kyoto Protocol greenhouse gases: carbon dioxide, methane, nitrous oxide, hydrofluorocarbons, perfluorocarbons and sulphur hexafluoride. The Ecometrica database contains the relevant factors for each set of emission factors, making it easy for you to accurately and fully account for your emissions, and facilitating compliance with greenhouse gas reporting protocols. In addition, the database contains multiple sets of global warming potentials in order for you to correctly convert emissions from individual greenhouse gases to carbon dioxide equivalent based on the requirements of the protocol you use.\n Reputable Sources\n The database contains only factors and assumptions published from reputable sources, so you can be confident in the accuracy of your results. Among the trusted institutions from which Ecometrica sources data for the database are:\n The Intergovernmental Panel on Climate Change\n The Department for Environment, Food and Rural Affairs\n The Department of Energy and Climate Change\n The International Energy Agency\n The Environmental Protection Agency\n Environment Canada\n The Western Climate Initiative\n Carbon Trust\n Climate Registry\n GHG Protocol Brazil\n The Government of India\n Swedenergy\n Central Japan Railway Company\n Deutsche Bahn\n \n Every data piece entered into the database by our expert analysts undergoes a rigorous quality assurance process. Data is reviewed prior to and following integration into the database to ensure there are no errors, while regular testing allows you to be confident in the functionality of Ecometrica systems. Maintenance is built into the offering since the data within the database is constantly being reviewed and updated as new factors are made available.\n If you\u2019re a developer interested in accessing our API, contact us today for an informal chat.","item_date":"Jun 18 2012 17:56:05","display_item_date":"06-18-2012","url":"http:\/\/ecometrica.com\/products\/our-impacts\/api\/","source":"ecometrica.com"},{"title":"News industry watches as Twitter moves toward the news system of the future","details":"1. First, its hard to see this as a tweet, under any reasonable definition of a tweet. It has more than 140 characters. And a lot of other features that are not available to tweets. \u00a0 \n  \t\t2. Its not very convincing to argue that theres a magic to 140 characters while giving much more to some members of the network. Lets hope that Twitter, Inc doesnt have the chutzpah to do that in the future.  \u00a0 \n  \t\t3. Why stop at a synopsis, why not include the whole article? This is where the partners have to start worrying. Because the day will come when Twitter comes to them and says Our users want the full article, not just the synopsis. So either the news org provides the whole article, or they compete with other providers who do.  \u00a0 \n  \t\t4. After that, its only a matter of time before Twitter is putting ads on those pages, or otherwise monetizing. There will be a split, but it will be less attractive to the publisher than if the news resided on their own site. \u00a0 \n  \t\tIt would have been easier for the news industry to compete with Twitter if they had made this move themselves a couple of years ago by starting a Twitter-like network, with the all the features Twitter is now introducing (which all were already in the publications RSS feeds).  \u00a0 \n  \t\tThe product we all want is the realtime feed of Twitter, the river of news -- and the full text of the stories. A couple of years ago only the news industry could have provided that. Technically, thats still true, but in the intervening time, Twitter has gotten stronger, and more people expect to get their news on Twitter. Wait much longer and that will be an insurmountable advantage. \u00a0","item_date":"Jun 18 2012 17:51:01","display_item_date":"06-18-2012","url":"http:\/\/scripting.com\/stories\/2012\/06\/17\/whatsNextForTwitter.html","source":"scripting.com"},{"title":"Windows 8 Hackathon in Mountain View June 22-24 hosted by Microsoft and Hackers+Founders - Adventures of a SoCal Technical Evangelist - Site Home - MSDN Blogs","details":"Join us for the Win 8 hackathon and be among the first to see the new look (pretty!) and feel (touch screens!) of Windows before the public launch in Q3.\n                  Not only do you guys get to play with new technology, did I mention that there are only 300 apps in the Win 8 store right now? And that the MSFT store is set to be the biggest one of its kind ever?! That sounds like opportunity knocking to me. If yall make something great and put it in the app store, you may end up with a little cash flow.\n                  Equally important, you can make new friends, eat like a little piggy, have fun and win prizes*.\n                  Before Hackathon: Attendees will receive an email regarding Win 8 prior to the event, so that setup can be completed before arrival.\n                  Friday, June 22, 6:00 PM - 9:00 PM: Dinner; Intro to Win 8; Meet the Experts, Time to brainstorm and form teams; Fun!\n                  Saturday, June 23, 9:00 AM - 9:00 PM (new time): Invent; Play; Ask the experts; Eat!\n                  Sunday, June 24, 9:00 AM - 6:00 PM: Judging begins at 3:00 PM. Prizes awarded at 5:00 PM. Outta there by 6:00pm\n                  Prerequisites \n                  Please bring a notebook computer and your personal identification. \n                    Be sure to download the Windows 8 Consumer Preview.\n                    And, dont forget to download Visual Studio 11 Express for Windows 8 and seize the future of development.\n                 \nThis free event is brought to you by Microsoft and Hackers+Founders. However, you are responsible for booking and funding your own travel and accommodations. Please note that there is limited space available for this event, so be sure to register early.","item_date":"Jun 17 2012 17:26:31","display_item_date":"06-17-2012","url":"http:\/\/blogs.msdn.com\/b\/alicerp\/archive\/2012\/06\/14\/windows-8-hackathon-in-mountain-view-june-22-24-hosted-by-microsoft-and-hackers-founders.aspx","source":"blogs.msdn.com"},{"title":"Hackathon Brings Together 150 Creative Change-Makers to \u201cReinvent Business\u201d | Blog | design mind","details":"How can technology and a group of creative minds help build a more human and truly social enterprise that is designed for the 21st century world and restores trust between business and society? That was the big question tackled by the interdisciplinary Reinvent Business hackathon \u2013 a collaborative, rapid ideation and software programming event \u2013 that we hosted in our San Francisco studio this past weekend.\n Co-developed by frog and LRN, and in partnership with Blumberg Capital, BSR, Carnegie Mellon University, Cue Ball, Dachis Group, Fast Company, Net Impact, Silicon Valley Bank, and the World Economic Forum, Reinvent Business brought together a diverse group of 150 software developers, designers, gamers, filmmakers, storytellers, and business leaders to design and build innovative products and services that have the capacity to change corporate behavior from within. Based on the belief that social technology and design present a unique  opportunity to drive higher levels of transparency, empathy, and  self-governance within companies, our goal was to create concepts and  prototypes for software applications that translate values and  principles into concrete interactions and tangible experiences at the  workplace.\n The event began on Friday evening with a private reception for participants hosted by Blumberg Capital. The next morning, the official contest kicked off with a rousing speech by LRN founder, chairman, and CEO Dov Seidman (also the author of the book How), who framed the hackathon challenge in philosophical terms and stressed the urgency of creating more humanist businesses: \u201cWe need to cultivate the humanity inside of our organizations, deepen the connections between business and society, and shape the kinds of positive behaviors we need in a more interconnected and interdependent world.\u201d  \u201cTrue reinvention can only happen with multiple stakeholders contributing to the solution, sharing their ideas from the bottom up and not just the top down. We believe in the power of convening. Our hope is that by bringing together some of the brightest minds and passionate leaders we can create sustainable ideas for shaping organizations capable of thriving uniquely in the 21st century,\u201d he said.\n \n Inspired by Seidman\u2019s words, frog then facilitated a 45-min immersion activity along five main themes that helped the participants translate a somewhat abstract challenge into a series of specific opportunity areas around which to form teams. Subsequently, the teams (with 7-8 members each) were organized to brainstorm, sketch, design, and code their concepts. A group of frog mentors helped them shape their designs, ensuring the greatest levels of quality, adoptability, and potential for positive impact.\n \n The winning team (which had worked on a web-based platform called SkillCloud) received a $5,000 cash prize from Silicon Valley Bank and a free all-day workshop from frog and LRN to further refine their idea. The concepts and prototypes created at the event will also be promoted by all hosting parties and featured in a special online issue of frogs magazine design mind.\n \n The outcome of the hackathon by far exceeded our expectations: We were overwhelmed by the turn-out (we had a long waitlist, and some of the participants had flown in from Mozambique, New Zealand, and Romania for this event), the passion and commitment of everyone involved, the amazing spirit throughout the event, and the enthusiastic feedback afterwards.\n We will be posting a more comprehensive documentation of all concepts and final presentations soon. Stay tuned!\n \n The second placed team Loopool and its final pitch...\n \n And the proud winners: SkillCloud!\n Coverage\n                                                     \n                  Tim Leberecht is the CMO of frog and the publisher of design mind.","item_date":"Jun 17 2012 17:26:16","display_item_date":"06-17-2012","url":"http:\/\/designmind.frogdesign.com\/blog\/hackathon-brings-together-150-creative-change-makers-to-reinvent-business.html","source":"designmind.frogdesign.com"},{"title":"Developers can go off the rails during this weekend's Hackatrain - Chicago online marketing | Examiner.com","details":"On Saturday, June 16, from 9am to 6pm, developers from far and wide, and all types of programming languages, will converge upon the CTA Brown Line to participate in Hackatrain, the first hackathon event ever held on a train, founded by entrepreveloper\u00a0Tom Ordonez.\n This is everything you wanted to know about a hackathon on a train, but were afraid to ask.\n What is Hackatrain, and how frequent is this event?\nThe Hackatrain is the 1st hackathon inside a CTA train in Chicago. Developers can build any type of web or mobile applications inside a train. It takes place inside the moving Brown Line in Chicago and at a secondary hacking space.\n The Hackatrain tries to replicate the conditions of a daily commute where time, wifi and power is limited. A daily commute usually takes 30-60 minutes. It proves that developers can build awesome code under extreme conditions.\n     Advertisement             \n   \n  What types of coordination with the CTA goes into putting together an event such as this?\nThe CTA helped us find the resources we needed to make this happen. Some teams are building CTA-related apps and are using the train and bus APIs to build their applications.\n What activities are planned and applications used to participate in Hackatrain?\nParticipants come from all programming backgrounds: Java, C#, PHP, Ruby, Phyton, ObjectiveC, Rails and other languages.\n There are a couple of teams that are building apps about the CTA:\n Meetup inside the CTA train: Sit next to someone awesome.\n Even though I code a lot in the train now, sometimes I wish to sit next to someone awesome to talk to. I tried doing this a couple of times, but people look at me weird. Once I asked a guy Hey, nice day, huh? He looked at me like he wanted to punch me. On another occasion, I asked a woman how her day was. She changed seats.\n Most commuters are attached to an electronic device, so maybe they need this communication channel to start a real-life interaction.\nTake pictures of broken stuff at CTA stations, then report them I think this idea is very cool.\n I looked around and the CTA does not offer anything like this. CTA could also use internally to maintain the tracks then report back to their home-base, or concerned commuters that would like to help the CTA. It would be like a crowdsourcing issue-reporting app.\n A list of all projects can be found by visiting: http:\/\/hackatrain.com\/blog\/2012\/6\/5\/the-hackatrain-apps.html\n Hackathons have gained a lot of attention across various cities lately. With major corporations sponsoring a great deal of these events in hopes to attain new talent, are hackathons the new (Tech) working interview?\u00a0\n I organized a similar event in Miami back in December of 2011, and it had a similar theme, but was more educational. It was called Code Retreat and was an original idea of Corey Haines from Chicago. He traveled all around the world teaching developers how to write better code.\n A hackathon brings software developers into one place to build software. It is a great way to learn from each other. Software development is a giant octopus with tentacles.\n There are too many things to learn and by building these types of events, you can bring developers from different backgrounds to learn from each other. Pete Morano from Hackatopia organizes a lot of these and developers learn a lot from them.\n What specific Brown Line stop will the Hackatrain\u2019s starting point by 9am Saturday? \nThe station and starting points will be revealed on Saturday.\n Follow me on twitter @tomordonez for updates and follow the topic #hackatrain.\u00a0","item_date":"Jun 17 2012 17:25:51","display_item_date":"06-17-2012","url":"http:\/\/www.examiner.com\/article\/developers-can-go-off-the-rails-during-this-weekend-s-hackatrain","source":"www.examiner.com"},{"title":"The Hacktastic Hackathon","details":"Startup Weekend is a global network of passionate leaders and entrepreneurs on a mission to inspire, educate, and empower individuals, teams and communities. Come share ideas, form teams, and launch startups.\n               Startup Weekend Global Site             \n                                         \nLooking to satisfy that Startup Weekend itch before the event? \u00a0Ohio State\u2019s TCO has announced that they will be holding single-day hack events periodically throughout the year -and you are in luck, because the next one will be just in time to dust off those hacking skills and get you ready for the 54-hour frenzy of Summer Startup Weekend!\n The upcoming Hacktastic Hackathon will be the week before Columbus Startup Weekend on Thursday, June 21st from 9am-8pm. Beyond that, this hackathon is unique in that the focus is on a sector of technology that typically gets ignored: Agriculture and Food Science. What will you do between those crazy hours you ask? Well, let me tell you:\n The event starts with idea collection leading up to the Thursday Hack-Fest. These ideas are then displayed the morning of the event. Each attendee gets 5 votes to distribute as they please and the men are separated from the boys. The strong survive and those ideas will become a flurry of ideation, hacking, thinking & doing (until 5pm).\n From there, the show-off shall begin. Attendees will come check out the solutions and products that were formulated less 12 hours earlier. Teams will then need to convince folks that attend the mingle\/judge portion of the event that their idea is not only good\u00a0 -its bad ass.\n There then shall be outright braggery (that\u2019s a real word. It\u2019s located somewhere on the internet), networking and judgement (dun dun DUNN). When all is said and done, winners shall be crowned and prizes handed out.\n The previous (and first) TCO Hackathon was a huge success, with a mix of over 45 attendees ranging from your typical startup hack kid, experienced developers to OSU Medical Center Staff, faculty and students. Hacktastic attendees spent their day working on health and iPhone related projects to create amazing results in just 13 hours. Three of the five ideas from the last event are still in motion -with two having strong potential to receive funding from the OSU Medical Center, and one is going into Fast Switch for incubator action.\n  \u201cWhat I like about this event is that it\u00a0doesn\u2019t\u00a0penalize those that cant code, and forces everyone who thinks they can tell a story of what could be to prove it. I also want everyone at the party, staff, students, deans,\u00a0faculty, GEE (he was at our first one), everyone get to the table and think, dream and make it so.\u201d\u00a0","item_date":"Jun 17 2012 17:25:44","display_item_date":"06-17-2012","url":"http:\/\/columbus.startupweekend.org\/2012\/06\/14\/event-announcement-the-hacktastic-hackathon\/","source":"columbus.startupweekend.org"},{"title":"Contra Costa College students participate in hack-a-thon for a good cause | abc7news.com","details":"SAN PABLO, Calif. (KGO) --  Pulling an all-nighter is nothing new for college students. In an event held at Contra Costa College and sponsored by chip giant Intel, engineering students held a 32-hour hack-a-thon for a good cause.  The apps they created will help teach younger students math.\t\n \n   It starts out with typing, scribbling and even a little card-playing. But within 32 hours, each team hopes to turn the code theyre writing into a playable online game.  Between now and then, there wont be much sleeping going on. \n For these young engineering students its a first glimpse at a Silicon Valley tradition called a hack-a-thon. \n People tend to get together with the four crucial ingredients which are power, food, coffee and Internet, not in any particular order, and focus on a problem for a particular amount of time, said Josh Bancroft with Intel developer relations. \n As a way of taking a project that might otherwise take months, and doing it in a very short period of time, said computer science instructor Tom Murphy. \n That means learning on the job and feeding off of the excitement. \n When Im in the groove, all I can really think about is how can I fix this, change that, add some more, said student Alejandro Ramirez Escanellas. All Im thinking about is the code, really, and what I want the code to do, and I just get really happy when the code does what I want it to do. \n But as these college students learn to be better programmers, theyre also helping younger students learn about math. Each of the apps they create here is an educational game that will help teach middle school Algebra, part of a larger initiative called Code for Good.  \n I think that people here are very much motivated by the fact that they are going to be helping kids learn through the production of these applications. I think its different than what youd see if we were creating apps to make money. \n The students admit theyre learning a little math themselves, but theyre also learning things like teamwork and how to solve problems in the real world. \n Its OK to bang your head on the wall as much as you can until you figure it out, said student Ferissa Lagasca.","item_date":"Jun 17 2012 17:25:31","display_item_date":"06-17-2012","url":"http:\/\/abclocal.go.com\/kgo\/story?section=news\/technology&id=8700552","source":"abclocal.go.com"},{"title":"Hackathon | Roblox Blog","details":"Last week, we announced that ROBLOX Game Conference 2012 will feature an all-day Hackathon, where attendees can use their scripting and logic skills to come up with quick solutions to ROBLOX puzzles. We put the idea to practice on Friday afternoon, gathering eight ROBLOX developers for our own mock Hackathon. The winning developers, known on ROBLOX as \u201cQuantumSama\u201d and \u201cHotThoth\u201c, didn\u2019t receive tangible prizes \u2014 actual RGC participants will \u2014 but a brief Friday evening of bragging rights.\n The mock Hackathon gave us a chance to test several challenges, a timer and leaderboard system, and helped us see some ways we can make this event fun for ROBLOX users of many tiers. Watch this video for an inside look at our dry run on Friday, June 8th.","item_date":"Jun 17 2012 01:05:14","display_item_date":"06-16-2012","url":"http:\/\/blog.roblox.com\/tag\/hackathon\/","source":"blog.roblox.com"},{"title":"Silicon Beach Hackathon - Hacking at a Beach Party? Only in LA!","details":"Southern California has finally emerged as power house in the tech scene, and in honor of this accomplishment, Silicon Beach Fest is throwing a major Hackathon during the festival from June 21st to 23rd.  The Hackathon will consist of three days of 15 invite-only teams going head-to-head to win over $50,000 worth of prize money.  \n  Each team will have just over 48 hours to present their creations to the esteemed panel of CEO Judges, including the CEO\u2019s of DataPop, ParkMe, GumGum and an EIR from IdeaLab.  On the development side, prestigious teams from Originate, Hautelook, Disney Interactive and many more will be competing in the codefest.  \n  What\u2019s separating this Hackathon from others is a dozen CTO coaches on-hand to lead hackers to the promised land. With some of the CTO coaches coming from companies like Hautelook, Gamzee, DataPop, Mogreet and PageWoo, this unique combination of new and established talent will lead to some interesting projects in software development.  \n  For more information on the Silicon Beach Fest Hackathon please visit: http:\/\/siliconbeachfest.com\/hackathon.html.  Coders should please fill out the application on the site.  \n  The Hackathon will be held beginning June 21st at 5pm to June 23rd at 820 Broadway, Santa Monica, California.\n  Introducing the CEO Judges:  \nJason A. Manasse, AccuScore\u00a0\u00a0\u00a0\u00a0  \nSam Friedman, ParkMe  \nJason Lehmbeck, DataPop  \nOphir Tanz, GumGum  \nLee Essner, IdeaLab  \nGreg Cohn, Ad Hoc Labs  \nBrent Freeman, Roozt  \nTed Dhanik, Engage BDR  \nNick Bicanic, EchoEcho\n  Introducing the CTO Coaches:","item_date":"Jun 17 2012 01:05:02","display_item_date":"06-16-2012","url":"http:\/\/www.prweb.com\/releases\/2012\/6\/prweb9608799.htm","source":"www.prweb.com"},{"title":"APIs Prove Wine is Better Than Beer","details":"As more of a beer lover than wine connoisseur, it pains me to say it: in the API world, wine has beer beat. Our directory lists 9 wine APIs, but only 5 beer APIs. However, there are noticeable differences in the type of data made available by these APIs.\n\n\nWine APIs tend to be centered around cataloging wine, or finding it to purchase online. Beer APIs are more heavily oriented toward the places that serve or brew the beers. There are also a couple databases of beers, similar to wine cataloging.\n\n\nHere\u2019s the current list of nine wine APIs:\n\n\n\u00a0\u00a0Adegga API: Social Wine Discovery service\n\n\n\u00a0\u00a0AVIN API: Wine tracking number repository\n\n\n\u00a0\u00a0Cruvee Partner Link API: Wine database update system\n\n\n\u00a0\u00a0Cruvee Social API: Social media search for wine industry\n\n\n\u00a0\u00a0Cruvee Wine Search API: Wine industry search service\n\n\n\u00a0\u00a0ShipCompliant Marketplace API: Wine shipping information service\n\n\n\u00a0\u00a0Snooth API: Wine recommendations service\n\n\n\u00a0\u00a0Wine-Searcher API: Wine search service\n\n\n\u00a0\u00a0Wine.com API: Wine reference and resources\n\n\nAnd the five beer APIs:\n\n\n\u00a0\u00a0Beer Mapping API: Brewery data and mapping services\n\n\n\u00a0\u00a0Brewery DB API: Open library of breweries and beers\n\n\n\u00a0\u00a0Inapub API: Pub search tool\n\n\n\u00a0\u00a0RateBeer API: beer database and rating tool\n\n\n\u00a0\u00a0Untappd API: Social beer discovery app\n\n\nBeer does have wine beat on being first to market. We added the Beer Mapping API in 2008 as the first beer API. It was almost a year before the launch of the wine recommendation service\u2019s Snooth API.\n\n\nPhoto by Duncan Hull","item_date":"Jun 17 2012 01:04:52","display_item_date":"06-16-2012","url":"http:\/\/blog.programmableweb.com\/2012\/06\/15\/apis-prove-wine-is-better-than-beer\/","source":"blog.programmableweb.com"},{"title":"Google Announces The \u201cDevelop For Good\u201d Hackathon","details":"","item_date":"Jun 17 2012 01:04:44","display_item_date":"06-16-2012","url":"http:\/\/www.webpronews.com\/google-announces-the-develop-for-good-hackathon-2012-06","source":"www.webpronews.com"},{"title":"Female Friendly <b>Hackathon<\/b> In NYC Today And Tomorrow - AllTwitter","details":"","item_date":"Jun 15 2012 22:58:59","display_item_date":"06-15-2012","url":"http:\/\/Sent%20to%20you%20via%20Google%20Reader","source":"Sent%20to%20you%20via%20Google%20Reader"},{"title":"API Strategies for Big Data - If Data Were Oil","details":"The world is changing.  There is a new oil, a new resource, a new  competitive advantage in great abundance. It\u2019s  Big Data.  The oil will drive innovation or distribution for  emerging platforms. Be one.@apinomicdrew@apinomic.com \nWhy are we calling it Big        Data?Because there is a lot of it?Because a lot of companies think they have it?Big because it is the domain of big companies?Because it is breeding big ideas?We needed another conference track topic?We just like the word \u201cbig\u201d \nData is happeningMonetarilySociallyCulturallyPoliticallyTechnologicallyGlobally \nData is\u2026Disruptive            Transformational   To the data poor        To the data rich \nWhat if data were a           liquid\u2026What would we do with it then?How would we manage it? \nWhat would be our resulting  actions if data were a fluid           resource?Monetarily - package, productize \uf0e0 distributeSocially \u2013 curate, cooperate \uf0e0 shareCulturally \u2013 harness new ideas \uf0e0 improvePolitically \u2013 harness the will, profile \uf0e0 listenTechnologically \u2013 innovate faster \uf0e0 buildGlobally \u2013 collaborate, open up \uf0e0 engageGovernance \u2013 regulate, standardize \uf0e0 reduceuncertainty \nJust Focusing on Business   and People (for now)  Business needs data toproduce, compete, survive.                                People need data to                             learn, advance, and grow. \nImagine a future\u2026\u201dOil traded near its lowest closing price in eightmonths in New York before the Organization ofPetroleum Exporting Countries meets to discuss...\u201dPersonal Data traded near its lowest closingprice in eight months in New York before theOrganization of Data Exporting Companies Google facebook LinkedIn IRS  Axciom   Amazon(ODEC) meets to discuss\u2026 IBM \nSo what do Big Data  Companies have in      Common?        They don\u2019t just manage data.                      They worship it.They think, build, and run like a platform. \nWhat is a platform?   A data centric, technology obsessed, peopleempowering, agile, high value, high energy, beast of a            company\u2026large or small.           Their most core competencies:                     Distribution                          &                     Innovation \nBig Data Map \u2013 Platform                Assessment         High                  Underperforming.                 Think: Offering APIs                                                  Stay                                             Paranoid, but                                                                 $                                            enjoy the currentValue of Data                                   valuation   Assets                       Change your          Underperforming.                      industry (fast)      Think: Partnerships                                             and Consuming          Low                                     APIs.                Low            Capability for Deep        High                              Innovation or Broad                                  Distribution \nBig Data Map \u2013 Personal                 Assessment         High                  Underperforming.           Stay curious, but                   Take more risks           enjoy the current                  with ideas. Life is           happinessRate of YOUR             Short.Learning from the Internet                       Change your           Underperforming.                      industry (fast)         Deepen personal                                            research and rate of          Low                                   new ideas.                Low           Capacity for Original        High                                Ideas or Broad                                  Networking \nLitmus test \u2013 \u201c20 years of     experience\u2026\u201d  One year of experience repeated 20  times? \nData Distribution will Help Define Data         Rich and Data Poor     It is why there are more than 5,000 Public APIs from           businesses, up more than 5 fold in 3 years. \nAPI\u2019s = The drill bits and      pipes for platform growthAPIs \u2013 empowering digitalpartnersAPIs \u2013 accelerating internalinnovationMobile API \u2013 moments ofaccess or captureAPI Mash ups \u2013commercializingcomplementary data setswith value added partners \nLaws of APInomics    Without managed APIs you cannot scale any    serious data distribution. \uf0e0Platform challenged.    You cannot control and measure the flow of your    oil to others. \uf0e0Platform blind or isolated.    You cannot efficiently tap into the vast natural    deposits of oil (data) in the world. \uf0e0Platform    starved.    APIs are not an experiment, they are a business.    Platform wake up call.*** These \u201claws\u201d are likely to be disrupted within       18 months\u2026thank to technology. \nA business without APIs or   data, or a means to        distribute. \nAnd APIs can do this too\u2026 Get you into fun hackathons Attract new talent Onboard new partners quickly Experiment in mobile more cheaply Test the uniqueness of your data Test the appeal of your brand Attract developers to test new sh__ for you Stay relevant in digital \nWhat does APInomic do?Measure your data assets \u2013 a baselineEstablish your API potential \u2013 a business caseMap your APIs to partners \u2013 a roadmapDeliver your API Strategy \u2013 a creative path todata richAverage engagement is 2-3 months. That\u2019s it! \nAPInomic = Acceleration +               Direction.    \u201cThe Jiffy Lube for getting better data                 efficiencies.\u201d \u201cThe AAA for getting better data directions.\u201d(And we know what tow trucks to call when you            stumble with APIs).     A Brand without an API Strategy is a Brand without a     Strategy. \nThe world is changing.   There is a new oil, a new resource, a new competitive   advantage in great abundance. It\u2019s Big Data.   The oil will drive innovation or distribution for   emerging platforms. Be one, or be disrupted like you   can imagine.@apinomicdrew@apinomic.com \nNext up for innovation or  further disruption\u2026 The Post Office\u2026","item_date":"Jun 15 2012 21:39:22","display_item_date":"06-15-2012","url":"http:\/\/www.slideshare.net\/dbartkiewicz\/api-strategies-for-big-data-if-data-were-oil","source":"www.slideshare.net"},{"title":"CyclingBuddy Rolls Out Open API","details":"CyclingBuddy is a \u201cfree platform that allows members to find other members to go cycling with.\u201d Described as the \u201cSocial Networking platform for cyclists,\u201d CyclingBuddy unites cyclists around the globe and promotes sharing routes, training tips, and overall opinions on cycling. In a strategic move, CyclingBuddy recently opened the CyclingBuddy API to allow developers to integrate with its platform. CyclingBuddy founder and CEO, Tony Piedade, commented: \u201cwe are really good at motivating people to cycle, while Apps are great at recording progress and performance.\u201d Piedade envisions great partnership opportunities where \u201ceverybody wins.\u201d\n\n\n\n\n\nAlthough the social network for cycling launched merely a few months ago, CyclingBuddy has gained substantial traction. In a couple of months, CyclingBuddy members have logged over 150,000 miles and 1000+ routes across 14 countries. Perhaps more importantly, CyclingBuddy has developed strategic partnerships that may lead to integration with their newly launched open API. Strategic partners include Camelbak, Madison, Cycletta, and Summit Different.\n\n\nCyclingBuddy announced the open API a few days ago, and little public information is currently available. Interested developers should contact CyclingBuddy directly: API@cyclingbuddy.com. Given the popularity of Apps surrounding CyclingBuddy\u2019s sister site: JoggingBuddy.com, expect a wave of integration and press releases to flood the web in the coming months.","item_date":"Jun 14 2012 18:34:48","display_item_date":"06-14-2012","url":"http:\/\/blog.programmableweb.com\/2012\/06\/14\/cyclingbuddy-rolls-out-open-api\/","source":"blog.programmableweb.com"},{"title":"Laying Foundations for the School of Data ","details":"Recently, a small team gathered in Berlin for the School of Data kick-off sprint. After three days fueled by coffee, felt-tip pens and a multi-coloured array of post-it-notes, the sprint left us with a true appreciation of the amazing community we are working with, and an exciting new structure to underpin the School of Data. Read on for more details, and to find out how you can get involved\u2026\n  \n  As we have stated in previous blog posts, the purpose of the School of Data is:\n  \u201cto provide online training for data \u2018wrangling\u2019 skills \u2013 that is, the ability to find, retrieve, clean, manipulate, analyze and represent different types of data.\u201c\n  Excellent. But what exactly does that mean? What basic competencies allow those abilities to be developed? How are they best taught? And is it possible to run a School of Data which caters both for those with no technical skills, and for those who already have significant data expertise?\n  In Berlin, we gathered a small team of project leaders and community experts from the Open Knowledge Foundation, P2PU and the Tactical Technology Collective, and mixed them with some leading data wranglers to address these and many other questions. For one adrenalin-fueled afternoon, around 40 community members also joined us online for a virtual sprint to test our ideas and contribute their own. A huge thanks to everyone who joined in \u2013 the afternoon was productive and hugely inspiring! Three days later and we had answered some questions, raised several more, and, most excitingly, had a framework to support the School of Data. This framework is outlined below.\n  The Data Pipeline\n  In our discussions, we quickly established that data wrangling takes place in several stages; in order to process data, it must be moved through a \u2018pipeline\u2019. The basic stages of the pipeline are outlined in the diagram below.\n  \n  Raw data must usually travel through every stage of the pipeline \u2013 sometimes going through one stage more than once. However, several different data wranglers could be involved. For example, a civil servant may collect, process and present some data in a basic table; someone from a civil society organisation may then find, clean and analyse that data for their own needs; and finally, they may pass it on to a data visualisation expert or journalist to help them build a story for the public.\n  \n  To fully understand data, wranglers need to be familiar with every stage of the pipeline, but, depending on their interests, some may want to hone their skills at one specific stage.\n  The School of Data will cater for both models by offering a combination of learning \u2018journeys\u2019 \u2013 in which learners will travel the data pipeline from start to finish \u2013 and \u2018plunges\u2019 \u2013 which will look at one specific area in more depth.\n  Once the pipeline was in place, the challenge was to identify the skills that are needed along each step of the way. Cue many more post-it-notes!\n  \n  At every stage of the data pipeline, a variety of skills could be employed to wrangle data. The appropriate technique usually depends on the nature of the data itself and exactly what you want to do with it. The abilities of the data wrangler will also play a part in which skills are deployed \u2013 some techniques are more technical than others!\n  With the excellent guidance of Stefan Urbanek, our posters and post-it-notes were transformed into the following skills map.\n  \n  The map shows the level of prior-knowledge required in order to develop various skills. \u2018N\u2019 indicates that no technical knowledge is necessary, whilst levels 1, 2 and 3 become gradually more advanced. For level 3, prior programming ability is likely to be necessary.\n  Learners at the School of Data should assess their knowledge and gauge which challenges are appropriate. In time, guidelines will be added to help you work this out. Generally, common sense applies \u2013 absolute beginners shouldn\u2019t attempt a level 3 course on fuzzy matching, whereas someone with a lot of experience may want to skip through a level 1 class on pivoting and get straight on to data simulation.\n  This is the moment for a dose of realism: the School of Data won\u2019t be able to address all of these skills in its first iteration! Neither is this list exhaustive, and we welcome your feedback and, most of all, your input. See below for more on how to comment and get involved.\n  So what now?\n  \n  Many of you have been asking great questions about the School of Data. We have tried to answer some of these in our new, expanded list of Frequently Asked Questions \u2013 take a look!\n  One of the key questions has been about the timeline for the School of Data. We expect to launch the first iteration of the School during Autumn 2012. Another question has been about how you can help us! The enthusiasm during the virtual sprint was inspiring, and the energy of the emerging School of Data community is one of the most exciting aspects of the School of Data to date. Below, we have outlined in more details how you can get involved\u2026\n  Get Involved!\n  The first learning challenges at the School of Data are going to be produced internally, to ensure that we cover a good balance of the basic skills from the outset.\n  Whilst we are creating and curating this content, we would appreciate your help in a number of ways:\n  Add content to the Data Wrangling Handbook! In some ways this is the key task at present. The Data Wrangling Handbook will function as the \u2018course textbook\u2019 for the School of Data, and we need your help to make it as comprehensive and effective as possible! If you have experience with a particular tool, technique, process or pitfall, we\u2019d love you to write a short chapter. The skeleton outline can be viewed online here \u2013 we will be fleshing out the details and adding content over the coming months. To offer feedback and to contribute, either (1) submit an issue to our tracker, (2) edit directly on Github, or (3) email schoolofdata [@] okfn.org with details of what you would like to add or change.\n If you know of useful materials and resources, share them with us here. Datasets can be added directly to the School of Data group on the DataHub\n If you would like to help edit, proof-read and test-run challenges and courses, sign up to the School of Data development mailing list, and introduce yourself. NB: there are two mailing lists! This one is higher traffic and interactive discussion is fully encouraged.\n If you want to stay in touch with news about the School of Data, sign-up to the quieter announce list, or follow the School of Data on Twitter: @SchoolofData\n \n Support the School of Data in their local area \u2013 perhaps by running physical courses through local networks and organisations\n \nIf you think you might be interested in getting involved in these or other ways, sign-up to the mailing list, and do drop an email to schoolofdata [@] okfn.org to introduce yourself.\n \nKick-starting the School of Data! Earlier this year, we announced plans to launch the School of Data. Thanks to the generous support of Open Society Foundations and the Shuttleworth Foundation, we\u2019re now ready to go! We\u2019re holding a kick-off sprint next week, and we invite...\n Announcing the School of Data The following post is by Rufus Pollock, Director and Co-Founder of the Open Knowledge Foundation, and Philip Schmidt, Co-Founder and Executive Director of Peer 2 Peer University. Today, we\u2019re announcing plans for a School of Data. The School will be...\n Diving into Data: The School of Data Journalism at the International Journalism Festival in Perugia This post is by Liliana Bounegru, Project Coordinator at the European Journalism Centre, and Lucy Chambers, Community Coordinator at the Open Knowledge Foundation. It is cross posted on DataDrivenJournalism.net and journalismfestival.com. In the past investigative reporters would suffer from a...","item_date":"Jun 14 2012 17:23:19","display_item_date":"06-14-2012","url":"http:\/\/blog.okfn.org\/2012\/06\/14\/laying-foundations-for-the-school-of-data\/","source":"blog.okfn.org"},{"title":"How Engineers Disappear Into the Twitter Collective","details":"What do you need to run a web service that broadcasts thousands of messages a second between millions of people across the planet? Among other things, you need a really big office in one of the world\u2019s most attractive cities.\n Earlier this week, after busting the seams on its old digs in the hip South of Market section of San Francisco, Twitter officially moved into a brand-new 215,000-square-foot office just down the road. As the popularity of its micro-blogging service has grown, so too has the company itself. In March 2011, the company employed about 400 people, but today, it spans close to 1,000, with over 800 in San Francisco alone.\n A big part of this growth is engineering talent \u2014 a hotly contested resource in Northern California. As with so many other web outfits in the heart of the technology world  \u2014 both public and private \u2014 part of Twitter\u2019s strategy involves what it is commonly known as the \u201cacqui-hire.\u201d Earlier this year, the micro-blogging behemoth purchased at least two companies solely for their engineering talent: blogging company Posterous and security startup Whisper Systems.\n In each case, Twitter wasn\u2019t looking to buy a product or service. It was merely looking for brains. Both sets of engineers have been consumed by the Twitter collective. \n \u201cEverything that\u2019s being built here at Twitter is really coming from all of us,\u201d says Sachin Argawal, the former CEO of Posterous, tells Wired, gesturing around at a cafeteria full of hungry Twitter employees at the company\u2019s old office, a stone\u2019s throw from AT&T Park, home of the San Francisco Giants.\n \u201cWe\u2019re all able to participate in innovating and coming up with the ideas. It feels very much like a startup in that way.\u201d Most of the Posterous team is now part of the effort to add more than just text to your tweets, including photos and videos.\n Before it was acquired, Whisper Systems wrote security software for Google\u2019s Android mobile operating system. But Twitter wasn\u2019t interested in the software. It promptly open sourced code, giving it away to the world at large. What it wanted was Whisper founder Moxie Marlinspike \u2014 a well-known expert in secure sockets layer (SSL) encryption, a technology used to guard web traffic from prying eyes. Marlinspike is now charged with securing Twitter entire infrastructure, from the back-end databases to its front-end user interface.\n Meanwhile, another Whisper Systems founder, roboticist Stuart Anderson, works for a team that seeks to improve the algorithms that track user behavior. The company uses these \u201clearning algorithms\u201d to suggest people to follow on Twitter or serve up targeted ads.\n According to Adam Messinger, Twitter\u2019s vice president of engineering, weaving a separate team of engineers into an existing company is a \u201ctricky problem.\u201d But he has some practice. Before coming to Twitter, he was vice president of development at Oracle, overseeing \u2014 among other things \u2014 the Java platform, which Oracle had acquired with its purchase of Sun Microsystems.\n Twitter organizes its teams in a kind of \u201cmatrix.\u201d Each team is charged with handling a different part of Twitter infrastructure \u2014 such as its iOS application or its backend database \u2014 but Messinger will also grab engineers from across teams to create \u201cvirtual teams.\u201d These will then build new parts of infrastructure, as if they were startups. \n Typically, when Twitter purchases a new company, the CEO of the acquired outfit becomes a product manager who oversees a team, as was the case was Posterous boss Sachin Argawa. According to Messinger, the Posterous team immediately slipped into their new engineering roles. \u201cThey got up to speed on our giant Ruby stack in the first week and even made a bug fix,\u201d he says, referring to Ruby on Rails, the programming framework Twitter is based on.\n Like Messinger, the entire company has a long history with with acqui-hires. Elad Gil, Twitter\u2019s vice president of strategy, came by way of an acquisition. His company was called Mixer Labs, an outfit offering an application program interface, or API, that shared geographical data with other services across the web. At about the same time, Twitter started geo-tagging Tweets, and the company soon acquired Mixer so that its engineers could help connects all those micro-messages to data that tied them to the physical locations from which they were sent.\n From the outside, Twitter seems like a monolith \u2014 a single web service with a single mission. But underneath the surface, the operation is vast and varied. It\u2019s many companies in one.","item_date":"Jun 14 2012 17:04:03","display_item_date":"06-14-2012","url":"http:\/\/www.wired.com\/wiredenterprise\/2012\/06\/twitter-talent\/","source":"www.wired.com"},{"title":"TurboVote Aims to Make Voting Easy for Everyone","details":"Voter turnout is always a challenge for the United States. Typical turnout is 50% for\u00a0presidential elections, 40% for mid-term elections and only 10% in local, special and primary elections. One state, Oregon, changed its voting system to increase participation by switching to a vote-by-mail for every election. This way residents aren\u2019t limited by a certain day, time or location to let their vote be heard.\n Many states still struggle with voting turnouts and Seth Flaxman, co-founder of TurboVote, knew there had to be some way use the Internet to bring the simplicity of the Oregon-style voting system to everyone.\n Seth always believed strongly in democracy and in high school, he drove his friend to the polls so they could vote for the school budget. After moving away for graduate school, Seth missed a couple of elections. Dissatisfied with the lack of services to help people vote, Seth founded TurboVote with fellow Harvard Kennedy School of Government students Kathryn Peters and Amanda Cassel Kraft.\n  Paul Schreiber\n Before every election, TurboVote helps people get registered to vote and helps them vote by mail. After the envelopes go out in the mail, TurboVote sends email and SMS reminders so people don\u2019t forget. Powered by Twilio, CTO Paul Schreiber implemented two-way SMS that not only reminds people of election deadlines, but lets voters reach out to the team with questions. Most recently, the team launched a mobile and Spanish versions of the service.\n In September of 2010, TurboVote piloted the service at Boston University. In November 2010, the team raised seed money with a grassroots campaign on Kickstarter paired with funding from the Sunlight Foundation. Next fall, they kicked off their service with Harvard College. The team hasn\u2019t slowed down since launch with funding from multiple sources including the Knight Foundation and Google, and partnering with universities across the US.\n Keep watching for a bunch of new features from TurboVote, including the ability to sponsor mailings for your friends and family. TurboVote\u2019s goal is simple: help as many people vote by mail as possible to make sure those votes are counted. The team aims to bring on more nonprofit and collegiate partners to help achieve that goal.\n Visit www.Turbovote.org to sign up for mail-in ballot registration and election reminders. Follow on Twitter @turbovote and on Facebook.","item_date":"Jun 14 2012 03:12:00","display_item_date":"06-13-2012","url":"http:\/\/feedproxy.google.com\/~r\/twilio\/OBEN\/~3\/ZyyVxIUMyNs\/turbovote-aims-to-make-voting-easy-for-everyone.html","source":"feedproxy.google.com"},{"title":"Beyond the Sandbox With Twilio and Syracuse University","details":"We are continuously impressed with the innovative programs and entrepreneurial young minds breaking out of universities. From weekend hackathons for good to fresh companies\u00a0 launched on a bus, this new breed of battle-tested students are a group to watch in the startup world. That\u2019s why we\u00a0are proud to announce a partnership with the Syracuse University Student Sandbox Incubator and Syracuse IDEA program.\n Founded in 2009, the Student Sandbox is a twelve week program designed to give student entrepreneurs the chance to accelerate their startup by providing them with access to funding, workspace, a team of mentors, and many other handy resources. Like most student incubators, the Sandbox is educational and does not retain any equity in its companies.\n Witnessing the continued growth and success of this program, Syracuse University is now increasing their efforts to further cultivate entrepreneurship on campus. The University recently announced a partnership with The Tech Garden, a VC incubator in Central New York, to establish the\u00a0Raymond von Dran Innovation and Disruptive Entrepreneurship Accelerator (IDEA) accelerator.\n Similar to the University of Texas at Austin\u2019s\u00a01SemesterStartup, the IDEA program takes a hybrid approach to teaching entrepreneurship by supplementing courses with external resources. The IDEA program connects students from any discipline and provides each team with access to the Sandbox, free or discounted technical tools, and, most importantly, support from mentors in the local startup community. Courses\u00a0and an IDS (IT, Design, and Startups)\u00a0minor cover taking an idea to an actionable launch plan to discussing legal issues facing startups.\n To get an inside\u00a0perspective\u00a0of what these programs mean to students, we talked to the Program Manager for the Student Sandbox\u00a0and\u00a0Co-Founder\u00a0at\u00a0Student IDEA Connect Board, Elizabeth Ruscitto.\n \u201cWith the IDEA program, we help students start businesses. By providing an experiential learning environment, students can graduate with a real world understanding of what it takes to build a successful enterprise, while they create their own company.\u201d\n \n We join a list of other great partners and\u00a0mentors\u00a0supporting these thinkers of tomorrow, giving each team \u00a0Twilio credit and mentorship time. We\u2019re excited to be involved with this group and support all of the teams involved.\n If you have questions or want to learn more about this and other Twilio EDU initiatives, make sure you follow Jack Aboutboul, one of our developer evangelists who leads our EDU initiative.","item_date":"Jun 14 2012 03:12:00","display_item_date":"06-13-2012","url":"http:\/\/feedproxy.google.com\/~r\/twilio\/OBEN\/~3\/y39-uiz3qQ4\/syracusesandbox.html","source":"feedproxy.google.com"},{"title":"WolfraSMS, the Power of Search via SMS and WolframAlpha","details":"Martin Pannier was looking for a way to bring the power of search similar to Siri to a wider audience, why should iPhone 4s owners have all the fun? What he built is WolfraSMS: a simple SMS app that harnesses the search capabilities of Wolfram|Alpha, powered by Twilio.\n Martin is the co-founder and COO of SocialFolders, a service to organize your social content in the cloud. WolfraSMS is the very first app Martin built with Twilio \u2013 quite impressive. Follow Martin on Twitter @martinpannier and read his blog here.\n What exactly is WolfraSMS?\n WolfraSMS is\u2014very simply\u2014a way to ask the Wolfram|Alpha computational search engine questions, by sending a simple SMS. You can send a variety of questions, such as \u201cWhere was Obama born?\u201d or \u201cWho was the third president of the United States?\u201d to the WolfraSMS number and it will send you back the answer in SMS form.\n What was the inspiration for building WolframSMS?\n I love Siri. Being able to ask questions and get the answer, and not only a series of links, is quite remarkable. But Siri is only available to iPhone 4S users. I wanted to bring some of the power of Siri to the 99%!\n SMS is incredible for its ubiquity: anywhere, and with any phone, you can send and receive SMS messages. Geeks don\u2019t quite realize the power of this ubiquity, as they always tend to have huge data plans and sophisticated smartphones.\n Match the two\u2026 boom: WolfraSMS.\n How are you using Twilio?\n Twilio powers the whole SMS hub.\n What other technologies are you using to build\/support it?\n The app was coded in Python and is hosted on Heroku.\n How did you get started developing with Twilio?\n With WolfraSMS :) It\u2019s actually my first Twilio app.\n What\u2019s next for WolframSMS?\n This is a side project for me, so with limited time on my hands, what\u2019s realistically next is easter eggs and more thorough use of the Wolfram|Alpha answers\u2014as for brevity reasons, only the most relevant part of the answers is returned. If I had more time, I would include the same search engines as Apple uses for Siri\u2014starting with Yelp.\n Then Siri would really be available to anyone in the world!","item_date":"Jun 14 2012 03:12:00","display_item_date":"06-13-2012","url":"http:\/\/feedproxy.google.com\/~r\/twilio\/OBEN\/~3\/sqKXZBtxtGI\/wolfrasms-the-power-of-search-via-sms-and-wolframalpha.html","source":"feedproxy.google.com"},{"title":"Twilio Client Expands to Android to Enable Cross-Platform VoIP Calls","details":"Following the February launch of Twilio Client for iOS, we are very excited to expand our mobile tools with the launch Twilio Client for Android. This makes it possible for any Android application to make and receive voice calls over a cellular data or WiFi connection without ever using standard cell phone minutes.\n With this launch, Twilio Client is now available for Android, iOS and web browsers to implement VoIP communications into any mobile app. Seamlessly talk between these platforms and traditional phones to let your users talk over whatever medium they choose.\n Building Apps That Talk in the Cloud\n Using Twilio Client for Android, developers have access to a few key features that will make building communications apps easy.\n Real-time Presence \u2013 Build buddy lists into any app letting users know who is online and set up notifications when a user is available for voice chat\n App Backgrounding \u2013 The SDK can receive voice calls even when a different app is in use, letting users switch between apps easily to respond to incoming calls.\n Cross-Platform Interoperability \u2013 Twilio Client supports Android, iOS, and web browsers so apps can make calls between any platform, including bridging VoIP calls with traditional phone calls.\n Voice Over Data and WiFi connections \u2013 Make and receive calls over data and WiFi connections without using carrier minutes\n \n Ready to get started on building a mobile app that makes and receives phone calls? \u00a0We\u2019ve got documentation, helper libraries, sample code, and an active community of developers ready to answer your questions.\n Get Started with Twilio\n Twilio Client\n Twilio Client for Android Quickstart\n Download Android SDK\n \nIf you are looking for iOS information take a look at the Twilio Client iOS Quickstart, iOS User Guide and download the SDK here.\n Need more help? \u00a0Can\u2019t code but want to find someone who can? \u00a0That\u2019s cool! \u00a0Contact us at sales@twilio.com and we\u2019ll connect you with members of our developer community who can help. Can\u2019t wait to see what you build.\n \t\t\t\t\t\t\t\t\t\t\t\n \t\t\t\t\t\t\tMeghan is the community manager at Twilio. Find her on Twitter @megmurph and email at murphy [at] twilio [dot] com.\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t\tView all posts by Meghan Murphy \u2192","item_date":"Jun 14 2012 03:12:00","display_item_date":"06-13-2012","url":"http:\/\/feedproxy.google.com\/~r\/twilio\/OBEN\/~3\/v6ybF2E4DOw\/twilio-client-expands-to-android-to-enable-cross-platform-voip-calls.html","source":"feedproxy.google.com"},{"title":"Twilio and Box, the Contest!","details":"","item_date":"Jun 14 2012 03:12:00","display_item_date":"06-13-2012","url":"http:\/\/developers.blog.box.com\/2012\/05\/01\/twilio-and-box-the-contest\/","source":"developers.blog.box.com"},{"title":"Introducing the V2 API","details":"","item_date":"Jun 14 2012 03:12:00","display_item_date":"06-13-2012","url":"http:\/\/developers.blog.box.com\/2012\/04\/25\/introducing-the-v2-api\/","source":"developers.blog.box.com"},{"title":"Seamlessly Integrate Email and Box with Context.IO","details":"","item_date":"Jun 14 2012 03:12:00","display_item_date":"06-13-2012","url":"http:\/\/developers.blog.box.com\/2012\/04\/02\/seamlessly-integrate-email-and-box-with-context-io-2\/","source":"developers.blog.box.com"},{"title":"Introducing the Box APEX Library for Force.com","details":"","item_date":"Jun 14 2012 03:12:00","display_item_date":"06-13-2012","url":"http:\/\/developers.blog.box.com\/2012\/03\/13\/introducing-the-box-apex-library-for-force-com\/","source":"developers.blog.box.com"},{"title":"Twitter sentiment mirrors Facebook stock prices?","details":"If you had traded based on signals today to buy\/sell Facebook stock, how would you have done?\n Probably pretty well!\n We\u2019ve been closely monitoring Twitter\u2019s social data and its relation to the Facebook stock price. What we found is pretty interesting. The positive and negative views on Twitter mirrored the fluctuations in Facebook stock price today.\n The graph below plots Twitter chatter related to the IPO against Facebook stock prices. You\u2019ll see that every time the volume of negative comments increase, Facebook stock price dropped within 20 minutes \u2013 noting an interesting correlation between Twitter sentiment and stock price fluctuations.\n \n                                              [...] conclusions. Turns out, the ups and downs of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] conclusions. Turns out, the ups and downs of how Twitter&#8217s consumers felt about the stock fairly significantly mirrored the price of Facebook&#8217s stock as the day [...]\n \n                                              [...] conclusions. Turns out, the ups and downs of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] conclusions. Turns out, the ups and down of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] conclusions. Turns out, the ups and downs of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] interesting conclusions. Turns out, the ups and downs of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] interesting conclusions. Turns out, the ups and downs of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] conclusions. Turns out, the ups and downs of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] conclusions. Turns out, the ups and downs of how Twitter&#8217s customers felt about the inventory quite significantly mirrored the cost of Fb&#8217s inventory as the day [...]\n \n                                              [...] conclusions. Turns out, the ups and downs of how Twitter&#8217s users felt about the inventory fairly much mirrored the cost of Fb&#8217s stock as the day [...]\n \n                                              [...] conclusions. Turns out, the ups and downs of how Twitter&#8217s customers felt about the inventory quite significantly mirrored the cost of Facebook&#8217s stock as the day [...]\n \n                                              [...] conclusions. Turns out, the ups and downs of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] conclusions. Turns out, the ups and downs of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] conclusions. Turns out, the ups and downs of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] conclusions. Turns out, the ups and downs of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] conclusions. Turns out, the ups and down of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day progressed. [...]\n \n                                              [...] interesting conclusions. Turns out, the ups and downs of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] conclusions. Turns out, the ups and downs of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] conclusions. Turns out, the ups and downs of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] interesting conclusions. Turns out, the ups and downs of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] conclusions. Turns out, the ups and downs of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] conclusions. Turns out, the ups and downs of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] conclusions. Turns out, the ups and downs of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] conclusions. Turns out, the ups and downs of how Twitter&#8217s users felt about the inventory pretty much mirrored the price tag of Facebook&#8217s inventory as the day [...]\n \n                                              [...] conclusions. Turns out, the ups and downs of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] conclusions. Turns out, the ups and downs of how Twitter&#8217s end users felt about the inventory rather a lot mirrored the cost of Fb&#8217s inventory as the day [...]\n \n                                              [...] conclusions. Turns out, the ups and down of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] down &#959f &#1211&#959w Twitter\u2019s users felt &#1072b&#959&#965t t&#1211&#1077 stock pretty much mirrored t&#1211&#1077 price &#959f Facebook\u2019s stock &#1072&#1109 t&#1211&#1077 day [...]\n \n                                              [...] conclusions. Turns out, the ups and downs of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] conclusions. Turns out, the ups and downs of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] interesting conclusions. Turns out, the ups and downs of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] conclusions. Turns out, the ups and downs of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] conclusions. Turns out, the ups and down of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] conclusions. Turns out, the ups and downs of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] interesting conclusions. Turns out, the ups and downs of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]\n \n                                              [...] interesting conclusions. Turns out, the ups and downs of how Twitter\u2019s users felt about the stock pretty much mirrored the price of Facebook\u2019s stock as the day [...]","item_date":"Jun 14 2012 03:12:00","display_item_date":"06-13-2012","url":"http:\/\/blog.datasift.com\/2012\/05\/18\/twitter-sentiment-mirrors-facebook-stock-prices\/","source":"blog.datasift.com"},{"title":"DataSift adds $7.2M: The story so far and focus for the future","details":"The last few months have been an amazing journey for us. It was only back in November last year that we launched DataSift. That seems like a lifetime ago when we look back at what we\u2019ve achieved since then\u2026\n with the help and support of Twitter we\u2019ve launched our Historics platform to unlock insights from 2+ years of Tweets\n we\u2019ve on-boarded 20+ new data sources (including YouTube, Blogs, Forums and NewsCred powered News )\n we\u2019ve added around 200 new customers, both entrepreneurs building their socially-intelligent applications to large enterprises that recognize that social is becoming table-stakes for business.\n \nOn top of that, as passionate Big Data junkies we held 50 events across the world last week as part of www.bigdataweek.com, including a live Twitter Q&A with Todd Park the CTO of the United States. Huge kudos goes to our whole team in getting us where we are today, but especially our founder, Nick Halstead, who has the vision and passion that keep us all moving.\n Maybe our biggest surprise to me is the breadth of use-cases that we\u2019re seeing companies use our Social Data platform for. Social Media Monitoring and \u201cBreaking News\u201d are obvious applications that companies build.\u00a0 But we\u2019re seeing everything from Business Intelligence, Stock-Trading models, Public-health applications and social-TV guides being built with DataSift.\u00a0 A big part of Nick\u2019s vision has always been to democratize the social data market \u2013 opening it to both small and large companies. We are starting to see this play-out.\n But we\u2019ve still got a lot of work ahead of us. Our combo of Social Data + Big Data + Real-Time has the opportunity to transform the industry. With Lean Start-up as our guide, we\u2019ve tested and seen the demand. Now we need to grow faster to help us move faster in response to this.\n That\u2019s why we\u2019re delighted to announce that our existing investors have invested additional $7.2M investment in DataSift to accelerate this growth. And I think that\u2019s the real-story behind our story \u2013 having awesome investors that share your vision and are eager to double-down with us on this market. We\u2019re lucky to have Roger Ehrenberg (IA Ventures) and Mark Suster (GRP Partners) as part of our team.\u00a0 They\u2019ve been instrumental in helping us get to where we are today.\n So what\u2019s next? We don\u2019t want to spoil the news, but over the next few months you\u2019ll see new services and applications from DataSift as we continue to build out our social-data platform. One of the biggest reasons that people choose DataSift is not just that we can filter and deliver raw social data at massive-scale, but we can structure it into a \u201cready-to-analyze\u201d format with sentiment, Klout Topics Natural Language Topic analysis, etc. for companies to consume. We\u2019re unique in providing this in the market. What we\u2019re working on next will take these capabilities further to give our customers not just a way to monitor social interactions, but to measure and metric them.\n And if you want to help shape the future of social and big-data, we\u2019re hiring!\n \t\t\t\t \t\t                                                                                      http:\/\/techdiem.com\/2012\/05\/02\/datasift-raises-7-2m-inside-round-to-help-make-sense-of-big-data-beyond-social-interactions\/                            DataSift raises $7.2m inside round to help make sense of \u2018big data\u2019 beyond social interactions | TechDiem.com                                                  \n                                              [...] May 2, 2012DataSift, a rising star in the world of \u2018big data\u2019 collection and analysis, has raised $7.2 million from existing backers GRP Partners and IA Ventures to help accelerate growth as worldwide demand [...]\n \n                                              [...] a rising star in the world of \u2018big data\u2019 collection and analysis, has raised $7.2 million from existing backers GRP Partners and IA Ventures to help accelerate growth as worldwide demand [...]\n \n                                              Congratulations. It\u2019s great to see a UK tech company doing so well.\n The ability to curate and build streams of data, and then exposing such a large amount of \u201cready-to-analyze\u201d data in an accessible way is very exciting.\u00a0\n Data is core to building interactive and engaging realtime web applications. So,\u00a0I\u2019m particularly looking forward to seeing data from DataSift being delivered to web and mobile\u00a0front-end\u00a0applications via Pusher (http:\/\/pusher.com) as we start to see the realtime web, and the user experiences it can enable, evolve even further.\n \n                                              [...] Article: DataSift :The Story So Far and Focus for the Future. DataSift :The Story So Far and Focus for the Future: [...]\n \n                                              Raising money, while certainly a sign that a good story has been told and more importantly, believed, should not be confused with doing well.\u00a0Making money is what you should be looking for.","item_date":"Jun 14 2012 03:12:00","display_item_date":"06-13-2012","url":"http:\/\/blog.datasift.com\/2012\/05\/02\/datasift-adds-7-2m-the-story-so-far\/","source":"blog.datasift.com"},{"title":"Congratulations to SportsData LLC, The Latest API as a Product","details":"Here at Mashery, we admire the trendsetting that our customers bring to their markets. This year, creating APIs as products is one of the ways we see customers pushing the envelope in their markets. APIs as products help companies scale across new channels at rapid speeds because they simplify how developers discover, use and compensate companies for their products via APIs. Today we congratulate SportsData LLC on their new API, and on delivering a significant advancement in how the sports industry is evolving how it offers data for apps, games and websites.\n SportsData LLC aims to disrupt the fantasy sports and sports app data market by democratizing sports data APIs with the five principles of APIs as Products (Product, Price, Place, Promotion, People). It\u2019s a simple principle, really:\n \u2022\tTurn data into a product by offering APIs that give developers tiers of content based on their needs.\n \u2022\tOffer data through APIs at a standard price, so developers know exactly what they are getting and paying when they sign up.\n \u2022\tGive developers a destination where they can learn about your API, share ideas and ask questions to get the most from your API by learning and sharing answers.\n \u2022\tGet the word out by engaging with vehicles like sample data sets, which will help developers understand just what they can do with an API, and get the word out about its value out to developers wherever they are.\n \u2022\tMake your API about more than data. Show developers the face of the people behind your API. SportsData LLC\u2019s founders stand for exactly this.\n By adopting the principles of API as a product, SportsData LLC is able to standardize their offering, and offer the best developer experience in the fantasy sports and sport enthusiast app development market today. Now that\u2019s what we call a recipe for disruption!","item_date":"Jun 13 2012 04:45:59","display_item_date":"06-12-2012","url":"http:\/\/blog.mashery.com\/content\/congratulations-sportsdata-llc-latest-api-product","source":"blog.mashery.com"},{"title":"California Lottery Tries its Luck with an API","details":"Californians have long had the ability to track lottery updates on their mobile phones, or get updates via the lottery\u2019s Twitter account. However, with the release of the CA Lottery API, developers may have entered the scene as California\u2019s latest player of the odds.\n\nThe API provides an information stream about lottery games currently running across the state of California. Information such as games played, winners\u2019 names, and amounts won can be pulled from the lottery\u2019s database.  Odds, history, real-time updates; previously available at the lottery\u2019s website; can be pulled from the API. As one of the top ten lottery states, with regards to largest payouts, allowing developers to manipulate lottery data may prove beneficial to California chance takers.\n\nAlthough your odds at picking a perfect NCAA bracket, taking a trip to the ER after a pogo stick accident, finding a four leaf clover, or being struck by lightning remain greater than winning the Mega Millions, perhaps API integration may close the gap. For more information on the California Lottery API, click here.","item_date":"Jun 12 2012 20:12:12","display_item_date":"06-12-2012","url":"http:\/\/blog.programmableweb.com\/2012\/06\/12\/california-lottery-tries-its-luck-with-an-api\/","source":"blog.programmableweb.com"},{"title":"New Geolocation Summer \u201912 Features and Mobile Apps","details":"In Summer \u201912, a new pilot program feature is introduced to support Geolocations. The new composite field type, geolocation, consists of two components: longitude and latitude. It is specifically designed to hold the geo-coordinates of a location for any objects, most commonly address related objects, such as Contact, Account, restaurants, shops, etc. When a geolocation field is added to these objects, and coordinates are set, users can perform radius based search. Some examples: to find the records of restaurants within 5 miles of my current location, to find all the homes within 15 miles of a sports arena, etc.\n Geolocation for Mobile Apps\n Mobile applications will find this new feature especially handy for providing mobile location-based search on Salesforce objects. Just like a regular custom field type, the geolocation field can be added to an object through object setup wizard. Decimal point precision can be set. Users have the choice to display coordinates in decimal point or degree, minute, and second notations. List views for objects with a geolocation field have a WITHIN operator to conduct radius based searching and filtering. You can perform distance calculation between two geolocation points using the DISTANCE(loc1, loc2, distance_unit) function inside formula fields.\n Also, SOQL is enhanced with DISTANCE and GEOLOCATION functions that let you write SOQL queries to perform location-based searches. For example, to find the names and phone numbers of all restaurantswithin 1 mile of 1 Market Street, San Francisco, California, which has a geo coordinate of 37.794915,-122.394733, you can write a SOQL query as:\n The Geolocation pilot is currently limited to Sandbox and Developer Edition orgs with Summer \u201912. To request access to the pilot feature, please contact salesforce.com support and request to participate in \u201cGeolocation Field Type Pilot program.\u201d\n   \t\t\t\t\t\t\t\t\t\t\t\n  \t\t\t\t\t tagged geolocation, mobile, mobile application developmentBookmark the permalink. Trackbacks are closed, but you can post a comment.\nTweet","item_date":"Jun 12 2012 18:31:15","display_item_date":"06-12-2012","url":"http:\/\/blogs.developerforce.com\/engineering\/2012\/06\/new-geolocation-features-and-mobile-apps.html","source":"blogs.developerforce.com"},{"title":"SOA center of gravity shifts to cloud APIs","details":"Just the other day, SOA Software announced it was beefing up its \u201cAtmosphere\u201d product family to support both\u00a0 intra- and inter-enterprise applications. SOA Software is one of many vendors recognizing that application programming interfaces (APIs) are now a must-have for many enterprises seeking to build and extend their sharable services. And we\u2019re not just talking about the likes of Google, Twitter and Amazon employing open APIs to build up their user bases and developer ecosystems \u2014 non-IT enterprises are opening up APIs as well.\u00a0 Is this the new face of service oriented architecture?\n\n\nAs the SOA Software announcement describes it, APIs are an extension of SOA principles to both private and public cloud capabilities:\n\n\nAPIs make application development simpler and faster, as developers can call upon already existing resources, rather than re-inventing the wheel each time a service or application is built. However, the path between SOA and the open API world isn\u2019t a seamless one, as a report in ComputerWorld explains. Often, the two concepts are at odds with one another. For example, while SOA practices and approaches are fairly mature, cloud APIs lack common standards. In addition, unlike most enterprise-focused SOA projects, cloud APIs tend to be externally focused:\n\n\nIn the article, TIBCO\u2019s Ivan Casanova agrees that APIs and SOA differ in overall purpose, but also have similarities as well:","item_date":"Jun 12 2012 17:34:55","display_item_date":"06-12-2012","url":"http:\/\/www.zdnet.com\/blog\/service-oriented\/soa-center-of-gravity-shifts-to-cloud-apis\/9109","source":"www.zdnet.com"},{"title":"Amazon Web Services Blog: Amazon S3 - The First Trillion Objects","details":"Late last week the number of objects stored in Amazon S3 reached one trillion (1,000,000,000,000 or 1012). Thats 142 objects for every person on Planet Earth or 3.3 objects for every star in our Galaxy. If you could count one object per second it would take you 31,710 years to count them all.\n  We knew this day was coming! Lately, weve seen the object count grow by up to 3.5 billion objects in a single day (thats over 40,000 new objects per second).\n  Our customers have taken advantage of S3s relatively new object expiration feature and have used it to delete over 125 billion objects since we released it at the end of last year. In other words, even though weve made it easier to delete objects, the overall object count has continued to grow at a very rapid clip.\n  Animoto has been using S3 for quite some time (I first blogged about them in 2008). Animoto CEO Sunil Kumar told us that \u201cWe really love the feature for deleting multiple objects, but the icing on the cake was the object expiration feature.\u00a0 It\u2019s amazing how S3 keeps thinking up and launching features that make our lives even easier.\u201d\n  On behalf of the Amazon S3 team, Id like to thank you for all of the amazing ways that youve found to put S3 to use. We really enjoy hearing and reading about your applications.","item_date":"Jun 12 2012 17:21:33","display_item_date":"06-12-2012","url":"http:\/\/aws.typepad.com\/aws\/2012\/06\/amazon-s3-the-first-trillion-objects.html","source":"aws.typepad.com"},{"title":"88 Local APIs: Yahoo Local, Yelp and Zvents","details":"Our API directory now includes 88 local APIs. The newest is the WebReserv API. The most popular, in terms of mashups, is the Yahoo Local Search API. We list 55 Yahoo Local Search mashups. Below you\u2019ll find some more stats from the directory, including the entire list of local APIs.\n\n\n\n\n\nIn terms of the technical details, REST and JSON lead the way. There are 64 local REST APIs and 7 local SOAP APIs. Our directory lists 54 local JSON APIs and 49 local XML APIs.\n\n\n\n\n\nThe most common tags within local are 26 shopping local APIs, 22 places local APIs and 19 mapping local APIs.\n\n\n\n\n\nOn the mashup side, we list 111 local mashups. We named MapSavings.com as mashup of the day two weeks ago.\n\n\nFor reference, here is a list of all 88 local APIs.\n\n\n\u00a0\u00a08coupons API: Local deals aggregator\n\n\n\u00a0\u00a0Adility API: Local coupon service\n\n\n\u00a0\u00a0AlterGeo API: Local recommendation service\n\n\n\u00a0\u00a0BiteHunter API: Dining Deals Search Service\n\n\n\u00a0\u00a0BlockChalk API: Local community social space\n\n\n\u00a0\u00a0Boomloop API: Local events and community\n\n\n\u00a0\u00a0Bullseye Store Locator API: Store location search service\n\n\n\u00a0\u00a0Bview Content API: Local business info service\n\n\n\u00a0\u00a0Bview Local Search API: Local retail search service\n\n\n\u00a0\u00a0Cing API: Deal and coupon service\n\n\n\u00a0\u00a0CityGrid  API: Local business directory search service\n\n\n\u00a0\u00a0CityGrid Advertising API: Local advertising service\n\n\n\u00a0\u00a0CityGroups API: Local public directory\n\n\n\u00a0\u00a0DealsGoRound API: Online marketplace for daily deals\n\n\n\u00a0\u00a0DealSurf API: Daily deal aggregator service\n\n\n\u00a0\u00a0deCarta MapSearch Engine API: Local search services\n\n\n\u00a0\u00a0Dokiru API: Social location based service\n\n\n\u00a0\u00a0EveryBlock API: Hyperlocal data and news aggregator\n\n\n\u00a0\u00a0Florist One API: Flower sales and delivery\n\n\n\u00a0\u00a0Food Genius API: Restaurant and food data\n\n\n\u00a0\u00a0foursquare Merchant API: foursquare merchant platform\n\n\n\u00a0\u00a0Fwix Location API: Business listing service\n\n\n\u00a0\u00a0Fwix Wire API: Local news gathering service\n\n\n\u00a0\u00a0GeoReach Neighborhood API: Location based adversting service\n\n\n\u00a0\u00a0GigPark API: Local recommendations service\n\n\n\u00a0\u00a0Google Places API: Local business and point of interest service\n\n\n\u00a0\u00a0Groups Near You API: Local groups search service\n\n\n\u00a0\u00a0HAMweather Aeris API: Weather data service\n\n\n\u00a0\u00a0Happenr API: Events database\n\n\n\u00a0\u00a0Happenstand API: Bay area events listing service\n\n\n\u00a0\u00a0HomeCook API: Local food businesses community\n\n\n\u00a0\u00a0HotUKDeals API: UK local deals service\n\n\n\u00a0\u00a0Hyperpublic API: Geographic data collection service\n\n\n\u00a0\u00a0Jotly API: Rating application\n\n\n\u00a0\u00a0KidsEatFor  API: Restaurant deal search service\n\n\n\u00a0\u00a0Lemonade Stand API: Community Sales Service\n\n\n\u00a0\u00a0Localmind API: Location based Q&A\n\n\n\u00a0\u00a0MashSpots API: Local search and mapping\n\n\n\u00a0\u00a0Mates API: Location-based networking service\n\n\n\u00a0\u00a0MerchantCircle.com API: Online marketing service for local businesses\n\n\n\u00a0\u00a0Meteostone Weather API: Weather data service\n\n\n\u00a0\u00a0Metro Traffic API: Real-time traffic data service\n\n\n\u00a0\u00a0Milo API: Local shopping service\n\n\n\u00a0\u00a0Mycitymate Location API: Local user reviews and city guides\n\n\n\u00a0\u00a0MyDealBag API: Local deals aggregation service\n\n\n\u00a0\u00a0NAVTEQ Traffic API: Local traffic information service\n\n\n\u00a0\u00a0Nokia Ovi Places API: Online mapping service\n\n\n\u00a0\u00a0Onboard Points of Interest API: Real estate listings and search service\n\n\n\u00a0\u00a0OneSearchAway API: Social search service\n\n\n\u00a0\u00a0OpenGovernment API: State and local government data\n\n\n\u00a0\u00a0Outside.in API: Hyperlocal news and information aggregator\n\n\n\u00a0\u00a0Patch API: Community-specific news and information service\n\n\n\u00a0\u00a0PeekaCity API: Location-based services\n\n\n\u00a0\u00a0Peixe Urbano API: Brazilian daily deals service\n\n\n\u00a0\u00a0PlaceIQ API: Location data and mapping service\n\n\n\u00a0\u00a0Postcode Anywhere Store Finder API: UK business listings service\n\n\n\u00a0\u00a0Praized API: Local user reviews and search service\n\n\n\u00a0\u00a0Qype API: Local reviews service\n\n\n\u00a0\u00a0Retailigence API: Location-based retail inventory\n\n\n\u00a0\u00a0Rvolve Hyperlocal Ads API: Hyperlocal advertising platform\n\n\n\u00a0\u00a0Safe2pee API: Gender neutral bathroom search\n\n\n\u00a0\u00a0Sensis Business Search API: Australian business directory search service\n\n\n\u00a0\u00a0ShreveSearch Search API: Shreveport-Bossier LA area search services\n\n\n\u00a0\u00a0SinglePlatform API: Restaurant Menu and Local Business Storefront API\n\n\n\u00a0\u00a0SmartPea Grocery  API: Grocery store deal and shopping list service\n\n\n\u00a0\u00a0SoGeo API: Location aware business platform\n\n\n\u00a0\u00a0Sqoot API: Local deals aggregation service\n\n\n\u00a0\u00a0TelVue PEG.TV API: Cloud-based streaming video service\n\n\n\u00a0\u00a0Tippr API: Daily deals aggregation service\n\n\n\u00a0\u00a0TrustedPlaces API: Local user reviews and city guides\n\n\n\u00a0\u00a0Urban Mapping Neighborhoods API: Urban geo-spatial data services\n\n\n\u00a0\u00a0VodoModo API: Geolocation video provider\n\n\n\u00a0\u00a0WebReserv API: Online business search and booking service\n\n\n\u00a0\u00a0welovelocal API: Local business search and recommendation service\n\n\n\u00a0\u00a0Wishpond API: Local retail search service\n\n\n\u00a0\u00a0World Weather Online API: Global weather information services\n\n\n\u00a0\u00a0World Weather Online City Search API: City lookup service\n\n\n\u00a0\u00a0World Weather Online Time Zone API: Time zone data service\n\n\n\u00a0\u00a0xAd API: Mobile advertising service\n\n\n\u00a0\u00a0Yahoo Local Search API: Local search service\n\n\n\u00a0\u00a0Yellow API: Canadian business listings service\n\n\n\u00a0\u00a0YellowBot Location API: Local business search service\n\n\n\u00a0\u00a0YellowBot Reputation Management API: Local business search service\n\n\n\u00a0\u00a0Yelp API: Local user reviews and city guides\n\n\n\u00a0\u00a0YouPage API: Business and local events directory\n\n\n\u00a0\u00a0YourStreet API: Geocoding news article service\n\n\n\u00a0\u00a0Zomato API: India Restaurant and lifestyle guide\n\n\n\u00a0\u00a0Zvents API: Local events search and community","item_date":"Jun 12 2012 17:12:34","display_item_date":"06-12-2012","url":"http:\/\/blog.programmableweb.com\/2012\/06\/12\/88-local-apis-yahoo-local-yelp-and-zvents\/","source":"blog.programmableweb.com"},{"title":"Data in use from public health to personal fitness","details":"Back in 2010, the first health data initiative forum by the Dept. of Health and Human Services introduced the public to the idea of an agency releasing internal data in forms easy for both casual viewers and programmers to use. The third such forum, which took place last week in Washington, DC, was so enormous (1,400 participants) that it had to be held in a major convention center. Todd Park, who as CTO made HHS a leader in the open data movement, has moved up to take a corresponding role for the entire federal government. Open data is a world movement, and the developer challenges that the HDI forum likes to highlight are standard strategies for linking governments with app programmers.  \nTodd Park on main stage.\n    Following my attendance at a privacy access summit the previous day, the HDI forum made me think of a government bent on reform and an open-minded public crossing hands over the heads of the hidebound health institutions that blunder onward without the benefits of tapping their own data. I am not tossing all hospitals, doctors, and clinics into this category (in fact, I am constantly talking to institutions who work with available data to improve care), but recording and storage of information in health care generally retards anyone interested in change.  \n  The datapalooza was already  covered on Radar by Alex Howard, so here Ill list some of the observations I made during the parts I attended.  \nHealth and Human Services chooses torrents over leaks\n    Able to attend the forum only on the first day, I spent a lot of it in a session on HHS data sets at Healthdata.gov because I wanted to know exactly what the department has to offer and how the data is being used.  \nHHS staff at break-out session.\n    Several things impressed me about the procession of HHS staff that crossed the stage to give five- or ten-minute presentations on data sets. First was the ethos of data sharing that the department heads have instilled. Each staff person showed visible pride in finding data that could be put on the Web. A bit of competitive spirit drives different departments that may have more or fewer resources, and data that comes naturally in a more structured or less structured form. One person, for instance, said, Were a small division and dont have the resources of the others, but we managed to release several data sets this year and one has an API.  \n  Second, the department is devoting resources to quality. Ive heard several complaints in the field about lack of consistency and other problems in public health data. One could hardly avoid such issues when data is being collected from hundreds of agencies scattered across the country. But the people I talked to at the HHS forum had ways of dealing with it, such as by requiring the researchers who collect data to submit it (so that trained professionals do the data entry), and running it through quality checks to look for anomalies.  \n  Third, the department knows that outside developers coming to their site will need extra help understanding the data being collected: what the samples represent, what the scope of collection was, and so forth.  In addition to a catalog powered by a Solr search engine, HHS provides direct guidance to the perplexed for those developing apps. They are also adding Linked Data elements to help developers combine data sets.  \n  A few examples of data sets include:  \n  The Center for Medicare & Medicaid Services offers aggregate data on emergency visits, hospital readmission rates (a major source of waste in health costs), and performance measurement.  \n   The Administration for Children and Families has a Head Start locator that helps parents find services, aggregate data on people who apply for Low Income Home Energy Assistance, etc.  \n   The Agency for Healthcare Research and Quality has longitudinal data abut spending on health care and its effect on outcomes, based on an annual survey, plus a service offering statistics on hospital treatments, morbidity, etc.  \n   The Assistant Secretary for Planning and Evaluation tracks workforce development, particularly in health IT, and measures the affordability of health care reflected in costs to employers, patients, and the government.  \n  \n  Recently, HHS has intensified its efforts by creating a simple Web interface where its staff can enter data about new data sets. Data can be uploaded automatically from spreadsheets. And a new Data Access and Use Committee identifies data sets to release.  \n  So now we have public health aids like the Community Indicators Data Portal, which maps the use of Medicaid services to poverty indicators, infant mortality, etc.  \n  HealthMap, created by Childrens Hospital Boston, is used by a fascinating range of projects. They scoop in huge amounts of data--mostly from news sites, but also blogs, and social networks--in multiple languages around the world, and apply a Bayesian filter to determine whats a possible report of a recent disease outbreak. After a successful flu-tracking program based on accepting reports from the public, they did a dengue-tracking program and, in Haiti, a cholera-tracking program.  \n  But valuable as HHS data is to public health, most of it is not very sexy to the ordinary patient or consumer. If youre curious how your Medicare charges compare with average payments for your county, go ahead and mine the data. But what about something immediately practical, such as finding the best hospital for a procedure?  \n  Recently, it turns out, HHS has been collecting and releasing data on that level, such as comparative information on the quality of care at hospitals. So a datapalooza like the HDI forum really takes on everyday significance. HHS also provides the Healthcare.gov site, with services such finding insurance plans for individuals and small groups.  \n  Other jurisdictions are joining the health data movement. Many countries have more centralized systems and therefore can release large amounts of data about public health. The United Kingdoms National Health Service was featured at the HDI forum, where they boasted of posting 3,000 health indicators to their web site.  \n  The state of Louisiana showed off a cornucopia of data, ranging from user restaurant ratings to ratings of oyster beds. Pregnancy risk factors, morbidity rates, etc. are broken down by race, sex, and other demographics. The representative freely admitted that the state has big health problems, and urgently called on developers to help it mine its data. The state recently held a Cajun codefest to kick off its effort. HHS also announced five upcoming local datapaloozas in other states around the U.S.  \n  \n  I talked to Sunnie Southern, a cofounder of a Cincinnati incubator called Innov8 for Health. They offer not only challenges for new apps, but guidance to help developers turn the apps into sustainable businesses. The organization also signs up local hospitals and other institutional users to guarantee a market to app developers. Southern describes Innov8 for Health as a community-wide initiative to support local developers and attract new ones, while maintaining deep roots among multiple stakeholders across the health care, university, startup, investors, and employer stake holders. At the inaugural class, which just took place, eight companies were chosen to receive intensive mentoring, introductions and connections to potential customers and investors, and $20,000 to start their company in 12 weeks. Health data is a core element.  \nHow far can a datapalooza take the health care field?\n    Health apps are a fast-growing segment of mobile development, and the government can certainly take some of the credit, along with VC and developer recognition that theres a lot of potential money to be made fixing health care. As Todd Park said, The health innovation ecosystem is beautifully chaotic, self-propelled, and basically out of control. That means the toothpaste cant be put back in the tube, which is a good thing.  \n  The HDI forum is glitzy and exciting--everybody in health care reform shows up, and the stage show is slickly coordinated--but we must remember the limits of apps in bringing about systemic change. Its great that you can use myDrugCo$ts.com to find a discount drug store near you. Even better, if your employer hooks you up to data sets provided by your insurer, myDrugCo$ts.com can warn you about restrictions that affect costs. But none of this will change the crazy pricing in the insurance plans themselves, or the overuse of drugs in medicine, or the inefficient development and testing methods that lead to high medication prices in the first place.  \nCaucus of Society for Participatory Medicine and friends.\n    Transparency by one department on one level can lead to expectations of transparency in other places too. As pricing in health care becomes more visible, it will become less defensible. But this requires a public movement. We could do great things if we could unlock the data collected by each hospital and insurance agency, but they see that data as their competitive arsenal and we are left with a tragedy of the anti-commons. It would be nice to say, You use plenty of public data to aid your decision-making, now reciprocate with some of your own. This can be a campaign for reformers such as the Society for Participatory Medicine.  \n  At the HDI forum, United Healthcare reported that they had enough data to profile patients at risk for diabetes and brought them in for a diabetes prevention program. This is only a sample of what can be done with data that is not yet public.  \nAetna presenter shows CarePass on the main conference stage.\n    Aetna is leading the way with a service called CarePass, currently holding a developer challenge. CarePass offers Aetnas data through an API, and they partner with other major data centers (somewhat as Microsoft does with HealthVault) to hook up data. Practice Fusion is also offering some data to researchers.  \n Even those bright-faced entrepreneurs launching businesses around data from HHS and elsewhere--certainly their success is one of the goals of the open data movement, but I worry that they will recreate the silos of the health care field in the area of patient data. What are they collecting on us as we obsessively enter our personal statistics into those devices? Who will be able to use the aggregate data building up on their servers?  \n  So there are hints of a qualitative change that can come from quantitative growth in the release and reuse of health care data. The next step involves the use of personal data, which raises its own litany of issues in quality and privacy. That will be the subject of the last posting in this series.","item_date":"Jun 12 2012 17:08:25","display_item_date":"06-12-2012","url":"http:\/\/radar.oreilly.com\/2012\/06\/data-in-use-from-public-health.html","source":"radar.oreilly.com"},{"title":"Facebook Integration for WordPress","details":"The plugin was built by Facebook engineers in collaboration with open source partners, and makes it simple for anyone to make their WordPress site more social \u2013 no coding required. The plugin will also work on mobile and support internationalization.\n  Social Publishing\n  Once the plugin is installed, you can cross-post content published to WordPress to your Facebook Timeline and the Facebook Pages you manage. You can also mention the names of Pages and friends as you post to further distribute your content.\n  \n  WordPress Widgets\n  The following social plugins are available as WordPress widgets:\n  Activity Feed: Shows readers their friends\u2019 activity on the site, such as likes and comments.\n Recommendations: Gives readers personalized suggestions for pages on your site they might like, as well as a Recommendations Bar option to give users the option to add content to their Timeline as they read.\n Customizable Like, Subscribe and Send buttons.\n Comments Box: Makes it easy for people to comment on your site and post back to Facebook, and includes moderation tools. The plugin also features automatic SEO support for Facebook Comments, so search engines can index them to improve your site\u2019s visibility.\n \nTechCrunch, Buzz Media and The Next Web are already using the plugin to connect with their audiences while providing users with more engaging and personalized experiences. The plugin is available for all sites on WordPress.com VIP as well.\n  WordPress powers 16.6 percent of the web, from The New York Times to People Magazine, and attracts more than 600 million unique visitors each month. We hope the plugin makes it possible for WordPress content to be shared even more widely among people.","item_date":"Jun 12 2012 16:57:19","display_item_date":"06-12-2012","url":"http:\/\/developers.facebook.com\/blog\/post\/2012\/06\/12\/facebook-integration-for-wordpress\/","source":"developers.facebook.com"},{"title":"Amazon RDS MySQL Now Starting at Just $19 a Month","details":"Im always happy to be able to talk about AWS options that lower costs and add flexibility for our customers.\n  To that end, I am pleased to announce the availability of Micro instances for Amazon RDS for the MySQL database engine. The t1.micro RDS instance is a low cost instance type designed for low traffic web applications, test applications and small projects.\n  As you may already know, Amazon RDS makes it easy to set up, operate, and scale a relational database in the cloud. Our customers have asked us for a lower priced instance type that could satisfy the needs of their less demanding applications. The micro RDS instance allows you to run a fully-featured relational database, starting at just $19 a month (2.5 cents an hour).  Micros RDS instances also support Multi-AZ deployments and Read Replicas.\n  Micro DB instances provide a small amount of consistent CPU resources, and also allow you to burst CPU capacity when additional cycles are available. The Micro DB instance type is available now in all AWS Regions. See the RDS Pricing page for more information on the On Demand and Reserved Instance pricing.","item_date":"Jun 12 2012 01:48:39","display_item_date":"06-11-2012","url":"http:\/\/aws.typepad.com\/aws\/2012\/06\/amazon-rds-mysql-now-starting-at-19-month.html","source":"aws.typepad.com"},{"title":"Amazon Web Services Blog: NASA Saves nearly $1M Per Year By Using AWS","details":"Linda Cureton, the CIO of NASA wrote a blog post to detail some of the IT reforms that are being put in to place to better serve NASAs mission and the American people. NASA has been an advocate and user of cloud and shared services for the last couple of years.\n   In her post (IT Reform at the National Aeronautics and Space Administration), Linda noted that NASA has moved many applications to AWS, with calculable cost savings:\n  NASA shifted to a new web services model that uses Amazon Web Services  for cloud-based enterprise infrastructure. This cloud-based model  supports a wide variety of web applications and sites using an  interoperable, standards-based, and secure environment while providing  almost a million dollars in cost savings each year.\n  You may enjoy reading the following case studies to learn more about how NASA puts AWS to use:\n  AWS Case Study - NASA Jet Propulsion Lab\u2019s Desert Research and Training Studies.\n  AWS Case Study - NASA JPL and Amazon SWF.","item_date":"Jun 12 2012 01:48:12","display_item_date":"06-11-2012","url":"http:\/\/aws.typepad.com\/aws\/2012\/06\/nasa-saves-nearly-1m-per-year-by-using-aws.html","source":"aws.typepad.com"},{"title":"New Playground for the DFP API - Google Ads Developer Blog","details":"Today, we\u2019d like to present the new DFP API Playground for test networks, built on Google App Engine using the new ads Java client library.  \n\nTest networks can be created using the playground application \n\n OAuth2.0 authentication using the Java client library with automatic token refresh\n\n Uses a push queue to make API calls in background tasks and stream results back using the Channel API\n\n Please note that the playground only makes test networks accessible, so you will not be able to view production or old sandbox networks through the application. We\u2019ve made the project open-sourced so you can get more details on integrating DFP API with App Engine through the project repository. \n\nTest Network Creation\n\n The new playground makes it simple for API developers to create test networks and get started with DFP API. All you have to do is log in with a Google account that\u2019s not already associated with a test network and you will be prompted to create a new one.\n\n 1. Visit http:\/\/dfp-playground.appspot.com\/. You\u2019ll be asked to sign in with your account if you haven\u2019t already done so.\n\n 2. Authorize the application to make requests to DFP API on your behalf (you\u2019ll only have to do this once). \n\n \n\n 3. Click the \u201cCreate Test Network\u201d button.\n\n \n\n 4. And you\u2019re done! You can now configure your test network (language, currency, etc) by following the link to the DFP user interface.  \n\n \n\n We are always looking for ways to help API developers be more productive, so if there are any features you would like to see in the playground application or in the API, please leave us a comment at our forum.  We\u2019d also love to chat with you at one of our Office hours hangouts.\n\n\u00a0-\u00a0Jeffrey Sham, DFP API Team   Email This  BlogThis!  Share to Twitter  Share to Facebook","item_date":"Jun 12 2012 01:47:59","display_item_date":"06-11-2012","url":"http:\/\/googleadsdeveloper.blogspot.com\/2012\/06\/new-playground-for-dfp-api.html","source":"googleadsdeveloper.blogspot.com"},{"title":"Inside the Baltimore Hackathon","details":"The second annual Baltimore Hackathon kicked off on Friday, drawing 75 engineers, programmers, designers, and DIYers to Advertising.com\u2019s Tide Point offices. After 48 hours of coding, building, and creating, a panel of judges awarded $500 prizes in six categories: technical complexity, smart design, civic service, aesthetics, crowd favorite, and hacker\/DIY. But for both the organizers and the participants, the event was about much more than money.\n Matthew Forr, an organizer and volunteer, sees the hackathon as a way to strengthen the local tech community while recognizing and rewarding local talent. \u201cIt\u2019s a great opportunity to make connections and form partnerships,\u201d Forr said. Participant Alan Grover, who was working on a plug-in for Firefox to enable secure peer-to-peer web applications, agreed. \u201cOne of the great things about it is the potential for future collaboration,\u201d he said. Grover also appreciated the social element of the hackathon. \u201cIt would be great to provide even more support to get us talking to each other,\u201d he said. \u201cGeeks can be bad at that.\u201d\n   \n  Nick Gauthier and Adam Bachman, whose gaming platform\u2014Cartridge\u2014was one of the winning projects, said the group environment provides energy and inspiration. \u201cIt got us off our butts,\u201d said Bachman, who helped to organize the event. \u201cIt takes a lot of energy to start something,\u201d added Gauthier. \u201cThe whole thing is kind of like a life hack as well as a tech hack. It gives you a dedicated block of time to focus on a project, and you\u2019re surrounded by people doing the same thing.\u201d\n \u201cIt\u2019s also great that it\u2019s cross-disciplinary,\u201d Bachman said. \u201cIn tech, your social circles tend to get segregated \u2013 PHP, Java, hardware \u2013 they don\u2019t usually overlap. But here, the prize categories aren\u2019t tied to any particular technology, so you get people from all disciplines.\u201d He noted that one underrepresented group\u2014not only at the hackathon, but in the tech industry in general\u2014was women. \u201cWe need to do a better job of reaching out to female programmers and getting them to these events.\u201d Gauthier added, \u201cI\u2019d love to see a hackathon for women by women. Software for women is an awesome opportunity \u2013 they\u2019re huge consumers of tech, but they tend to be underserved.\u201d (The Atlantic just published an article expressing the same sentiment.)\n Eliot Pearson of Advertising.com, one of the event\u2019s sponsors, also expressed a desire to see more women and minorities participating, but stressed that the Baltimore tech community is inclusive and welcoming. \u201cAdvertising.com and AOL [Advertising.com\u2019s parent company] are proud to be a part of this event,\u201d Pearson said. \u201cWe believe in diversity, we believe in technology, and we believe in Baltimore. We\u2019d love to host the hackathon again.\u201d\n Given the success of this year\u2019s event, which sold out, it seems likely that the Baltimore Hackathon will return in 2013. Jason Denney, one of the driving forces in organizing this year\u2019s hackathon, said, \u201cThe interest is definitely there in the community. The biggest challenge is getting the sponsors and putting it all together. Once we got Looking Glass [Denney\u2019s employer and an event sponsor] and Northrop Grumman [lead sponsor] onboard, it all came together.\u201d\n Visit the Baltimore Hackathon website to see the full list of participants, sponsors, and winners.","item_date":"Jun 12 2012 01:15:43","display_item_date":"06-11-2012","url":"http:\/\/www.examiner.com\/article\/inside-the-baltimore-hackathon","source":"www.examiner.com"},{"title":"AT&T Mobile App Hackathon - Education (Palo Alto)","details":"So this last weekend we had a very special hackathon that was focused on education and produced in partnership with AT&Ts very own Aspire program, which is focused on reducing high school drop out rates. The event was kicked off by Beth Shiroshi,\u00a0VP of AT&T Aspire,\u00a0Lucien Vattel, founder and executive director of GameDesk, and Banny Banerjee, Associate Professor at Stanford. In addition to the big names at the event, AT&T anncounced earlier this year a relaunch of the Aspire program with a quarter billion commitment specifically to education. Through some excellent internal connections, we were introduced to the Aspire group and the rest is history. Moving forward, AT&T Mobile App Hackathon will deliver at least four hackathons focused on education for AT&T Aspire, so be looking for more exciting events!\u00a0\n With the awesome turn out of over 200 developers, teachers and students this last weekend in Palo Alto, we are very excited for future events. By my watch, I believe the 30 pizzas were inhaled in under 30 minutes as well as the entire fridge of drinks which totalled over 250 bottles. I have to say that this is a great problem to have and well have to budget more for food and possibly get a better head count. We have been toying with the idea of a five dollar speed bump just to make sure that developers are serious about coming and so that we can get a better count for food. The funds would go into a new prize called audience choice where the bucket-o-cash would go to the team that the audience chooses as their favorite team\/idea\/pitch. Let me know your thoughts on this: @alex_donn.\n I really wish I could have stayed for the final presentations, but I had to catch an early flight out to Israel to run the next hackathon and I am currently at the Sheraton Tel-Aviv writing up this event review. The crazy part is that it is 05:45AM here and I am still awake after being up for the last 36 hours with only four or five hours of sleep. Hm. I wonder how much longer I can keep this up? Anyone into body hacking?\n As for the education hackathon there were 32 teams that presented an excellent array of applications with the first place team being a stand-out application as described by other attendees I spoke with after the event.\u00a0The overall winning team was a combination of teachers and developers, which is what we were really looking to achieve.\u00a0\n The prize selection weighed in significantly as well with $60k in AT&T Aspire donations to non-profits and $10k in cash prizes for the top teams. Apigee, Tiggzi, Amazon, AT&T Cloud, AT&T API and Pearson also kicked in a significant amount of prizes.\n Without further delay, here is the list of winners:\n \n Read With Me - Read with me is a web app that replaces cumbersome and time consuming methods for assessing reading fluency in classrooms through the use of a digital, paired-device solution that allows for real-time, meaningful analysis of reading fluency scores. \u00a0The scores can be saved, recorded, graphed, and shared with other educators and parents.\n Hiword - An application that uses an innovative method to assist users in remembering foreign language.\n Dropout Doctor - An app that is focused on showing kids that are considering dropping out the result of their choices both statistically and visually.","item_date":"Jun 11 2012 21:12:17","display_item_date":"06-11-2012","url":"http:\/\/developerboards.att.lithium.com\/t5\/AT-T-Developer-Program-Blogs\/AT-amp-T-Mobile-App-Hackathon-Education-Palo-Alto\/ba-p\/32132?nobounce","source":"developerboards.att.lithium.com"},{"title":"Pixorial Adds to its Story With its API","details":"Pixorial, a video storytelling platform, has been slowly evolving over the last 5 years into a feature-rich and deeply integrated video sharing and editing tool. Pixorial offers smart phone applications, a nifty uploading utility, and seamless use within Google Drive. Now, with the release of the Pixorial API, integration can be taken into the developers hands. \n\nPixorial continues to update its feature set and recently announced upcoming feature additions:\n\n16.9 widescreen-quality viewing on iOS and Android mobile devices\n\n Support for a wider range of video formats (even the insanely obscure ones, like LAVFI and wc3movie)\n\n Improved playability and viewing quality on Apple TV and Google TV\n\n Faster processing of uploaded and edited videos\n\n Non\u2013destructive editing, which leaves you with an unaltered original copy your edited videos.\n\n Greater video quality while you\u2019re using Producer, our advanced editor.\n\n Higher-quality playback of videos that you\u2019ve shared with others.\n\n \n\n\n\n\n\nThe RESTful Pixorial API would already be a great option for developers that need to embed video and who knows what the company will come up with next.\n\n\n\n\n\nThe Pixorial API is one of 224 video APIs that are listed in the ProgrammableWeb directory.","item_date":"Jun 11 2012 20:24:34","display_item_date":"06-11-2012","url":"http:\/\/blog.programmableweb.com\/2012\/06\/11\/pixorial-adds-to-its-story-with-its-api\/","source":"blog.programmableweb.com"},{"title":"EBSCO Releases API for Discovery Service","details":"EBSCO Publishing, which provides online research databases for libraries, has released a revamped tool to make it easier for their customers to provide search results in the interface of their choice but with EBSCO touches.\n    The company is now selling an upgraded EBSCO Discovery Tool Application Programming Interface (API), which allows a technical staff to reproduce the Discovery Service result list and detailed record exactly as seen in the EBSCO product or to customize the display to fit a different interface.\n    EBSCOs Discovery Service lets libraries integrate results from multiple sources and adds features such as relevancy ranking, full-text linking, persistent links, custom links, book jacket images, and other components. The addition of the API enables the display of EBSCO premium content results to appear as part of the librarys own results. The API requires the use of EBSCOhost, the platform by which the company makes its content available to customers.\n    EDS API gives libraries that are heavily invested in their [online public access catalog] or VuFind implementations, or that have developed their own solutions an easy integration option that greatly enhances every search with superior content and features, said Ron Burns, senior director of EBSCOhost product management. The API includes documentation, an XSD XML schema definition, sample XSLTs, a VuFind demo, and other sample applications and code. According to a company spokeswoman, pricing is based on a number of factors, particularly full-time enrollment of the institution that has adopted EBSCO Discovery Service.\n                                                                                    Dian Schaffhauser is a writer who covers technology and business for a number of publications. Contact her at dian@dischaffhauser.com.","item_date":"Jun 11 2012 20:21:49","display_item_date":"06-11-2012","url":"http:\/\/campustechnology.com\/articles\/2012\/06\/11\/ebsco-releases-api-for-discovery-service.aspx","source":"campustechnology.com"},{"title":"Concurrent launches open source API to ease Hadoop development","details":"One developer is forwarding his alternative app framework to MapReduce to make big data management in the Hadoop era easier.\n\nConcurrent CEO and Founder Chris Wensel is the creator of an open source data workflow API, Cascading, which is used by Twitter, Amazon and Razorfish.\n\nLast week, the company was officially launched as was Cascading 2.0, which is now available under an Apache 2.0 license.\n\nWensel sees growing adoption of the API as big data management explodes.  He created Cascading to help him develop complex Hadoop applications easier and first released the code in 2007.\n\n\u201cI was writing Hadoop applications and it was an extremely painful process. I started writing a framework to give me a different model . MapReduce is a simple way to parallelize data computations but it\u2019s hard to solve real problems,\u201d Wenzel said in a recent interview.\n\nTrue Ventures and Rembrandt announced $900,000 in seed funding last year.\n\n\u201cWe have seen a lot of innovation and investment in the data center at the storage, database, and around new technologies like Hadoop to accommodate the exploding data growth.  We think a new equally important category will also emerge around data processing and management in order to give context to these growing volumes of data,\u201d wrote True Ventures\u2019 general partner Puneet Agarwal in a bog last year.","item_date":"Jun 11 2012 20:03:39","display_item_date":"06-11-2012","url":"http:\/\/www.zdnet.com\/blog\/open-source\/concurrent-launches-open-source-api-to-ease-hadoop-development\/11185","source":"www.zdnet.com"},{"title":"Apple's best asset: Developers and its app economy","details":"Apple touted new MacBook Airs, MacBook Pros and a look at iOS6, but its primary asset\u2014app developers\u2014was highlighted early at its WorldWide Developer Conference.\n\n\nTim Cook, Apple CEO, highlighted some jaw dropping figures:\n\n\n\n\n\nIn other words, Apple\u2019s design and ecosystem garner all the headlines, but the biggest moat around its business model is its developer community. If developers stick with iOS\u2014and there\u2019s no reason they wouldn\u2019t follow the money adn those links to real credit card accounts\u2014Apple will continue to get the exclusive apps and deliver content to its users.\n\n\nBottom line: Developers are Apple\u2019s edge over Android.","item_date":"Jun 11 2012 19:43:08","display_item_date":"06-11-2012","url":"http:\/\/www.zdnet.com\/blog\/btl\/apples-best-asset-developers-and-its-app-economy\/79702","source":"www.zdnet.com"},{"title":"Asana\u2019s First API is Out in The wild","details":"Asana, Silicon Valley\u2019s celebrity task management and productivity service, recently launched it\u2019s long awaited,  Asana API.\u00a0This  initial API  looks like a small step in the right direction.  It\u2019s designed in a RESTful structure, and provides simple access to Asana\u2019s basic functionality.  There\u2019s nothing out of the ordinary for the moment but, Asana assured us that this is just the start of their platform initiative. \n\nSome expected to see interesting work in the real-time space, with all their hype around the site\u2019s real-time functionality and their  Luna framework .  For now, they\u2019re just testing the waters \u2013 but that\u2019s what you\u2019d expect from a company with Eric Reis as an advisor.\n\nThe current API is missing many items considered obvious for the regular crowd of API developers, such as Oauth  The Asana team acknowledged those limitations, and has a list of new clear enhancements for the future.  However, aside from standard pieces like Oauth, the team remains open to use cases, and after gathering feedback, intends to pursue developer features that are more out of the ordinary.\n\nAlthough the Asana team has no specific plans for the future, the team clearly wants to go beyond the obvious API enhancements, and make the platform another point of innovation for the company.  Asana\u2019s site, openly advertising its most frequent customer requests, alludes to the addition of webhooks in the near-future.  I was also told of an improved means of querying in their API as a highly likely area of enhancement.  The Asana team recognizes that a basic API, while vital, will just be one piece if tasks are to truly become the central point of activity.\n\n\n\n\n\nTo our liking, Asana has taken a pretty liberal stance on its rules of use.  API access is easy to obtain for anyone interested in development, and app development is permitted for pubclic distribution, or personal use, or company use.  A few sample applications are available, but I\u2019m sure they can use more\u2026","item_date":"Jun 11 2012 19:43:00","display_item_date":"06-11-2012","url":"http:\/\/blog.programmableweb.com\/2012\/06\/11\/asanas-first-api-is-out-in-the-wild\/","source":"blog.programmableweb.com"},{"title":"Hackathons Aren't Just for Hacking","details":"On the eve of Facebook\u2019s public offering, while the world obsessed over the company\u2019s market value and direction, employees gathered in their new Menlo Park headquarters and got to work, hacking away and inventing new products for the site. This hackathon, a sort-group brainstorm and software-coding party, was not much different than the other 30 hackathons Facebook has held since 1996. Participants huddled on couches or in conference rooms with white boards, plugging away at new apps. (I can\u2019t tell you what they are, says the organizer, Pedram Keyani. \u201cI like my job. I want to keep it.\u201d) They ate food from Jing Jing, the company\u2019s favorite Chinese restaurant in downtown Palo Alto, and downed energy drinks. Employees even wore spirited matching T-shirts that read: \u201cStay Focused and Keep Hacking\u201d (with Facebook stock worth tens of billions of dollars ready to hit the public markets, and a good chunk of it belonging to people in the room, they had reason to be distracted). But unlike past hackathons, where developing features like the \u201cLike\u201d button, \u201cTimeline\u201d and \u201cFacebook Chat\u201d was the only objective, this event also had a public relations motive. \u201cAs a company, we were going to hack that night. We were going to hack until the ringing of the bell the next morning,\u201d says Keyani. \u201cIt\u2019s kind of a reminder, no matter what people write about us, or what people think of us or how big we get, the reason we do what we do is because we like to build. We want to keep it that way.\u201d\n Hackathons originated in the world of startups as a way for employees to take time off from their everyday job to code and develop new ideas. The products of these of hackathons \u2014 the smart services and new ways of attacking a problem \u2014 that are produced, have until recently been the sole reason for their existence. And they work: TimeHop and My Next Trip were created during similar software coding sprints at FourSquare. But as with Facebook, companies are finding that these energy-drink-fueled all-nighters can have business purposes that have absolutely nothing to do with hacking.\n 1. Hackathon as recruiting event\n While Facebook limits its events to internal employees, companies like bitly, Meetup, and Foursquare open their offices to outside programmers and developers for weekend hackathons. (If that doesn\u2019t sound like a fun way to spend a weekend, you may not have coding seared into you soul.) First they lure some of the tech world\u2019s most passionate citizens with the chance to code alongside their peers, and then they hit them with their really cool offices and other perks. Meetup has a rooftop for parties. Bloomberg has unlimited free snacks and a stunning view from its Manhattan office. For the more downtown set, bitly has its trendy meatpacking district location replete with laid-back, denim-clad employees and vodka bottles on desks.\n Once participants are daydreaming about what life could be like if they toiled for the hackathon host, company recruiters can study how they work during the event. \u201cYou see what people can actually do, rather than what they are saying in an interview,\u201d says Kathryn Fink, a Community Manager at Meetup. Foursquare recruited Max Stoller after he banged out DontEat.at, an app that will notify you if you check into a restaurant that is at risk of being closed for health code violations, during a hackathon. Bitly hired Mike Dewar, one of its data scientists, after he impressed other engineers with his fast and elegant coding. \n 2.  Hackathon as ploy by non-engineering outfits to impress the world with their data-savvy thinking (and simultaneously to get some help from the tech community)\n Last year the City of New York sponsored a 48-hour hackathon where specially selected programmers, designers, and city officials were invited to design a new website for the government. Rachel Sterne, the city\u2019s chief digital officer, said the main goal was to build relationships with the private sector. \u201cIf the only outcome was that New York City was welcomed to a dozen hackathons afterwards,\u201d says Sterne \u201cThat would be huge.\u201d \n Similarly Bloomberg Ventures, the venture capitalism arm of Bloomberg, holds hackathons to nurture new ideas in which to invest. \u201cBloomberg has always been a leader in data science,\u201d says Matt Turck, the company\u2019s Managing Director. \u201cNow we are doing community building exercises to ensure we remain thought leaders.\u201d \n 3. Hackathon because we\u2019re too cheap to hire our own massive research-and-development team\n Why hire groups to test out new products and ideas, when you can invite programmers into your office to work with your product for free? New York City, for example, could have hired expensive professionals to give them proposals on their new website (they are having contractors bid to actually design it), but instead, they held a hackathon where, according to Sterne, the government got to \u201csee what the brightest minds in technology and design thought would be the right direction for the website.\u201d Those ideas are being implemented now.\n Startups such as bitly give outsider engineers early access to new and upcoming parts of their service to see what they will do. It\u2019s not so much because they don\u2019t have their own engineers coming up with ideas, but they could never afford to bring in the breadth of engineers that a hackathon does. \u201cWe\u2019re opening specific things that we want people to test and try out and use,\u201d says Matt LeMay, platform manager of the company. \u201cIt\u2019s a sneak peek at some of the things we are working on, as we are working on them, so that we can get feedback from people who are part of the community that we are building.\u201d\n 4. Hackathon as party\n  When asked to describe hackathons most participants will fire back enthusiastically with words like \u201cfun,\u201d \u201cexciting,\u201d and \u201cstimulating.\u201d And for the right population of engineer, they truly are. Tech companies hold hackathons for their employees in lieu of expensive beachside outings, drunken happy hours, and company \u201ctherapists,\u201d to ensure their treasured employees don\u2019t flee. \n This raises the question, why do tech professionals love hackathons so much? \u201cIt\u2019s problem solving but that\u2019s too generic a description,\u201d says Jared Lander, a New York-based data scientist and consultant. \u201cThe computer is doing something that you\u2019re telling it to do. You\u2019re figuring out how to tell a computer how to solve these problems, how to jump through the hoops to make it happen. It\u2019s creating something. It\u2019s very much like being an artist.\u201d\n Keyani, from Facebook, describes the feelings he gets when going into a Hackathon, almost breathlessly: \u201cWhat if I could get rid of the rules? What if gravity didn\u2019t exist? What could I build?\u201d \n Given the beating Facebook\u2019s stock and the company have taken since the IPO, that is probably a question everyone is asking. And what better way to come up with new products, to solve the problem of making money from mobile users? That\u2019s right, a hackathon. No wonder then, that when Facebook announced its next hackathon to employees in a status update, a majority of the company, including the business team, \u201cLiked\u201d it within a few hours.","item_date":"Jun 11 2012 18:13:51","display_item_date":"06-11-2012","url":"http:\/\/www.wired.com\/business\/2012\/06\/hackathons-arent-just-for-hacking\/","source":"www.wired.com"},{"title":"In San Francisco, a hackathon that humanizes the business plan","details":"SAN FRANCISCO \u2014 \u201cWe want to build companies that we want our kids to work in, and that we\u2019re proud of,\u201d said Kal Patel, a judge at the Reinvent Business Hackathon hosted by frog design last weekend here in San Francisco. Patel, who leads global development and the European expansion of business consultancy LRN, said this as a prelude to announcing the three top teams from the two-day hackathon, in which 20 seven-member teams were tasked with developing a product, platform or system that can \u201cbuild a more human and truly social enterprise.\u201d\n Clearly, that\u2019s a tall order. But that\u2019s the point; frog design and LRN collaborated to develop the hackathon in the hopes of inspiring great ideas for humanizing the way businesses operate.\n \n Tim Leberecht, frog design\u2019s chief marketing  officer, greeted me as I arrived at frog\u2019s SOMA headquarters late Sunday afternoon, just in  time to watch all 20 three-minute presentations. It was an unseasonably  warm and sunny day in San Francisco, but the offices were packed with  slightly sweaty hackathon participants. Their excitement was palpable.\n \u201cNone of the teams have dropped out,\u201d a surprised, exhilarated Leberecht said.\u00a0With an expected attendance of around 80 participants, the hackathon received more  than 200 applicants and some of them \u2014 coders, designers, students, filmmakers, business leaders and engineers \u2014 had to be wait-listed. Each team contained seven participants, many of whom didn\u2019t know each other before the work started up on Saturday morning.\n The panel of judges were given only a half-hour to deliberate and decide which of the 20 ideas \u2014 all of them formed and turned into a product or service prototype or concept during over the weekend \u2014 was the best, and therefore awarded a $5,000 prize from Silicon Valley Bank.\n The ideas and presentations were judged based on:\n Originality of idea\n Feasibility of implementation\n Likelihood of adoption\n Fidelity of code\n Possible impact and capacity to make positive change\n \nThe winning idea is \u201cDecision Icon,\u201d a kind of social network that gives employees within an organization the ability to share their personal interests and abilities outside the workplace. Organizations could then build out teams based on these individuals\u2019 interests \u2014 skills that wouldn\u2019t necessarily be on their resumes and certainly not integral to their jobs. The hope is that the platform would become an idea generation engine.\n \n \u201cWe want to be the matchmaker between motivated people\u201d within companies, explained one of the designers behind the idea.\n \n The members of the second-place Loopool team displayed their acumen for coding, having created a very snazzy demo over the weekend, as well as pulling together a very entertaining presentation, complete with a song. Loopool would rebrand customer resource management (\u201dCRM\u201d) software as collaborative relationship management software. The software would give consumers a platform for providing product feedback in a way that would be shared not just with the company that makes the product, but with a larger community and in a manner that would elicit a public response.\n \u201cThis tool lets companies evolve, like open source technology,\u201d one Loopool team member explained.\n To my ears, Loopool sounded a lot like products that already exist, such as Get Satisfaction, which empowers consumers by giving them a platform for complaining or praises companies or products, and, in turn, gives companies a tool for directly responding to those consumers.\n The third place winner was Sentimetric, a software platform billed as \u201cGoogle Analytics for emotion.\u201d This is a tool that employers would use to chart, measure and track the way employees are feeling about their workplace on a day to day basis. The idea is that by being more keyed into how employees are feeling, employers will be able to respond proactively to changes in the emotional landscape, thereby (hopefully) boosting happiness and retention. It would be targeted toward small companies that are growing, and adding staff, very quickly.\n \n All three top teams will receive free mentoring from frog design and LRN, and all of the participating teams will be featured in an upcoming issue of frog\u2019s Design Mind magazine. As the hackers filed out of the frog design offices and onto 3rd Street, they all seemed quite content that they\u2019d toiled away all weekend. And Leberecht hopes the conversation, and hacking, will continue. \n \u201cThis may be turned into a series of events, held in different locations, or held a larger platform,\u201d he said.","item_date":"Jun 11 2012 17:29:49","display_item_date":"06-11-2012","url":"http:\/\/www.smartplanet.com\/blog\/design-architecture\/in-san-francisco-a-hackathon-that-humanizes-the-business-plan\/6873","source":"www.smartplanet.com"},{"title":"Creativity gets a boost with 'hackathon'","details":"","item_date":"Jun 11 2012 17:26:42","display_item_date":"06-11-2012","url":"http:\/\/www.todayonline.com\/Technology\/EDC120611-0000003\/Creativity-gets-a-boost-with-hackathon","source":"www.todayonline.com"},{"title":"Twitter\u2019s Hashtag Pages Could Be The New AOL Keywords","details":"Earlier today, Twitter made its most aggressive grab for TV marketing dollars, with the release of a TV ad during the Pocono 400 and the launch of the corresponding Twitter.com\/#NASCAR hashtag page.\n\n\nI said at the time that the campaign reminds me somewhat of those AOL keywords that used to get tagged about during commercials and at the end of television episodes. See, once upon a time AOL \u2014 loving parent of this wonderful tech blog \u2014 was the way that a good part of the American population got onto this thing called the Internet. And so, for a time it was the go-to place to find out more about all your favorite brands, products, TV shows, movies \u2014 you name it.\n\n\nSee Twitter, like AOL before it, wants to be the destination for users who wish to engage with a certain brand. It wants to own the URL that runs at the end of an ad. Actually, scratch that \u2014 it wants to own the hashtag that appears during the ad or TV show, to become synonymous with where the conversation happens.\n\n\nNow, Twitter isn\u2019t the on-ramp to the Internet that AOL was\u2026 But it\u2019s got something even better: Twitter has a base of very vocal and very engaged users. And these users are essentially creating a ton of content, which smart brands are starting to leverage to promote their goods and services. In a way, it\u2019s already the place where brand conversation happens.\n\n\nThat\u2019s what\u2019s so brilliant about #NASCAR and what we can only assume will be future hashtag landing pages: The brands themselves don\u2019t have to actually create anything new. Without actually knowing what happens on the back end of a hashtag page, it seems clear that Twitter has built a pretty slick way to curate and repurpose content that is being tweeted out by its users.\n\n\nUnlike AOL Keywords \u2014 and today\u2019s rough equivalent, Facebook Pages \u2014 where the onus is on the brand to create, upload, and promote the most compelling content, Twitter\u2019s managed to get its users to do the hard work for brands. Now all the brands have to do is surface those conversations, make them available in an easy-to-read way. #NASCAR shows how that can be done pretty successfully.\n\n\nIt\u2019s the act of curation rather than creation which is why I think Twitter\u2019s strategy has legs, and why it could beat out other methods of social marketing. Because at the end of the day, there\u2019s nothing better than getting your biggest fans to promote your brand for you.","item_date":"Jun 11 2012 17:18:27","display_item_date":"06-11-2012","url":"http:\/\/techcrunch.com\/2012\/06\/10\/twitter-hashtag-pages-aol-keywords\/?icid=tc_home_art","source":"techcrunch.com"},{"title":"Internal Elastic Load Balancers in the Virtual Private Cloud","details":"One of the challenges we\u2019ve heard about many times from customers is the challenge of load balancing between tiers of an application.  While Elastic Load Balancing addresses many of the complexities of building a highly available application, it doesn\u2019t help when you need to balance the load between multiple back-end instances.  Until now.  As of today, you can create an internal load balancer in your VPC and place your non-internet-facing instances behind the internal load balancer.  Here\u2019s a simple overview:\n  \n  The internet-facing load balancer has public IP addresses and the usual Elastic Load Balancer DNS name.  Your web servers can use private IP addresses and restrict traffic to the requests coming from the internet-facing load balancer.  The web servers in turn will make requests to the internal load balancer, using private IP addresses that are resolved from the internal load balancer\u2019s DNS name, which begins with \u201cinternal-\u201c.  The internal load balancer will route requests to the application servers, which are also using private IP addresses and only accept requests from the internal load balancer.\n  With this change, all of your infrastructure can use private IP addresses and security groups so the only part of your architecture that has public IP addresses is the internet-facing load balancer.  Because the DNS record is publicly resolvable, you could also use a VPN connection and address the internal load balancer from your on-premise environment through the VPN tunnel.\n  Getting started is easy.  Using the AWS Console, simply select the checkbox to make your new load balancer an internal load balancer.  Everything else stays the same.\n  \n  As part of this change, we\u2019ve also relaxed the constraints on the size of the subnet you need to attach the load balancer to.  You can now attach a load balancer to your subnets that have a \/27 or larger size.\n  I\u2019m looking forward to hearing about the new scenarios this enables for you.  Let us know what you think!","item_date":"Jun 11 2012 17:11:50","display_item_date":"06-11-2012","url":"http:\/\/aws.typepad.com\/aws\/2012\/06\/internal-elastic-load-balancers.html","source":"aws.typepad.com"},{"title":"Big ethics for big data - O'Reilly Radar","details":"As the collection, organization and retention of data has become commonplace in modern business, the ethical implications behind big data have also grown in importance. Who really owns this information? Who is ultimately responsible for maintaining it? What are the privacy issues and obligations? What uses of technology are ethical \u2014 or not \u2014 when it comes to big data?\n  These are the questions authors Kord Davis (@kordindex) and Doug Patterson (@dep923) address in Ethics of Big Data. In the following interview, the two share thoughts about the evolution of the term big data, ethics in the era of massive information gathering, and the new technologies that raise their concerns for the big data ecosystem.\n  How do you define big data?\n  Douglas Patterson: The line between big data and plain old data is something that moves with the development of the technology. The new developments in this space make old questions about privacy and other ethical issues far more pressing. What happens when its possible to know where just about everyone is or what just about everyone watches or reads? From the perspective of business models and processes, impact is probably a better way to think about big than in terms of current trends in NoSQL platforms, etc.\n  One useful definition of big data \u2014 for those who, like us, dont think its best to tie it to particular technologies \u2014 is that big data is data big enough to raise practical rather than merely theoretical concerns about the effectiveness of anonymization.\n  Kord Davis: The frequently-cited characteristics volume, velocity, and variety are useful landmarks \u2014 persistent features such as the size of datasets, the speed at which they can be acquired and queried, and the wide range of formats and file types generating data.\n  The impact, however, is where ethical issues live. Big data is generating a forcing function in our lives through its sheer size and speed. Recently, CNN published a story similar to an example in our book. Twenty-five years ago, our video rental history was deemed private enough that Congress enacted a law to prevent it from being shared in hopes of reducing misuse of the information. Today, millions of people want to share that exact same information with each other. This is a direct example of how big datas forcing function is literally influencing our values.\n  The influence is a two-way street. Much like the scientific principle that we cant observe a system without changing it, big data cant be used without an impact \u2014 its just too big and fast. Big data can amplify our values, making them much more powerful and influential, especially when they are collected and focused toward a specific desired outcome.\n  Big data tends to be a broad category. How do you narrow it down?\n  Douglas Patterson: One way is the anonymization of datasets before theyre released publicly, acted on to target advertising, etc. As the legal scholar Paul Ohm puts it, data can be either useful or perfectly anonymous, but never both.\n  So, suppose I know things about you in particular: where youve eaten, what youve watched. Its very unlikely that Im going to end up violating your privacy by releasing the information that theres one particular person who likes carne asada and British sitcoms. But if I have that information about 100 million people, patterns emerge that do make it possible to tie data points to particular named, located individuals.\n  Kord Davis: Another approach is the balance between risk and innovation. Big data represents massive opportunities to benefit business, education, healthcare, government, manufacturing, and many other fields. The risks, however, to personal privacy, the ability to manage our individual reputations and online identities, and what it might mean to lose \u2014 or gain \u2014 ownership over our personal data are just now becoming topics of discussion, some parts of which naturally generate ethical questions. To take advantage of the benefits big data innovations offer, the practical risks of implementing them need to be understood.\n   How do ethics apply to big data?\n   Kord Davis: Big data itself, like all technology, is ethically neutral. The use of big data, however, is not. While the ethics involved are abstract concepts, they can have very real-world implications. The goal is to develop better ways and means to engage in intentional ethical inquiry to inform and align our actions with our values.\n  There are a significant number of efforts to create a digital Bill of Rights for the acceptable use of big data. The White House recently released a blueprint for a Consumer Privacy Bill of Rights. The values it supports include transparency, security, and accountability. The challenge is how to honor those values in everyday actions as we go about the business of doing our work. \n  Do you anticipate friction between data providers (people) and data aggregators (companies) down the line?\n  Douglas Patterson: Definitely. For example: you have an accident and youre taken to the hospital unconscious for treatment. Lots of data is generated in the process, and lets suppose its useful data for developing more effective treatments. Is it obvious that thats your data? It was generated during your treatment, but also with equipment the hospital provided, based on know-how developed over decades in various businesses, universities, and government-linked institutions, all in the course of saving your life. In addition to generating profits, that same data may help save lives down the road. Creating the data was, so to speak, a mutual effort, so its not obvious that its your data. But its also not obvious that the hospital can just do whatever it wants with it. Maybe under the right circumstances, the data could be de-anonymized to reveal what sort of embarrassing thing you were doing when you got hurt, with great damage to your reputation. And giving or selling data down the line to aggregators and businesses that will attempt to profit from it is one thing the hospital might want to do with the data that you might want to prevent \u2014 especially if you dont get a percentage.\n  Questions of ownership, questions about who gets to say what may and may not be done with data, are where the real and difficult issues arise. \n  Which data technologies raise ethical concerns?\n  Douglas Patterson: Geolocation is huge \u2014 think of the flap over the iPhones location logging a while back, or how much people differ over whether or not its creepy to check yourself or a friend into a location on Facebook or Foursquare. Medical data is going to become a bigger and bigger issue as that sector catches up.\n  Will lots of people wake up someday and ask for a do over on how much information theyve been giving away via the frictionless sharing of social media? As a teacher, I was struck by how little concern my students had about this \u2014 contrasted with my parents, who find something pretty awful about broadcasting so much information. The trend seems to be in favor of certain ideas about privacy going the way of the top hat, but trends like that dont always continue.\n  Kord Davis: The field of predictive analytics has been around for a long time, but the development of big data technologies has increased accessibility to large datasets and the ability to data mine and correlate data using commodity hardware and software. The potential benefits are massive. A promising example is that longitudinal studies in education can gather and process significantly more minute data characteristics and we have no idea what we might learn. Which is precisely the point. Being able to assess a more refined population of cohorts may well turn out to unlock powerful ways to improve education. Similar conditions exist for healthcare, agriculture, and even being able to predict weather more reliably and reducing damage from catastrophic natural weather events.\n  On the other hand, the availability of larger datasets and the ability to process and query against them makes it very tempting for organizations to share and cross-correlate to gain deeper insights. If you think its difficult to identify values and align them with actions within a single organization, imagine how many organizations the trail of your data exhaust touches in a single day.\n  Even a simple, singular transaction, such as buying a pair of shoes online touches your bank, the merchant card processor, the retail or wholesale vendor, the shoe manufacturer, the shipping company, your Internet service provider, the company that runs or manages the ecommerce engine that makes it possible, and every technology infrastructure organization that supports them. Thats a lot of opportunity for any single bit of your transaction to be stored, shared, or otherwise mis-used. Now imagine the data trail for paying your taxes. Or voting \u2014 if that ever becomes widely available.\n  What recent events point to the future impact of big data?\n  Douglas Patterson: For my money, the biggest impact is in the funding of just about everything on the web by either advertising dollars or investment dollars chasing advertising dollars. Remember when you used to have to pay for software? Now look at what Google will give you for free, all to get your data and show you ads. Or, think of the absolutely pervasive impact of Facebook on the lives of many of its users \u2014 theres very little about my social life that hasnt been affected by it.\n  Down the road there may be more Orwellian or Minority Report sorts of things to worry about \u2014 maybe were already dangerously close now. On the positive side again, there will doubtless be some amazing things in medicine that come out of big data. Its impact is only going to get bigger.\n  Kord Davis: Regime change efforts in the Middle East and the Occupy Movement all took advantage of big data technologies to coordinate and communicate. Each of those social movements shared a deep set of common values, and big data allowed them to coalesce at an unprecedented size, speed, and scale. If there was ever an argument for understanding more about our values and how they inform our actions, those examples are powerful reminders that big data can influence massive changes in our lives.\n  This interview was edited and condensed.\n   Ethics of Big Data \u2014  This book outlines a framework businesses can use to maintain ethical practices while working with big data.\n  If your data practices were made public, would you be nervous?\n Who owns patient data?\n  The World Economic Forum recently described personal data as a new economic asset class\n  Why you cant really anonymize your data\n  FTC calls on Congress to enact baseline privacy legislation and more transparency of data brokers","item_date":"Jun 11 2012 17:10:59","display_item_date":"06-11-2012","url":"http:\/\/radar.oreilly.com\/2012\/06\/ethics-big-data-business-decisions.html","source":"radar.oreilly.com"},{"title":"Show Me the Money from Canada: Not Just Another Loonie API","details":"Based in Vancouver, BC, Payfirma has created an XML-based API that promises custom forms and full shopping carts to keep you in control of developing the flow of your credit card processing. The idea is to make business\u2019s lives simpler by reducing development costs with the Payfirma API that accepts Visa, Mastercard, Discover, and American Express among others.\n\nPayfirma states the API also is pre-integrated with third party shopping carts, and has a payments profiles API to keep customers on the retailer\u2019s site while preventing the storing of card data on the retailer\u2019s server. This eliminates online credit card risks and is supported by Visa and Mastercard Secure Code.\n\nIt\u2019s easy to see how Payforma developed in Canada. As Rolando Fuentes reports in Techvibes.com, Canada leads the U.S. in what Mastercard dubs the \u201cMobile Payment Readiness Index,\u201d measuring a series of factors from mobile payments to consumer readiness and regulation. (Thailand was number 1.)\n\nIf the satisfaction of one customer is any indication, Payfirma could be on to something big. Foodster delivers takeout in Vancouver from any restaurant for flat fee of $10. Customers order online with Payforma managing the transactions. According to Tom Hoang, the Foodsters\u2019 Business Development Manager, \u201cPayfirma gives us a secure, and robust way to handle credit card transactions, even American Express. This was something we didn\u2019t have before when we used PayPal\u2026we\u2019re paying fees that are fair, and we have the support we need.\u201d\n\nPayforma is headed by Michael Gotkurk, who previously founded Versapay. As CEO, Gotkurk led Versapay to become Canada\u2019s fastest growth company in 2009. Following up with a successful IPO in 2010, he then moved on to found Payfirma.\n\nPayfirma\u2019s client list includes Greyhound, The UPS Store, and BMW. Last December we dove into a new class of payment APIs, including the Dwolla API.","item_date":"Jun 11 2012 17:08:48","display_item_date":"06-11-2012","url":"http:\/\/blog.programmableweb.com\/2012\/06\/11\/show-me-the-money-from-canada-not-just-another-loonie-api\/","source":"blog.programmableweb.com"},{"title":"SPARC to Host Mobile App Hackathon","details":"SPARC, a Best Place to Work winner, opened registration today for its second annual mobile application hackathon being held on Saturday, August 25, 2012. Hackathon 2.0 will bring together teams of developers from around the country to compete for over $10,000 in prizes and giveaways, provide an energy-filled environment for spectators, and feature all the beer, caffeine, and tasty food trucks attendees can handle. The can\u2019t miss 16-hour event will be held at SPARC\u2019s headquarters on 2387 Clements Ferry Road in Charleston, SC.\n  SPARC\u2019s Hackathon 2.0 offers software engineers a chance to meet peers from around the industry, show off their mobile app skills in a creative environment, and have some fun along the way. Prizes will be awarded to teams who build the best applications, including two iPads for the \u201cbest in show\u201d winner. The engaging nature of the competition, coupled with SPARC\u2019s award-winning culture, produces a can\u2019t-miss event for engineers - or spectators, who are also welcome at the event. \n  We\u2019re hosting the Hackathon to open-source our culture to the industry as a whole. Where else can you form a team with someone from a different company or school, come hang out with some of the brightest developers in our industry, AND compete for cash and prizes doing what you love? Creativity and innovation thrive in that type of environment, and we want to share it with the world, said Amanda Noyes, S\/W Engineer & Event Organizer.\n  Participants of Hackathon 2.0 will work in teams using their own equipment to develop applications based on a chosen concept or category. Teams can develop native apps for Android, iOS, BlackBerry, Windows Phone 7, or use a framework such as PhoneGap that builds to an APK or IPA. Demos will be given at the event on SPARC equipment, and all code will be checked into SPARC\u2019s Github repository then released under a GPLv2 license after the event. \n  Check-in and setup will be from 8:00 - 9:30 a.m., followed by idea revelation from 9:30 - 10:00 a.m. Application development will take place from 10:00 a.m. - 10:00 p.m., followed by judging and awards from 10:00 p.m. until midnight. \n  Visit hackathon.sparcedge.com to register (as a team or spectator) or to learn more. \nSPARC provides software product development services and support with customers in the commercial, federal, and defense contracting industries. SPARC offers a broad range of solutions in order to enhance customer-focused results in an industry driven by state-of-the-art technology. We infuse our services with robust scientific methods, complemented by modular information management technologies. Check us out online at http:\/\/www.sparcedge.com.\n  About GitHub   \nGitHub is a social network for programmers. Git is a distributed software management program created by Linus Torvalds, orginally for the Linux Kernel Development. GitHub is a hosted Git repository. Github allows you to take part in collaboration by forking projects, sending and pulling requests, and monitoring development. Visit http:\/\/www.github.com to learn more.","item_date":"Jun 11 2012 17:03:12","display_item_date":"06-11-2012","url":"http:\/\/www.prweb.com\/releases\/mobile\/hackathon\/prweb9586000.htm","source":"www.prweb.com"},{"title":"Twitter airs its first television commercial","details":"All doubt about Twitters position in the media world was laid to rest this weekend as the company aired its first ever television commercial during the Pocono 400 NASCAR race.\n     The ad, which encouraged NASCAR fans to see what he sees (referring to a driver), was used to promote the special NASCAR hashtag page which Twitter created for NASCAR and announced last week. That page uses a combination of algorithms and curation to surface the most interesting Tweets to bring you closer to all of the action happening around the track, from the garage to the victory lane.\n  Its an interesting and potentially lucrative model for the San Francisco-based company, particularly in light of the fact that Twitter and live television have a very good relationship.\n  But a television commercial? Its an important milestone for the company, one that leaves little doubt Twitter is a mainstream media company -- like it or not.\n  While hashtag pages may not be the internets replacement for AOL keywords (and Twitter probably hopes they wont be given AOLs fate), they could prove to be very important for the company, making them worth promoting prominently (read: on television). After all, two of Twitters biggest problems are the services signal-to-noise ratio and its ability to retain users.\n  In the case of the NASCAR hashtag page, Twitter is curating tweets and photos that NASCAR fans would be most interested in, removing much of the noise, and it gives registered users who are racing fans a reason to come back. Incidentally, it also gives racing fans who arent registered on Twitter a way to interact with the Twitter service. Some of those unregistered users, of course, might opt to register to follow their favorite drivers after seeing that their tweets and photos contain content they wont find anywhere else.\n  Because of the potential benefits, expect to see Twitter ink more media partnerships that bring the Twitter brand to a small screen near you during commercial breaks.\n                 Patricio Robles is a tech reporter at Econsultancy. Follow him on Twitter.","item_date":"Jun 11 2012 17:00:45","display_item_date":"06-11-2012","url":"http:\/\/econsultancy.com\/us\/blog\/10087-twitter-airs-its-first-television-commercial","source":"econsultancy.com"},{"title":"CyclingBuddy.com opens up API for App Developers","details":"CyclingBuddy.com; The Social Networking platform for cyclists has today made the strategic decision to open its API allowing App developers to integrate with it.\n  Since launch CyclingBuddy members have manually logged over 150,000 cycling miles and shared over 1,100 of their favourite cycling routes.  CyclingBuddy has already attracted members from 14 countries around the world, a remarkable level of growth considering the site is just over 3 months old.  Opening up its platform to developers is a sharp signal of its intent to continue to build a solid platform of services for cyclists.\n  Tony Piedade, founder and CEO said  CyclingBuddy is a great community of cyclists of all ages and abilities, all looking to find someone else to cycle with. There are a multitude of Apps out there that are extremely good at recording and monitoring performance.  We want to invite these App developers to integrate into CyclingBuddy.com.  Its a great partnership! -  we are really good at motivating people to cycle, while Apps are great at recording progress and performance once they have got out of the door  -  everybody wins\n  Developers interested in the API should email API@cyclingbuddy.com \n  [End]\n  For more information email our PR team  PR@cyclingbuddy.com \n \t\t\t\t\t\t \t\t\tThis press release was distributed by SourceWire News Distribution on behalf of CyclingBuddy.com. For more information visit http:\/\/www.dwpub.com\/sourcewire","item_date":"Jun 11 2012 16:59:44","display_item_date":"06-11-2012","url":"http:\/\/www.sourcewire.com\/news\/72387\/cyclingbuddy-com-opens-up-api-for-app-developers","source":"www.sourcewire.com"},{"title":"US Census API Makes Giant Data Dumps Optional","details":"There\u2019s still government hoops to jump through, but the US Census is making much of its recent data available by the slice, rather than the whole pie. The US Census Bureau API provides access to 2010 Census data, including demographics, income and how many women in New York take public transportation to work (and that\u2019s excluding taxi cabs).\n\nThe government department has certainly moved on to advanced computer hardware since the photo above was taken in the 1950s. But it\u2019s been slow to embrace the new open data movement. In a way, it was far ahead of its time, providing data long before Data.gov aimed to open up goverment. However, one of the issues as a developer I\u2019ve always had with government data is that it demands a hefty download and a lot of learning arcane methods for relating data.\n\n\nThis agency\u2019s first foray into API-land is not without obscure government codes. For example, California is state number 06, and each county has a unique code, too. For those who have been pouring over the data for years, it makes sense. For others, there\u2019s a learning curve.\n\n\nAnd heck, for the first time we can get directly at US Census data for the tiny trickle of information we care about. And it\u2019s a REST and JSON API, even supporting JSONP. There\u2019s a lot here for a developer to like. Goodbye downloads, goodbye spreadsheets.","item_date":"Jun 11 2012 00:09:58","display_item_date":"06-10-2012","url":"http:\/\/blog.programmableweb.com\/2012\/06\/07\/us-census-api-makes-giant-data-dumps-are-optional\/","source":"blog.programmableweb.com"},{"title":"If a Tweet Falls In the Forest\u2026","details":"Twitter has proven to be an effective way for the average Internet user to get @ your favorite NBA player or indie rocker. And yet as the site grows, the noise of individual tweets is being drowned out. Enter Thunderclap, a Twitter app from\u00a0Chinatown-based De-De (for \u201cdesign and development,\u201d not the Chinese word for \u201clittle brother\u201d).\n Yesterday,\u00a0Rolling Stone\u00a0writer, noisemaker and hero-journalist Matt Taibii sent out the\u00a0inaugural Thunderclap\u00a0at noon, causing 1,921 people to tweet simultaenously at Senator Tim Johnson (D-SD) and Senator Debbie Stabenow (D-MI) in order to entreat the pair to keep their hands off the Dodd-Frank Act (and also plug Mr. Taibbi\u2019s recent article, \u201cHow Wall Street Killed Financial Reform\u201c). The tweets were exposed to a combined 4 million people, Thunderclap estimates.\n Thunderclap is a platform for people who want to send a directed petition signed by a crowd of people to a specific person or group via Twitter. Post a tweet, state your case, and ask others to join you\u2014and if they do, it\u2019ll trigger a mass of identical tweets all at the same time. Somewhat like Kickstarter, the app requires a message to reach a tipping point for the trick to work. Hopeful Thunderclappers must set a minimum number of supporters\u2014Mr. Taibbi set his at 500\u2014and a timeframe to reach the goal. If no one\u2019s interested, the Thunderclap fails.\n \u201cKickstarter has opened up a new world of crowdfunding. We think Thunderclap will be a new way of \u2018crowdspeaking,\u2019\u201d said Hashem Bajwa, who was\u00a0the digital strategy lead at ad agency Droga5 before founding De-De in January. The agency is best described as \u201cBetaworks meets Mad Men,\u201d he said, and Thunderclap is the first of many playful vanity projects that will surely earn mindshare for the fledgling agency. De-De works out of a dilapidated building in Chinatown where the elevator is manned by a Chinese man who sings on Fridays. \u201cI call it the \u2018Silicon Sweatshop,\u2019\u201d Mr. Bajwa said. \u201cI kind of like that we\u2019re not at General Assembly.\u201d\n The next Thunderclaps are already in the works. Today at noon another Thunderclap is scheduled to blast out from 128 accounts: \u201cTell Congress to provide simple access to congressional data. Innovation in government depends on it! @RepHalRogers.\u201d David Cascino, who posted the Thunderclap, only has 224 followers on Twitter; another project by Soraya Darabi, who has more than 400,000 followers on Twitter, did not reach its goal.\n This shows that even small-fry tweeters can amplify their voices, Mr. Bajwa said. \u201cThe person\u2019s following who creates a Thunderclap is not as important as the social quality and \u2018shareworthy-ness\u2019 of their message,\u201d he said. \u201cYou dont have to have a huge following to have a successful Thunderclap.\u201d\n Having a large following certainly helps, however. Mr. Taibbi\u2019s effort earned endorsements from Sarah Silverman and Cory Doctorow, which helped goose the numbers. Thunderclap tracks the number of clicks each Thunderclap receives after all the tweets have been sent out. The link in Mr. Taibbi\u2019s Thunderclap got at least 2,000 clicks after it was fired off, Mr. Bajwa said. Since the campaign had about 2,000 supporters, he figures the Thunderclap at least doubled awareness for the issue.\n Thunderclap was inspired in part by the human mic at Occupy Wall Street, Mr. Bajwa said, wherein one speaker\u2019s words are echoed sentence-by-sentence by the crowd, so that\u00a0hundreds of people\u00a0can hear. \u201cTwitter is a very\u00a0easy place to say something but an incredibly difficult place to be heard,\u201d he said.\n This idea has been tried at least once before, with New York-based\u00a0Kommons, which attempted to direct tweets at public figures. Kommons failed to gain traction and pivoted to a different business. But it didn\u2019t have the tipping point element, which seems key to Thunderclap\u2019s success.\n Thunderclap will introduce a version for Facebook in the next two weeks, Mr. Bajwa said, and he\u2019s been approached by at least three big names wanting to do a Thunderclap.\n There are two major challenges, though. Thunderclap spits out such a mass of tweets at a time that it could be considered spam by Twitter and banned. \u201cYeah, I really hope Twitter doesn\u2019t shut us down,\u201d Mr. Bajwa said. If the microblogging platform decides to cut Thunderclap off, he has some ideas for workarounds\u2014the Thunderclap may not include an @ reply in the future, for example, if Twitter decides to restrict its access.\n The other challenge is more essential. Just as the growing volume of tweets decreased the impact of each tweet, a growing number of Thunderclaps may decrease the impact of a Thunderclap. Imagine a senator whose feed is inundated with 500 duplicative tweets a day about an issue, where each tweet required no more commitment than a few clicks. It might be impressive at first, but Thunderclap will have to keep innovating once the novelty wears off.\n \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tAdrianne Jeffries is the editor of Betabeat. Follow Adrianne on Twitter or via RSS. ajeffries@observer.com","item_date":"Jun 11 2012 00:08:43","display_item_date":"06-10-2012","url":"http:\/\/betabeat.com\/2012\/06\/tomorrow-more-than-1600-people-will-tweet-with-matt-taibbi\/","source":"betabeat.com"},{"title":"Mobile App Hackathon at START ","details":"The START coworking space is getting ready to launch and they are hosting a happy hour and a mobile-app hackathon to celebrate. \n The aptly name START Kickoff Appy Hour will be Friday June 8th from 6pm-9pm. The START Hackathon will be Saturday June 9th 7am \u2013 7pm. \n The good news is that both events are completely free, and there are over $5,000 worth of prizes at the hackathon. But, there is one catch\u2026 you must register here. Sorry for the late notice, but REGISTRATION CLOSES TODAY (6\/5\/2012)\n The details from the event registration page are as follows:\n Join us on Saturday 6\/9\/2012 for 12 hours of adrenalin pumping redbull downing app development and be prepared to demo to win major street cred with your peers. The #1 winner or team will receive 3 months of complimentary usage of space at START.\n               About the author\n                      \n \t\tI am a programmer and co-author of the book JBoss in Action. I am a Principal Consultant at Improving Enterprises where I work with amazing consultants to solve challenging problems for knowledgeable clients.    When I\u2019m not cranking out code, I teach and write about software, startups, and education.    I\u2019m currently helping to organize three groups:    Houston Startup Support Network  Houston Lean Startup Circle  Houston Startup Weekend    I\u2019m working on three startups \/ projects:  --dripita.com-- Dripita lets you queue up your Twitter updates and sends them out for you periodically.  --FanLaunch--- A group payment platform for paying content creators to release their work for free non-commercial use under a Creative Commons license.  --MyKidCanCode.com-- An e-book I\u2019m writing to help parents teach their children how to program.","item_date":"Jun 11 2012 00:04:34","display_item_date":"06-10-2012","url":"http:\/\/www.startuphouston.com\/2012\/06\/05\/mobile-app-hackathon-at-start\/","source":"www.startuphouston.com"},{"title":"Surveying 1000 APIs - Think APIs are for Startups Only? Think Again! - 3scale","details":"As the API Ecosystem grows, at 3scale we often asked where growth is fastest and how to characterize certain trends. While this is challenging to do, sites like Programmableweb provides great data of many public APIs and regularly feature trend analysis (see John Musser\u2019s excellent \u201cHot or Not\u201d Presentation Gluecon for example).\n To dig deeper though, we decided to try to classify a snapshot of 1000 of the latest APIs recently registered on Programmableweb, with some surprising results. \n The study took the latest 1000 APIs registered in the Programmableweb directory prior to the 2nd of May 2012 and analysed them in various ways. Hence the sample stretches from the 17th of January until the 2nd of May 2012. The directory is now over 6000 APIs and those latest 1000 were added in just 90 days \u2013 so it\u2019s recent snapshot and should tell us something.  The APIs were classified in a number of dimensions ranging from the size of the company launching the API, the sector it operates in and the business model behind the API.\n The most interesting finds were \u2026 \n  It\u2019s not just web startups!: A common perception of APIs is that the majority of APIs are run by web technology startups. However, in the sample, only 17% of the companies providing APIs were web technology startups by our definition of less than 3 years old with a primarily digital \/ web based product).\n \n  The long tail is still missing in action: A second assumption is that APIs would likely \u201cmirror the web\u201d in terms of distribution by the size of the company providing them \u2013 mirroring a long tail distribution of few large companies providing APIs and many tiny companies. However, from the data it appears that APIs are primarily concentrated on medium sized and small companies, as well as with a few large organisations with many APIs. The true long tail isn\u2019t quite there yet. \n \n  Mobile \u201cOnly\u201d is rare \u2013 but Multi-channel is huge: Thirdly, with mobile being such a driving force in the current evolution of the Web we expected to find many APIs which primarily focused on powering mobile applications \u2013 however only 10.7% of the APIs in the sample could be said to have a pure mobile focus. There were though many of the rest (another 51% of the total) which clearly targeted both mobile and non-mobile re-use. This result is likely to be somewhat skewed since many private mobile APIs are not to be listed on Programmableweb to date, however it still seems to suggest that we are entering a \u201cmulti-channel\u201d world rather than a purely mobile one.\n \n  Business Models are getting there: A final common perception is that API business models are still in flux and there are no clear patterns emerging. In the sample there were a diverse range of business models and the majority of APIs launched had clear business value to the provider \u2013 either in terms of direct monetization or indirect collateral benefit. It also seemed there was convergence to a few common models.\n \n The chart below shows the estimated size distribution of the companies launching APIs in the sample set (based on data from Crunchbase, the company\u2019s web sites and general web search). Interestingly the distribution does not follow a clear long-tail distribution as the Web does. In fact:\n  Large companies are well represented: launching significant numbers of APIs.\n \n  There is a \u201cmissing\u201d long tail with companies of medium and small size dominating API launches and not the much longer tail of relatively few micro \/ very small organisations (the number of micro-organizations is similar to the number of \u201csmall\u201d organisations when in a long-tail distribution it would be much greater).\n \nThis seems to indicate that to date it still requires a certain size of company \u2013 10 employees upwards and more likely 50 employees upwards to launch an API, rather than an explosion of APIs from very small organisations. This tendency is likely due both to technical constraints (it remain technically challenging to launch an API, despite platform and tool advances) and business constraints \u2013 APIs are likely to appeal primarily to organisations with standardized products and services to distributed but not yet to those with niche bespoke \/ custom products.\n  API Providers by Size: V. Small indicates < 10 employees, Small 11-50 employees, Medium 51-500, Large 501-2000, Very Large 2001+.\n Not only for Web Technology Startups\n The study also separated those considered web technology startups from others since this category was expected to be one of the most significant sources of APIs. However, only 17% of the organizations were identified in this category.\n The definition we used was less than 3 years old and with a business focused primarily on web\/digital product or distribution. While this misses out some others that might well be considered startups in other spaces we wanted to figure out how many companies were in the Tech Startup ecosystem specifically since these together with a few giants, these tend to dominate the much of the API tech news.\n Not all of the startups identified were small \u2013 some were already organisations of significant size \u2013 having raised multiple rounds of capital in their 3 years.\n Company Size by Business Area\n Figuring out which companies do what was also interesting and a few clear patterns emerged from the sample:\n  Very small API providers (35%) tended to providing novel APIs as extensions to SAAS applications (APIs are now becoming a standard part of the SAAS Armoury!), but a surprising number provide primarily infrastructure style services (29%) such as SMS resale or tools \/ utilities.\n \n  Small and Medium API providers also focus on similar areas, but also add Data Services (20%).\n \n  Large and very Large organizations overwhelmingly (54%) released APIs focused on infrastructure services, with payment and ecommerce services a distant second (18%). \n \n  Governments primarily focused on Data APIs that covered both raw information services in categories ranging from Seismic Activity and Weather data to government spending \u2013 secondary to these were APIs in areas such as compliance, procurement and tendering.\n \n One of the biggest growth drivers for APIs is certainly mobile \u2013 with mobile apps increasingly needing to call back to network services to deliver the right user experience. To understand how much of a driver mobile was we also classified the APIs as to whether they had an exclusive (or near exclusive) focus on delivering for mobile, the objective of delivering for both mobile and non-mobile or (finally) being primarily for fixed Web re-use.\n The figure below shows the results of this split. Surprisingly, less than 11% of the APIs in the survery seemed to have a pure mobile focus. However the significant majority (62% total) were either pure mobile or mixed both Mobile and Non-mobile targets.\n  API split between those which focus purely on mobile, target both or focus primarily on fixed line use.\n The trend is even starker if Scientific APIs (125 in the study) are factored out since these all fell into the non-mobile category \u2013 in this case 70% of non-scientific APIs addressed mobile to some extent. This indicates that mobile is of very significant driver, but multiple channels are even more important.\n Business Models Diversity\n The business models applied by many of the APIs have shifted to provide clear value to both the API provider and the user of the APIs. Figure 3 shows the split in the sample between primary business models. The business model categories in the classifications were defined as:\n  Direct API Monetization (is-API): API service in which fees are applied directly to API usage, either to make calls to the API or for the immediate effects of API calls (such as SMS sending, cloud infrastructure operations or pay per query search services).\n \n  Product Extension APIs (Product Addition): these APIs provide additional features for customers of an existing (generally browser based) application as additional features. The value to the user is a new access to their existing data, the value to the API provider is potentially monetary (API access requires higher tiers of service) or indirect (the API increases service stickiness).\n \n  Product Projection APIs: these APIs enable third party partners, resellers and application developers to build on top of an existing product and act as channels or enhancers. The value to third parties is increased functionality or revenue share and the value to API providers is the creation of new distribution channels and a richer experience for their users.\n \n  Non-Profit: general free to use APIs for wide re-use or with a donation system to support the maintenance. \n \n  Government: APIs are provided by public agencies and by governments at no cost to the user but with rate limits and restrictions imposed (although some government APIs in certain cases are paid APIs for volume usage or re-use for commercial gain).\n \nSome APIs also applied mixed business models, but in most cases the focus was clear. Suprisingly, the business models applied for commercial APIs split almost evenly between the first three models (26% focusing on customer enhancements, 23% on direct monetization and 20% on reseller style APIs) \u2013 indicating the emergence of clearly distinct value propositions. The remaining APIs were relatively evenly split between Government, Scientific and Non-profit models. \n  APIs by Business Model Type\n The sample size of the study is small and no doubt some artifacts are accentuated or muted as a result, our classifications also might not always have been spot on, so we\u2019d advise against taking mission critical business decisions based on it. The sample represent approximately 1\/6th of the overall programmableweb directory at the time of writing however, so perhaps provides some useful hints to current trends. \n Over the coming weeks, we\u2019ll try to dig down into some of the elements of the study to tease out patterns. \n Follow us on twitter and tweet out your thoughts!\n Thanks to Programmableweb for the hard work in maintaining the directory, feedback on some of our initial data and also their own regular trends analysis!\n Update: there\u2019s a comment thread on hackernews: http:\/\/news.ycombinator.com\/item?id=4079615\n  Data refers to the 1000 APIs registered in Programmable Web between the dates of 17\/1\/2012 and 2\/5\/2012.\n \n  Company Size data was captured from crunchbase, press articles, google searches and company sites.\n \n  Company Business Model information applies to the API in question, companies may have different Business Models for different APIs.","item_date":"Jun 09 2012 19:52:07","display_item_date":"06-09-2012","url":"http:\/\/www.3scale.net\/2012\/06\/surveying-1000-apis-think-apis-are-for-startups-only-think-again\/","source":"www.3scale.net"},{"title":"RESTing Anywhere: Using Kinvey's API with the Lua Programming Language","details":"With our REST API, you can use Kinveys backend service from any language\/runtime that supports HTTP. We currently provide iOS, Android, and Javascript libraries to make it easier for those platforms, but the story doesnt end just there. \n I found out that the guys at Gideros Mobile recently added Kinvey support to their Lua libraries. Their blog post highlighted how easy it was to use Kinvey to write Lua tables to the cloud. \n I wasn\u2019t familiar with Lua, so I decided to learn a bit about it and see how much effort it would be to send data back and forth to a Kinvey backend.\n              \n                                                                                                 Lua is not quite as easy to install and add packages as Node or Ruby, but it wasn\u2019t too difficult. In addition to the base Lua language, to talk to Kinvey you\u2019ll need some additional libraries to handle HTTP requests and JSON. To follow this example, you\u2019ll need Lua and those libraries. These are the steps I used:\n  My example below uses the Lua Socket and Lua Json\u00a0libraries, which can be installed through the luarocks command.\n  Once you have all this built and installed, you\u2019re ready to talk to Kinvey.\n  \n The technique to building a library to talk to our backend is to get the REST request right by setting the appropriate headers, method, and url. Here\u2019s the Lua code to POST a new \u201cJames Bond\u201d entity to our sample app. This is all based on the \u201cTry It Now\u201d samples in our documentation.\n  \n In the above example all the magic is in the http.request command, and the rest is shuffling the data around. The next code snippet will read that same object back from the backend and reconstruct the table.\n  And that\u2019s how to use our API from Lua. If you\u2019re interested in using Kinvey from a real Lua app, I recommend checking out the Gideros library here. They\u2019ve taken on the work of wrapping the HTTP requests and JSON conversion with a higher-level API. \nTo connect Kinvey to a different programming language, use this pattern to build up a library:\n  Take your \u201cobject\u201d data, convert to json\n  Pass to a Kinvey app url with the appropriate credentials\n  Read it back and convert from the JSON string to the \u201cobject\u201d type\n  \n The power of HTTP makes adding a backend to any app that simple! Be sure to sign up here to test it out for yourselves.","item_date":"Jun 09 2012 19:51:17","display_item_date":"06-09-2012","url":"http:\/\/www.kinvey.com\/blog\/item\/169-resting-anywhere-using-kinveys-api-with-lua-programming-language","source":"www.kinvey.com"},{"title":"Census Bureau Developer APIs","details":"To improve access to data and encourage innovation, the Census Bureau has  begun to provide API access to some data sets, beginning with the 2010 Census  Summary File 1 and the 2010 American Community Survey (five year data).   We invite developers to use these beta APIs, join our Developer Forum,  and provide feedback to help us move forward with continued API development.\n  Please read the Terms of Service for using the API.   \nAccessing Data\n  Data is accessible to software developers through a stateless HTTP GET request. In order to access data, you will need to insert a key into the request URL in order for query results to be returned. If you do not have a key assigned to you, you will be directed to the Request a Key page where you can register for one.\n   Here is an example of the request URL for 2010 Census SF1 data  that has an assigned key inserted: \n Here is an example of the request URL for ACS 2010 5 Year data for Total Population for California and New York: \n  Here is an example of the request URL for ACS 2010 5 Year data for Gross Rent as a Percentage of Household Income, 10.0 to 14.9 percent for all counties in CA:    http:\/\/thedataweb.rm.census.gov\/data\/2010\/acs5?key=b48301d897146e8f8efd9bef3c6eb1fcb864cf&get=B25070_003E,NAME&for=county:*&in=state:06 \n  2010 Census SF1 white population of 12 year olds in Alabama","item_date":"Jun 09 2012 19:51:02","display_item_date":"06-09-2012","url":"http:\/\/www.census.gov\/developers\/","source":"www.census.gov"},{"title":"Growing quality apps with the App Center","details":"The App Center helps high-quality apps grow by promoting those that people enjoy the most. It includes all types of social apps, including those built for Facebook.com, iOS, Android, and the web.  People can find apps through their friends, browse by category, or get personalized recommendations. \n  The App Center is available on Facebook.com and in our iOS and Android apps.  If a user is browsing the App Center from their computer, they can easily send a mobile app to their phone.  Whenever a mobile app requires a download, users will be sent to the respective install page in the Apple App Store or Google Play. \n  Growth tied to quality\n The App Center lists apps based on quality signals from users. To see if your app meets the requirements, you can view your status in the App Dashboard.\n  Keep your app detail page up-to-date even if it isn\u2019t currently listed, as it will appear in Facebook search for people who haven\u2019t installed your app.  Your app will be automatically included in the App Center once it meets our quality thresholds. \n  The App Center is beginning to rollout to users in the United States today. We will provide more details on the localization tools and international availability in the coming weeks.\n  The App Center is launching with over 600 apps, including new apps like Ghost Recon Commander, Jetpack Joyride, and Ghosts of Mistwood.  More apps are being added every day. Learn how to include your app or visit the App Center.","item_date":"Jun 09 2012 19:50:49","display_item_date":"06-09-2012","url":"http:\/\/developers.facebook.com\/blog\/post\/2012\/06\/07\/growing-quality-apps-with-the-app-center\/","source":"developers.facebook.com"},{"title":"Oakland to Host the First Solar Hackathon: Call for Developers","details":"The hackathon fever is taking over the Nation. After being hosted at San Francisco, New York City, Boston, Boulder, Santa Clara, now the cleanweb hackathon is coming to Oakland for the first ever solar focussed event.\n Can you build an app in a weekend? Do you thrive in dynamic teams? Are you a creator, visionary and executor all in one?\n  This weekend\u2019s hackathon in Oakland promises to provided another platform for all those hackers looking to intersect the emerging field of clean energy, tech and the internet, also called cleanweb. Like the other hackathons, the competition will run through the weekend where hackers and developers will spend 30 hours developing innovative clean energy apps.\n But the hackathon in Oakland is a little different. It is the first solar-focussed hackathon with sponsors like Sungevity rooting for some solid apps that will accelerate solar energy adoption.\n The Solar Hackathon\u00a0brings innovative programmers, designers, and business minds together for one weekend to develop ways to accelerate solar energy adoption. \u00a0Participants build web-based solutions to make it easier, more efficient, and more enjoyable for people to power their lives with sunshine.\u00a0We are trying to spark the creativity to lead the solar industry into a new era. \u00a0\u00a0To do this we need people who defy old notions and have the ingenuity to play a role brightening our energy future.\n What else is new at the Oakland Solar Hackathon?\n The organizers are looking for two types of participants in each team:\n Type 1: those who can take a problem or a parameter and turn it into a functional idea that can both be used and appeal to judges and consumers like entrepreneurs, business women\/men and creative minds\n  Type 2: those who can take the marketable idea and convert it into marketable product through web design, programing, or the creation of a mobile application like web designers\/developers, app designers\/developers, programmers, graphic designers\n But obviously the competition would love to have people who do not prescribe to these guidelines but can add a different perspective in the development process. \u00a0Once participants register they can sign up with\u00a0Hacker League\u00a0to meet other participants and form balanced teams of 2-3 people.\n Each team will be matched with a coach that will provide one-on-one advice.\u00a0\u00a0Coaches consist of a mix of experienced entrepreneurs, investors, startup incubator members, startup attorneys, and subject experts focused on development, design, and marketing and will\u00a0assist the team in creating a useful, appealing, and polished product.\u00a0.\u00a0Coaches often provide both immediately actionable and long-term advice.\n Speakers\n The event will offer several speeches by prominent community and fields members that can offer invaluable expertise, advice and motivation. In fact, after registration opens on June 8, opening comments will be made by the mayor of Oakland at 7pm.\n Event details!\n5:30 PM Friday, June 8th \u2013 Sunday June 10th\n55 Harrison St, Oakland\n(Quick 10 minute BART or 20 minute Ferry from San Francisco)","item_date":"Jun 09 2012 19:50:34","display_item_date":"06-09-2012","url":"http:\/\/ecopreneurist.com\/2012\/06\/07\/oakland-to-host-the-first-solar-hackathon-call-for-developers\/","source":"ecopreneurist.com"},{"title":"Join the Facebook Team at Our Toronto HACK","details":"Come code with us at our HACK in Toronto on Saturday, June 16th at 99 Sudbury. Developers in Toronto and North by Northeast conference attendees are encouraged to attend.\n  The HACK is a day-long competition where you build new social apps with Facebook. Get started hacking right away, or attend our technical talks on the fundamentals of our APIs, mobile SDKs and Open Graph. Our engineers will be around to answer questions and to help you make the most of our APIs and tools. Well be giving out prizes for Best Overall App, Best Open Graph Integration, Best Game and Best Mobile App.\n  Registration is free. Check out the schedule below and sign up now to secure a spot in the HACK. We hope to see you there!","item_date":"Jun 09 2012 19:50:24","display_item_date":"06-09-2012","url":"http:\/\/developers.facebook.com\/blog\/post\/2012\/06\/07\/join-the-facebook-team-at-our-toronto-hack\/","source":"developers.facebook.com"},{"title":"mHealth apps are just the beginning of the disruption in healthcare from open health data","details":"Two years ago, the potential of government making health information as useful as weather data felt like an abstraction. Healthcare data could give citizens the same blue dot for navigating health and illness akin to the one GPS data fuels on the glowing map of geolocated mobile devices that are in more and more hands. \n  After all, profound changes in entire industries, take years, even generations, to occur. In government, the pace of progress can feel even slower, measured in evolutionary time and epochs. \n  Sometimes, history works differently, particularly given the effect of rapid technological changes. Its only a little more than a decade since President Clinton announced he would unscramble global positioning system data (GPS) for civilian use. President Obamas second U.S. chief technology officer, Todd Park, estimated that GPS data is estimated to have unlocked some $90 billion dollars in value in the United States. \n  In the context, the arc of the Health Data Initiative (HDI) in the United States might leave some jaded observers with whiplash. From a small beginning, the initiative to put health data to work has now expanded around the United States and attracted great interest from abroad, including observers from England National Health Service eager to understand what strategies have unlocked innovation around public data sets. \n  While the potential of government health data driving innovation may well have felt like an abstraction to many observers, in June 2012, real health apps and services are here -- and their potential to change how society accesses health information, deliver care, lowers costs, connects patients to one another, creates jobs, empowers care givers and cuts fraud is profound. The venture capital community seems to have noticed the opportunity here: according to HHS Secretary Sebelius, investment in healthcare startups is up 60% since 2009. \n  Headlines about rockstar Bon Jovi rocking Datapalooza and the smorgasbord of health apps on display, however, while both understandable and largely warranted, dont convey the deeper undercurrent of change. \n  On March 10, 2010, the initiative started with 36 people brainstorming in a room. On June 2, 2010, approximately 325 in-person attendees saw 7 health apps demoed at an historic forum in the theater of Institute of Medicine in Washington, D.C, with another 10 apps packed into an expo in the rotunda outside. All of the apps or services used open government data from the United States Department of Health and Human Services (HHS). \n  In 2012, 242 applications or services that were based upon or use open data were submitted for consideration to third annual Health Datapalooza. About 70 health app exhibitors made it to the expo. The conference itself had some 1400 registered attendees, not counting press and staff, and was sold out in advance of the event in the cavernous Washington Convention Center in DC. On Wednesday, I asked Dr. Bob Kucher, now of Venrock Capital and the Brookings Institution, about how the Health Data Initiative has grown and evolved. Dr. Kucher was instrumental to its founding when he served in the Obama administration. Our interview is embedded below:\n  \n  Revolutionizing the healthcare industry --- in HHS Secretary Sebeliuss words, reformulating Wired executive editor Thomas Goetzs latent data to lazy data --- has meant years of work unlocking government data  and actively engaging the developers, entrepreneurial and venture capital community. While the process of making health data open and machine-readable is far from done, there has been incontrovertible progress in standing up new application programming interfaces (APIs) that enable entrepreneurs, academic institutions and government itself to retrieve it one demand. On Monday, in concert with the Health Data Palooza, a new version of HealthData.gov launched, including the release of new data sets that enable not just hospital quality comparisons but insurance fees as well.\nTwo years later, the blossoming of the HDI Forum into a massive conference that attracted the interest of the media, venture capitalists and entrepreneurs from around the nation is a short-term development that few people would have predicted in 2010 but that a nation starved for solutions to spiraling healthcare costs and some action from a federal government that all too frequently looks broken is welcome.   The immense fiscal pressure driving innovation in the health context actually means belated leveraging of data insights other industries take for granted from customer databases, said Chuck Curran, executive director and general counsel or the  Network Advertising Initiative, when interviewed at this years HDI Forum. For example, he suggested, look at the dashboarding of latent\/lazy data on community health, combined with geographic visualizations, to enable \u201chotspot\u201d-focused interventions, or info about service plan information like the new HHS interface for insurance plan data (including the API).\n  Curran also highlighted the role that fiscal pressure is having on making both individual payers and employers a natural source of business funding and adoption for entrepreneurs innovating with health data, with apps like My Drugs Costs holding the potential to help citizens and businesses alike cut down on an estimated $95 billion dollars in annual unnecessary spending on pharmaceuticals. \n  Curran said that health app providers have fully internalized smart disclosure : it\u2019s not enough to have open data available for specialist analysis -- there must be simplified interfaces for actionable insights and patient ownership of the care plan. \n  For entrepreneurs eying the healthcare industry and established players within it, the 2012 Health Data Palooza offers an excellent opportunity to take the pulse of mHealth, as Jody Ranck wrote at GigaOm this week: \n  Roughly 95 percent of the potential entrepreneur pool doesn\u2019t know that these vast stores of data exist, so the HHS is working to increase awareness through the Health Data Initiative. The results have been astounding. Numerous companies, including Google and Microsoft, have held health-data code-a-thons and Health 2.0 developer challenges. These have produced applications in a fraction of the time it has historically taken. Applications for understanding and managing chronic diseases, finding the best healthcare provider, locating clinical trials and helping doctors find the best specialist for a given condition have been built based on the open data available through the initiative.\n  In addition to the Health Datapalooza, the Health Data Initiative hosts other events which have spawned more health innovators. RockHealth, a Health 2.0 incubator, launched at its SXSW 2011 White House Startup America Roundtable. In the wake of these successful events, StartUp Health, a network of health startup incubators, entrepreneurs and investors, was created. The organization is focused on building a robust ecosystem that can support entrepreneurs in the health and wellness space.\n  This health data ecosystem has now spread around the United States, from Silicon Valley to New York to Louisiana. During this years Health Datapalooza, I spoke with Ramesh Kolluru, a technologist who works at the University of Louisiana, about his work on a hackathon in Louisiana, the Cajun Codefest, and his impressions of the forum in Washington: \n  \n  One story that stood out from this years crop of health data apps was Symcat, an mHealth app that enables people to look up their symptoms and find nearby hospitals and clinics. The application was developed by two medical students at Johns Hopkins University who happened to share a passion for tinkering, engineering and healthcare.  They put their passion to work - and somehow found the time (remember, theyre in medical school) to build a beautiful, usable health app. The pair landed a $100,000 prize from the Robert Wood Johnson Foundation for their efforts. In the video embedded below, I interview Craig Munsen, one of the medical students, about his application. (Notably, the pair intends to use their prize to invest in the business, not pay off medical school debt.)\n  \n  There are more notable applications and services to profile from this years expo - and in the weeks ahead, expect to see some of them here on Radar, For now, its important now to recognize the work of all of the men and women who have worked so hard over the past two years create public good from public data.  \n  Releasing and making open health data useful, however, is about far more than these mHealth apps: Its about saving lives, improving the quality of care, adding more transparency to a system that needs it, and creating jobs. Park spoke with me this spring about how open data relates to much more than consumer-facing mHealth apps: \n  \n  As the US CTO seeks to scale open data across federal government by applying the lessons learned in the health data initiative, look for more industries to receive digital fuel for innovation, from energy to education to transit and finance. The White House digital government strategy explicitly embraces releasing open data in APIs to enable more accountability, civic utility and economic value creation. \n  While major challenges lie ahead, from data quality to security or privacy, the opportunity to extend the data revolution in healthcare to other industries looks more tangible now than it has in years past. \n  Business publications, including the Wall Street Journal, have woken up to the  disruptive potential of open government data As Michael Hickins wrote this week,  The potential applications for data from agencies as disparate as the Department of Transportation and Department of Labor are endless, and will affect businesses in every industry imaginable. Including yours. But if you can think of how that data could let someone disrupt your business, you can stop that from happening by getting there first. \n  This growing health data movement is not placed within any single individual city, state, agency or company. Its beautifully chaotic, decentralized, and self-propelled, said Park this past week.   \nThe Health Data Initiative is no longer a government initiative, he said. Its an American one. ","item_date":"Jun 09 2012 19:50:10","display_item_date":"06-09-2012","url":"http:\/\/radar.oreilly.com\/2012\/06\/mhealth-healthdata-ehealth-innovation-opendata.html","source":"radar.oreilly.com"},{"title":"Speedier Shortening","details":"We mean it when we say we\u2019ve been reading all your tweets and emails. And we\u2019re listening! In the week since our release, we\u2019re already making adjustments, so that saving and shortening links in the new bitly is a whole lot easier. \n Now, when you paste a link into the \u201cPaste a link here\u2026\u201d box at the top of the page, it will be instantly saved when you paste it (no second click needed)\u2014 and the shortlink will be right there ready for you to copy. You can still edit the privacy, notes, or bundles, and of course share it.\n \n For our browser tools (the bitmarklet and Chrome extension), the process is a little different. A common complaint prior to the new bitly was that 1-click saves via our browser tools created too many accidental shortlinks. Now you have the opportunity to confirm your save after your initial click. \nIf you need even more speed though, turn on 1-click saving in the browser tools by opening the dialog\u2019s settings panel and checking the box. Bitmarks added via 1-click will be saved with your default privacy setting.\n \n This new functionality is available on the website today, and we\u2019re rolling out an update to the bitmarklet and Chrome extension over the next 24 hours.\n \n Other Updates Thus Far\nHere are a few more of the many changes we\u2019ve already made in response to what we heard this week:\n Copy and customize buttons on the newly expanded stats page\n Chrome extension will now automatically add highlighted text to the notes field.\n iPhone app 1.01 \u2014 fixes crashes when running on iPad, expands support for creating a new account without Facebook or Twitter.\n Bulk actions got easier with the new dropdown button.\n Gazillions of small bug fixes and UI tweaks.\n \nAnd Keep the Feedback Coming!\n Living things - from earthworms to pufferfish to people - all want to grow and strive to be more than they are. Products need to evolve and grow too, and bitly is no exception. Our goal is to make collecting, finding, and sharing content better for all of our members.\n This relaunch \u00a0is to help more people take advantage of what bitly has to offer and expose useful, but buried, features in the old bitly - features that people wanted but most could not find or use well. We also added features that you have been asking for all along, such as the ability to easily find previously saved links, decide what links are private versus public, collaborate on bundles and more.\n Our team sees every tweet, reads every comment, and will respond directly to all emails that are sent to us. We hear all of you! And we appreciate all your feedback. We\u2019ll keep rapidly iterating to improve and smooth out rough edges based on our conversations with you.\n Keep the feedback coming. Tweet @bitly or shoot us an email at support@bitly.com .","item_date":"Jun 09 2012 19:49:47","display_item_date":"06-09-2012","url":"http:\/\/blog.bitly.com\/post\/24568850534\/speedier-shortening","source":"blog.bitly.com"},{"title":"Should the mainstream media see Twitter as competition?","details":"Twitter unveiled an interesting partnership with the NASCAR auto-racing circuit on Thursday, in which the real-time information network has created a kind of portal for the Pocono 400 race this weekend. The site looks like a fairly normal Twitter feed, with one important difference: in addition to highlighting tweets using an algorithm, an editor hired by Twitter will also be selecting or \u201ccurating\u201d the stream. If that sounds like the kind of thing a media company might do, it\u2019s probably because it is the kind of thing a media company would do \u2014 the NASCAR deal takes Twitter even further into the realm of being a media entity. Should traditional media players be concerned?\n\nTwitter CEO Dick Costolo has repeatedly protested that Twitter doesn\u2019t see itself as a media entity and doesn\u2019t want to be one, and there are some obvious reasons why the company wouldn\u2019t want to be seen that way, including the fact that it has been spending a lot of time negotiating partnerships with existing media players such as ESPN. It would likely be a lot harder to make the case that media companies should see the service as a partner if Twitter was promoting itself as a media company (Costolo may also be concerned that media companies don\u2019t typically get nosebleed valuations the way tech companies do).\n\nDespite the protests, however, there is plenty of evidence that Twitter is a media entity \u2014 not a traditional one, perhaps, but a media company nevertheless. For one thing, its business model relies entirely on advertising around content, which is fundamentally the same model most media companies depend on (Facebook could be seen as a media company for the same reason). The content itself may be entirely user-generated, but then so was much of the content in the early years of the Huffington Post, now a major media player in its own right.\n\nUntil recently, Twitter was just a platform that provided easy access to this real-time content created by others. But deals like the NASCAR partnership \u2014 and offerings like the email summary powered by its Summify acquisition \u2014 are pushing Twitter further and further into the \u201ccuration\u201d business, and that is essentially an editorial function.\n\nSelecting tweets and photos about a car race might not seem like journalism, but what Twitter is doing is very similar to what a site like Huffington Post or even a newspaper or sports site might do with an event like NASCAR. A traditional media outlet might also have a columnist write some thoughts about the race as well or send a reporter down into the pits to interview drivers, but pulling together real-time reactions from those involved and from spectators has also become a big part of the media response to a major event.\n\nIf something like the \u201cArab Spring\u201d revolutions in Egypt were to flare up again, Twitter could quite easily adapt the NASCAR model to such an event (although there wouldn\u2019t be the obvious commercial relationship). An editor or editors could function in much the same way that NPR editor Andy Carvin did during the revolutions in Egypt and Tunisia, curating tweets and even fact-checking photos and videos in real time. That is fundamentally an editorial function, whether Twitter sees it that way or not \u2014 and as Columbia University journalism professor Emily Bell notes, the network is already an important source of news.\n\nI have no idea whether Twitter has any intention of expanding its editorial ambitions, but the NASCAR arrangement could be an interesting precursor to a number of different scenarios: for example, the company could develop (or acquire) tools and services like Storify or Storyful that make it easier to curate and verify real-time news reports, and it could either offer those to existing media players or it could employ them itself with its own news staff. For these reasons and others, blogging pioneer Dave Winer has said he believes that news organizations should absolutely see Twitter as competition \u2014 and suspects that the company might even buy a traditional media player at some point.\n\nThe most compelling reason for Twitter to move into this kind of area is that it could increase the engagement that users have with the network, something that is fairly crucial when it comes to appealing to advertisers. Being a platform and allowing anyone to distribute content through your network is great \u2014 but coming up with reasons why users should spend time on your pages (and look at the ads) is also pretty attractive from a business point of view. Can Twitter do both at the same time? Can it offer itself as a partner for media companies while also tip-toeing into the editorial end of the business?\n\nIf nothing else, the Twitter deal with NASCAR reinforces the fact that for traditional media companies, competition is everywhere \u2014 just as Twitter itself can hire editors to curate and aggregate content, so brands and advertisers like the auto-racing entity are becoming publishers and content creators in their own right, with all the same tools that media outlets have at their disposal. Everyone is a media entity now.","item_date":"Jun 09 2012 03:58:11","display_item_date":"06-08-2012","url":"http:\/\/gigaom.com\/2012\/06\/08\/should-the-mainstream-media-see-twitter-as-competition\/","source":"gigaom.com"},{"title":"Business Challenges Solved by APIs: What Every Executive Should Know","details":"It seems like everyone either has or needs an API these days. Analysts are touting how critical it is for businesses to engage their customers across every touchpoint. APIs are supposed to fuel innovation for these new applications, new channels, and, ultimately, new ways to generate revenue.\n  But getting your API strategy off the ground could be more difficult than it sounds. Even with the right strategy and architecture, there are very real technical challenges in your way.\n  Join API experts from Elastic Path Software as well as API Evangelist Kin Lane for a frank discussion on the current state of APIs.\n  Webinar takeaways:\n \nLearn how APIs are critical to the way consumers engage with businesses today\n \tUnderstand how your current challenges can be solved by a smart API strategy\n \tAppreciate what technical roadblocks await your team, and how to solve them\n \n Elastic Path is the leader in digital commerce technology and expertise for enterprises selling digital goods and content. Major global brands such as Google, Time Inc, and Virgin Media rely on Elastic Path to monetize digital relationships with their customers in ways that are frictionless, social, and everywhere.","item_date":"Jun 09 2012 03:39:55","display_item_date":"06-08-2012","url":"http:\/\/www.elasticpath.com\/webinar\/api-for-execs-resource","source":"www.elasticpath.com"},{"title":"Business Challenges Solved by APIs: What Every Executive Should Know","details":"It seems like everyone either has or needs an API these days. Analysts are touting how critical it is for businesses to engage their customers across every touchpoint. APIs are supposed to fuel innovation for these new applications, new channels, and, ultimately, new ways to generate revenue.\n  But getting your API strategy off the ground could be more difficult than it sounds. Even with the right strategy and architecture, there are very real technical challenges in your way.\n  Join API experts from Elastic Path Software as well as API Evangelist Kin Lane for a frank discussion on the current state of APIs.\n  Webinar takeaways:\n \nLearn how APIs are critical to the way consumers engage with businesses today\n \tUnderstand how your current challenges can be solved by a smart API strategy\n \tAppreciate what technical roadblocks await your team, and how to solve them\n \n Elastic Path is the leader in digital commerce technology and expertise for enterprises selling digital goods and content. Major global brands such as Google, Time Inc, and Virgin Media rely on Elastic Path to monetize digital relationships with their customers in ways that are frictionless, social, and everywhere.","item_date":"Jun 09 2012 03:38:25","display_item_date":"06-08-2012","url":"http:\/\/www.elasticpath.com\/webinar\/api-for-execs-resource#.T9LFIcuIvRw.twitter","source":"www.elasticpath.com"},{"title":"What every executive needs to know about API technology","details":"Yes, it seems like a very technical subject best left in the hands of your IT department. \n  But just as the web and mobile jumped quickly from sideshow to center stage in sales and marketing, how good your APIs are will ultimately have a significant impact on your bottom line \u2013 and potentially, the future of your business.\n  Here\u2019s what you need to know about them.\n     What exactly am I trying to solve?\n  By now, you\u2019re probably well aware of the market forces and trends leading every industry into an era of disruptive digital selling. Emerging products and services are exerting tremendous pressure on companies to innovate, differentiate, and extend the reach of their offerings \u2013 whether functionally, demographically, or geographically. At the same time, aggressive competition is driving down margins, while rapidly changing consumer attitudes, preferences and behaviors create a moving target for marketers and product strategists.\n  To succeed in this environment, any digital selling that your enterprise undertakes must obviously be frictionless, relevant, and pervasive to consumers. But on your end, it must also be fast, scalable, and cheap. Cheap to try, cheap to deploy, and cheap to shut down if specific projects fail (and they will).\n  This is where API technology excels.\n  How do APIs solve these challenges?\n  In theory, a good set of APIs transforms your big, old-fashioned ecommerce platform into a sleek engine that delivers commerce-as-a-service. This means that all of the advanced capabilities you\u2019ve come to rely on \u2013 promotions, personalization, offer management, shopping carts, and so on \u2013 are no longer locked away within a single platform, but instead are available for use by any application or developer.\n  In this perfect world, if you can imagine or build a customer experience, you can quickly loop it into your existing ecommerce, business intelligence, and CRM systems by calling on APIs instead of hacking away at the underlying software.\n  A great API allows you adapt quickly and cheaply to a shifting market, but in a scalable and sustainable way, rather than constantly making difficult and expensive changes to enterprise software. This magical ecosystem looks something like this:\n  \n  Aren\u2019t my IT guys taking care of it?\n  Unfortunately, this is where the promise of APIs crashes headlong into their present reality.\n  If you put yourself in the shoes of most enterprise IT or commerce platform technologists, their number one priority is not to build out this magical ecosystem for your business, but simply to ensure that the underlying functionality they own is exposed for use by third parties. This may be a subtle distinction, but it\u2019s the difference between a good API that gets the job done, and a great one that works like a superpower.\n  Many existing APIs do a terrible job at making it easy for other applications to actually access business functions. Some are good. Almost none are great. When left in the hands of IT, the typical API ecosystem ends up looking something like this:\n  \n  It\u2019s a bottom-up view of APIs from the perspective of the code, rather than a top-down one where the potential applications are king. Where underlying functionality is merely exposed, creating a viable customer experience out of those functional pieces requires serious time, money, skill, and deep, in-depth knowledge of the software that hosts those capabilities.\n  This defeats the purpose of having an API in the first place, and does little to alleviate the time, effort, or cost needed to conduct disruptive commerce.\n  Compare the promise with the reality. The only difference between the two is in the quality of the commerce API, and how easily independent developers can create products and services from your core capabilities. This is how API technology can either make or break your entire business strategy. \u00a0\u00a0\n  How can I make this happen?\n  Independent API evangelist Kin Lane has seen first-hand how API programs at major companies have resulted in both spectacular successes and failures. He stresses that the creation of an API strategy must be a coordinated effort between business stakeholders and technologists, because API technology decisions have moved beyond the IT sphere to affect the very ability of an enterprise to respond effectively to the market. If you\u2019d like to learn more about this, check out my recent conversation with him in Business Challenges Solved by APIs.\n  Just like with ecommerce and mobile that came before, business executives can no longer consider APIs to be a sideshow best left to IT. They must get involved in understanding, making decisions, and ultimately owning what will be the next big chapter in digital commerce and marketing.\u00a0\n                 David Chiu is the digital commerce strategist at\u00a0Elastic Path\u00a0and a guest blogger for Econsultancy.\n       \n         Online Communities is a multipart series that guides marketers through the construction, upkeep, and leverage of a digital community. This report, Part One, Starting A Community explores the foundational concepts of community, the advantages for companies who take the time to build them, and explores the preliminaries of using the largest community-facilitating digital services.\n       \n         Econsultancys Marketing Attribution: Understanding Value Across the Customer Journey, sponsored by Google Analytics, is a survey and collection of\u00a0interviews with marketers about the benefits\u00a0 of attribution across multiple channels, various approaches and technologies used, and the keys to success.","item_date":"Jun 09 2012 03:35:52","display_item_date":"06-08-2012","url":"http:\/\/econsultancy.com\/us\/blog\/10072-what-every-executive-needs-to-know-about-api-technology","source":"econsultancy.com"},{"title":"Thomson Reuters acquires Apsmart","details":"Mobile is creating huge opportunities for publishers, but building mobile applications isnt the forte of most publishing organizations.\n  So a growing number of companies are turning to M&A to acquire the skills they need to make progress with mobile initiatives.\n     The latest example of this: today, Thomson Reuters announced the acquisition of London-based Apsmart for an undisclosed amount.\n  Founded in 2009, Apsmart was founded by venture capital firm DN Capital and the creator of the original Shazam iPhone application, Rahul Powar. The companys focus: building connected, data-driven applications for iOS and Android devices.\n  Thomson Reuters is no stranger to mobile and has a number of product offerings with mobile components, but the company obviously felt a greater need for expertise in this area. According to the companys press release:\n    The acquisition of Apsmart will enhance Thomson Reuters mobile product creation, design and development, allowing the company to deliver even more expert-enriched content, news and solutions through the interfaces that professionals want on the mobile devices they use.\n    As TheNextWebs Robin Wauters notes, the Financial Times acquired an app developer, Assanka, earlier this year, suggesting that we may be seeing more acquisitions like this in the future.\n  While mobile is a huge market and the most successful of app developers can afford to remain independent, the reality is that competition is fierce and for many upstarts, being acquired and working for a large publisher with a huge customer base is a very decent outcome.\n                 Patricio Robles is a tech reporter at Econsultancy. Follow him on Twitter.","item_date":"Jun 09 2012 03:30:33","display_item_date":"06-08-2012","url":"http:\/\/econsultancy.com\/us\/blog\/10076-thomson-reuters-acquires-apsmart","source":"econsultancy.com"},{"title":"Join Retailigence At The Big Brand Hackathon","details":"Top mobile developers will be converging in San Francisco on June 16th and 17th to compete in The Big Brand Hackathon sponsored by Kraft Foods and The Home Depot.\n At the event, developers will be working to turn marketing concepts into code and ideas into apps. Prizes will be awarded for best use of API\u2019s as well as two 1st prizes of $8,000 for best application featuring Kraft and The Home Depot.\n Retailigence will be making its local product search API freely available to participating teams. Developers and spectators are both welcome at the event. You can purchase your $10 tickets via the Hackathon\u2019s Eventbrite page.\n We hope to see you there!","item_date":"Jun 09 2012 03:27:27","display_item_date":"06-08-2012","url":"http:\/\/www.retailigence.com\/blog\/join-retailigence-at-the-big-brand-hackathonjoin-retailigence-at-the-big-brand-hackathon\/","source":"www.retailigence.com"},{"title":"Census to launch API for demographic, economic app builders next month","details":"The Census Bureau plans to launch an application programming interface in the next month that will stream its data straight to developers, Stephen Buckner, director of the bureau\u2019s Center for New Media and Promotion, said Monday.\n\nDevelopers will be able to use data from the APIs to build apps that help homebuyers find neighborhoods with similarly aged children or help restaurants, movie theaters and bowling alleys find the prime locations for their target audiences, he said.\n\nThe API, which officials are testing now, will include 2010 census information, plus data from the bureau\u2019s American Community Survey, he said.\n\nBuckner was speaking at a panel discussion on mobile technology at the Management of Change conference sponsored by American Council for Technology-Industry Advisory Council, a government technology industry group.\n\nThe Census Bureau also is working on a handful of mobile apps, he said. The first app, called America\u2019s Economy, will display much of the economic data Census gathers. Later apps may list local demographic data, which will be updated based on the smartphone or tablet\u2019s location.\n\nIn the long run, Buckner said, he wants the bureau to be a source for mobile data, not an app builder.\n\n\u201cWe can\u2019t be in the business of developing mobile apps forever,\u201d he said. \u201cThat\u2019s not in the government\u2019s interest. The creativity about the way people want to use our data is outside government, not inside. So we want to expose core things that show the range of things you can do with our data.\u201d\n\nThe 2010 census marked the first time temporary workers collected information on handheld computers rather than on paper, Buckner said. The workers also used the computers to link census data with Global Positioning System tags, he said, and to fill out and submit their timesheets, saving the bureau time and money.\n\n2010 is just a starting point, though, he said. By 2020, citizens may be filling out census forms on mobile devices, themselves.\n","item_date":"Jun 08 2012 02:53:36","display_item_date":"06-07-2012","url":"http:\/\/www.nextgov.com\/emerging-tech\/2012\/06\/census-launching-api-demographic-economic-app-builders-next-month\/56065\/","source":"www.nextgov.com"},{"title":"Helping Monetize The Mobile Web","details":"Game developers building on the mobile web have been able to use Facebook Payments to monetize their apps for some time. A full list of supported countries can be found here.\n  However, this flow for mobile payments can be long, with a total of up to seven steps, including SMS verification.  At Mobile World Congress this year we announced an improved mobile payments flow for mobile web apps. Today we are starting to roll it out. This low friction (two steps) carrier billing is now available on the majority of carriers in the US and the UK and will be rolled out to additional operators worldwide.\n  The payment flow is simple. Users who want to pay for a virtual or digital good in a mobile web app open the payment dialog and confirm their purchase.  This payment flow requires no typing and looks like this:\n  Developers who already integrated Facebook Payments on the mobile web don\u2019t need to do anything to benefit from this new flow.  Mobile web developers interested in adding Facebook Payments to their mobile website can integrate through our payments API. Instructions and tips can be found here. To see the new payment flow in action, check out SkyScraper City on m.facebook.com","item_date":"Jun 06 2012 21:38:56","display_item_date":"06-06-2012","url":"http:\/\/developers.facebook.com\/blog\/post\/2012\/06\/06\/helping-monetize-the-mobile-web\/","source":"developers.facebook.com"},{"title":"Innovator Spotlight: XO Group","details":"XO Group Inc., formerly The Knot Inc., is a lifestage media and technology company dedicated to providing information, products, and services to couples planning their weddings and future lives together. Trading on the NYSE under XOXO, the XO Groups flagship brand is The Knot, the Internet\u2019s most-trafficked one-stop wedding planning solution. Other brands include: The Nest, The Bump, WeddingChannel, and GiftRegistry360.com. \n\n Jason Sirota, XO Group\u2019s chief architect spoke with Apigee about how XO Group is using APIs to simplify the architecture and integration of their internal systems as well as expand reach to their trusted partners.\n\nWhat is XO Group\u2019s API strategy?\n\n XO Group uses APIs across all of our brands. We use APIs specifically for communicating internally between different platforms, mostly for data management and data movement. Some examples include our content management system that needs to access information from our tools platform or our local platform where it retrieves that information via HTTP APIs. We usually put Apigee in between those two systems to allow us to monitor and control the access between these systems at a high level.\n\n We also use APIs for our external partners - our API strategy is divided between internal communication and trusted partners. From a technical perspective, we don\u2019t use SOAP at all. For our API strategy, we decided to use a web-based XML or JSON over HTTP approach.\n\n What problem were you trying to solve with your API strategy?\n\n Our problem would manifest as we would develop a system, especially a database, and that database would over time be accessed directly by a number of applications across our enterprise. When somebody wanted to retire an application they would not realize that the database layer was actually called by a number of different applications. For example, shutting down a database would throw a major amount of errors on some part of our site. What we realized very quickly was that sharing information at the database connection level wasn\u2019t going to scale to the eight to twelve verticals we were planning on releasing.\n\n In 2007-2008, we experienced a quadrupling in the amount of our content. We went from having one to two vertical teams talk back and forth to each other without much problem to having eight or nine vertical teams where they all needed to exchange data, but doing so at the database level was really brittle. So we moved toward an approach to have the ability for individual teams to expose services over HTTP. The team knows that if they want to shut down that application they will also need to shut down that service.\n\n What APIs are you exposing to your partners?\n\n We have exposed access to our membership system to allow users to create and access membership information on some of our trusted partner sites. The other area we\u2019ve been able to successfully expose is all of our mobile applications. For example, our mobile apps on iPhone use only our HTTP APIs to access data. The mobile development of iPhone apps is driving the need for APIs and the inter-connectedness of our verticals as mobile becomes more the focus of our business.\n\n How do you work with Apigee\u2019s API platform?\n\n We put Apigee in the middle \u2013 in between the access layer and the database layer. This helps us understand what the usage patterns are for any particular platform and how that platform is being accessed. We also added the capability of adding or removing access to individual applications at the API layer and also at the action level. This allows us to say, this set of APIs can be accessed by these applications but this other set of APIs cannot. This was very easy for us to do with Apigee. We didn\u2019t have to develop an entire security layer because a security layer was already built in. Apigee\u2019s API platform provides us the capability to understand where our performance bottlenecks exist. For example, when we launch access to an API from a particular application and if we notice that application is seeing significant traffic, we have the ability to easily add client-side caching. Apigee allows us to look at the inter-connectedness of our systems and be able to monitor and manage them.\n\n What benefits have you seen with APIs?\n\n On the business side, we look at media and the user experience. On the technical side, we focus on being able to move quickly and offer new experiences at a rate that our customers want. The big benefit for the business is not having to worry about rebuilding things. So if we create APIs as part of our standard part of our development process, we can meet the specific business needs of our partners who need access to our data. With Apigee it\u2019s as simple as creating a key and setting up the correct access.\n\n What is your vision for your API Program?\n\n Our vision is to reach a state where all of our systems are built completely from APIs. We would like to be able to rebuild the front-end of a system without having to change the data platform. We want to do this in a tiered format so that we can segment our systems and build interesting user and customer experiences without having to go all the way down the stack. Our goal and philosophy for our API program is to have everything we do be exposed as a service.","item_date":"Jun 06 2012 21:38:34","display_item_date":"06-06-2012","url":"http:\/\/blog.apigee.com\/detail\/innovator_spotlight_xo_group\/","source":"blog.apigee.com"},{"title":"Beat the Traffic Paves a Social Road for Transportation","details":"In 2011, we learned that even our cars and trucks were going \u201csocial\u201d when Salesforce.com announced its partnership with Toyota. However, the social capabilities of the partnership were limited in scope (car diagnostics, tune up reminders, etc.). What if your car could prevent you from driving a certain route because it knew what areas were traffic-heavy? That car sounds \u201csocial\u201d in a fuller sense; it might be possible with the new\u00a0Beat the Traffic\u00a0API.\n\nBeat the Traffic debuts its new API at Telematics Detroit 2012, in the US\u2019 biggest auto city, among 1,800+ executive attendees. Andre Gueziec, CEO of Triangle Software (makers of Beat the Traffic),\u00a0commented on the release: \u201cWe\u2019re looking forward to being in the automotive capital of the universe and learning from visionaries in the world of telematics.\u201d The API allows developers to integrate real-time traffic data into the website, application, or even vehicle of choice. With over 3,300 traffic incidents reported each day across almost 600,000 miles of road, Beat the Traffic\u2019s data set alone is of premium value.","item_date":"Jun 06 2012 21:38:02","display_item_date":"06-06-2012","url":"http:\/\/blog.programmableweb.com\/2012\/06\/06\/beat-the-traffic-paves-a-social-road-for-transportation\/","source":"blog.programmableweb.com"},{"title":"Big Data Philanthropy for Humanitarian Response","details":"My colleague Robert Kirkpatrick from Global Pulse has been actively promoting the concept of \u201cdata philanthropy\u201d within the context of development. Data philanthropy involves companies sharing proprietary datasets for social good.\u00a0I believe we urgently need big (social) data philanthropy for humanitarian response as well. Disaster-affected communities are increasingly the source of big data, which they generate and share via social media platforms like twitter. Processing this data manually, however, is very time consuming and resource intensive. Indeed, large numbers of digital humanitarian volunteers are often needed to monitor and process user-generated content from disaster-affected communities in near real-time.\n \n Meanwhile, companies like Crimson Hexagon, Geofeedia, NetBase, Netvibes, RecordedFuture\u00a0and\u00a0Social Flow\u00a0are defining the cutting edge of automated methods for media monitoring and analysis. So why not set up a Big Data Philanthropy group for humanitarian response in partnership with the Digital Humanitarian Network? Call it Corporate Social Responsibility (CRS) for digital humanitarian response. These companies would benefit from the publicity of supporting such positive and highly visible efforts. They would also receive expert feedback on their tools.\n This \u201cEmergency Access Initiative\u201d could be modeled along the lines of the International Charter\u00a0whereby certain criteria vis-a-vis the disaster would need to be met before an activation request could be made to the Big Data Philanthropy group for humanitarian response. These companies would then provide a dedicated account to the Digital Humanitarian Network (DHNet). These accounts would be available for 72 hours only and also be monitored by said companies to ensure they aren\u2019t being abused. We would simply need to \u00a0have relevant members of the DHNet trained on these platforms and\u00a0draft the appropriate protocols, data privacy measures and MoUs.\n I\u2019ve had preliminary conversations with humanitarian colleagues from the United Nations and DHnet who confirm that \u201cthis type of collaboration would be see very positively from the coordination area within the traditional humanitarian sector.\u201d On the business development end, this setup would enable companies to get their foot in the door of the humanitarian sector\u2014a multi-billion dollar industry. Members of the DHNet are early adopters of humanitarian technology and are ideally placed to demonstrate the added value of these platforms since they regularly partner with large humanitarian organizations. Indeed, DHNet operates as a partnership model. This would enable humanitarian professionals to learn about new Big Data tools, see them in action and, possibly, purchase full licenses for their organizations. In sum, data philanthropy is good for business.\n I have colleagues at most of the companies listed above and thus plan to actively pursue this idea further. In the meantime, I\u2019d be very grateful for any feedback and suggestions, particularly on the suggested protocols and MoUs. So I\u2019ve set up this open and editable Google Doc for feedback.\n Big thanks to the team at the\u00a0Disaster Information Management Research Center (DIMRC) for planting the seeds of this idea during our recent meeting. Check out their very neat Emergency Access Initiative.","item_date":"Jun 06 2012 07:14:00","display_item_date":"06-06-2012","url":"http:\/\/irevolution.net\/2012\/06\/04\/big-data-philanthropy-for-humanitarian-response\/","source":"irevolution.net"},{"title":"University of Waterloo Open Data API","details":"All calls are made to the following URL with the required parameters for a given service. \nCurrently, the API is in beta mode and is restriced to 3500 calls per month for a given key. \nThis number may increase in the future.\nIf you have any inquiries or suggestions, please feel free to contact us at \nUpdates\nRequest Limit is now 3500 (Feb 14)\nAPI Signup now open to everyone (Feb 14)\nWatPark data added (Feb 10)\nAPI Moved to \/public\/v1\nJSONP Callback Added (Nov 22)\nFall 2011 Exam Schedule Added (Nov 18)\nAPI Moved to University of Waterloo Servers (Nov 7)\nAPI Adopted by the University of Waterloo. (Oct 4)\nInitial Launch. More methods to follow. (Aug 4)\nThis site is not now affiliated with University of Waterloo\nA project by Kartik Talwar","item_date":"Jun 06 2012 06:07:41","display_item_date":"06-05-2012","url":"http:\/\/api.uwaterloo.ca\/","source":"api.uwaterloo.ca"},{"title":"Programmatic Access to AWS Billing Data","details":"You can now access your AWS billing data programmatically. This has been a much-requested feature and Im confident that it will be put to good use right away.\n\n  To get started, all you need to do is to provide an Amazon S3 bucket for your billing data, give the AWS Billing system permission to write to it, and visit the Billing Preferences page to enable programmatic access:\n\n Once you have done this, we will generate an Estimated bill several times per day and store it in the bucket, where you can download and process it as desired. We will also generate a Final bill at the conclusion of each billing period.\n\n  Billing Reports are generated as CSV files and include plenty of details:\n\n  \n\n  Here is a list of the fields (read the documentation for more information):\n\n  \n\nAs you can see above, several of the fields make sense only in the context of Consolidated Billing. This option, which is used by many of our enterprise customers, allows payment for a number of AWS accounts to roll up to a designated payer account (see my blog post for more info), while also making the AWS volume tiers easier to reach.\n\n  The reports are stored in the S3 bucket of your choice, and you are responsible for all storage costs. You can delete the reports after you process them, and you may want to use S3s Object Expiration feature to keep things under control.\n\n  Im really looking forward to seeing the apps emerge that take advantage of this data. If you create one, leave a comment!","item_date":"Jun 06 2012 06:05:48","display_item_date":"06-05-2012","url":"http:\/\/aws.typepad.com\/aws\/2012\/06\/new-programmatic-access-to-aws-billing-data.html","source":"aws.typepad.com"},{"title":"New and Updated Developer Documentation for Summer \u201912","details":"For your reading pleasure, we\u2019ve updated the developer documentation on developer.salesforce.com and database.com. It reflects the new Summer \u201912 Salesforce release (API version 25.0). In addition, there are some new docs you might want to check out.\n The Open CTI Developer\u2019s Guide is now available. This guide describes how to use Open CTI to build computer-telephony integration systems that integrate with Salesforce.\n Customizing Case Feed with Visualforce is now available. This guide describes how to customize Case Feed with Visualforce components.\n The REST Metadata API Developer Guide is now available. This guide describes the Metadata REST API, a RESTful API for retrieving and deploying setup information in your organization. Note that this feature is currently pilot. To sign up for the pilot, contact salesforce.com.\n The SOQL and SOSL Reference is now available for both Force.com and Database.com. This reference information was formerly part of the SOAP API Developer\u2019s Guide and has been moved into its own reference guide.\n \nAlso, the Web Services API Developer\u2019s Guide has been renamed to SOAP API Developer\u2019s Guide to avoid confusion with other Salesforce Web service APIs, such as the REST API.\n Let us know what you think! Also, remember that earlier versions of the documentation are always available at http:\/\/wiki.developerforce.com\/page\/Earlier_Reference_Documentation (and at http:\/\/devcenter.database.com\/page\/Earlier_Reference_Documentation for Database.com).\n   \t\t\t\t\t\t\t\t\t\t\t\n  \t\t\t\t\t tagged Announcements, API, documentation, Summer 12Bookmark the permalink. Trackbacks are closed, but you can post a comment.\nTweet \n  \n                                              Today some part of the new  REST Metadata API Developer Guide is confusing me. I developer my page goo.gl\/duj9q using api version 24.0 . Demo video @ http:\/\/goo.gl\/5Oi9x , I will have to study this new document to understand the changes related with V25.0","item_date":"Jun 06 2012 06:04:22","display_item_date":"06-05-2012","url":"http:\/\/blogs.developerforce.com\/tech-pubs\/2012\/06\/new-and-updated-developer-documentation-for-summer-%e2%80%9912.html","source":"blogs.developerforce.com"},{"title":"Oracle Sues Patent Troll for Behaving Like Oracle","details":"Fresh off its failed attempt to prove that Google\u2019s Android operating system infringed on its Java patents, Oracle has sued a small company called Lodsys, complaining that the Texas-based outfit has been harassing its customers with questionable claims of patent infringement. \n \u201cLodsys has repeatedly threatened numerous Oracle customers with assertion of the Patents-in-Suit against Oracle\u2019s Web Commerce Products,\u201d Oracle\u2019s suit read (.pdf). \u201cLodsys is not entitled to any royalties from Oracle or any of its customers, nor does Oracle or any of its customers need a license to the Patents-in-Suit.\u201d\n Lodsys has made a name for itself in recent years with its efforts to pressure independent iOS and Android application developers into paying licenses fees for use of four patents it owns. A year ago, it sued a group of small developers in  the patent-friendly Eastern District of Texas.\n In bizarre fashion, it told the developers that if its claims were wrong, it would pay them $1,000 apiece.\n Apparently, Lodsys is also going after Oracle customers. The database giant says that its suit is an attempt to disarm Lodsys so that it will stop harassing Oracle\u2019s customers with letters, phone calls and emails about obtaining a license to its four patents. \n Walgreens and REI, among other Oracle customers, were recently named as defendants in a suit brought by Lodsys.\n Oracle drew scorn for its claims that Google infringed on two of its Java patents in building Android, and in the end, a jury shot down the claims. \n In the suit against Lodsys, Oracle filed eight claims against four patents: U.S. patents 5,999,908, 7,133,834, 7,222,078 and 7,620,565.\n Lodsys can best be described as a shell company, saying on its homepage that these four patents are the core of its business. The company does not make any products but says that its goal is to \u201cembrace and empower invention by supporting an Innovative Economy\u201d by licensing its patents.\n \nGot a secret? Email caleb_garling [a] wired.com. Caleb covers tech, but loves other stuff like sports, fiction, beer, fun in remote places and music featuring guitars. Encircle on Google+, subscribe on Facebook or\nRead more by Caleb Garling \nFollow @calebgarling on Twitter.","item_date":"Jun 05 2012 20:32:49","display_item_date":"06-05-2012","url":"http:\/\/www.wired.com\/wiredenterprise\/2012\/06\/oracle-patent-troll\/","source":"www.wired.com"},{"title":"Twilio Celebrates 100,000 Developer Accounts","details":"Were celebrating the DOers and heroes changing the way we communicate. In celebration of what you do we want to highlight the milestones youve achieved. Share your milestone moment with us in a short video to participate in a special feature debuting at TwilioCon this fall.  \t\t\nThree DOers will be chosen to join us at TwilioCon - all expenses paid - to share their stories in person. Read the contest details. \n  \t\n  \t\tWe want you to join the community and share your story at TwilioCon this year so we will randomly select three people to do just that in an all expenses paid trip to TwilioCon 2012. This package includes three nights in a hotel in San Francisco, a flight stipend and the full conference pass. Submissions will be accepted from Monday June 4th until 12:01am Tuesday July 13th. Read submission rules for more details. Contest void where prohibited.","item_date":"Jun 05 2012 19:35:39","display_item_date":"06-05-2012","url":"http:\/\/ahoy.twilio.com\/milestones","source":"ahoy.twilio.com"},{"title":"API Business Models Take Center Stage","details":"The number of public APIs that are available has been growing at a phenomenal pace with our directory now clocking 6000+ APIs. While this number is definitely going to keep increasing, it is also important to understand the reasons why organizations want or should have an API in the first place. API\u2019s are increasingly getting associated with business models and not just using technology to expose some information.\n\n\nIn a talk at The Next Web Conference, Adam DuVander spoke about an API being three key things: Apps, Partners and Income (video). As a business model, you want your product to enable developers to write apps on top of it, push it across to as many partners or channels, which in turn will mean more business for you. In other words, APIs are now being seen in the larger context of how they could enable new business models on the Web, rather than just looking at them as a technology.\n\n\nA recent survey by Vordel, a Cloud Application Gateway provider, has also confirmed the above premise. The survey asked participants of a webinar as to why their enterprises were adopting APIs. The results shown below indicate that half the respondents adopting APIs to build out new business channels.\n\n\n\n\n\nThe survey also threw up another interesting data point: More than 75% of the organizations have appointed dedicated Managers\/Architects to oversee the API strategy.\n\n\nAPIs are now a key part of an organization strategy and the API business models are much diverse in range than they were before. For a great overview of API Business Models in 2012, check out John Musser\u2019s Open APIs: What\u2019s Hot, What\u2019s Not? Presentation at Gluecon 2012 and find out the right one for your organization.","item_date":"Jun 05 2012 19:02:32","display_item_date":"06-05-2012","url":"http:\/\/blog.programmableweb.com\/2012\/06\/05\/api-business-models-take-centre-stage\/","source":"blog.programmableweb.com"},{"title":"An Architecture of Participation","details":"What happens when half of the worlds population lives in cities? When over 3 billion people are online? When there are more than 15 billion connected devices? - Old organizational models hit end of life. People behave differently. Organizations behave differently. What worked in the old world doesnt work in the new.\n  Through the ages, people have collaborated around common goals. Joint creation and joint production are no new ideas. It could be argued that the old religious scriptures were crowdsourced. Most other activity back then was strictly controlled by a ruling leader or harsh environmental conditions. But when people engaged in what for them was the newest and most intriguing topics, they worked together. They collaborated.\n  What is changing now is that participatory models are becoming the rule, not the exception. The world used to be about command and control. Someone told you what to do. There still is a lot of that. But collaborative innovation is taking over. We are coming to a stage in our civilization where regular functions are masterfully automated and industrialized, and our focus as human beings can and will  increasingly be on innovation. In the area of innovation, the most powerful creation happens in teams, groups and crowds - across organizational boundaries. When we architect for such participation, we can multiply the power of innovation.\n  Linus Torvalds stumbled over this mechanism over 20 years ago. In an act that was part abandonment, part invitation, he somewhat unknowingly threw out an intriguing challenge to software developers all over the world: Work with me to build a free operating system. And people did - willingly, spontaneously, and brilliantly. Soon, a number of free and open source software projects were defining the Architecture of Participation - a model for how to engage people with different ambitions, different mandates, different employers (or no employer) and different communication habits in joint projects that unpredictably but inevitably produce superior results.\n  Thats the essence of the Architecture of Participation. You construct rules of engagement that allow disagreeing people to let their work products agree. A system where the designer invites input from contributors. The end result is an ecosystem that evolves fast than any individual initiative, resulting in a work product with fewer deficiencies.\n  The Architecture of Participation is more than open, and more than crowdsourcing. Open, strictly speaking, means that you share your production with others. It doesnt necessarily mean participation. Crowsourcing means many people contributing their production. It doesnt necessarily mean that they would exchange value with each other. Its not enough to be open and its not enough to crowdsource. We must build an architecture of participation where different participants with different agendas can exchange ideas and models. Its not easy to do that, but it also is not impossible.\n  A beauty of a well-functioning architecture of participation is that there is no significant distinction or conflict between the public good and the private good. Its just good. Its good for each one, and it is good for all. It does not matter whether there are free riders and freeloaders in the system, because the moment they take any action whatsoever, they become at least marginally useful to the entire system.\n  Millions of freeloaders providing a marginal benefit amounts to much more than a small number of contributors each providing a big benefit. This is why the size of the ecosystem matters. With 3 billion people on the internet, freeloaders are more abundant and more useful than when we had just 3 million people on the internet (which was approximately the time when the Linux project started).\n  This is why the architecture of participation is now overtaking systems of command and control.  The volumes are so large that any attempt to be fully in control inevitably leads to a group too small to have a meaning. The people you can control are vastly outnumbered by the people you can only influence, but not control.\n  Let us also be clear that the Architecture of Participation is not anarchy and also not democracy. Every architecture of participation has an architect. There is a steward of the project. The steward can be a single individual (think Linus Torvalds), a team (think the creators of the Apache web server) or a company (think MySQL AB). The steward of the project sets the rules of engagement. If the rules are too strict or egregious, people will not participate. If there are no rules, people will not know how to participate. In the ideal architecture of participation, there is a steward of the project that sets priorities and design goals and that ensures that the field is open for participation by anyone and everyone. To scale collaboration, it makes sense to create useful interfaces - useful APIs that allow individual initiatives to evolve at their own pace while interacting with each other through the agreed interface.\n  Architectures of participation exist all over the technology sector today. Its not any longer just about open source. Wikipedia brings together those who can express facts and concepts in writing. Facebook brings together those who can express their daily lives. oDesk and the Mechanical Turk bring together those who have work capacity to provide to others.  Kiva.org brings together those who have a penny to spare for someone who is working hard. Twitter brings together those who can express useful information very briefly. The Human Genome Project brings together insight about DNA.  The Khan Academy brings together the best in educational practices. The Linux Foundation continues to bring together those who can express computer behavior in the form of kernel code.\n  We are only in the early stages of the architecture of participation. Cloud computing is a participatory endeavor. The mobile application space will soon explode with participation. Large traditional corporations are launching social initiatives and participatory fora. National governments are opening up for citizen participation. The list is longer.\n  The ideal architecture of participation combines the best of ownership of design with the best of collaboration by the masses. If you have no architect, you have no participation. But if you have no participation, it matters little what the architect does. When the architect (a person or a team) is a master of the trade and also a welcoming recipient of contributions and participation, amazing results ensue.\n  M\u00e5rten Mickos","item_date":"Jun 05 2012 18:42:16","display_item_date":"06-05-2012","url":"http:\/\/www.eucalyptus.com\/blog\/2012\/06\/05\/architecture-participation","source":"www.eucalyptus.com"},{"title":"Reaching a Million APIs and What to do When We Get There","details":"This guest post is by Steven Willmott. Steven is the CEO of\u00a03scale networks , a company that provides infrastructure services for over 100 APIs. The content draws on a presentation made at Gluecon in May 2012.","item_date":"Jun 05 2012 18:32:54","display_item_date":"06-05-2012","url":"http:\/\/blog.programmableweb.com\/2012\/06\/05\/reaching-a-million-apis-and-what-to-do-when-we-get-there\/","source":"blog.programmableweb.com"},{"title":"Prelert Announces Open API Support for SQL","details":"Prelert\u00ae, the technology leader in machine learning predictive analytics for infrastructure and application management, today announced an SQL Open API for both its Real-Time and Diagnostics products. Prelert Diagnostics is now the industrys first downloadable predictive analytics product to include an Open API. \nPrelert turbocharges existing infrastructure and application monitoring tools with an easily deployed layer of 3rd generation machine learning analytics. These highly advanced analytics process millions of metrics and log notifications, without the need for rules, templates or thresholds, and identify behavioral activity patterns that are the early warning signs of performance problems. Prelert applies Big Data analytics to huge volumes of management data to provide IT support personnel with the reduced set of information they need to identify, diagnose and resolve problems before users are impacted.   \nIT teams responsible for mission critical applications and services have an enormous resource - a vast amount of management data that is largely untapped by their current tools, explains Mark Jaffe, Prelert CEO.  Prelert is the first fully self-learning analytics product to analyze all of that data and provide valuable insights that enable higher service levels, in less time while reducing the need for human intervention. \nPrelert continues to lead the industry in providing high value analytics software with a focus on fast time to value and eclectic data collection, said Dennis Drogseth, Vice President of Research for IT Megatrends at Enterprise Management Associates.   Their support for an Open API is further support for their compelling vision.  \nPrelert turbocharges IT teams with the power of Machine Intelligence\u2122 through easily consumed products that provide significant value in hours.  Todays complex IT environments generate more monitoring data than humans can efficiently structure or analyze. Prelert harnesses the power of 3rd generation machine learning Big Data analytics to quickly expose actionable patterns hidden in huge volumes of data so that IT experts can provide higher services at lower costs in less time.\n \n For the original version on PRWeb visit: www.prweb.com\/releases\/prweb2012\/6\/prweb9571314.htm","item_date":"Jun 05 2012 18:29:41","display_item_date":"06-05-2012","url":"http:\/\/www.sfgate.com\/cgi-bin\/article.cgi?f=\/g\/a\/2012\/06\/05\/prweb9571314.DTL","source":"www.sfgate.com"},{"title":"The 21st Century Supply Chain","details":"Published  \t\t\t\t  June 4th, 2012\t\t\t\t  by Trevor Miles  \t\t\t\t  @milesahead   \t\t\t\t  \t\t\t  \t\t\t\tRami Karjian of Flextronics casually threw out the comment that \u201cdata is the new oil\u201d during a session at the recent Gartner Supply Chain Conference in Palm Desert. The session titled \u201cNew Business Rules \u2013 The Flextronics Next-Generation Supply Chain Strategy\u201d was hosted by GT Nexus. Given Flextronics\u2019 role as a contract manufacturer in the high-tech\/electronics supply chain space, with aspirations of providing supply chain managed services, it is immediately obvious how they can make very effective use of data, especially extended or \u201ctrue\u201d end customer demand data and \u201ctrue\u201d component availability data. Of course, this is true for any number of participants in the supply chain and there has been a lot written about the use of social platforms to capture \u2018big data\u2019 on consumer behavior. But anyone who has worked with the likes of Flextronics will recognize that the \u2018speed\u2019 of their business (driven by low margins, high volumes, and rapidly changing products) is like no other. Knowing sooner and acting faster are core capabilities and competencies they must have in order to survive. It is easy to see where data fits into this equation.\n But I hope Rami will not be upset with me when I state that the term seemed too catchy to have come from a supply chain guy (I am writing as a supply chain guy, so I mean no disrespect!) The catchiness of the phrase just reeks of Analyst or Strategy Consultant. So I went Googling for the term.\u00a0 I was correct. The term seems to have been used originally by Andreas Weigend of Stanford in an interview with Forbes dating from September 2011. (One caution: The interview is sponsored by SAP.) In the interview Weigend says, with my emphasis, that\n \u201cMost people try to play the social media game faster, but it is not your game to play anymore. To be successful, you have to understand what the game is made of . . . it is data and identity. This forces the change from a transactional economy to a relationship economy. The companies who get this, will win.\u201d\n There is no doubt that Weigend is referring to the customer or demand side of the supply chain, really to \u2018big data\u2019 epitomized by social media platforms such as Facebook and Twitter. But here is the problem for social media: Customers aren\u2019t compelled to engage in any manner with the OEM; Suppliers are. So while undoubtedly social data can be mined for information, social data cannot be relied on as the sole source of information to drive the business. Instead, social data can be used to augment, to enhance, the structured data that is being used to drive the business.\n I was struck by what Ray Wang said in this context in the same Forbes interview, namely that\n \u201cThe market has moved beyond just marketing, service, and support use cases,\u201d says Wang. \u201cWe see 43 use cases that span across key enterprise business processes that impact eight key functional areas, from external facing to internal facing including PR\/marketing, sales, service and support, projects, product life cycles, supply chain, human capital management, and finance.\u201d\n Companies will get real value within the internal and supply sides from platforms that are social in nature, whereby social concepts will finally make collaboration a reality by adding context and nuance to \u2018dumb\u2019 EDI exchanges of data between computers. The activities of cross-functional and cross-organizational exploration and discovery through what-if analysis followed by negotiation, which are so important to decision making processes. These capabilities, which are\u00a0required for decision making are not supported by EDI and ERP platforms, which instead focus on executional or transactional processes. This point was made very strongly at the Gartner Supply Chain Conference by Jim Cafone of Pfizer who said that\n \u201cERPs are great if you want to talk to yourself, but who can afford to do that in today\u2019s value chains?\u201d\n Jim was referring to the fact that much of today\u2019s value\/supply chain exists outside of the four walls of the organization because so much of the supply chain has been outsourced leading to the conclusion (using the context of \u2018data is the new oil\u2019, that ERP is like crude oil) -\u00a0it needs to go through refinement and to be augmented by additives to be useful. It needs to be refined and augmented because, in order to operate a value\/supply chain effectively today, it is necessary to reach across functional and organizational boundaries even for so-called structured data, making even structured data \u2018big data\u2019. As Jim Cafone stated, without reaching across the organizational boundaries, you are only \u2018talking to yourself\u2019. We cannot operate today\u2019s supply chains effectively with this lack of visibility. (And let\u2019s face it, many ERP deployments resemble an oil spill \u2013 the mess you\u2019re in isn\u2019t what you expected, and it\u2019s going to cost an awful lot to fix.)\n To be of any value, the data provided by visibility needs to be \u2018refined\u2019 by being broken down into \u2018specific useful parts\u2019, turning visibility into actionable insight. I don\u2019t mean to\u00a0diminish the value of visibility because without it there can be no actionable insight. The true value comes from the supply chain orchestration made possible by actionable insight.\n So let us decompose \u2018actionable insight\u2019 quickly.\n Turning Data into Visibility\n Existing social network platforms, such as Facebook, are principally about sharing or \u201cpushing\u201d information, but most business processes require an interaction between at least two people, each of which is responsible for an aspect of the decision and who need to reach a consensus and compromise in order to take action. Often these interactions require input from as many as 5-10 people. Identifying who needs to know can often be an insurmountable barrier to reaching a timely decision. I want to draw the distinction here between the people who need to know, which implies responsibility to take action, and the people who want to know, which implies an interest but not a responsibility. Existing social technologies address the \u2018want to know\u2019, but not the \u2018need to know\u2019 aspect. On the other hand, existing machine-to-machine exchanges of structured data between organizations are desperately in need of the context and nuance provided by social platforms. Having both turns data into visibility.\n Turning Visibility into Insight \n Knowing the state of something \u2013 inventory, capacity, etc. \u2013 in the supply chain, which is what visibility provides, is useful, but not valuable. Value is derived from knowing what that state means to your financial and operational metrics, and, perhaps even more importantly, to your projected operational and financial metrics. To achieve this insight you must be able to compare the current state to a desired state and to evaluate if the difference is important, which you can only determine by having a complete representation of your supply chain \u2013 BOMs, routings, lead times, etc. \u2013 so that you can link cause to effect. You cannot do this by \u2018talking to yourself\u2019. Only if you have and end-to-end representation of your supply chain can you link a tsunami in Japan to your revenue projections for the next 2 quarters. Of course many people can do this given enough time. Doing this quickly \u2013 knowing sooner and acting faster \u2013 is what brings value. While events as large as the Japanese tsunami make the linkages between the supply chain nodes obvious, the daily operation of a supply chain is subjected to thousands of little \u2018tsunamis\u2019, the compounded effect of which, in terms of reduced revenue and increased costs, can often be the difference between making or missing a quarter.\n Turning Insight into Actionable Insight\n Knowing that something is not quite right \u2013 visibility \u2013 and knowing the financial and operational impact or consequence of this mismatch \u2013 insight \u2013 is only valuable if you can act on this insight quickly \u2013 actionable insight. To do this you must be able to link the impact to the person responsible for the impact, not only the cause. People take action. Of course under certain circumstances decisions can be automated, but only mundane decisions, decisions that make little difference, can be automated. \u2018Big\u2019 decisions should always be left to human judgment. And to do that we need to link the cause and effect to the people who need to take action. Hence actionable insight.\n Supply chain is a \u2018team sport\u2019. Each function may have their own span of control and metrics, but any decision made by one function will almost always have an impact on at least one other function and more likely have multiple impacts on multiple functions. But currently, each node in the supply chain, in fact each function in the supply chain, usually operates in isolation with Engineering making design changes with little thought as to how these impact Manufacturing or Procurement, with Marketing planning promotions with little consideration of available capacity or material availability, with Sales accepting order quantities and delivery dates with little consideration of the cost required to achieve both. Orchestration is the coordination and synchronization of these separate functions into a unified response to real demand. To achieve orchestration requires a platform on which a team of people across functional and even organizational boundaries can explore and understand the financial and operational consequences of possible actions quickly and collectively.\n Too often when I write about these topics I forget to include the word \u2018projected\u2019. Without a doubt a great deal of value can be derived from risk recovery, but risk avoidance and risk mitigation are even more valuable. Being able to determine that revenue or margin targets will (future) not be achieved by the end of the quarter if we continue to create in the current manner is a lot more valuable than being able to determine why we have (passed) missed the quarter. Knowing sooner that something will happen if we do not change course allows us time to investigate ways to avoid the risk or to take the opportunity. But knowledge without action brings little benefit. I have written in the past about George Stalk\u2019s concept of Competing Against Time and the related OODA concept \u2013 Observe, Orient, Decide, Act \u2013 idea from the US military strategist Colonel John Boyd, both of which explain very crisply the competitive advantage of knowing sooner and acting faster.","item_date":"Jun 05 2012 00:22:42","display_item_date":"06-04-2012","url":"http:\/\/blog.kinaxis.com\/2012\/06\/data-is-the-new-oil-know-sooner-act-faster\/","source":"blog.kinaxis.com"},{"title":"Data as Currency & Dealmaker","details":"The amount of data collected by companies in all sectors and markets has grown exponentially in the past decade. Businesses increasingly interact with customers through social and business networks, through apps (and APIs) and therefore businesses collecting data from new and diverse locations outside the walls of their enterprises and systems-of-record.\n The following is a perspective on 5 ways in which data is changing how we do business in 2012 and beyond.\n Data as currency and dealmaker\n Similar to the discovery of oil in Texas at the turn of the last century, enterprises that have been collecting and storing data will be the ones primed to leverage their data for new opportunities and striking new business deals.\n Add to this new data sources like the explosion of social data, which provides a window into your real-world and real-time customers\u2019 behavior. The data accumulates quickly and changes frequently, and the ability to capture, analyze and derive insights from it will be key to offering true customer-centric value across companies, and even entire industries.\u00a0\n Data is fast becoming the de-facto currency for facilitating business deals. Enterprises will be able to command monetary and opportunistic conditions in return for providing access to their data. Google\u2019s custom search for websites is an example. By providing indexed data and search functionality to websites, Google in return has the ability to show ads and generate revenue on the website.\n We will also see the emergence of data network effects: enterprises will be able to enrich their existing data sets by requiring that other enterprises who purchase their data return (or feedback) the enriched data to augment the original data set. Enterprises sitting on the most desired data will be able to continuously add value to their existing data sets.\n Collaborations through data\n I believe a new model of collaboration based on data is emerging. Enterprises realize that they can partner with other enterprises and create innovative, new business value by sharing and operating with shared or semi-shared data stores.\n Apart from shared data storage and processing costs, enterprises will be able to leverage faster time-to-market and build enhanced data-driven features using such collaboration models. They could use shared data stores as primary operating data backends thereby realizing near real-time data updates and availability for their products and services.\n The academic world has several examples and parallels to this notion where data sets are frequently generated, updated and shared between various research projects leading to better quality and more expansive research and insights.\n Data marketplaces\n Data can be defined by attributes such as \u201cLatency of delivery for processing\u201d, \u201cquality\u201d, \u201csample ratio\u201d, \u201cdimensions\u201d, \u201ccontext\u201d, \u201csource trustworthiness\u201d, and so on. As data becomes a key enabler of business, it becomes an asset that can be bid upon and acquired by the highest bidder. The price associated with acquiring data will be determined by the data attributes. For example, a real-time feed of a data source might cost more than historical data sets. Data sets at 100% sample ratio might cost more than data at lower fidelity.\n Ability to access and synthesize data is a competitive edge. To gain and maintain this edge, enterprises will have to add the cost of acquiring data to their variable operating costs. At the same time, enterprises will have to protect their data as they they protect other corporate assets. Protection (and insurance) against loss, theft, corruption will be required to ensure continued success.\n End users will stake claim to their data\n With the rise of social networks and even with the consumerization of IT, data is also becoming more personal. We trade our personal data for services every day as we interact with Facebook and other sites.\n End users who are the generators of data that enterprises collect and use to improve their businesses will stake claim to their data and demand a share of the value with the enterprise. In addition, end users will demand and gravitate towards enterprises that give them the ability to track, view and control the data they generate for enterprises.\u00a0Enterprises may have to either \u201cforget\u201d users because users demand it or compensate them for their data.\n The\u00a0jury \u00a0 is \u00a0 still \u00a0 out\u00a0but the tide may already have turned in this direction in Europe. Data protection regulations may allow for a \u201cright to be forgotten\u201d law through which users will have the right to demand that data held on them be deleted if there are \u201cno legitimate grounds\u201d for it to be kept. This includes if a user leaves a service or social network, like Google or Facebook - the company will have to permanently delete any data that it retains.\n Data disintermediation\n The concept of disintermediation - of removing the middlemen and vendors and giving consumers direct access to information that would otherwise require a \u201cmediator\u201d\u00a0 has been an active topic in the information industry and gains momentum in 2012 as data becomes currency.\n We will see more and more enterprises exposing their data schemas, formats and other related capabilities publicly though a common data description language and data explorer capabilities accessible by both humans and machines.\n Enterprises (or their automatic agents) will be able to crawl the web (or some other data network) and discover new data sources that serve their needs. Enterprises will have the ability to walk the data models and understand the structure and schema of various data sets and understand the intricacies of using these data sources.\n We\u2019d love your feedback \u2013 whether you agree, disagree, have additional points-of-view or questions. Youll find a great community of your peers over on the\u00a0api-craft forum.","item_date":"Jun 04 2012 23:08:36","display_item_date":"06-04-2012","url":"http:\/\/blog.apigee.com\/detail\/data_as_currency_dealmaker\/","source":"blog.apigee.com"},{"title":"EMR Interface and API | Electronic Medical Records API","details":"Our interactive API connects the best EMR applications available. Our focus on connectivity has already connected over 65,000 pharmacies via Surescripts, interactive directions and access to more than 150 medical labs.\n   \t\n  Our free EMR serves as a platform bringing together the best applications.  We\u2019ve already delivered Practice Fusion connectivity to pharmacies through Surescripts, interactive directions and lab partners including LabCorp.\n  \n  Our free Electronic Medical Record system is a platform; a community that brings together the best EMR applications available for medical professionals. Practice Fusions focus on EMR integration has already delivered video training through YouTube and Vimeo, connectivity to 65,000 pharmacies through Surescripts, interactive directions and access to 150 medical labs through partners including Quest and BioReference.\n  \n  There are hundreds of EMR devices and solutions that could be valuable for the Practice Fusion community\u201a more than our team could ever create internally. Thats why we launched our\u00a0API Challenge in October 2010 for over 30 development teams and why were excited to be opening up our API to new developers shortly.\n  Our vision for the API includes both solutions for your practices workflow and for your patients. Imagine prescribing a mobile phone mood tracker application to a patient on a new antidepressant right from their electronic chart and then being alerted if their mental health declines significantly. Or, adding specialty-specific features to your Practice Fusion account with just a few clicks. Our goal is to bring you a wide variety of new choices that work seamlessly within your existing EMR application.\n  Meaningful data connection between various healthcare platforms is the ultimate goal of Washingtons health IT investment. The Practice Fusion API is a first step toward future medical record interoperability; creating opportunities for seamless communication with hospitals, medical groups, patient platforms and more.\n  Just a few things the API can deliver within the EHR:\n  Medical billing solutions\n  Lab and partner connections\n  Hospital interoperability\n  Connections to digital medical devices\n  Specialty-specific EHR features\n  Mobile applications\n  Telemedicine features\n  \nContact us if you are developer interested in opportunities to work on Practice Fusions API.","item_date":"Jun 04 2012 23:00:13","display_item_date":"06-04-2012","url":"http:\/\/www.practicefusion.com\/pages\/emr-api-interface-healthcare-interoperability.html","source":"www.practicefusion.com"},{"title":"Practice Fusion and the Value of Electronic Health Records","details":"While in Texas recently I took the opportunity to spend time with Practice Fusion, an Electronic Medical Record (EMR) vendor that I\u2019ve covered previously. Practice Fusion is an interesting vendor, providing a free EMR service to physicians. It boasts that its solution is used to manage the records of over 31 million patients across the US through primarily small and mid-sized primary care practices. Practice Fusion must be on to something as they\u2019ve managed to attract $38M worth of venture funding from some big names.\n Anyway, I was invited to attended a Practice Fusion event where I met with a number of practitioners using Practice Fusion to run their operations. it was interesting talking with these folks about the benefits that a move to a cloud practice management solution is driving efficiencies in the healthcare industry. I wanted to dig into this a little further and took the opportunity to communicate with an internist based in San Francisco, Allan Treadwell. Treadwell is affiliated with UCSF as a volunteer clinical instructor and also has a solo primary care practice near the UCSF hospital.\n I let Treadwell tell his own story about how he came to be using electronic health records;\n I took over this practice almost 5 years ago, and during that time converted the practice from a traditional insurance based practice to a retainer practice. The practice has become much smaller in terms of numbers of patients, but I have been able to improve the level of care significantly.\n When I assumed this practice 5 years ago the records and appointment system was entirely on paper. Scheduling appointments was difficult and time consuming, since any changes had to be made by rifling through a paper appointment book to make the requested changes.\u00a0 Any patient who would call asking about the date of an upcoming appointment would have to wait for my office manager to find it, after flipping pages in the appointment book. At that time I was already interested in exploring electronic medical records, but had held off on fully researching it due to a couple of constraints. One was cost. As a solo practitioner I don\u2019t have a lot of cash to spend on an expensive system. I had worked at offices previously that had problems implementing EMR systems, and the cash outlay was substantial. I couldn\u2019t afford to get the wrong system. Secondly, my need at the time was primarily for scheduling which I felt would be far easier to implement initially than a complete EMR system.\n It was about then that I heard about Practice Fusion, which at the time was offering only an electronic scheduling service. It seemed perfect for my needs, and I decided to try it out at least on a trial basis. My entire office became dependent on it for my scheduling needs. Then, as Practice Fusion grew and offered more services, I continued to expand my usage accordingly. I now use it for all of my medical records, scheduling, internal messaging, electronic prescribing, drug interaction, lab results \u2013 in essence it has fundamentally changed almost every aspect of my practice. I continue to keep a shadow paper chart for the days when my internet connection is down, but that has only happened once in the past five years. Not only does my office run more efficiently and safely, I can also access the information from any computer. If, for example, a patient calls while I am at home or away from the office, I can quickly pull up all of their medical information in an instant.\n Treadwell isn\u2019t an evangelist at all costs, he rightly points out some areas for improvement, citing the need for a mobile application in his particular situation. More broadly than specific functionality however, the example of Treadwell shows the efficiencies that can be gained in particular by small practices moving from primarily paper-based solutions to electronic ones.\n Practice Fusion\u2019s approach of providing a free solution that is monetized by targeted advertising to practitioners will certainly turn some potential customers off. In particular targeted advertising in the health sector has a degree of \u201cyuck factor\u201d that many will be wary of. personally I am relaxed about anonymised data being used for contextual advertising, even in this most personal of areas, health.\n While there is a downside to the advertising approach, it does have the significant benefit of putting cutting edge EMR solutions into the hands of even the smallest practice and this is a disruptive and enabling trend. Clearly the investment that the US Government has made into EMR is playing in Practice Fusion\u2019s favor but I believe that even without this extra attention, the benefits and efficiencies driven by EMR would have gained the attention of practitioners. If we imagine a few years in the future when EMR have replaced the majority of manual systems, and the core functionality within these sorts of systems has been built out further, we can imagine how much more efficiency medical practitioners will be \u2013 that\u2019s a positive change, and one which validates the value that cloud computing can bring.","item_date":"Jun 04 2012 23:00:13","display_item_date":"06-04-2012","url":"http:\/\/www.cloudave.com\/20139\/practice-fusion-and-the-value-of-electronic-health-records\/","source":"www.cloudave.com"},{"title":"Money APIs Power The New Wall Street","details":"From billion dollar companies to the neighborhood coffee shops, merchants of all sizes are seeking new ways to offer secure and affordable mobile payments systems that expand their ability to conduct business. The PayPal API got its start this way, first with online auctions, then with thousands of websites and individuals. It\u2019s now one of over 150 payment APIs, many of which exist outside of traditional finance.\n\n\nAccording to a study, 87% of the U.S. adult population has a mobile phone, and 44% of those phones are Internet-enabled. 84% percent of smartphone users have accessed the Internet on their phone in the past week. That translates into a sizeable mobile wallet market.\n\n\nThe growing list of public \u201cmoney APIs\u201d, are at every step of the transaction process. They process payments, deliver funds, add point of sale functionality to any app.\n\n\nPayAnywhere, a leader in mobile point of sale solutions, just announced the addition of two new Android software developer kits(SDKs) that allow developers to easily embed credit card point of sale functionality into apps for Android devices.\n\n\n\n\n\nThe Basic and Advanced Android SDKs give instructions on how to start the transaction and call, initialize, and customize the library. Receiving information back from the library is also detailed in the API documentation.\n\n\nOther \u201cmoney APIs\u201d making an impact:\n\n\nPayPal API \u2013 gateway platform for sending receiving online payments\n Dwolla API \u2013 send payments or integrate a point of sale system, GRID application acts as your identity to the end user, low payment system fees.\n LevelUp API - users can set up their own QR code that allows them to pay various merchants via scanning the QR code as point of sale.\n Google Payment Express enables merchants to accept Google Wallet on the mobile web. It is a cloud-based method of accessing and storing.\n Mastercard API \u2013 enable payments , offer deals, enable  transfer money through the MasterCard network.\n Visa API \u2013 lets users purchase goods and services without sharing credit card information with the seller.\n \n\n\nAre Apple\u2019s iWallet and Facebook getting ready to make a splash on the New Wall Street too?","item_date":"Jun 04 2012 22:22:47","display_item_date":"06-04-2012","url":"http:\/\/blog.programmableweb.com\/2012\/06\/04\/money-apis-power-the-new-wall-street\/","source":"blog.programmableweb.com"},{"title":"Open Government Data: The Book by Joshua Tauberer","details":"This book is the culmination of several years of thinking about the principles behind the open government data movement in the United States. In the pages within, I frame the movement as the application of Big Data to civics. Topics include principles, uses for transparency and civic engagement, a brief legal history, data quality, civic hacking, and paradoxes in transparency.","item_date":"Jun 04 2012 20:34:21","display_item_date":"06-04-2012","url":"http:\/\/opengovdata.io\/","source":"opengovdata.io"},{"title":"Amazon Web Services Blog: Lots of SAP News to Start the Week","details":"Last week one of my colleagues stopped me in the hall and said Jeff, you have to tell our customers about all of the great work that we are doing with SAP. I asked him for some details and he was happy to oblige.\n  SAP Business All-in-One is an important piece of enterprise software, a package that is mission critical for many companies. It includes Enterprise Resource Planning (ERP), Customer Relationship Management (CRM), Supplier Relationship Management (SRM), and Business Intelligence (BI) functions.\n  The first big piece of news is that we are expanding the range of SAP certified solutions on AWS. This includes:\n  SAP Business All-in-One on Linux.\n  SAP Business All-in-One on Microsoft Windows.\n  Expanded certification for SAP Rapid Deployment solutions on Windows Server 2008 R2.\n  Expanded certification for SAP Business Objects on Windows Server 2008 R2.\n  \nSecond, AWS partner VMS (a German management consultant staffed by a number of ex-SAP executives), has published a new SAP TCO analysis. Their research shows that running SAP on AWS can results in an infrastructure cost savings of up to 69%, when compared to on-premises or colo-based hosting. You can read the executive summary to learn more.\n  VMS computed a CWI (Cloud Worthiness Index) value of 59 for SAP running on AWS. The CWI was designed to quantify the economic value of the cloud, and is based on VMSs measurements of over 2,600 SAP systems. It accounts for TCO and best practices, both with and without the cloud. You can read more about the CWI here.\n  Third, we have announced a number of other SAP offerings on AWS:\n  You can now process Big Data on SAP on AWS using their new HANA in-memory database. SAP has published a comprehensive getting started guide and they are also offering a 30-day free trial for testing and evaluation.\n  SAP Afaria makes it easy to build and deploy mobile applications that connect mobile workers to busines data. Afaria handles a number of important aspects of this including password and certificate management, an application portal, and a management console. You can launch Afaria from the AWS Marketplace (register for the 14-day free trial if you dont have a license).\n  \nYou can find case studies and technical papers on our SAP microsite.\n  -- Jeff;","item_date":"Jun 04 2012 18:19:46","display_item_date":"06-04-2012","url":"http:\/\/aws.typepad.com\/aws\/2012\/06\/sap-start-the-week.html","source":"aws.typepad.com"},{"title":"Research Data via Web Services","details":"Cortellis for Informatics uses sophisticated application programming interfaces (APIs) to enhance the research and development process by delivering real-time access to information and tools for biopharmaceutical competitive intelligence, drug pipeline, and drug research and development.\nEach API integrates Thomson Reuters data with proprietary content and public information sources into a comprehensive view that is customized to meet the needs of a company, team or individual researcher. Data can be shared in a number of ways, including dashboards, custom reports and alerts, and intranet and mobile applications.\nCortellis for Informatics streamlines information sharing across all levels of the organization. Instead of using multiple systems to manage workflows, researchers access up-to-the-minute news in the office, in meetings or at the bench. This ability to deploy high-quality Thomson Reuters information across the whole company in real time within internal systems provides greater opportunities for collaboration in the scientific search and discovery process.\nThomson Reuters","item_date":"Jun 04 2012 18:09:11","display_item_date":"06-04-2012","url":"http:\/\/www.dddmag.com\/products\/2012\/06\/research-data-web-services","source":"www.dddmag.com"},{"title":"Envirofacts Application Programming Interface (API)","details":"The Envirofacts Multisystem Search Form allows users to search multiple environmental databases (program systems) for facility information, including toxic chemical releases, water discharge permit compliance, hazardous waste handling processes, Superfund status, and air emission estimates.  A facility is a business, site, or place subject to environmental regulations or of environmental interest. The Multisystem search report includes an API link for report data, which provides the facility location including the latitude, longitude, and associated program systems.  For example, if a user is   interested in seeing the facility location and associated program systems for facilities located in Greenwich, CT, the API Uniform Resource Locator (URL) can be obtained by using the Envirofacts Multisystem Search Form              \n\n  Envirofacts has developed a Representational State Transfer (RESTful) data service API to all of its internal data holdings.            This functionality provides an Envirofacts user with the ability to query any table through the use of a URL.            The default output is in Extensible Markup Language (XML), which can be utilized in other applications, as well as tools such as Microsoft Excel or Access.            Output options of Comma-Separated Values (CSV) and Excel can be requested in the URL. The entire Envirofacts database metadata          is available online so that all tables and columns within Envirofacts are documented. Having the table structures available in this manner makes most of         Envirofacts data easily available through the service. \n\nUse the Quick Start Search option or click the Advanced link to go to the full Multisystem Search. The Quick Start Search returns results with a scale of 2.5 miles. The advanced search has a scale of 10 miles. To return a larger number of records use the Advanced Search by clicking the Advanced link. \n\n  \n\n      The Multisystem Search page will load.  The Envirofacts database can be searched using any combination of the following criteria: facility name, geography, facility industrial classification, or pollutant. Enter Greenwich in the City field and CT in the state field.  \n\n  \n\n        Click the Search button to obtain the Multisystem Search Results page that includes the API Link for Report Data.  \n\n        Copy and paste the link to view the report data.\n\n      The default URL specifies 500 rows.  http:\/\/iaspub.epa.gov\/enviro\/efservice\/multisystem\/minLatitude\/41.00064\/maxLatitude\/41.138792\/minLongitude\/-73.722807\/maxLongitude\/-73.566961\/rows\/1:500\n\n  The minLatitude, maxLatitude, and minLongitude, maxLongitude values represent the top right and bottom left corners of the map.  These parameters can be modified to bring up points anywhere in the US. \n\n  Please note that at this time the only output format available for the Multisystem API is XML.\n\n  The following table shows the breakdown of the components to  show Rows 10-15 of the default URL   : http:\/\/iaspub.epa.gov\/enviro\/efservice\/multisystem\/minLatitude\/41.00064\/maxLatitude\/41.138792\/minLongitude\/-73.722807\/maxLongitude\/-73.566961\/rows\/10:15","item_date":"Jun 04 2012 18:04:29","display_item_date":"06-04-2012","url":"http:\/\/www.epa.gov\/developer\/efapi_multisystem.html","source":"www.epa.gov"},{"title":"Understanding our API","details":"Earlier this month, we published the SLC Developer Reference Guide. Included in the guide are the API specification and data model. For those who are not familiar with APIs, the term stands for Application Programming Interface, which simply put, is a way for applications to communicate with each other. In many ways this is the bridge that SLC has created to empower the creation of future applications. A popular example is on the website Yelp.com. While their own website will store, aggregate and sort user review data, they use a Google Maps API to geographically represent that data. So, when you search for a restaurant or review on Yelp, you can see it placed on a Google map on the same page. The applications are communicating on the back-end, and combining data to present a solution for the user.\n\n   Translating this into an example specific to the SLC\u2019s goals, imagine if you were to create an application to present a student\u2019s progress and recommend specific learning topics aligned with their district\u2019s learning objectives. The SLC would pull all three data points of information \u2013 student progress, learning topic, and learning objectives \u2013 and then communicate the information to the application.* After a few short data calls, voila! The application has the ingredients it would need to cook up a recommended course of study. On the front-end, the user wouldn\u2019t see all of the different communications that occurred, but would see the combined result based on their simple button clicks.\n\n   For our developer community, the API will serve as a reference guide from which they can understand how to frame the development of their future applications. We will soon be making available a listing of specific real-world scenarios that teachers have highlighted as opportunities for vendors to solve with future applications (which we hinted at in our recent Teacher Wishlist blog post). The next step will be releasing the SLC Sandbox, which will allow developers to test the applications to make sure they work with the SLC technology infrastructure and data model. \n\nBelow is a list of some of those \u201cdata points\u201d that applications will be able to reference through the API \u2013 combining any of these, what kind of applications would you create? \nLet us know in the comment section!\n\nClass schedule and school calendar\nAcademic record and assessments\n\nTo see more details of our API specification, explore our Developer Reference Guide.\n\n   *States, districts and schools using the technology will retain ownership and control of their data, and of the privacy and security policies that apply to how that data is collected, retained and safeguarded. In other words, as the legal owner of source system data, districts continue to control what instructional data applications get access to student data, and the SLC will support transfer of student data only to applications approved by the districts.","item_date":"Jun 04 2012 18:02:20","display_item_date":"06-04-2012","url":"http:\/\/slcedu.org\/blog\/understanding-our-api","source":"slcedu.org"},{"title":"Bricsys Makes Engineering Software, APIs Free to Students","details":"Bricsys has launched a global academic program, which provides free engineering software and application programming interface (API) access to students and schools.\n   One piece of software now available free of charge from Bricsys is Bricscad V12 Platinum, computer-aided design (CAD) software. Features of Bricscad V12 Platinum, which can be used on both Windows and Linux operating systems in 3D modeling rendered mode, include the ability to:\n   Read and write .dwg files;\n  Detect automatic design intent;\n  Add and control 3D model constraints;\n  Keep the number of clicks to a minimum with the use of the quad tool;\n  Add commands, hatches, and blocks in a tabbed window through the tool palette; and\n  View blocks in drawings that are not open and place them in the drawing being worked on.\n  \nBy making a variety of APIs available to them the move is designed to allow students and teachers to come up with new engineering software and CAD developments. The company will also create and launch an e-store where students and schools can sell their apps online. Payment and sales will be handled by Bricsys.\n   The academic program is available to download for free for the first 12 months to students, teachers, and schools. After the first year, educational institutions must renew their membership.\n   The company, which has European headquarters in Belgium and United States headquarters in New Hampshire, is a member of the Open Design Alliance, which supports open industry-standard formats for the exchange of CAD data and the maintenance of valuable data stored by legacy design systems, according to its Web site. Membership fees provide members with access to the Teigha software development platform.\n   For more information, visit bricsys.com.\n                                                                                                                                                                                                    About the Author\n                                                                                    Tim Sohn is a 10-year veteran of the news business, having served in capacities from reporter to editor-in-chief of a variety of publications including Web sites, daily and weekly newspapers, consumer and trade magazines, and wire services. He can be reached at timothyjsohn@gmail.com and followed on Twitter @editortim.","item_date":"Jun 04 2012 17:56:39","display_item_date":"06-04-2012","url":"http:\/\/campustechnology.com\/articles\/2012\/06\/04\/bricsys-makes-engineering-software-apis-free-to-students.aspx","source":"campustechnology.com"},{"title":"Dear Oracle: The Java APIs Are Not a Work of Art","details":"Oracle said the Java APIs were like a beautiful painting. Google said they were more like a file cabinet. And in the end, Judge William Alsup came closest to agreeing with Google, comparing an API to a library that organizes the Java programming language. \n \u201cEach package is like a bookshelf in the library,\u201d Alsup wrote with last week\u2019s much-anticipated ruling in the epic legal battle between Google and Oracle. \u201cEach class is like a book on the shelf. Each method is like a how-to-do-it chapter in a book. Go to the right shelf, select the right book, and open it to the chapter that covers the work you need.\u201d\n His ultimate point was that the organization of a library is not subject to copyright. Yes, he said, the books are copyrightable, but not the way the books are organized. \n In other words, Google did not infringe on Oracle\u2019s copyright when it cloned 37 Java APIs in building its Android mobile operating system. Though Google copied the organization of the APIs, it built the code behind them on its own \u2014 or at least mostly on its own. \u201cThe Java and Android libraries are organized in the same basic way but all of the chapters in Android have been written with implementations different from Java but solving the same problems and providing the same functions.\u201d\n With his ruling, Judge Alsup effectively brought an end to the six-week trial over Google\u2019s use of Java in Android. After suing Google in 2010, claiming both copyright and patent infringement, Oracle had sought a portion of Google\u2019s Android revenues, but in the wake of Alsup\u2019s ruling, it\u2019s entitled to almost nothing \u2014 though the database giant has already said it will appeal. \n \u201cThis reaffirms our longstanding understanding of the law: that these APIs were free for anyone to use as we did, taking just the declarations and doing our own independent implementations. That\u2019s the way developers use Java. You can\u2019t say a language is free for everyone to use and then hold back the nouns and the verbs.\u201d \u2013 Google\n If Alsup had ruled otherwise, says Bret Bocchieri, an intellectual property lawyer with the international law firm Seyfarth Shaw LLP, Oracle could have potentially reaped a \u201cmind-staggering amount\u201d of damages. But he didn\u2019t. \n What\u2019s more, Alsup\u2019s ruling allows a world of software companies and individual developers to breathe a sigh of relief. In the software world, cloning APIs is a common practice. Several cloud platforms, for instance, mimic the APIs of Amazon\u2019s massively popular Elastic Compute Cloud. An API is an application programming interface, a way for two pieces of software to talk together, and the general assumption has always been that these interfaces are not subject to copyright. When Oracle tried to argue otherwise, it caused at least a little hand-wringing among software outfits across the industry. But on Thursday, Alsup put an end to all that.\n \u201cTo accept Oracle\u2019s claim would be to allow anyone to copyright one version of code to carry out a system of commands and thereby bar all others from writing their own different versions to carry out all or part of the same commands,\u201d read his 41-page brief. \u201cNo holding has ever endorsed such a sweeping proposition.\u201d\n Ed Walsh, an attorney with the international law firm Wolf Greenfield, isn\u2019t surprised by the ruling. But he also says that we shouldn\u2019t necessarily view the ruling as a decision that frees all APIs from copyright. He believes that the judge may have ruled in favor of Google at least in part because Sun, the original maker of Java, allowed Google to clone the APIs. Oracle sued Google after acquiring Sun. \n \u201cI think some element of the influence [for the ruling] was the view that Sun allowed people to use Java,\u201d Walsh said. \u201cSo that expanded the range of things [Oracle] could not protect by copyright.\u201d\n Catherine Lacavera, Google\u2019s director of litigation, says much the same thing. \u201cThis reaffirms our longstanding understanding of the law: that these APIs were free for anyone to use as we did, taking just the declarations and doing our own independent implementations,\u201d she told Wired. \u201cThat\u2019s the way developers use Java. You can\u2019t say a language is free for everyone to use and then hold back the nouns and the verbs.\u201d\n But Alsup goes much further, using great detail in describing what the Java APIs are and how they should be treated under the law. His library metaphor is an apt one. But he doesn\u2019t stop at metaphors. He seems to truly understand APIs. He realizes there\u2019s a difference between copying an interface and copying the code behind an interface.\n \n \u201cEvery method and class is specified to carry out precise desired functions and, thus, the \u2018declaration\u2019 (or \u2018header\u2019) line of code stating the specifications must be identical to carry out the given function,\u201d he says, after laying down his library metaphor.\n As of 2008, Java included 166 APIs, spanning more than six hundred classes, broken into more than six thousand methods. Google replicated the names and the operation of 37 API packages, but it used its own code to implement the methods and classes.\n During the trial, Oracle counsel Mike Jacobs often said that building an API was akin to writing a great symphony or, yes, painting a beautiful painting. And Judge Alsup did acknowledge that developing an API is a creative endeavor. But he added that at the conceptual level, such inventions can only be protected by patents. Oracle also tried the patent argument, but that didn\u2019t work either.\n Java relies on a particular vocabulary called \u201cmethod specifications\u201d that allow humans to tell the computer exactly what they want it to do. Alsup said that under the U.S. Copyright Act, no matter how creative a method specification may be, anyone \u2014 including Google \u2014 is entitled to use the same specifications as long as the line-by-line implementations are different. \u201cThe method specification is the idea. The method implementation is the expression. No one may monopolize the idea,\u201d Alsup wrote.\n The judge said that no court of appeals or district court has addressed whether APIs are subject to copyright. But he did point to other precedent, including the 1879 Supreme Court ruling in Baker v. Seldon \u2014 a case that examined whether accounting techniques are copyrightable. The court ruled that bookkeeping methodology could only be protected by patents and that protection under copyright law would \u201cfrustrate the very purpose of publication.\u201d \n \u201cIt is true that Baker is aged but it is not pass\u00e9. To the contrary, even in our modern era, Baker continues to be followed in the appellate courts.\u201d\n He also cited 1994\u2032s Apple Computer, Inc. v. Microsoft Corp., 1992\u2032s Computer Associates International, Inc. v. Altai, and 1986\u2032s Whelan Associates, Inc. v. Jaslow Dental Laboratory, Inc. \u2014 all of which examined whether various aspects of computing are subject to copyright. For Alsup, the upshot is this: If there are only a few ways to express an idea, then no one can claim copyright.\n Names and short phrases are not copyrightable, he said, and copyright protection never extends to any idea, procedure, process, system, method of operation, or concept \u2014 regardless of its form. He also said that functional elements essential for interoperability are not copyrightable. And that includes the Java APIs.\n In many ways, the Google-Oracle battle was a letdown. But in some cases, it rose above the usual monotony. The highlight came when Alsup told the court he had learned to code in Java \u2014 a way of showing Oracle that he wouldn\u2019t let the company pull the wool over his eyes. It was quite a performance, and after looking back on six weeks in his courtroom, where he hit both the attorneys and expert witnesses with the sharpest of questions, we take him at his word. In his ruling, he went so far as to write out lines of code that illustrate methods, classes, and packages. And, well, he got the ruling right.","item_date":"Jun 04 2012 17:54:00","display_item_date":"06-04-2012","url":"http:\/\/www.wired.com\/wiredenterprise\/2012\/06\/google-oracle-api-bookshelf\/","source":"www.wired.com"},{"title":"Alsup is Wrong: APIs Must be Given Copyright Protection","details":"Congratulations to Google for completing the trifecta, coming out victorious on all three key court challenges brought against them by Oracle, and thus making Java more free today than it was at any time in the past. But like so many technical decisions made by a non-technical judiciary and jury pool, Judge Alsup\u2019s conclusion\u00a0on the final bone of contention is wrong. He was wrong to declare that programming API\u2019s cannot be protected by copyright.\n  An API is more than just a collection of components\n  Maybe you cant copyright the idea that a programming language needs a String class. Perhaps its not that creative for an API designer to put a method named\u00a0rangeCheck\u00a0into a component named Integer. Maybe it\u2019s not that earth shattering to put a flush method in the FileWriter class. \u00a0But the Java API is more than just a collection of components with simple methods that, as Judge Alsup commented, a high school student could do.\n  I agree with Judge Alsup on the\u00a0rangeCheck dilemma. Indeed, a competent high school student could competently write the body of the rangeCheck method. I also think a high school class could come up with a decent list of methods that should be part of the Math class. But the Java API isnt just a simple collection of components that exist as little islands in a sea of bytecode.\n  The Java API is beautiful.\u00a0\n  Programmers and developers are often slagged as being a bunch of uncreative nerds, but the best programmers I know are also the most creative people I have ever met. The manner in which classes in the Java API are related through inheritance, associated through interfaces, and intertwined through aggregation is as beautiful to me as any Van Gogh or Rembrandt hanging on the walls of the\u00a0Getty. There is clearly intellectual property hidden there in the Java API, and since its an accepted concession of the US judicial system that intellectual property is copyrightable, then copyright laws should protect the Java API as well.\n  Java developers know the truth\n  Developers know it. Java developers also tend to want Java to be free, so theyre not out there rioting against the courts decision. But Java developers know the truth. Developers know how a solid API justfeels\u00a0right. Developers can see and feel the symmetry, insight, beauty and creativity that goes into a well-developed API. And every developer worth his salt knows how miserable and painful it is to work with an API that was developed hastily, haphazardly and without forethought regarding the real challenges a programmer will encounter when leveraging it to implementing a solution.\n  Good APIs vs bad APIs\n  For example, the Java API is far from perfect. Lets face it, they screwed up the Date class. Every project Ive ever been on is on fire with yellow warning lights in the column of every line of code that invokes the deprecated methods of java.util.Date. Theres still debate on TheServerSide about what should be done with the Date class, and the lack of forethought into the complexity of designing that API component, over ten years ago, still permeates its way through modern enterprise applications. And more to the point, if every component in the API was delivered with so many inherent shortcomings, the Java API would have been tossed in the recycle bin long before its first official release. The reason the Java API is so universally and enthusiastically loved is because of the intellect and insight that went into designing it, and there should be some legal way to protect that type of effort.\n  Every developer who takes their first programming class is completely mystified by the utter silliness of all of the abstract classes and seemingly useless interfaces that pepper the Java API. Implementing all of these codeless interfaces always seems academic and laborious to a newcomer. But theres always a point where a developer leaves the college classroom and starts doing some real development where the light goes on and they hit that Eureka moment where they finally appreciate the benefit and beauty of the way the various Java components participate in interface based polymorphism, or inheritance based abstraction.\u00a0\n  Maybe only developers know what developers know?\n  I dont think any lawyer or Judge on a district court would have any appreciation or understanding of what was written in the previous paragraph, but every Java developer does. Beauty, symmetry, form and functionality are only created out of nothing in the Bible. When it happens in the real world, its a product of intellect and ingenuity, and when that type of ingenuity manifests itself in an API, the minds responsible for it are entitled to some type of protection for their creation.\n  When looking at the API question, perhaps too much emphasis gets placed on the minutia of the code. Looking at individual classes and methods would be like looking at a door and a support wall and telling Frank Lloyd Wright than none of his architectural drawings were worthy of protection because theyre simply a bunch of doors, walls and support beams. Its easy to see the inherent beauty of a architectural drawing of a magnificent building. Its not nearly as easy to see how the same degree of beauty and creativity manifests itself in a magnificent programming API. Thats the problem we run into as the judiciary goes from making complicated legal decisions to making complicated technical decisions.\n  Perhaps software is too complex and ephemeral to protect these days. Perhaps the ability of companies like Oracle to protect their intellectual property is being steamrolled by the open source ethos that is so rampant in the IT community.\n  Today, Java is more free than it was yesterday, but thats only because Judge Alsup made the wrong call by declaring that the Java API could not be protected by copyright.\n  Judge rules for Google, against Oracle, in copyright case","item_date":"Jun 04 2012 00:58:01","display_item_date":"06-03-2012","url":"http:\/\/www.theserverside.com\/news\/thread.tss?thread_id=65210","source":"www.theserverside.com"},{"title":"A Day in the Life of a Developer Evangelist","details":"In the past two years, the explosion in web technologies and apps has created a new profession: It\u2019s called Developer Evangelism, and it\u2019s seriously awesome.\n \u201cIt is an incredible job \u2013 although you still have to explain to a lot of people what you do,\u201d says Christian Heilmann, the Principal Technical Evangelist at Mozilla who literally wrote the book on the profession called\u00a0Developer Evangelism, announcing it as a \u201cnew role in IT companies\u201d in 2009.\n The role doesn\u2019t involve coding all day long (although there\u2019s plenty of that involved). It\u2019s not marketing, PR or sales, although there are plenty of shared adventures to be had. It\u2019s not recruiting, but that happens too. One thing is for sure, it doesn\u2019t involve a lot of sleep. So what is it? We asked Heilmann to define it:\n A developer evangelist is first and foremost a translator. Someone who can explain technology to different audiences to get their support for a certain product or technology. It needs to be someone who is technical but also capable to find the story in a technical message\u2026\u00a0A good developer evangelist can get techies excited about a product by pointing out the benefits for developers who use the product on an eye-to-eye level.\n Much like how the boom in social media birthed new positions that were two parts marketing, one part customer service, developer evangelism is a unique blend of business, product development, customer care and everything in between. They\u2019re spokespersons, mediators, and translators between a company and both its technical staff and outside developers. Although the role is nothing new (Guy Kawasaki was one of the first at Apple), there definitely is an uptick (and competition) for hiring for this position now \u2014 so what do modern day \u201cdeveloper evangelists\u201d look like?\n Twilio\u2019s Rob Spectre\n \n \u201cI\u2019m pretty much a professional skateboarder. Twilio\u00a0gives me sneakers, clothes and sends me around the world to do my thing. It\u2019s just not as cool.\u201d\n In the month of March,\u00a0Rob Spectre,\u00a0Twilio\u2018s Developer Evangelist\u00a0was home in New York City for a total of 7 days.\u00a0First he was at SXSW, where Spectre joined the Twilio team to promote the brand in Austin, Texas. \u201cIt was super guerilla. We coated 2 SUVs in temporary Twilio decals and cruised around launching t-shirts out the windows\u2026 Unlike hackathons, which are dev centric, conferences like SXSW and LeWeb are purely about brand play for us. We go to see and be seen\u2026\u00a0It was non-stop hustling from 9-2am.\u201d\n With a slew of hackathons and meetups in between like Boston\u2019s Startup Weekend and Philly\u2019s\u00a0PennApps, he also spent a week in Barcelona for\u00a0Mobile World Congress. \u201cI was pretty much in a coma after that\u2026I need to find a way to make this sustainable. People are starting to tell me I look tired,\u201d he says laughing. \u201cBut really, I\u2019ve had 2 weekends off since the beginning of the year and one was our company ski trip.\u201d\n What does your day look like? \u201cI\u00a0usually get up around 8 and take a run across the Williamsburg Bridge. Then I get coffee and answer email on the way back from the coffee shop on my phone. Today, I cut a little code in the morning. At noon, we have a team lunch. At 2, I\u2019m meeting with a company to talk about our API. It\u2019s all coffee meetings after that and living off my calendar.\u201d\n Before joining Twilio in August 2011, Spectre was a full-timer at Boxee, a company that lets you cruise the Internet and a slew of fun apps right on your TV. \u201cComing from an engineering background, Twilio\u00a0is my first \u2018soft job\u2019, that is, a non technical job,\u201d he says.\n The Twilio team at SXSW\n When Spectre visited PennApps\u00a0in January of this year, it was at the height of the SOPA\/PIPA debates. He helped 3 students on the\u00a0Grassroutes\u00a0team build a\u00a0deceptively simple widget using Twilio\u2019s API that\u00a0lets visitors call all of their Congressional representatives right from their computers with one click.\u00a0\u201dThese kids get no support from administration or faculty. The school only gave them a hallway to run this thing,\u201d says Spectre.\n A number of well-known companies have tapped into Twilio\u2019s API including GroupMe, Hulu (for customer service) and ZenDesk (for customer service).\u00a0While Spectre has seen a few fun developments using the API such as a hacked\u00a0microwave that uses Twilio SMS to text you when your food is done, this year, Twilio is getting serious for election season as nearly every single campaign is using its API from Obama to Ron Paul. \u201cThey\u2019re like circuses,\u201d says Spectre. \u201cYou have a finite amount of time to make your impact. You have to look bigger than you are and it has to be huge and sensational, but then when it\u2019s over, everyone packs up and it disappears.\u201d\n For the campaign trail, Twilio delivers an insane amount of fluidity in comparison to antiquated options such as renting a call center, connecting outbound lines and buying loads of hardware. Instead of paying these monthly overhead fees, using Twilio cost just $1 per month.\n This past February,\u00a0Twilio released its voice client iOS SDK, which enables developers to create call centers, mobile Skype alternatives, voice-enhanced mobile games, dating apps and social dialers, one of which replaces the keypad with faces of your friends along with other cool features. \u201cIf group messaging was all the rage last year\u2026 social dialing will be a big part of this year,\u201d says Spectre.\n Spectre is one of Twilio\u2019s eight developer\u00a0evangelists: There are three in NYC, one in Austin, TX, one in Seattle, one in Charlotte, one in San Francisco and then there\u2019s\u00a0Stevie Graham, who holds down Twilio\u2019s international footing in\u00a0London.\n Foursquare\u2019s Akshay Patil\n \n \u201cWe see such creativity and are inspired by our developer community so we\u2019re not overly territorial.\u00a0Our attitude to our developers is that \u2018they can build what they want to build\u2019.\u00a0Meanwhile\u00a0we remain confident we\u2019re building the right\u00a0experience\u00a0for our user base.\u201d\n Foursquare\u2018s developer community just reached 20,000, measured by the number of people who create a consumer key to hit its API. \u201cAs our community expands we are going to need people to mature our API offerings. Right now, we\u2019ve given our community attention and love but we\u2019re going to have to be smarter in the ways we communicate, explain our tools and help them,\u201d says\u00a0Akshay Patil, Foursquare\u2019s Platform Evangelist.\n Unlike most developer evangelists, Patil doesn\u2019t travel as much as he could. With requests coming in from all over the world, Patil stays focused at home base in New York City. This year, he will build his developer community team with an additional 5 people, making frequent travel more likely.\n Previous to Foursquare, Patil was a Google Engineer for 6 years and met Foursquare execs Dennis Crowley, Alex Rainert and Harry Heymann while they were working on Dodgeball. He\u2019s also the creator of\u00a0Partychat, which lets you create chat rooms with your friends or co-workers using Google Talk or XMPP.\n Does he have the Foursquare API memorized?\u00a0\u201dYes, at this point, pretty much. I even have dreams in which I\u2019m doing my job and launching cool features,\u201d says Pital.\n The Foursquare team and hackers at NASDAQ\n Foursquare\u2019s API community grows with its user community and is particularly strong in San Francisco and NYC, which may come as no surprise. Patil also says the API gets a lot of love from places like Montreal,\u00a0Paris, London and Tokyo. We asked him to list his favorite uses of the Foursquare API. Check them out below!\n Best uses of Foursquare\u2019s API:\n Timehop: The ultimate daily nostalgia kick\u00a0that reminds you of your Foursquare checkins, Instagrams, tweets, Twitter photos and Facebook posts from a year ago, each day.\n Dont Eat At:\u00a0Sends a text message when you check into a restaurant that is at risk of being closed for health code violations.\n Plan Your Next Trip: The app\u00a0uses Foursquare\u2019s Explorer API to generate a personalized two-day vacations for travelers. (As the winner of\u00a0Foursquare\u2019s Global Hackathon, Plan Your Next Trip\u2019s creator, Benjamin Netter was flown to NYC from Paris to have dinner with Foursquare co-founder Naveen Selvadurai. That\u2019s one pimpin\u2019 API!)\n \u00a0AMEE Location Footprinter:\u00a0Tracks your carbon footprint based on your Foursquare check-ins.\n TripsQ:\u00a0Uses your Foursquare history to map the travel you\u2019ve been doing and shows you how far you\u2019ve flown, what legs you\u2019ve done, and your Co2 footprint based on flights.\n 4sqwifi: Enables you to find nearby venues which offer WiFi \u2014 and their password.\n Peer in Paris: This app\u00a0lists all of the public toilets in Paris, depending on your location. It also lets you check-in if you\u2019d like to.\n The BlindSpot cane: This cane for blind people\u00a0doubles as a smartphone and uses GPS and bluetooth technology, including location based services like Foursquare check-ins to help visually impaired people operate with more normalcy.\n Foursquare-powered Social Cooler:\u00a0It\u2019s pretty straight forward: check-in to the\u00a0Social Cooler on Foursquare, and watch it magically pop open.\n \nCraziest use of the Foursquare API: \u201cThis guy\u00a0built something\u00a0you could pee on and it could check you in. He used an\u00a0Arduino\u00a0board with a moisture detector and our API to check you into a venue\u2026 People\u00a0love social media and excrement! They\u2019ll always find a new way to combine the two,\u201d says Patil.\n Dwolla\u2019s\u00a0Michael Schonfeld\n \n \u201cI get to come in and do whatever the fuck I want and whatever I do people are still happy with it.\u201d\n In mid-April, the Iowa-based startup Dwolla poached\u00a0two of New York City\u2019s most talented players including Aviary\u2019s former Biz Dev Alex Taub\u00a0and\u00a0Nerve Dating\u2019s\u00a0Michael Schonfeld.\u00a0While the two work remotely in New York City\u2019s General Assembly campus, the 20-person team gets together every day on Google Hangout to tell each other what they\u2019re working on.\n As Dwolla\u2018s new Developer Evangelist, I asked Schonfeld how he explains his job to his mom.\u00a0\u201dMost people hear evangelist and think it\u2019s something religious. So, I\u00a0tell my mom that I get other developer to use our API tools, which still means nothing to her.\u201d\n Schonfeld is originally from\u00a0Israel and moved to Los Angeles, California to attend Cal State. After he graduated he knew he wanted to dive into a startup scene. \u201cLos Angeles isn\u2019t the biggest startup scene so it was either New York City or San Francisco. And this just seems cooler,\u201d he says.\n Whats in your job description?\u00a0\u201dI don\u2019t have one. At least, not that I\u2019m aware of. What I do is\u00a0get as many people using our API and getting those API calls maximized in whatever way possible. I do whatever I need to do to make that happen, which means going to hackathons and events all the time, even really obscure events like Kansas City Hackathons.\u201d\u00a0Last month, Schonfeld attended hackathons in\u00a0Kansas City, Iowa and Los Angeles.\n Most productive hours of operation?\u00a0\u201d10pm to 3am, basically \u2019til I just pass the fuck out and can\u2019t work anymore.\u201d\n Do you ever dream about Dwolla\u2019s API?\u00a0\u201dYes, this is horrible. When I start working somewhere, I start thinking about the syntax and lingo. It gets infinitely worse when I get drunk and start thinking about it because it\u2019s hard to build stuff when you\u2019re intoxicated.\u201d\n \u201cI heard about Dwolla first from\u00a0HackerNews\u00a0and that the\u00a0whole big thing was that they were really cheap for transactions. I met Alex Taub during oHours in San Francisco and we stayed friends through Twitter. He told me Dwolla was looking for a developer evangelist and a business development head, so we decided to join together,\u201d explains Schonfeld.\u00a0\u201dNow, we go out together and meet new companies.\u00a0Alex pimps me out and says, \u2018If you guys want this, Michael will integrate this for you in a matter of days\u2019\u2026and then I do.\u201d\n Michael Schonfeld\u00a0and Alex Taub at New York City\u2019s General Assembly\n The coolest API user Schonfeld has recruited thus far is Justin Kan, the founder of Justin.TV, who wanted\u00a0an inexpensive, reliable and easy way of paying out his Execs (personal helpers). Justin started a new venture out in SF called\u00a0GetExec\u00a0that allows people to find Execs in real time (\u00e0\u00a0la TaskRabbit). \u201cHe was great to work with, because he\u2019s both the Founder, and the Programmer. It\u2019s very rare to have a company dive right into integration during our first meeting\/phone call \u2013 and that\u2019s exactly what happened with Justin,\u201d says Schonfeld.\n Schonfeld is currently working on an integration with\u00a0reddit, \u201cthe front page of the Internet\u201d that will allow users to fundraise for causes they find in\u00a0subreddits, as well as integrations with NYC-based art startup,\u00a0Artsicle\u00a0and Cindy Gallop\u2018s stealth project\u00a0Make Love Not Porn.\n Most recently, Schonfeld, who tweets by the name @BaconSeason, put out an impresive product with a boring name:\u00a0The Payment Fees\u00a0Calculator, which\u00a0compares the fees you\u2019d pay when moving money around, using one of the leading payment gateways in comparison to Dwolla.\n SoundCloud\u2019s Paul Osman\n \n \u201cAs a developer evangelist, I think it\u2019s important that you stay a developer. It\u2019d be hard to put myself into a developer\u2019s shoes otherwise. By building apps with our platform, I can get that 3rd party perspective and point things out that I might not have noticed otherwise.\u201d\n Paul Osman, the Developer Evangelist at SoundCloud,\u00a0left Mozilla this past February, because he wanted to work more closely with developers again and people in the developer community. When asked how he defines a Developer Evangelist he said:\u00a0\u201dSomebody who represents developer\u2019s interests inside of the company.\u201d\n Unlike Twilio\u2019s Spectre, travel for Osman has been fairly light including one trip to San Francisco\u00a0for Music Hack Day and a week of bonding in the SoundCloud office.\u00a0His day-to-day schedule includes monitoring the support channels and making sure people who post questions to the company mailing list or over the web are helped out. \u201cBasically, anyone struggling with the SoundCloud API, I try to help out. I also spend quite a lot of time writing sample code and trying to figure out where there are holes in our documentation,\u201d he says.\n Osman also reaches out to developers who use SoundCloud\u2019s API in particularly neat ways or ways they\u2019d like to showcase. One such app, written at\u00a0Music Hack Day was\u00a0whsprs.co, which\u00a0lets you play a game of telephone using SoundCloud. The app, built by\u00a0Ian Butterworth\u00a0lets users connect with their SoundCloud account, and using the Javascript SDK they can record a \u2018whisper\u2019. Other users can look for a whisper recorded by someone nearby and repeat it, and so on and so on.\n Looking forward, Osman says he\u2019d like to see more visibility into what people are doing with the SoundCloud API. (The company currently has an app gallery that shows a few things going on.) \u201cI think people who use SoundCloud in interesting ways aren\u2019t getting the attention they deserve,\u201d he says.\n Osman is working on a number projects at SoundCloud Developers including more frequent blog posts, developer contests, more curation and more aggressive improvement of their API documentation.\n Twitter\u2019s Jason Costa\n \n \u201cTwitter feeds us\u00a0breakfast, lunch and dinner. Thank god it\u2019s catered. Now, if we can just get our laundry done here\u2026\u201d\n In March,\u00a0Twitter announced\u00a0it had 140 million active users sending 340 million tweets per day. The San Francisco-based startup\u2019s value has been estimated at $4 -$10 billion with revenues of $100 \u2013 $110 million per year, and grandmas the world over are still confused as to exactly what the micro-blogging platform is, and why everyone in the world can\u2019t stop tweeting.\n As Twitter\u2019s Head of Platform,\u00a0Jason Costa gets to wear many different hats and work in diverse areas. The three main things he does on day-to-day basis are first, addressing pent up market demand in the ecosystem, and second, matchmaking between the multitude of global brands and large companies that want to get a lot more value of out Twitter.\u00a0Costa tells me a few of the more notable companies using Twitter\u2019s API include\u00a0SocialFlow,\u00a0Crowdbooster,\u00a0Simply Measured,\u00a0Lanyrd\u00a0and\u00a0CrowdTwist.\n His third job and primary focus is growing the ecosystem and spurring adoption of the platform, which recently hit 1.5 million registered apps. Has he memorized Twitter\u2019s API? You bet he has. Does he dream about it? \u201cI definitely have,\u201d he says. \u201cI\u2019m always working with our API, looking for certain issues and trouble shooting both for internal consumption and for our developer community.\u201d\n Costa comes from a very impressive background starting with a degree from USC in Engineering as well as a graduate degree from MIT\u2019s Sloan. He joined Twitter in 2008 after working at Facebook as a Product Manager on its mobile team and at Google as a Technical Program Manager.\n Twitter\u2019s developer community is hot the world over, especially in Tokyo, Seoul, New York City, London and Berlin.\u00a0While Costa tries to be conservative with travel, it\u2019s not uncommon for him to visit 9 cities and 3 different continents to meet with thousands of devs in the course of a few months. \u201cI love meeting with developers, but it needs to be done in a scalable way,\u201d he says.\n Currently, when people host Twitter meetups in cities such as Malaysia, Seoul, London, New York, Amsterdam and Dublin, Twitter supports them by appointing a local community ambassador to take care of the logistics and then Skype-ing in for a Q&A. For example, Twitter Developer Tea Times are popular in Malaysia and Singapore.\n Instead of traditional hackathons, Twitter hosts 72-hour \u201cinformation dissemination exercises\u201d that draw developers and partners from all over to meet the Twitter team and be able to ask questions. The next similar event will be held in June when Twitter hosts its\u00a0Platform Services Open House\u00a0in New York City.\n Sagely Advice from Mozilla\u2019s Christian Heilmann\n \n After interviewing the 5 developer evangelists above, we asked Heilmann\u00a0(Principal Technical Evangelist at Mozilla and author of the handbook:\u00a0Developer Evangelism) for a few bits of sagely advice that he was kind enough to share with us below.\n On Travel: \u201cIt is very important to be visible. This could be in-person, around the globe but it is also as important to be online with answers in the right discussion threads, screencasts, videos and the like,\u201d says Heilmann.\n \u201cTraveling can be really hard \u2013 my schedule was and still is nuts which is why I want [Mozillas] Evangelism Reps program to kick off so people can speak in my stead. This is not easy as once you made a name of yourself as being a good speaker with great talks conference organisers will always want you specifically. It is very important to say no from time to time and give good reasons. Don\u2019t leave people hanging wondering if you\u2019d do it or not.\n Heimann recommends choosing events that will record or stream talks that way you\u2019ll be able to reuse them in the future.\n On Becoming a Developer Evangelist: \u201cIf you haven\u2019t worked in delivery of products and felt the pain of developers out there, don\u2019t try to become an evangelist. You need to come from the trenches or you\u2019ll be ripped to shreds by a very knowledgable and cynical audience,\u201d he says. \u201cAlso be aware that your integrity is your main weapon. You need to be known as someone who cares and knows about technology, not as a spokesperson for a certain product.\u201d\n On Working with your Company: \u201cInformation you give out needs to be backed up by the people who deliver the products you talk about or it can get very embarassing very quickly,\u201d he warns. \u201cYou are not a performing monkey for engineers in your company, you are their channel out to the world and translate what they do to different audiences.\u201d\n The Future of Developer Evangelism: \u201cI can see evangelism\u2026 to become a more known part of companies and merge with or take over a lot of the traditional marketing and PR roles. More and more companies realise that developers are just another audience you need to care for as much as you do for end users. I am lucky to work with product people and PR people who get what I want to achieve and work with me instead of seeing me as a threat. I hope that more companies understand that.\u201d","item_date":"Jun 03 2012 18:33:13","display_item_date":"06-03-2012","url":"http:\/\/thenextweb.com\/dd\/2012\/06\/03\/a-day-in-the-life-of-a-developer-evangelist\/?awesm=tnw.to_1Ef9D","source":"thenextweb.com"},{"title":"WSDOT Traveler Information API","details":"To use WSDL services you must be assigned an Access Code. Please enter your email                  address to receive your code. Your email address will not be shared and will be                  used only to notify you of changes to our services.","item_date":"Jun 03 2012 18:16:01","display_item_date":"06-03-2012","url":"http:\/\/wsdot.wa.gov\/Traffic\/api\/","source":"wsdot.wa.gov"},{"title":"DNS Made Easy Releases REST API 2.0","details":"DNS Made Easy (dnsmadeeasy.com), the leading IP Anycast managed DNS service provider, announced the production release of DNS Made Easy API 2.0 today. This version brings a number of additional features to the DNS Made Easy RESTful API previously only available through the DNS Made Easy control panel. API version 2.0 builds on core functionality from the first API release by adding access and modification calls for DNS Failover and System Monitoring records, query usage, and domain templates. \n  Users of the DNS Made Easy API can now enable DNS Failover and System Monitoring on A records, as well as configure and view DNS Failover and System Monitoring settings on A records under one or more domains. DNS Failover and System Monitoring is an add-on service to the DNS Made Easy Managed DNS service designed to increase redundancy of end systems.\n  Domain query usage is also now accessible through the DNS Made Easy API. Users have the ability to retrieve daily statistics for one or more domains under their account.  Query usage data can provide valuable insight into Internet traffic patterns for customer domains, as well as provide easy access to individual domain usage tracking and cost analysis. \n  A third significant update included in API 2.0 is the ability to view, create, modify, and apply domain templates. Domain templates have long been a useful feature in the DNS Made Easy control panel to make changes to many domains concurrently. The addition of domain templates to the DNS Made Easy API will bring ease of management to users that have many domains within DNS Made Easy\u2019s Managed DNS service.\n  DNS Made Easy REST API 2.0 closes the gap on the functionality that was only available in our world renown control panel, states Steven Job, President of DNS Made Easy. With the release of these new REST API features DNS Made Easy is continuing to show our strength in engineering excellence.\n  More information about DNS Made Easy suite of services is available at http:\/\/www.dnsmadeeasy.com.\n  About DNS Made Easy   \nDNS Made Easy is a subsidiary of Tiggee LLC and is a leader in providing global IP anycast enterprise DNS services. DNS Made Easy implemented the industrys first triple independent anycast cloud architecture for maximum DNS speed and DNS redundancy. DNS Made Easy originally launched their DNS services and web management portal in 2002 and has grown to manage hundreds of thousands of customer domains receiving more than 5.0 billion queries per day. DNS Made Easy has a proud history of industry leading uptime and ROI since launching their DNS service. DNS Made Easy has been the preferred choice for most brands on the Internet compared to the over-priced and under-performing enterprise IP anycast alternatives.\nTiggee LLC (tiggee.com) is a privately held company, formed in 1999 to provide innovative internet solutions for businesses. Putting your business on the Internet is our business. Tiggee LLC\u2019s subsidiaries include DNS Made Easy (IP anycast enterprise managed DNS services \u2013 dnsmadeeasy.com), VPSit (enterprise virtualization environments \u2013 vpsit.com), Host Made Easy (dedicated server hosting and co-location services \u2013 hostmadeeasy.com), Top Level DNS (TLD name services \u2013 topleveldns.com), Resolving Name Server (resolvingnameserver.com), Show My Host (showmyhost.com), Reverse DNS Trace (reversednstrace.com), IPv6 Verify (ipv6verify.com), and Domains Made Easy (domainsmadeeasy.com).","item_date":"Jun 03 2012 18:15:18","display_item_date":"06-03-2012","url":"http:\/\/www.prweb.com\/releases\/dns\/management\/prweb9564929.htm","source":"www.prweb.com"},{"title":"OData and Impact on API Design (video & slides)","details":"The video and slides for the session are below. Thanks to @gbrail and @jhingran and our moderator @brianpagano. Wed love to continue the discussion on the\u00a0api-craft forum.\n (To those who were on the live webcast - our sincere apologies for the audio & technical difficulties.)\n \n OData Introduction and Impact on API Design (Webcast)  View more presentations from Apigee","item_date":"Jun 03 2012 18:14:54","display_item_date":"06-03-2012","url":"http:\/\/blog.apigee.com\/detail\/odata_and_impact_on_api_design_video_slides\/","source":"blog.apigee.com"},{"title":"Update to LinkedIn\u2019s Privacy Policy","details":"We\u2019d like to let our members know that a week from now, on June 7th 2012, we\u2019re planning to update our\u00a0Privacy Policy and\u00a0User Agreement to provide you greater control over your data with further clarity on our terms.\n Here are a couple of key areas the updated Policy will encompass:\n We have shortened the time we keep personally identifiable information obtained through our plug-ins and our off-site advertising to 24 hours.\n We have enhanced member privacy controls by making your public profile settings determine what information can be accessed outside of LinkedIn by search engines and third parties.\n \nEnsuring more privacy and control over your personal data remains our highest priority. We\u2019ll be updating this post with links to the latest version of our Privacy Policy.\n Stay tuned for more information in the weeks to come.\n       \n        \t    Topics:              Privacy","item_date":"Jun 03 2012 18:14:36","display_item_date":"06-03-2012","url":"http:\/\/blog.linkedin.com\/2012\/06\/01\/new-linkedin-privacy-policy\/","source":"blog.linkedin.com"},{"title":"Hey Pinterest, When Will a Public API be Available?","details":"Last week I read an article written by Kin Lane\u2019s Lack of Pinterest API is a Lack of API Business Strategy. The article points out that Pinterest has been developing a public API since May 2011. Though there once was API documentation for Pinterest, the company has yet to officially make the  Pinterest API available to the public.\n\n\nIt is understandable that it is taking a long time for Pinterest to develop a public API. API development is very complex and many factors need to be considered and addressed before building and releasing an API. Lane provides several suggestions for Pinterest to use appropriate API building blocks in the development of their API. The suggestions include Pinterest providing to web developers; Branding Guidelines, a UI \/ UX Toolkit, a real time Roadmap, and a fair Terms of Use Policy.\n\n\n\n\n\nCompanies like Twitter, Facebook and Google have been using web APIs for some time now as a means to market their products and services, and to help create a loyal fan base. Now that Pinterest is the third most-popular social network in the U.S. (behind Facebook and Twitter), many web developers are looking for Pinterest to finally release an API. There is even an Unofficial Pinterest API Group (a closed\/non-public Facebook Interest Group) that currently has 2,702 members interested in developing web applications using the Pinterest API.\n\n\n The Pinterest site support section states that \u201cWe currently do not have a public API,\u201d however you can fill out a \u201cbrief contact form to be one of the first to know when it\u2019s ready.\u201d \n\n\n There is clearly a great deal of interest in a Pinterest API. So the question is \u201cHey Pinterest, When Will a Public API be Available?\u201d","item_date":"Jun 03 2012 05:54:32","display_item_date":"06-02-2012","url":"http:\/\/blog.programmableweb.com\/2012\/06\/01\/hey-pinterest-when-will-a-public-api-be-available\/","source":"blog.programmableweb.com"},{"title":"Open API's to Fuel Your Software E-commerce Program","details":"The serious e-commerce business is going to find every way possible to build its business into a stronger, more formidable opponent in the Online market place. Plopping up a quick shopping cart does not in itself create this strength. More sophistication is required on your part. This is where APIs come into play.\n You can add your own products to many shopping sites through APIs, you can add products from big sites into your website through their APIs, or you can build your own API to others to integrate their products and services into your website or webapp.\n You should integrate your product catalog or database into any sites that offer this, where many customers already look for their purchases, if you have your own products and services. But if you do not want to create your own products and you simply want to be an affiliate, there are many who offer APIs that allow you to tap into many of the big guys sites. If you want to be one of those big guys, though, you can do it by building your own API and offering it to online shops for them to tie into. Which route you take depends on your resources and your dedication to fueling your e-commerce venture.\nUtilizing Others APIs to Build Your E-commerce\n Amazon, eBay, CafePress, and many more offer APIs to either connect your product databases to their sites where your products are then found or to link products from their sites into yours. What could you do with that? You could build an interface, appliance, or application that draws from their product database. This relieves you of building your own database of products, saving you the money of data entry and maintenance. More importantly it relieves you of keeping inventory, shipping, returns, and many other headaches.\n Imagine spending time developing the user experience and not worrying about the product data. This also solves the dilemma of product data unification. One additional benefit is tapping into multiple product databases. You could draw on Amazon and eBay and present the items within a single user interface.\n You can lean on such APIs as those from CafePress and Zazzle to tap into on-demand manufacturing. Turn to SwapThing if you want to build a bartering project. If product search and comparison is up your alley, check out the APIs from Shopping.com, Amazon, Shopzilla, eBay, or CNET. Once you are ready to build a checkout, most payment gateways offer their own specialized APIs, like Google Checkout, Authorize.net, PaySimple, or Paypal. Coupons are a special niche that you can access through Zixxo. Reading the UPC and EAN is easy with the help of the UPC database API. Affiliate network APIs come from such sources as Commission Junction and LinkShare. You can even sink your teeth into review APIs of the likes of Yelp, CityGrid, and GoodGuide. Location oriented APIs are found at Milo and Retailingence.\n Now let us talk money for a moment. The most prevalent revenue models for APIs are Pay-per-Click (PPC) and Pay-per-Action (PPA). The former charges a fee for each click through made by a visitor, part of that fee is paid to the developer as well. The latter is akin to an affiliate commission awarded when a visitor, referred to a place like Amazon.com, makes a purchase.\n The real gem in this is niche shopping segments. They take from multiple sources and pull it all together in one place. The combinations are untapped. If I were to focus somewhere, it would be here.\nLets Mash It Up!\n Take a little of everything and tie it into every presence your e-commerce has on the Internet, then give your customers everything in one place. There are certain elements you really should not overlook when mashing up an e-commerce project. Let us look at four angles of mashups: Social Mash, Comparison Mash, Location Mash, and Review Mash.\nSocial Mashing\n The most obvious oversight someone could make is missing the Social Mash. There is nearly no overhead from these, yet they are extremely effective. Forget the mere postings and likes and move beyond to displaying your entire catalog or even shopping carts in Facebook. Tie in the product info, browsing history, and shopping cart from your e-commerce platform and you can make a bang up user experience.\nComparison Mashing\n During all this time you should be thinking mobile mashup orientation, because, in fact, comparison of products is what most mobile users rely on their phones for. Shopping.com, Redlaser, and ScanLife are all relevant. The last two take bar code scans from your phone and cough up the details. Your website or webapp can tap into these powerful capabilities through these companies APIs, placing your products into the pool of available ones.\nLocation Mashing\n Your inventory should be tied into location sites like ShopKick, Groupon, and Google Maps through their APIs. Then your products will show up in searches on these sites and local customers in your area will find you. You could also tie in your products that you offer on your site, drawn from other APIs on other sites, to the search function for location.\nReview Mashing\n Review mashing, along with social and comparison mashing, are big on steering the customers buying habits. You can tie in with review sites so they can pull reviews from your site, but be absolutely certain they have a link back to you. The most notables in this category are Amazon, Pickii, and Yelp.\nConstructing Your Own API to Expand Your E-commerce.\n Another route you could go with your e-commerce business is to create your great e-commerce site and develop an API for resellers to use in tying in their product inventory to your site. They maintain their product inventory and you maintain the user experience. It takes some programming know-how, but you could manage to construct it with a scripting language which is easy, like Php.\nThe Final Word\n This is the age of networking and nothing says networking in the e-commerce world louder than API. If you ignore this prong of your e-commerce strategy, you will certainly flounder about, leaning only on SEO. You need to branch out, think bigger, extend your reach. Certainly, these big sites you plug into with your product database are also your competitors. That is a concern, but you should see this from a different angle. You have the possibility of leveraging their traffic to pick up some sales and win some customers. How you capture those customers is the challenge in this part of the game. You must develop that part of your strategy most carefully, because this is how you will really kick the likes of Google in the ass, while using their own APIs. And we all know we would love to do that to the big G.","item_date":"Jun 03 2012 05:53:09","display_item_date":"06-02-2012","url":"http:\/\/www.digitalfacts.net\/2012\/06\/open-apis-to-fuel-your-software-e.html","source":"www.digitalfacts.net"},{"title":"Why APIs? Anatomy of an Open API Initiative","details":"We\u2019ve looked at internal, partner, and customer API initiatives. This time we\u2019ll look at the Open API initiative, probably the most  familiar of strategies given the success of companies like Twitter,  Foursquare, Facebook . . .\n A lot of companies are inclined to start with an Open API using Twitter, Foursquare, or Facebook as the archetype. We generally recommend against this approach. Rather, we recommend an open strategy after a business has learned lessons and mitigated risks by executing an internal or partner model first.\n After the API team has learned from internal and partner projects there will be a vast amount of institutional wisdom and courage for opening the API to the world of innovative developers.\n Anatomy of an Open API Initiative\n There are no absolutes and it doesn\u2019t always work out that the flow is from internal to partner to open. Netflix has seen huge success by starting with an open API, then working with partners who were building streaming services for specific connected devices, and are now using their API more and more internally as well. For example, internal developers re-engineered their private streaming API by using the public API. \u00a0\n Here are some of the use cases served by an open API initiative.\n Breakthroughs\n This is the most common of all use cases and a well-known paradigm given the successes of companies like Twitter, Foursquare, or Facebook. This is the case for innovation by leveraging the creativity and know-how of 100s of 1000s of developers around the world using your API to create cool apps and make big breakthroughs.\n If you target an Open API initiative, chances are that your internal or partner strategies will go well too. It is important to be careful about setting expectations. A lot of things need to go right to get huge numbers of developers successfully creating apps on your APIs.\n A company may have a geographical or demographic niche that represents a nice new value proposition for the business. But it may not have the resources or the budget to get the value proposition into those niches. With an open API program, a developer can create an app that would represent enough revenue to be a change in her lifestyle and at the same time impact the original business in a positive way. A win-win!\n Directed\n Netflix started with a directed approach. They ran a contest in the academic researcher marketplace and offered a substantial dollar reward to incent researchers to use the Netflix API to create a better movie recommendation system. We\u2019ve seen a similar approach - inspiring research to help solve a problem - used successfully in the automotive industry. It extends R&D budgets and resources beyond the borders of your business and spurs innovation on a broad scale.\n The Enterprise API Engine\n Weve observed that many successful API initiatives are done in stages starting Internal and expanding to partners and customers and then possibly open, each stage building on the preceding one.\n But again, there are no absolutes, and generally when you know you need an API, you should start where you have the most amount of pain. Start where the business drivers are - which might be Internal if you are meeting demand for mobile and social apps, or Partner if you need to innovate with partners to deliver on a backlog of business development opportunities, or Open if you need to inspire a broad community of app developers to innovate and create growth and new opportunities.\n When a business gets all four scenarios working together it creates a strong enterprise API engine.\n You are on the road to having a robust platform that allows you to compete in the world of ubiquity but where budget and resources to keep up with that ubiquity are not keeping pace. An API strategy future-proofs your business.","item_date":"Jun 03 2012 05:34:37","display_item_date":"06-02-2012","url":"http:\/\/blog.apigee.com\/detail\/why_apis_anatomy_of_an_open_api_initiative\/","source":"blog.apigee.com"},{"title":"Government transparency: The best disinfectant","details":"We use cookies to support features like login and allow trusted media partners to analyse aggregated site usage.      Keep cookies enabled to enjoy the full site experience. By browsing our site with cookies enabled, you are agreeing to their use.      Review our cookies information for more details.\n                                                                                                              Government transparency\n           The best disinfectant\n       Hopes of \u201copen government\u201d under Barack Obama have been only partly fulfilled\n               \n BARACK OBAMA accepted an award honouring his administration\u2019s commitment to transparency on March 28th 2011. It was given by a coalition of open-government advocates. But the meeting was closed to reporters and photographers, and was not announced on the president\u2019s public schedule. Occasionally life provides perfect metaphors.\n On his first full day in office Mr Obama declared that \u201cgovernment should be transparent,\u201d and said that his administration \u201cis committed to creating an unprecedented level of openness in government\u201d. And so, in December 2009, he issued his Open Government Directive, which ordered federal departments to formulate and publish plans to become more transparent. Those plans were all duly published within five months. Also that December he created a new National Declassification Centre (NDC), designed to streamline the declassification of government documents.\n              In this section\u00bbThe best disinfectant\n \n   The federal government now publishes a vast array of data at Data.gov. At Recovery.gov meanwhile, citizens can track how their stimulus funds were spent. Foia.gov, launched in March 2011, lets people see whether agencies are fulfilling their obligations to disclose information under the 1966 Freedom of Information Act (FOIA). That act governs what information must be released to the public. It is federal, but all 50 states have their own versions governing what records and meetings are public. These regulations are commonly known as sunshine laws.\n Eric Holder, Mr Obama\u2019s attorney-general, issued FOIA guidelines to agency and department heads encouraging disclosure, and promising that his Justice Department would take a much narrower view of what constitutes a defensible withholding of information than his predecessor\u2019s had. Mr Obama has not once claimed executive privilege to avoid turning over information to Congress. His predecessor, George Bush, made six such claims during his two terms, and the previous Democratic president, Bill Clinton, made five.\n And yet in this arena, as in others, Mr Obama has been better at rhetoric than reality. David Sobel, senior counsel for the Electronic Frontier Foundation, a digital-rights group, says that despite Mr Holder\u2019s promise of greater FOIA disclosures, \u201cThose of us who file litigation in this area haven\u2019t really seen that happen.\u201d The NDC has made public a paltry 22.6m of the 400m pages of classified data that is currently in its backlog. The rate of document classification remains far higher than the rate of declassification.\n Mr Obama\u2019s administration is proving as fond of wartime secrecy as the administration he replaced. The American Civil Liberties Union is suing under FOIA to get it to reveal records of the use of drones by the CIA and the armed forces to kill particular people. The CIA\u2019s response has been to \u201cneither confirm nor deny the existence or non-existence of records responsive to this request\u201d.\n Yet perhaps none of Mr Obama\u2019s transparency promises has rung hollower than his vow to protect whistleblowers. Thomas Drake, who worked at the National Security Agency, was threatened with life imprisonment for leaking to the Baltimore Sun unclassified details of a wasteful programme that also impinged on privacy. The case against him failed\u2014ultimately he pleaded guilty to a misdemeanour charge of \u201cexceeding authorised use of a computer\u201d\u2014but not before he was hounded out of his job. Mr Obama\u2019s administration tried to prosecute him under the Espionage Act, a law passed in 1917 that prohibits people from giving information \u201cwith intent or reason to believe that it is to be used to the injury of the United States or to the advantage of a foreign nation\u201d. Mr Obama has indicted six whistleblowers, including Mr Drake, under the Espionage Act, twice as many as all prior administrations combined, for leaking information not to a \u201cforeign nation\u201d but to the press.\n All of this comes despite the fact that whistleblowers often do a great deal of good: in 2010, for instance, 77% of the $3.1 billion that America won in fraud-related judgments and settlements came from suits brought by them. Of course any government is entitled to keep some secrets, and of course people who leak genuinely damaging information ought to be prosecuted. But the prosecution of Mr Drake and others like him smacks more of vindictiveness and message-sending than justice.\n If the federal government is dragging its feet, however, several states are powering ahead. Sam Olens, Georgia\u2019s attorney-general, led an effort to update Georgia\u2019s open-records and open-meetings laws. Mr Olens said that the laws, which had not been overhauled in many years, had grown convoluted and ambiguous. He also cited complaints that local governments were ignoring open-records requests. The changes Mr Olens championed\u2014and which Nathan Deal, Georgia\u2019s Republican governor, signed into law on April 17th\u2014lower the cost of records obtained by the public from 25 cents a page to 10 cents, and require agencies to alert requesters if the records will cost more than $25 to provide (sunshine laws require agencies to provide information; they do not require it to be free). They increase the penalties for officials who violate the law, and let prosecutors bring civil charges, not just criminal ones, against violators.\n The law includes data, data fields and e-mail in its definition of public records, and it lets citizens bring their own devices to a government agency to make electronic copies. Georgia thus joins a growing number of states that explicitly open electronic communication to and from government officials to the public. Among the most ambitious such programmes is Florida\u2019s Project Sunburst, which its Republican governor, Rick Scott, announced on May 2nd. E-mails sent or received by the governor or 11 of his senior staff members will be posted to a searchable database within a week of transmission. Another of Mr Scott\u2019s initiatives provides extensive pension and salary information for state employees.\n Some complain that by posting such information Mr Scott, whose budgets have trimmed Florida\u2019s already lean workforce, is stoking public anger against state employees and their generous pensions. Others warn that Sunburst\u2019s requirements can be avoided by such archaic practices as talking in person or by phone rather than by e-mail. And of course government employees can always skirt open-records laws by using personal rather than official e-mail accounts.\n But even so, the data show that in states with strong FOIA laws politicians are less likely to be corrupt, and those that are corrupt are more likely to be caught. \u201cSunlight\u201d, wrote Louis Brandeis, a Supreme Court justice, nearly a century ago, \u201cis said to be the best of disinfectants.\u201d","item_date":"Jun 01 2012 05:00:10","display_item_date":"05-31-2012","url":"http:\/\/www.economist.com\/node\/21555924","source":"www.economist.com"},{"title":"No Copyrights on APIs: Judge Defends Interoperability and Innovation | Electronic Frontier Foundation","details":"Innovation for the win: A federal judge ruled\u00a0today that Javas APIs are not copyrightable. The federal district judge in the widely reported Oracle v. Google case ruled in favor of innovation and interoperability, allowing software to use\u00a0Application Programming Interfaces\u00a0without paying a license fee. Judge Alsups opinion is important\u00a0news for software developers and entrepreneurs.\u00a0\n To recap: Oracle, the current owner of Java, sued Google for, among other things, using Java APIs in its Android OS. Oracle claimed that Google infringed both its patents and copyrights. The Court disagreed, and Judge Alsup ruled that \u201cGoogle and the public were and remain free to write their own implementations to carry out exactly the same functions of all methods in question.\u201d\n Earlier, the jury summarily disposed of Oracles patent claims and also found that, assuming one could get a copyright on an API, Google might have infringed (the jury failed to answer whether Google\u2019s use was a legal fair use). All of this left open arguably the most important question: whether APIs could be copyrighted. \u00a0As we previously explained, the answer must be no under current law, and\u00a0extending copyright to APIs would have a disastrous effect on interoperability, and, therefore, innovation. We are glad to report that Judge Alsup agreed.\n The court clearly understood that ruling otherwise would have impermissibly \u2013 and dangerously \u2013 allowed Oracle to tie up \u201ca utilitarian and functional set of symbols,\u201d which provides the basis for so much of the innovation and collaboration we all rely on today. Simply, where \u201cthere is only one way to declare a given method functionality, [so that] everyone using that function must write that specific line of code in the same way,\u201d that coding language cannot be subject to copyright.\n Judge Alsup, a coder himself, got it right when he wrote that \u201ccopyright law does not confer ownership over any and all ways to implement a function or specification of any and all methods used in the Java API.\u201d Its a pleasure to see a judge so fundamentally understand the technology at issue; indeed the first part of the opinion reads like an Introduction to Java class (and, to be certain, if Oracle appeals, Judge Alsups lession will do a fantastic job teaching the appeals court how Java works). Its that fundamental understanding that allowed Judge Alsup to explain:\n That a system or method of operation has thousands of commands arranged in a creative taxonomy does not change its character as a method of operation. Yes, it is creative. Yes, it is original. Yes, it resembles a taxonomy. But it is nevertheless a command structure, a system or method of operation \u2014 a long hierarchy of over six thousand commands to carry out pre-assigned functions. For that reason, it cannot receive copyright protection \u2014 patent protection perhaps \u2014 but not copyright protection.\n Judge Alsup\u2019s opinion implicitly recognizes that the copyright laws, mostly recently overhauled in the 1970s, simply were not intended to cover claims like those made by Oracle in this case. Here, Oracle poured through 15 million lines of Android code searching for infringment, and found only nine lines (one function!) that had been copied from Java, a circumstance the Court found \u201cinnocuous and overblown.\u201d Such functionality may be subject to patenting, which has a shorter life span and more opportunities to challenge its validity, but Oracle\u2019s attempts to shoehorn its upatented APIs into copyright law were met with the proper rejection.\n Its not all good news for innovation: in yet just another example of an intellectual property system gone awry, this lawsuit has likely already cost each side millions (if not tens of millions) of dollars (and that\u2019s before damages). Those resources, including the person-hours, can and should be dedicated to developing new technologies and business models, not improving a few law firms bottom lines.\u00a0Oracle v. Google is just the latest in a long line of cases that ratchet up high-stakes litigation surrounding intellectual property rights \u2013 whether it be software patents or copyrights. This dangerous trend creates insurmountable barriers to entry and harms innovation. If this process has taught us anything, it is that this practice needs to stop. This is why EFF will continue to fight for an intellectual property system that has the breathing room to allow for innovation.\n And in the meantime, developers everywhere can breathe a sigh of relief\u00a0\u2013\u00a0this judge got it right.","item_date":"Jun 01 2012 02:02:44","display_item_date":"05-31-2012","url":"https:\/\/www.eff.org\/deeplinks\/2012\/05\/no-copyrights-apis-judge-defends-interoperability-and-innovation","source":"www.eff.org"},{"title":"Judge Frees Google's Android From Oracle Copyrights | Wired Enterprise | Wired.com","details":"The federal judge refereeing the billion-dollar fight between Oracle and Google over the Android operating system has dismissed Oracle\u2019s claim that the Java APIs used by Android are subject to copyright.\n The APIs are application program interfaces, code that lets one piece of software talk to another. The general assumption has long been that APIs aren\u2019t subject to copyright. But in suing Google over Android, Oracle insisted that they were, and after a six-week trial, the company\u2019s efforts to win serious damages from Google came down to this single point. \n But on Thursday, Judge William Alsup ruled that Oracle does not have the exclusive rights to the structure, sequence, and organization the 37 Java APIS in question.\n \u201cTo accept Oracle\u2019s claim would be to allow anyone to copyright one version of code to carry out a system of commands and thereby bar all others from writing their own different versions to carry out all or part of the same commands,\u201d read the ruling from Alsup. \u201cNo holding has ever endorsed such a sweeping proposition.\u201d\n Oracle said it would \u201cvigorously pursue an appeal\u201d of the decision. But the ruling is good news for many companies and developers across the tech industry that build software platforms that clone existing APIs. Most notably, this includes cloud services that mimic the APIs of Amazon\u2019s wildly successful EC2 service.\n \u201cThe court\u2019s decision upholds the principle that open and interoperable computer languages form an essential basis for software development,\u201d read a canned statement from Google. \u201cIt\u2019s a good day for collaboration and innovation.\u201d\n Oracle soon issued a response. \u201cOracle is committed to the protection of Java as both a valuable development platform and a valuable intellectual property asset,\u201d the statement read. \u201cThis ruling, if permitted to stand, would undermine the protection for innovation and invention in the United States and make it far more difficult to defend intellectual property rights against companies anywhere in the world that simply takes them as their own.\u201d \n With his ruling, Alsup said that in cloning the 37 Java APIs, Google wrote 97 percent of the code from scratch and that the remaining three percent was lifted in accordance with the law. He also said that out of the 166 Java software packages controlled by Oracle, 129 were in no way infringed upon by Google. Oracle cannot legally claim, he argued, that it owns all possible implementations and pieces of the command structures of all 166 APIs. \n Alsup added, however, that his order does not mean that the Java API packages are free for all to use without license or that the structure, sequence, and organization of all computer programs may be \u201cstolen.\u201d Google, he said, had simply acted appropriately under the U.S. Copyright Act.  \n In August of 2010, shortly after acquiring Sun Microsystems, the maker of Java, Oracle sued Google, claiming both copyright and patent infringement. But the meat of the case involved copyrights, as Oracle boss Larry Ellison pointed out during a public appearance on Wednesday, before Alsup\u2019s ruling came down.\n During the trial, Judge Alsup \u2014 who said he had learned to code in Java for the case \u2014 told the jury that when considering the arguments from Oracle and Google, it should assume that APIs are subject to copyright. Presumably, he wanted to avoid making a ruling on APIs and copyright if the jury had found that Google had not infringed. But in the end, the jury reached a partial decision that did not completely absolve Google and the Judge was forced to make a ruling. \n The jury found that Google had infringed on Oracle\u2019s copyrights cloning the APIs, but it couldn\u2019t decide whether this infringement constituted fair use under the law. \n On Wednesday, Ellison said he considered this a victory. And that was stretch even then. Now, claims of a win are even further fetched. Originally, Oracle sought to wring $7 billion in damages from Google, but after the ruling, it is entitled to next to nothing. Oracle only hope for significant damages is an appeal.\n Update: This article was updated with a comment from Oracle.","item_date":"Jun 01 2012 00:31:22","display_item_date":"05-31-2012","url":"http:\/\/www.wired.com\/wiredenterprise\/2012\/05\/oracle-google-judge-dismiss\/","source":"www.wired.com"},{"title":"Platform Updates: Operation Developer Love","details":"Today we added an FQL Query tab to the Graph API Explorer to make it easier to test and play with FQL. As before, you can run FQL queries through the Graph API by issuing an HTTP GET\n can be a single FQL query or a JSON-encoded dictionary of queries. This tab in the Graph API Explorer just gives you a quick way to experiment with queries.\n  Updated Open Graph Publishing Guidelines\n  To give people on Facebook the best experience, we\u2019re introducing several new Open Graph publishing guidelines. In instances where these changes require you to modify an existing app, you\u2019ll have 90 days to complete your update per our Breaking Change policy. These guidelines address feedback we\u2019ve received and codify best practices that encourage everyone working with our platform to put user experience first.\n  Additional publishing guideline\n  For stories about watching a video or reading an article, you must use the built-in watch and read actions. We\u2019ll no longer approve custom actions for these stories.\n  90-day update window\n  Even if we\u2019ve approved your custom action for reading an article or watching a video, you must migrate to using a built-in action, resubmit your app and get approved. Please review the requirements for working with read and watch actions.\n Built-in watch and read actions can only be published after someone engages with the content for 10 or more seconds. If a video is shorter than 10 seconds, the viewer must watch the entire video.\n Tagging people and places:\n \n   Only encourage people to tag friends when they have actually performed an action together.  \n   Only encourage people to tag a location if they are currently at that place.  \n   \n  You should allow people to turn sharing on or off for the content in your app, and the setting a user selects should persist. Learn more about our policies and guidelines:\n  Open Graph Checklist\n Built-in Action Types\n Facebook Platform Policies\n \n FBML apps will no longer work on Platform. The FBML Removal migration will appear and will be enabled for all apps. It will be possible to disable the migration, thereby re-enabling FBML, until July 5, 2012 when the migration and all FBML endpoints will be removed completely.\n  XMPP Connections must be done over TLS\n Apps connecting to Facebooks XMPP service will be required to use STARTTLS for all connections. We will start rejecting unencrypted connections.\n  Bugs activity between Tuesday, May 22 and Tuesday, May 29\n  150 bugs were reported\n 33 bugs were reproducible and accepted (after duplicates removed)","item_date":"Jun 01 2012 00:08:30","display_item_date":"05-31-2012","url":"http:\/\/developers.facebook.com\/blog\/post\/2012\/05\/30\/platform-updates--operation-developer-love\/","source":"developers.facebook.com"},{"title":"How important are all those ugly Tweet Buttons to news sites?","details":"A quick-and-dirty look seems to indicate around 1 in 5 tweets to a news organization comes from those Tweet Buttons.\n          By Joshua Benton\n  \t \n     News sites today are pockmarked with sharing buttons, those little \u201ctweet this\u201d or \u201clike that\u201d rectangles attached to seemingly every story these days.\n In some ways, it\u2019s not as bad as it used to be, when there were more social networks making a plausible claim on people\u2019s attention. (Digg that!) But even in a world that\u2019s mostly shaken out to Facebook and Twitter \u2014 plus maybe Google+ if you\u2019re generous, or LinkedIn if you write about business, or Pinterest if you\u2019re about shopping or food, or\u2026sigh. Well, even in a world that\u2019s mostly shaken out to a few social networks, those buttons are still a less than appealing set of warts on the body hypertext.\n That\u2019s the argument advanced by designer Oliver Reichenstein in a blog post (\u201cSweep the Sleaze\u201d) that got a lot of attention this week:\n But do these buttons work? It\u2019s hard to say. What we know for sure is that these magic buttons promote their own brands \u2014 and that they tend to make you look a little desperate. Not too desperate, just a little bit\u2026\n Don\u2019t worry. These buttons will vanish. The previous wave of buttons for Delicious and Digg and Co. vanished, Facebook and Twitter and G+ might vanish or they might survive, but the buttons will vanish for sure. Or do you seriously think that in ten years we will still have those buttons on every page? No, right? Why, because you already know as a user that they\u2019re not that great. So why not get rid of them now? Because \u201cthey\u2019re not doing any harm\u201d? Are you sure?\n Not surprising, coming from a designer best known in the technology world for building a minimalist writing app. \n But do those Like buttons and Tweet Buttons do any good? To get some data on that, we can thank Luigi Montanez, who posted a Ruby script that allows you to see, of the n most recent tweets containing a given URL string, how many of them were generated by a Tweet Button. (More detail here.) Luigi found that there\u2019s substantial variation among different sites.\n I downloaded the script and decided to start running it on a variety of news sites to see if I could suss out any patterns. This isn\u2019t hard science, for reasons I\u2019ll get into in a moment, but it\u2019s interesting! Here are the results from 37 news sites, first divided up by type of news site. (I looked at the 1,000 most recent tweets for each, rather than Luigi\u2019s 500.)\n \n The Y axis is the percent of the 1,000 most recent tweets that Twitter says were generated by a Tweet Button. So, 16.3 percent of those tweets to nytimes.com (The New York Times) came from such a button, versus 20.2 percent for wsj.com (The Wall Street Journal).\n Two broad observations:\n \u2014 Tech sites seem to be less reliant on the Tweet Button, as a percentage \u2014 as one might expect from sites with a social media-savvy audience. Presumably readers of The Verge are comfortable copying and pasting a URL into Twitter on their own, or tweeting to Verge content by seeing someone else\u2019s reference to it in their feed.\n \u2014 Sites with a clear ideological profile \u2014 Daily Kos on the left (38.8 percent) and Red State on the right (32.1 percent), for instance \u2014 are among the heaviest beneficiaries of the Tweet Button. Could that be because their readers are explicitly looking to those sites for links to share on their networks, and the Tweet Button is an easy way to speed up that process? But Fox News seems to be an interesting exception to that observation \u2014 only 5.1 percent of its tweets came through the button, versus 27.3 percent for MSNBC.\n Most other sites seems to fall into a broad middle, which makes sense. Here\u2019s that same data, but this time ordered by percentage rather than grouped by type of outlet:\n \n All right, let\u2019s all visit Caveat Central \u2014 a place of particular interest to any journalism graduate student who\u2019d like to tackle this in a more serious way and get a good AEJMC paper out of it:\n \u2014 This is just a snapshot of 1,000 tweets\u2019 worth of data, taken around 11:30 a.m. ET today. You\u2019d need a much bigger data set to do any serious analysis. The numbers could be substantially different at a different time of day or day of week, for instance, or at different points in a story\u2019s life cycle. You\u2019d also need to get raw counts over time \u2014 \u201cthe 1,000 most recent tweets\u201d might cover a span of 15 minutes for some sites and several days for another.\n \u2014 This script doesn\u2019t measure other Twitter app data \u2014 in other words, how many are using the Twitter iPhone app, how many are using news org-branded apps, how many are using TweetDeck, how many are using Twitter\u2019s web interface? Lots of interesting data potential in there. It would also be interesting to correlate the results with each news org\u2019s own Twitter presence; The New York Times\u2019 5.1 million Twitter followers no doubt make Tweet Button usage seem smaller in comparison. (It\u2019s also worth noting that some sites may have something that looks like a Tweet Button that\u2019s actually registered with Twitter under a different app name; those wouldn\u2019t show up here.)\n \u2014 Not all Twitter users are created equal, of course. How do the Tweet Button users compare to those using other means to tweet \u2014 particularly in the size of their Twitter followings? If Tweet Button users were numerous but had small followings, they might not be as valuable to news organizations as they\u2019d seem at first glance.\n \u2014 To do this justice, you\u2019d also need to analyze placement of Tweet Buttons on sites. Audience composition isn\u2019t the only reason Tweet Button behavior might change, of course; potentially much bigger is the placement of Tweet Buttons on each site. (Just below the headline? At story\u2019s end? Both?)\n But even this quick look would seem to support the idea that killing off Tweet Buttons would, for most news organizations, remove somewhere around 20 percent of their Twitter link mentions. Maybe more, if (as I\u2019d guess, although with no data) that those Tweet Button users are often something like a Tweeter Zero \u2014 an originator that enables a story\u2019s later spread through other means. (Here\u2019s where a tool like the Times\u2019 Cascade would be really useful.) Or maybe less, since some percentage of those Tweet Button users would still find other means to tweet without them. Here\u2019s hoping some Ph.D. student runs with this.\n \n     \t\t\t  \t\t\t\t \t\t\t\tTwitter is a social network and microblogging platform. The service is built on 140-character messages called tweets, which live on the web and can be read by anyone, although some users opt to make their accounts private. Twitter also allows\u2026\n   \t\t\t     \t\t\t\n                      A smaller consideration to take into account would be how useful the button is once somebody clicks. I actually like the buttons when implemented well.\u00a0\n Some buttons will include a tweet-tailored headline instead of one that would take up a bulk of the 140. Some sites include their handle, some don\u2019t (I prefer when they do to give them credit). Some have their full site name, some don\u2019t. Some include the section, etc.\u00a0Some share buttons just place the URL and nothing else.\n I think if you\u2019re going to use the button, tailor it for the service. The way sites handle that are a factor in whether I use the button or just craft it myself in Tweetdeck or Twitter.com.","item_date":"May 31 2012 18:39:54","display_item_date":"05-31-2012","url":"http:\/\/www.niemanlab.org\/2012\/05\/how-important-are-all-those-ugly-tweet-buttons-to-news-sites\/","source":"www.niemanlab.org"},{"title":"n0tice Promotes Open Journalism with Open API","details":"In 2011, the Guardian launched its local messaging bulletin: n0tice. Although the platform encouraged open journalism, the Guardian has expanded the effort with the launch the n0tice API last week. The very premise of n0tice offers value from a locality perspective. With the open API, the value of the local bulletin can be integrated with any developer\u2019s application.\n\n\n\n\n\nPerhaps the most intriguing integration of the API thus far lies in tracking the Olympic torch route to London for the 2012 Olympic Games. The Guardian itself is tracking the route with the n0tice API. As the torch makes its way to London, n0tice users are able to post pictures and text messages that the Guardian automatically adds to its news feed.\n\n\nMatt McAllister, the Guardian\u2019s director of digital strategy, commented: \u201cIt feels like we\u2019re sitting on this huge bundle of potential and it\u2019s just a matter of continuing to execute.\u201d A Wordpress plugin already allows developers to create curations (Wordpress sites can display content from the Guardian\u2019s n0tice community platform). FormbyFirst, a local site reporting on happenings in Formby, encourages users to post upcoming local events. In turn, FormbyFirst pulls five events from the API and feeds them to the FormbyFirst homepage.\n\n\nThe Guardian promotes the open API as an \u201copen journalism toolkit.\u201d Indeed the potential for open journalism is enhanced by allowing feet on the street, local users to post news. However, the integration potential is unlimited. Any app, site, or feed that can enhance its user experience with local data can benefit from the API. In its prototype platform launched last year, n0tice already adopted an international audience. Now that the functionality has been expanded via an open API, we could see a viral spread across the globe.","item_date":"May 31 2012 18:31:56","display_item_date":"05-31-2012","url":"http:\/\/blog.programmableweb.com\/2012\/05\/31\/n0tice-promotes-open-journalism-with-open-api\/","source":"blog.programmableweb.com"},{"title":"Google Geo Developers Blog: Join us at Random Hacks of Kindness this weekend","details":"On June 2nd and 3rd, volunteers around the world are coming together for the Random Hacks of Kindness Global Hackathon.  Born in 2009, Random Hacks of Kindness (RHoK) is the brainchild of a partnership among Google, Microsoft, Yahoo!, HP, NASA and the World Bank.  The RHoK international community is over 4000 strong, encompassing not only computer programmers but also engineers, designers, web experts, project managers and other tech-savvy do-gooders, as well as subject matter experts in areas as diverse as disaster risk management, climate change, water, health care and human rights.  This group of unlikely collaborators works together to define challenges facing humanity in local communities all around the world, and then to rapidly prototype solutions during an intensive weekend of round-the-clock work.\n \n Fueled by plenty of coffee and a strong desire to make a difference in the world, hackers for humanity working alongside subject matter experts have created innovative solutions to pressing problems in their communities.\nA team at RHoK in Trento, Italy designed a mobile application that connects charities distributing food to needy populations with restaurants and businesses with excess food to donate.\n A team at RHoK Philadelphia designed a web platform enabling homeless service providers to easily identify empty beds and open soup kitchens for Philadelphia and New Jersey homeless populations.\n A team at RHoK Washington D.C. designed a tool to visualize complex landslide risk algorithms, making the information accessible to local mayors and urban planners making building decisions.\n A team at RHoK Sydney created a crowdsourcing tool to allow Australian citizens to rapidly report and respond to bushfires.\n \n In the two short years since its inception, RHoK communities have sprung up in close to 50 cities around the world, with the support of over 180 diverse partner organizations, from government and academia, to the non-profit and private sectors.\n RHoK Global in June 2012 will be taking place simultaneously in 21+ cities globally, from Seattle to Santo Domingo, from Philadelphia to Prague. Googlers will be attending the events in San Francisco, Prague, and other locations, and we hope you can join us.\n Be a part of this global movement to make the world a better place through the innovative use of technology.  Register at www.rhok.org\/events and come out to hack for humanity in a city near you on June 2nd and 3rd, 2012!\nChristiaan Adams is a developer advocate with the Google Earth Outreach Team and Google.org\u2019s Crisis Response Team, where he helps nonprofits and disaster response organizations to use online mapping tools. When he\u2019s not at work, he likes to go hiking or mountain biking, using Google Maps, of course.","item_date":"May 31 2012 18:19:33","display_item_date":"05-31-2012","url":"http:\/\/googlegeodevelopers.blogspot.com\/2012\/05\/join-us-at-random-hacks-of-kindness.html","source":"googlegeodevelopers.blogspot.com"},{"title":"Open APIs Enable New Business Models for Operators","details":"For operators, offering web services to third-party developers reflects the age-old strategy of selling picks and shovels in a gold rush.\nAnalysys Masons research shows that operators share of revenue from mobile content and applications is declining steeply in developed markets. To offset the decline, operators need to increase network stickiness and value. Offering attractive new enablers \u2013 open APIs \u2013 to third-party developers is a promising way to do this.\nThis document provides:\n-\ta thorough analysis of the challenges and opportunities facing operators in the web services market, including the competitive landscape and new business models\n-\tmarket growth indicators and an examination of the potential revenue opportunities available to operators\n-\ta forecast for operator deployments of commercial web services offerings during 2012\u20132016\n-\ta detailed overview of operators current offerings and activities in the web services space.","item_date":"May 31 2012 18:15:39","display_item_date":"05-31-2012","url":"http:\/\/www.researchandmarkets.com\/research\/r2t5lk\/open_apis_enable_n","source":"www.researchandmarkets.com"},{"title":"Open API lessons for LinkedIn and Facebook ","details":"","item_date":"May 31 2012 17:15:46","display_item_date":"05-31-2012","url":"http:\/\/www.theregister.co.uk\/2012\/05\/31\/linkedin_closed_apis\/","source":"www.theregister.co.uk"},{"title":"Why APIs? Anatomy of a Customer API Initiative","details":"In this short series to explore different API strategies, weve looked at the characteristics and examples of\u00a0 Internal and Partner API Initiatives, which are often the first and second stages of a companys API strategy.\n The Customer API initiative is mostly used in one of two scenarios. The first is when offering software as a service (SaaS). (Look at Salesforce as an example where customers demand an API.) The second is when the customer of your business is another business (a B2B scenario).\n Here we look at a few scenarios for the Customer API initiative.\n \n The innovation use case comes into play when a customer has an idea for something they want to do with your existing feature set. Without an API they have only what\u2019s available out of the box. With an API, someone at the other company can use the API to create innovative apps. A great example of this is salesforce.com and their force.com offering which is a platform that can be fully extended with APIs and run-time components.\n Integration\n Integration is a well-known and common scenario that has been approached with different technologies for years. It\u2019s about keeping data synchronized between your business and your customers\u2019 businesses, facilitating business process integration between companies, and so on.\n Batch kick start\n This use case describes a scenario in which a business has signed a new user or customer and there\u2019s now a new batch of information and user data that needs to be available in that business\u2019 system from the customer. Providing an API to allow customer to write that data to your system is very effective.\n Next time, we\u2019ll finish our summary of the various flavors of API initiative we see deployed by looking at an Open API initiative.","item_date":"May 31 2012 17:08:08","display_item_date":"05-31-2012","url":"http:\/\/blog.apigee.com\/detail\/why_apis_anatomy_of_a_customer_api_initiative\/","source":"blog.apigee.com"},{"title":"LinkedIn\u2019s Big Year: IPO, 60K Devs and Billions of API Calls","details":"Facebook may be this year\u2019s most-discussed tech IPO, but last year certainly belonged to LinkedIn. The company, which is turning a modest profit, also has expanded its developer offerings. With that came many more developers to the LinkedIn API, more than doubling the number of registered devs in less than a year. And those apps created on the platform account for billions of API requests each month going to business social network.\n\n\n\n\n\nIn November, 2010, months before LinkedIn\u2019s IPO, it had 20,000 developers, according to LinkedIn\u2019s Adam Trachtenberg. Shortly after the stock debuted in May, 2011, the developer count had inched up to 25,000, when we wrote that adoption was bound to surge. It did, and then some. The company has 60,000 developers as of March, 2012, according to its press page. That\u2019s more than double the developers in less than a year, or triple in sixteen months, if you go back to the 2010 number.\n\n\nDeveloper counts don\u2019t even include the LinkedIn Share button, which has found its way onto 400,000 unique domains. The share button is likely the largest user of the LinkedIn API, which clocks in at 2 billion requests per month, making it an API billionaire.\n\n\nOther internal apps include the company follow button and Apply with LinkedIn.\n\n\nThe external app with perhaps the biggest splash is CubeDuel, which in early 2011 hit LinkedIn API limits. LinkedIn\u2019s Trachtenberg extended CubeDuel\u2019s limit twice as the app became popular. In the case of most apps that hit the API limits, they probably do so because of scraping or data mining, as alluded to in the LinkedIn forums.\n\n\n\u201cWe always take a member-centric approach,\u201d Trachtenberg said. \u201cOur goal is to enable the most successful integrations possible while making sure that all members are having a fantastic experience.\u201d\n\n\nThough, as CubeDuel\u2019s founder pointed out, LinkedIn hasn\u2019t had many wins on its platform (the company does have a modest app gallery). But with the rapidly expanding developer community, perhaps it\u2019s on its way there.","item_date":"May 30 2012 18:47:18","display_item_date":"05-30-2012","url":"http:\/\/blog.programmableweb.com\/2012\/05\/30\/linkedins-big-year-ipo-60k-devs-and-billions-of-api-calls\/","source":"blog.programmableweb.com"},{"title":"About.me API Enables Developers To Integrate Custom Apps","details":"About.me is a popular service that allows anyone to create a single page profile of yourself that can connect to your profiles on Facebook, Twitter and other popular services. The concept is simple but limiting to the services that it currently connects to. This is no longer the case with the release of the About.me API that now allows developers to write new applications that can integrate into the About.me profile page.\n\n\nThe About.me API now enables your application to integrated by About.me users into their single profile page. These applications would in turn be visible to users who visit About.me profile pages too, thereby increasing the visibility of your application. These applications would be hosted by the developer but they would be integrated seamlessly on the About.me sit, running inside of an iframe when you click on the application.\n\n\n\n\n\nTo get started with the API, you need to an about.me account and a developer ID. You need to get the developer ID by writing to platform@about.me. The documentation site is detailed with information on providing application metadata, allowing adding the application to about.me profile and much more.\n\n\nAre you an About.me user? What kind of applications do you think were absent till now and you would like to link to? This kind of API seems ripe for a Hackathon or Contest where the most interesting applications could get featured in the directory?","item_date":"May 30 2012 18:27:05","display_item_date":"05-30-2012","url":"http:\/\/blog.programmableweb.com\/2012\/05\/30\/about-me-api-enables-developers-to-integrate-custom-apps\/","source":"blog.programmableweb.com"},{"title":"Sacramento Hackathon Set For Saturday","details":"It\u2019s kind of like speed-dating for software developers. That\u2019s how organizers of an upcoming \u201chackathon\u201d in Sacramento describe the event.\n     \nTuesday, May 29, 2012     A group of technology entrepreneurs is holding the event to  jump-start some Silicon Valley-style thinking in Sacramento.\n    Were trying to bring the spirit of the start-up community  thats in the Bay Area, were trying to bring that here to  Sacramento.\u00a0\u00a0\n    Hackathons have been known to spawn new web and mobile app  developers in the Silicon Valley. Lujan says the same should be  happening in the capitol city.\u00a0\u00a0\n    Were hoping that new businesses will be formed and that  people will continue, after the hackathon, to build out their  products and hopefully build small businesses.\n    Its kind of making a joke because coders will sit in front  of a laptop for eight to ten hours at a time without moving and eat  cereal and drink high energy drinks.\n    To make participants feel at home, she says theyll have lots  of cereal on hand for the 30 hour event which begins Saturday at  noon in the offices of the online news source Sacramento  Press.","item_date":"May 29 2012 20:28:09","display_item_date":"05-29-2012","url":"http:\/\/www.capradio.org\/articles\/2012\/05\/29\/sacramento-hackathon-set-for-saturday","source":"www.capradio.org"},{"title":"Benefits Of Public APIs","details":"For the last 15 months or so there has been a lot of buzz around APIs, and more than a few companies are trying to figure out what they can gain from building an API to allow external parties to utilize their systems.\n Many companies already have private APIs that allow different software across platforms to utilize their functionality and data. Some are hesitant to open up APIs to third party developers, especially if the API will have direct or indirect access to business layers or payment systems.\n The perceived difficulties concern security, rigorous testing and the time and cost to set up different business models and rules for private and public use. However, more and more companies are seeing that the benefits of opening up APIs may well outweigh any additional costs.\n \n Imagine that your company is selling international text messages and that you have a website or a smartphone app facing the end users and a private API to access the underlying system. If your company is really good at making deals with international operators you might be able to offer competitive pricing to end users. Let\u2019s say another company is better than you are on user experience and virality, and that they have a massive user base for a smartphone app doing something similar: free messaging from smartphones to smartphone for instance.\n Now, if you allow them to use your API to send text messages to feature phones when their end users are trying to reach a phone that doesn\u2019t support their app, you could suddenly have a revenue stream that costs virtually nothing to maintain and customer acquisition. No matter how small that revenue stream may be, you have eliminated two very expensive obstacles.\n \n Another example is systems with user generated content, such as ratings, reviews, etc. Allowing third parties access to your precious data may encourage end users to create more content for you, as the data can be accessed and presented in a different context. An end user might be more prone to rate a movie using mobile on the subway going to work, than using a computer at home or at work, so why not let mobile developers have access to your movie database using a public API and get more reviews and ratings from their users?\n The scalability is potentially huge if you set up your infrastructure properly to allow third parties to utilize your systems. Public APIs have enormous possibilities and the only question is how you and your company can benefit from using them.","item_date":"May 29 2012 18:52:42","display_item_date":"05-29-2012","url":"http:\/\/www.exiconglobal.com\/blog\/benefits-of-public-apis\/","source":"www.exiconglobal.com"},{"title":"APIs: real decision-making tools","details":"Collecting, processing, analysing and displaying data on the same interface is excellent! Being able to create these operations from several different interfaces is even better!\n The data covers multiple fields and can be adapted to meet the needs of each of the interfaces involved, whilst being kept up to date at the same time. This is what I would call the basic idea of an Application Programming Interface (API).\n An API connects different platforms together so that data can be read and\/or written. Depending on the platform used, the different actions which can be performed by an API vary greatly.\n Let\u2019s take a look at some examples to illustrate the different possibilities offered by APIs.\n Example of a widget created with information from the Yahoo! Weather API\n Yahoo! Weather\n The \u201cYahoo!\u201d search engine has an API which obtains meteorological information from anywhere in the world by simply entering GPS co-ordinates into the request. The information which is retrieved (e.g. maximum and minimum temperature) can then be\u00a0integrated into different modules.\nRetrieving a map via the Google Maps API\n Google Maps\n Google has several different APIs which respond to the different needs of an application. One of the most used APIs is the \u201cMaps\u201d API. This interface offers users a wide range of interactions which can be performed on the different maps of the world.\n The localisation system used is similar to the one used by \u201cYahoo! Weather\u201d since it relies on GPS co-ordinates to focus on the map that is to be retrieved. How it works is that a fixed image of the map is retrieved, and tags are then be added to the map to make it interactive by using different parameters.\n How do APIs work?\n Each operation, or interaction, offered by an API is known as a \u201cmethod\u201d. The different methods are:\n Read only: the information is obtained so that it can be processed by the user;\n \nRead and write: the data is retrieved for processing. This method is also used to modify, add and delete information;\n \nWrite only: this type of method adds information, without obtaining information which is already present.\n \nHere is a diagram showing how an API works:\n \n To store the different methods in a logical fashion and to simplify how they are used and managed, they are organised into groups known as \u201cservices\u201d. A user may or may not have access to a service, in other words the set of methods that the service is made up of.\n How can I access information provided by APIs?\n Several different methods for accessing such information exist. Each API is free to offer the method which suits it the best, depending on the information to be transferred.\n Below is a list of the main formats used:\n  \n REST: a call to a URL is made, with different parameters so that the request can be refined. This method is used the most, with nearly 75%* of APIs using it. It is very easy to implement, and allows for the quick development of the application.\n \nSOAP: this request method requires a more accurate format, and also requires a more substantial call than the REST method. This system tends to be disappearing; I will not go into the technical details on how it is used.\n \n XML: the XML format is used the most for API responses since it can carry a large amount of information, whilst adhering to a strict format. 65% of APIs use this data format.\n \n JSON: JSON is a rapidly growing format which transfers information in a format which can be easily used in the majority of programming languages, especially JavaScript. The fact that it is easy to use means that applications are quick to set-up.\n \nThese different formats are only a selection of formats chosen from many others available. There are currently no standards which exist as far as request and response formats are concerned. Nevertheless, the REST format for requests and JSON format for responses seem to be en route to becoming the standards.\n What does the AT Internet API do?\n The AT Internet API, available to customers, allows them to perform a range of different actions including, managing their account and creating analyses. The API allows some customers to manipulate data, or to generate analyses which are relevant to their business. Customers can redistribute data in several different ways: a) internally to guide strategic company decisions and to guide their different teams, b) to their partners in order to provide them with information on the performance of their campaigns, and finally c) to the general public to rely on transparency.\n Which services does the AT Internet API have to offer?\n  AT Internet offers its customers a wide range of different services with read and\/or write actions for each so that they can get the most out of the API.\n Below is a short description of the two key webservices AT Internet has to offer:\n The\u00a0Reporting service is the main service offered by AT Internet, and is also the one which is used the most. This service will retrieve any analysis present on the interface, thanks to the use of an \u201canalysis code\u201d available in each analysis.\n \n Asynchronous methods: the analysis is generated at the same time as a call is made by a customer, with each call being allocated a \u201cnumber\u201d. The number which is allocated will then retrieve data via a second call.\n \nThe choice to use one type of call or another depends on the quantity of data to be retrieved. The synchronous method is used for low traffic volumes, low enough so that the transfer time is transparent. The asynchronous method retrieves larger sets of data which take longer to be calculated without blocking the application that uses the API when waiting for the data to be transferred.\n CampaignSources\n The solution offered by AT Internet helps users find out a visitor\u2019s source for all types of marketing campaign such as: sponsored links, affiliation, email campaigns, RSS feeds and Advertising.\n Campaigns are declared and managed on the interface. With the CampaignSources service users have a wider range of options for managing marketing campaigns associated with a site. It is possible to:\n Add a campaign;\n Close a campaign;\n Retrieve information from a campaign (or set of campaigns);\n Update a campaign (by changing its name and description).\n \nThis will provide the Marketing service in charge of these campaigns (which may not necessarily have access to the interface) with the opportunity to easily, if not automatically, declare campaigns as they are being launched.\n As we have just seen, APIs are important elements in managing information, and can form part of real applications, by reprocessing only the data that is provided by the different APIs.\n By integrating the APIs separately into internal tools, we can benefit from all of the advantages that a SaaS solution has to offer.\n This type of interface may turn out to be decisive in the strategic and operational management of a campaign, a site or even a company. It is therefore very interesting to take the time to study the possibilities offered by the different APIs relative to the market in which a company is growing to obtain the data which can then be integrated into an internal information system, and which in the end will help the decision-making process.","item_date":"May 29 2012 18:48:06","display_item_date":"05-29-2012","url":"http:\/\/blog.atinternet.com\/en\/index.php\/2012\/05\/23\/solutions\/apis-real-decision-making-tools\/676","source":"blog.atinternet.com"},{"title":"Daring to fail at 23: Australia's hackathon hero","details":"Peter Watts is just 23, but the Australian networks like a seasoned pro and his app is turning heads in the music industry.\n\n\nThe young entrepreneur from Sydney is on a six-month trip around the US spruiking his start-up Swarm.fm, financed by $20,000 in prizes he has won from coding competitions known as hackathons.\n\n\nWith Swarm.fm Watts is developing apps to help people find new music and learn more about the artists they love.\n\n\nHis latest hackathon triumph was the $10,000 grand prize at an event run by music streaming service Spotify. In 48 hours, Watts built an app that made it simple to add a list of friends and then collate their music activity from Facebook and other services, using the data to build a dynamic Spotify playlist based on what the users friends are listening to.\n\n\nFairfax Media followed Watts as he pitched his start-up to tech industry luminaries and music executives at a private party during the SXSW Interactive festival in March in Austin, Texas.\n\n\nThey have a whole lot of band managers, Lady GaGas manager and a few other high profile people in here that are interested in how technology can build the tools to help the artists really engage with their fans and really get their music out there, he says.\n\n\nPeople outside of tech are understanding that tech is what can really take them to the next level.\n\n\nWatts appears in the new video series on this website, Digital Dreamers, which tracks Australian entrepreneurs making it big in the US. Episode 1 debuted last week, while episode 2 is published today.\n\n\nAt SXSW, Watts was among 48 finalists out of 650 entrants to make it into the Music Accelerator start-up competition.\n\n\nWatts did not win - lost to an iPad app with guitar-teaching cartoon animals - but his networking and pitching paid off.\n\n\nIt was definitely a valuable exercise and I got some solid feedback. In particular, I made a very strong connection to a music tech industry veteran, which has had a flow-on effect, leading to angel investor interest, potential advisers and even a lawyer, he said following SXSW.\n\n\nWattss six-month journey through the US has also taken him to New York three times and San Francisco for extended periods.\n\n\nThe first half of the trip has been focused on building a network and refining the product. The next few months will be dedicated to growing the company and getting it out there, he said.\n\n\nWe have a big product launch coming soon, and are in the process of raising a seed round of investment.\n\n\nWatts, who lives in Mascot but plans to incorporate in the US, believes the fact that tech entrepreneurs such as he have to go offshore to succeed is a lost opportunity.\n\n\nI think its definitely bad for Australia ... but its a tough chicken-and-egg problem, he says.\n\n\nIts hard to convince those people to stay in Australia because there isnt the same environment for them to succeed.\n\n\nPeter Watts is one of the young entrepreneurs Asher Moses encountered on the road at SXSW in Austin, Texas. He and others tell their stories in Digital Dreamers, Episode 2. Coming next week: meet the Aussies making waves in Silicon Valley.","item_date":"May 29 2012 18:42:28","display_item_date":"05-29-2012","url":"http:\/\/www.canberratimes.com.au\/technology\/technology-news\/daring-to-fail-at-23-australias-hackathon-hero-20120525-1z98i.html","source":"www.canberratimes.com.au"},{"title":"Why Open Data isn\u2019t enough. | Open Knowledge Foundation Blog","details":"The debate around data in our community has been densely concentrated around the question of openness. That\u2019s not surprising. Words like \u201cfree\u201d and \u201copen\u201d have dominated the conversations in the digital commons for most of its existence, mainly because most of the digital commons has been centered on copyrightable works.\n  \n  Software, text, photos, videos, music are all creative works under the law, all carrying the powerful, relatively internationalized protections of copyright, and this very power allows creators to invert that power using free \/ libre copyright licenses. That reality has led to a set of definitions of freedom for software, for cultural works, and for knowledge, all of which are very centered on the intellectual property regimes surrounding digital objects. We\u2019ve also propagated the idea to hardware.\n  And that\u2019s carried over into the debate around data. We ask, \u201cis it open data?\u201d of the world.\n  But I spend a lot of time around data people for whom open is an afterthought. For many people it\u2019s Big Data, right down to the requisite O\u2019Reilly branded events. They\u2019re worried about whether we should leverage machine learning or domain experts, not openness. Or it\u2019s Social Data. They\u2019re worried about privacy policies and selling the data to as many vendors as possible. It\u2019s Blue Button, and Green Button. They\u2019re worried about getting data into people\u2019s hands. It\u2019s Quantified Self. They\u2019re worried about getting their own data into their own hands. In Washington and other capitals large and small it\u2019s on Government Data.  \n  Open is almost never mentioned.\n  And I think that\u2019s because we\u2019re so focused on intellectual property, on share alike and attribution and public domain, that we lose the bigger context.\n  Creative works came online in a cultural and technical context that allowed us to focus on freedom, and intellectual property. We have decades of history with software, photo, and video, and hundreds with text. We had a technical infrastructure ready to create, distribute, consume, and remix creative works: mailing lists, sharing websites, wiki software.\n  We don\u2019t have that with data.\n  Data is entering the world at a rate that is so fast it\u2019s almost incomprehensible to human brains. It\u2019s like trying to comprehend geologic time. The cost of generating data is so low in so many spaces, and dropping like a stone in so many others, that the real challenge is to do interesting things with it. The gulf between those who can do something with data and those who can\u2019t is a serious new case of digital divide, and licensing is just a tiny part of that gulf. Important, to be sure, but tiny.\n  There\u2019s a people gulf \u2013 190,000 machine learning experts and 1,500,000 managers in the US alone that don\u2019t exist, but need to, to take advantage of data. That gap is worse in the developing world, and will only accelerate in coming years.\n  But perhaps most important is a cultural gulf \u2013 we live in a world right now that (implicitly in most cases, but increasingly explicitly) accepts the natural state of data as transactional. We trade our data, rather than our cash, for services like Facebook, Google, apps, and more. We don\u2019t get a copy of it. We don\u2019t know who does. We\u2019re on the outside of the black box, but our data\u2019s on the inside.\n  So my argument is that we as an \u201copen\u201d movement need to understand and integrate our concerns over property rights into the broader debate. We need to talk about citizen\u2019s rights. We need to talk about the right to understand how our web searches are returned. We need to talk about how our privacy rights may be negatively impacted by more openness.\n  Because unlike the web, and the internet, which grew quietly in obscure corners of the world, allowing open designs to flourish, data has already drawn attention, money, and closed business models. We\u2019re in active competition against powerful, rich opponents to create an open ecosystem at the core of data, one that TCP\/IP and HTML didn\u2019t have to fight.\n  Here\u2019s hoping we can bridge the gaps before other, closed systems can do so for us. The good news is that open systems have a lovely little history of outcompeting closed ones, given time, freedom to compete, and even a small group of committed people.\n  \n Why Share-Alike Licenses are Open but Non-Commercial Ones Aren\u2019t It is sometimes suggested that there isn\u2019t a real difference in terms of \u201copenness\u201d between share-alike (SA) and non-commercial (NC) clauses \u2014 both being some restriction on what the user of that material can do, and, as such, a step...","item_date":"May 29 2012 18:39:09","display_item_date":"05-29-2012","url":"http:\/\/blog.okfn.org\/2012\/05\/28\/why-open-data-isnt-enough\/","source":"blog.okfn.org"},{"title":"The Barriers to Adaptability: Results from the Adaptability Hackathon Sprint 2","details":"Gartner EXP \/ Management Innovation Lab Hackathon invites a select group of IT leaders to crack a key issue\u2014how IT can help organizations become more adaptable.\u00a0 For the past two months more than 240 senior IT executives from around the world have been discussing technology\u2019s role in enterprise adaptability.\n This post is an update on the results of the second sprint in the Hackathon focusing on identifying the barriers to adaptability.\u00a0 The first sprint completed in April concentrated on why adaptability matters.\u00a0 Follow this [link] to see a post highlighting some of that discussion.\n In Sprint 2 of the Adaptability Hackathon, we discussed and dissected the barriers that stand in the way of our organizations becoming more adaptable, and began to share ideas about how technology might help us overcome them.\n When we tallied the votes at the end of the sprint, our hackathon participants had determined the following four barriers to be the most pressing to address:\n 1.\u00a0\u00a0\u00a0\u00a0 Too many big bets, not enough little ones \n 2.\u00a0\u00a0\u00a0\u00a0 A monopoly on capital allocation \n 3.\u00a0\u00a0\u00a0\u00a0 Monolithic structures \n For today\u2019s post, I thought I\u2019d dive a bit deeper into the conversation around each of these four barriers.\n Too many big bets, not enough little ones\n This barrier, far and away the one that received the most votes by our hackathon contributors, was based on the simple idea that adaptability depends on experimentation.\u00a0The more experiments an organization runs, the greater chance that one of them will result in a truly organization-altering innovation.\n Yet many companies today only make a few big bets, and don\u2019t have the management or technological infrastructure in place to make lots of smaller bets at once.\n In his barrier submission, entitled The problem with projects, David Clark of Gartner suggests that project management itself\u2014and the controls and process that come with it\u2014may be a reason why we are not able to make more bets more quickly.\n In his Good Stewardship barrier submission, Chris Bertelsen from Protective Life suggests that technology might be able to help us make smaller bets by providing less expensive ways to measure our progress and by providing platforms and methodologies that lower the governance bar and make experimenting less expensive.\n Borge Teigland, VP Global IT\/CIO at Norske Skogindustrier ASA, commented that IT itself might be one of the biggest barriers to innovation simply because of the big bets IT has placed in big ERP systems and locked-down infrastructure.\n Finally, Dan Kelly, Director of MIS at Nemschoff Chairs \/ Herman Miller suggested that \u201cstreamlining the specification process of new solutions\u201d might be the key to bringing technology and business closer together.\n Based on the contributor\u2019s comments and ideas, I think our challenge in addressing this barrier will be twofold. First, as IT professionals, we will need to look inward to see what we can change and streamline about our own technology choices and processes to make it easier for our organizations to experiment more and make more small bets without IT becoming a hindrance. But beyond that, I\u2019d like to see us explore ways we can help our organizations implement new technologies that make the process of experimenting (and measuring the results of this experimentation) easier, cheaper, and faster.\n A monopoly on capital allocation\n In most companies, there\u2019s only one place an employee can go to get funding for an unconventional idea\u2014up the chain of command.\u00a0 If the project doesn\u2019t dovetail with the boss\u2019 priorities or prejudices, it doesn\u2019t get funded.\u00a0 This paucity of funding sources puts a huge damper on innovation and adaptability.\n This was our second highest voted barrier, and according to Tom Honan, CIO at CapitalSource Bank, it is a critical barrier to address.\n \u201cImplemented to help identify and ensure capital spending is aligned with the organization\u2019s strategic plan and goals, it can serve to starve out many worthy endeavors,\u201d\n Tom said. He then went on to comment that at CapitalSource Bank they have been considering options to decentralize investment spending without losing alignment to the strategic plan. I hope Tom shares some of his ideas for how they might approach this as a mini hack during our next sprint.\n Paul Green, Jr. of The Morning Star Company built on Tom\u2019s comments by suggesting that perhaps by using technology to bring more people into the strategic planning process (as organizations like Wikimedia have done), more people might understand and share the strategic priorities. Eventually this might even lead to redefining the way strategic priorities are chosen in the first place, including more people in the process.\n Margaret Lam, Director of Enterprise Architecture at Seagate Technologies made another interesting observation about capital allocation. She suggests that organizations are almost always driven towards optimization, while entrepreneurial organizations are driven to take risks.\n \u201cWe look for ways to optimize resources, cost, assets, etc. If it is not a project with proven ROI, we don\u2019t want to do it. Entrepreneurial companies are exactly the opposite. They encourage ideas, innovations, forgive failures, and take risks. They are willing to provision time and resource to \u201cfund\u201d idea generation.\u201d\n As we begin to hack ways that we could eliminate the monopoly on capital allocation, I hope we\u2019ll consider ways that IT could enable organizations to act more entrepreneurial and less bureaucratic and perhaps even begin to involve more people at more levels in the organization in the capital allocation process.\n Monolithic structures\n Most big organizations today operate as monolithic structures\u2014not particularly nimble or flexible. But could technology help even big organizations become more adaptable? What could IT do to help disaggregate the organization?\n In Binary thinking; Singular thinking style, Jim Stikeleather, Executive Strategist at Dell Services, suggests that one issue with monolithic structures is that they encourage monolithic thinking:\n \u201cOrganizations drive themselves to the \u201cone answer\u201d and do not allow themselves to hold all possibilities open until the last possible minute, and then collapse like a probability wave into one state or another. Instead them must believe in the \u201cone truth\u201d and pursue it and only it until it either succeeds or fails,\u201d Jim said.\n What is Jim\u2019s answer to this challenge?\n \u201cIn reality, an organization has to practice both simultaneously in order to be aware of all possibilities; be able to stat rank probability, value and risk for each possibility; to \u201cgo where no man has gone before\u201d in preparing for those possibilities. It is the dialectic \u2013 thesis, anti-thesis then synthesis that these two conflicting modes of reasoning provide that enables an organization to adapt.\u201d\n Some of the discussion on this barrier centered on the idea of helping monolithic structures becomes more nimble and flexible. Colin Cunningham told a story about an exercise he was involved at in a Singapore bank where \u201cwe built the capability for sales staff, selected customers and technical people to interact together for 2 hours a day to identify improvements and issues in current live systems.\u201d His question: \u201cThis is at a tactical rather than strategic adaptability level. Could the concept be taken to the strategic level?\n As we begin to hack how technology could help us break down the monolithic structure of our organization, I hope we\u2019ll keep some of these thoughts in mind and look for ways that technology can help organizations become more flexible by becoming more multi-dimensional, supporting more ideas, answers, and strategies at once.\n In most organizations, senior executives dominate the strategy and decision-making process. Yet this becomes a problem when these executives have too much emotional equity invested in their past decisions or in short term financial goals. Could we protect the organization and increase its chances of adapting successfully by using technology to harness the intelligence of the entire company (not just the executives) in making decisions?\n Thierry Kuperman Le Bihan points out that the executives in charge may not always have the best long term adaptability interests of the organization at heart because of \u201cthe inevitable short-term interest from CEOs to make quick profits and leave the company in a better financial position when they leave. This objective prevents the CEO from thinking about the future of the company beyond a sequence of quarters.\u201d\n If Thierry is right, perhaps decentralizing decision making and including more people within the organization beyond the executive team in the process might provide a \u201ccheck and balance\u201d to ensure the long term future of the organization isn\u2019t just determined by short-term goals.\n As we consider how we might use technology to disaggregate some of the decision-making authority within our organizations, I\u2019d love to see hacks that use technology to balance the decision making power between the people at the top of the organization and those in the trenches who might have even better, more current information and\u2014collectively\u2014are prioritizing the long term adaptability of the business over short term financial objectives.\n Getting ready of the next sprint\n Over the past few weeks our discussion of the barriers to adaptability has proven to me that IT leaders have a unique vantage point from which to analyze these challenges. I\u2019m looking forward to seeing how we use our collective knowledge and experiences to begin to address these barriers over the next few weeks.\n This is the next step toward our final goal: identifying the management and technology \u2018hacks\u2019 that increase organizational adaptability.\n If you are not yet registered for the hackathon and would like to participate, please visit: www.mixhackathon.org\/gartner. \u00a0There is still time to participate.\n We have discussed the dynamics, identified the barriers, now let the hacking begin!\u00a0","item_date":"May 29 2012 18:31:24","display_item_date":"05-29-2012","url":"http:\/\/blogs.gartner.com\/mark_mcdonald\/2012\/05\/28\/hackathon-update-the-barriers-to-adaptability-results-from-the-adaptability-hackathon-sprint-2\/","source":"blogs.gartner.com"},{"title":"City Of Tampa Plans A Major Design Hackathon","details":"Change you can believe in got you feeling a little unsatisfied? How about change you can program?\n  Mayor Bob Buckhorn\u2019s office (City of Tampa) is working to create open access to many of the cities systems and databases to allow ad hoc, private teams of technologists to build and develop solutions for the city, local businesses and residents\/visitors.\n  To kickstart this initiative, they envision a hackathon.\n  Hackathons offer web designers and developers an opportunity to try out new ideas and collaborate with other coders in a fun and semi-managed environment.\n  Former San Francisco Mayor Gavin Newsome got the ball rolling with the concept in 2010, passing the first open data law in the United States (SF led the way with openness and technology? You don\u2019t say!).\n  Unsure of whether or not this would work? Here\u2019s a detailed explanation (via TED.com) of how the intelligent, digitally-skilled, and civic-minded among us have begun to re-program our cities with novel and useful apps: TED talk on \u201cCode for America.\u201d \n  The event will be June 22-24. For specifics about location, parking, remaining sponsorship opportunities and general information on the open data movement in Tampa, visit the citys website.\n  Remember, nerds change the world, and you should be nice to them.\n  Lastly, if you think this is a good idea and want to throw another hackathon in the Tampa Bay region, here\u2019s a list of tips and suggestions for your organizational pleasure.\n  \u201cStep-by-step, ferociously,\u201d -- Jeff Bezos.\n  Writer: Nathan Schwagler \n  Source: City of Tampa                            \t        Give us your email and we will give you our weekly online magazine. Fair?","item_date":"May 29 2012 18:24:22","display_item_date":"05-29-2012","url":"http:\/\/83degreesmedia.com\/innovationnews\/hack052912.aspx","source":"83degreesmedia.com"},{"title":"US CTO seeks to scale agile thinking and open data across federal government","details":"In the 21st century, federal government must go mobile, putting government services and information at the fingertips of citizens, said United States Chief Technology Officer Todd Park in a recent wide-ranging interview. Thats the first digital government result, outcome, and objective thats desired. \n  To achieve that vision, Park and U.S. chief information officer Steven VanRoekel are working together to improve how government shares data, architects new digital services and collaborates across agencies to reduce costs and increase productivity through smarter use of information technology. \n  Park, who was chosen by President Obama to be the second CTO of the United States in March, has been (relatively) quiet over the course of his first two months on the job. \n  Last Wednesday, that changed. Park launched a new Presidential innovation Fellows program, in concert with VanRoekels new digital government strategy, at TechCrunchs Disrupt conference in New York City. This was followed by another event for a government audience at the Interior Department headquarters in Washington, D.C. Last Friday, he presented his teams agenda to the Presidents Council of Advisors on Science and Technology.\n  The way I think about the strategy is that youre really talking about three elements, said Park, in our interview. First, its going mobile, putting government services at the literal fingertips of the people in the same way that basically every other industry and sector has done. Second, its being smarter about how we procure technology as we move government in this direction. Finally, its liberating data. In the end, its the idea of government as a platform. \n  Were looking for a few good men and women\n  In the context of the nations new digital government strategy, Park announced the launch of five projects that this new class of Innovation Fellows will be entrusted with implementing: a broad Open Data Initiative, Blue Button for America, RFP-EZ, The 20% Campaign, and MyGov. \n  The idea of the Presidential Innovation Fellows Program, said Park, is to bring in people from outside government to work with  innovators inside the government. These agile teams will work together within a six-month time frame to deliver results. \n  The fellowships are basically scaling up the idea of entrepreneurs in residence, said Park. Its a portfolio of five projects that, on top of the digital government strategy, will advance the implementation of it in a variety of ways. \n  The biggest challenge to bringing the five programs that the US CTO has proposed to successful completion is getting 15 talented men and women to join his team and implement them. Theres reason for optimism. Park shared vie email that:\n  ... within 24 hours of TechCrunch Disrupt, 600 people had already registered via Whitehouse.gov to apply to be a Presidential Innovation Fellow, and another several hundred people had expressed interest in following and engaging in the five projects in some other capacity. \n  To put that in context, Code for America received 550 applications for 24 fellowships last year. That makes both of these fellowships more competitive than getting in to Harvard in 2012, which received 34,285 applications for its next freshman class. There appears to be considerable appetite for a different kind of public service that applies technology and data for the public good.\n  Park is enthusiastic about putting open government data to work on behalf of the American people, amplifying the vision that his predecessor, Aneesh Chopra, championed around the country for the past three years. \n  The fellows are going to have an extraordinary opportunity to make government work better for their fellow citizens, said Park in our interview. These projects leverage, substantiate and push forward the whole principle of liberating data. Liberate data. \n  To me, one of the aspects of the strategy about which I am most excited, that sends my heart into overdrive, is the idea that going forward, the default state of government data shall be open and machine-readable, said Park. I think thats just fantastic. Youll want to, of course, evolve the legacy data as fast as you can in that same direction. Setting that as this is how we are rolling going forward \u2014 and this is where we expect data to ultimately go \u2014 is just terrific. \n  In the videos and interview that follow, Park talks more about his vision for each of the programs. \n  In the video below, Park discusses the Presidential Innovation Fellows program and introduces the first program, which focuses on open data: \n  \n  Park: The Open Data Initiative is a program to seed and expand the work that were doing to liberate government data as a platform. Encourage, on a voluntary basis, the liberation of data by corporations, as part of the national data platform, and to actively stimulate the development of new tools and services, and enhance existing tools and services, leveraging the data to help improve Americans lives in very tangible ways, and create jobs for the future. \n  This leverages the Open Government Directive to say look, the default going forward is open data. Also the directive to API-ize two high priority datasets and also, in targeted ways, go beyond that, and really push to get more data out there in, critically, machine-readable form, in APIs, and to educate the entrepreneur and innovators of the world that its there through meetups, and hackathons, and challenges, and Datapaloozas. \n  Were doubling down on the Health Data Initiative, we are also launching a much more high-profile Safety Data Initiative, which we kicked off last week. An Energy Data Initiative, which kicked off this week. An education data initiative, which were kicking off soon, and an Impact Data Initiative, which is about liberating data with respect to inputs and outputs in the non-profit space. \n  Were also going to be exploring an initiative in the realm of personal finance, enabling Americans to access copies of their financial data from public sector agencies and private sector institutions. So, the format that were going to be leveraging to execute these initiatives is cloned from the Health Data Initiative. \n  This will make new data available. It will also take the existing public data that is unusable to developers, i.e. in  the form of PDFs, books or static websites, and turn it into liquid machine-readable, downloadable, accessible data via API. Then \u2014 because were consistently hearing that 95% of the innovators and entrepreneurs who could turn our data into magic dont even know the data exists, let alone that its available to them \u2014 engage the developer community and the entrepreneurial community with the data from the beginning. Let them know its there, get their feedback, make it better. \n  Blue Button for America\n    \nPark: The idea is to develop an open source patient portal capability that will replace MyHealthyVet, which is the Veterans Administrations current patient portal. This will actually allow the Blue Button itself to iterate and evolve more rapidly, so that everY time you add more data to it, it wont require heart surgery. It will be a lot easier, and of course will be open source, so that anyone else who wants to use it can use it as well. On top of that, were going to do a lot of biz dev in America to get the word out about Blue Button and encourage more and more holders of data in the private sector to adopt Blue Button. Were also going to work to help stimulate more tool development by entrepreneurs that can upload Blue Button data and make it useful in all kinds of ways for patients. Thats Blue Button for America. \n  What is RFP-EZ?\n  \n  Park: The objective is buying smarter. The project that were working ON with the Small Business Administration on is called RFP-EZ. \n  Basically, its the idea of setting up a streamlined process for the government to procure solutions from innovative, high-growth tech companies. As you know, most high-growth companies regard the government as way too difficult to sell to. \n  That A) deprives startups and high-growth companies from the government as a marketplace and, B) perhaps even more problematically, actually deprives the government of their solutions. \n  The hope here is, through the actions of the RFP-EZ team, to create a process and a prototype that the government can much more easily procure solutions from innovative private firms. \n  It A) opens up this emerging market called the government to high-tech startups and B) infects the government with more of their solutions, which are radically more, pound for pound, effective and cost efficient than a lot of the stuff that the government is currently procuring through conventional channels. Thats RFP-EZ. \n  \n  Park: The 20% Campaign is a project thats being championed by USAID. Its an effort at USAID to, working with other government agencies, NGOs and companies, to catalog the movement of foreign assistance payments from cash to electronics. So, just for example, USAID pays its contractors electronically, obviously, but the contractor who, say, pays highway workers in Afghanistan or the way that police officers get paid in Afghanistan is actually principally via cash. Or has been. And that creates all kinds of waste issues, fraud, and abuse. \n  The idea is actually to move to electronic payment, including mobile payment \u2014 and this has the potential to significantly cut waste, fraud and abuse, to improve financial inclusion, to actually let people on phones, to enable them to access bank accounts set up for them. That leads to all kinds of good things, including safety: its not ideal to be carrying around large amounts of cash in highly kinetic environments. \n  The Afghan National Police started paying certain contingents of police officers via mobile phones and mobile payments, as opposed to cash, and what happened is that the police officers started reporting an up to a 30% raise. Of course, their pay hadnt changed, but basically, when it was in cash, a bunch of it got lost. This is obviously a good thing, but its even more important if you realize that when they were paid what they were paid in cash that they ultimately physically received, that was less than the Taliban in this province was actually paying people to join the Taliban \u2014 but the mobile payment, and that level of salary, was greater than the Taliban was paying. Thats a critical difference. \n  Its basically taking foreign assistance payments through the last mile to mobile. \n  MyGov is the U.S. version of Gov.uk\n  \n  Park: MyGov is an effort to rapidly prototype a citizen-centric system that allows Americans the information and resources of government that are right for them. Think of it as a personalized channel for Americans to be able to access information resources across government and get feedback from citizens about those information and resources. \n  How do you plan to scale what you learned while you were HHS CTO to the all of the federal government?\n  Park: Specifically, were doing exactly the same thing we did with the Health Data Initiative, kicking off the initiatives with a data jam \u2014 an ideation workshop where we invite, just like with health data, 40 amazing tech and energy minds, tech and safety innovators, to a room \u2014 at the White House, in the case of the Safety Data Initiative, or at Stanford University, in the case of the Energy Initiative. \n  We walk into the room for several hours and say, Heres a big pile of data. What would you do with this data? And they invent 15 or 20 news classes of products or services of the future that we could build with the data. And then we challenge them to, at the end of the session, build prototypes or actual working products, that instantiates their ideas in 90 days, to be highlighted at a White House \u2014 hosted Safety Datapalooza, Energy Datapalooza, Education Datapalooza, Impact Datapalooza, etc. \n  We also take the intellectual capital from the workshops, publish it on the White House website, and publicize the opportunity around the country: Discover the data, come up with your own ideas, build prototypes, and throw your hat in the ring to showcase at a Datapalooza. \n  What happens at the Datapaloozas \u2014 our experience in health guides us \u2014 is that, first of all, the prototypes and working products inspire many more innovators to actually build new services, products and features, because the data suddenly becomes really concrete to them, in terms of how it could be used. \n  Secondly, it helps persuade additional folks in the government to liberate more data, making it available, making it machine-readable, as opposed to saying, Look, I dont know what the upside is. I can only imagine downsides.  What happened in health is, when they went to a Datapalooza, they actually saw that, if data is made available, then at no cost to you and no cost to taxpayers, other people who are very smart will build incredible things that actually enhance your mission. And so you should do the same. \n  As more data gets liberated, that then leads to more products and services getting built, which then inspires more data liberation, which then leads to more products and services getting built \u2014 so you have a virtual spiral, like whats happened in health. \n  The objective of each of these initiatives is not just to liberate data. Data by itself isnt helpful. You cant eat data. You cant pour data on a wound and heal it. You cant pour data on your house and make it more energy efficient. Data is only useful if its applied to deliver benefit. The whole point of this exercise, the whole point of these kickoff efforts, is to catalyze the development of an ecosystem of data supply and data use to improve the lives of Americans in very tangible ways \u2014 and create jobs. \n  We have the developers and the suppliers of data actually talk to each other, create value for the American people, and then rinse, wash, repeat. \n  Were recruiting, to join the team of Presidential Innovation Fellows,  entrepreneurs and developers from the outside to come in and help with this effort to liberate data, make it machine-readable, and get it out there to entrepreneurs and help catalyze development of this ecosystem. \n  We went to TechCrunch Disrupt for a reason: its right smack dab center in the middle of people we want to recruit. We invite people to check out the projects on WhiteHouse.gov and, if youre interested in applying to be a fellow, indicate their interest. Even if they cant come to DC for 6-plus months to be a fellow, but they want to follow one of the projects or contribute or help in some way, we are inviting them express interest in that as well. For example, if youre an entrepreneur, and youre really interested in the education space, and learning about what data is available in education, you can check out the project, look at the data, and perhaps you can build something really good to show at the Education Datapalooza. \n  Is open data just about government data? What about smart disclosure? \n  Park: In the context of the Open Data Initiatives projects, its not just about liberation of government health data: its also about government catalyzing the release, on a voluntary basis, of private sector data. \n  Obviously, scaling Blue Button will extend the open data ecosystem. Were also doubling down on Green Button. I was just in California to host discussions around Green Button. Utilities representing 31 million households and businesses have now committed to make Green Button happen. Close to 10 million households and businesses already have access to Green Button data. \n  Theres also a whole bunch of conversation happening about, at some point later this year, having the first utilities add the option of what were calling Green Button Connect.  Right now, the Green Button is a download, where you go to a website, hit a green button and bam, you download your data.  Green Button Connect is the ability for you to say as a consumer, I authorize this third party to receive a continuous feed of my electricity usage data. \n  That creates massive additional opportunity for new products and services. That could go live later this year. \n  As part of the education data initiative, we are pursuing the launch and scale up of something called My Data, which will have a red color button. (It will probably, ultimately, be called Red Button.) This is the ability for students and their families to download an electronic copy of their student loan data, of their transcript data, of their academic assessment data. \n  That notion of people getting their own data, whether its your health data, your education data, your finance data, your energy use data, thats an important part of these open data initiatives as well, with government helping to catalyze the release of that data to then feed the ecosystem. \n  How does open data specifically relate to the things that Americans care about, access to healthcare, reducing energy bills, giving their kids more educational opportunities, and job creation? Is this just about apps?\n  \n  Park: In healthcare, for example, youll see a growing array of examples that leverage data to create tangible benefit in many, many ways for Americans. Everything from helping me find the right doctor or hospital for my family to being notified of a clinical trial that could assist my profile and save my life, and the ability to get the latest and greatest information about how to manage my asthma and diabetes via government knowledge in the National Library of Medicine. \n  There is a whole shift in healthcare systems away from pay-for-volume of services to basically paying to get people healthy. It goes by lots of different names \u2014 accountable care organizations or episodic payment \u2014 but the fundamental common theme is that the doctors and hospitals increasingly will be paid to keep people healthy and to co-ordinate their care, and keep them out of the hospital, and out of the ER. \n  Theres a whole fleet of companies and services that utilize data to help doctors and hospitals do that work, like utilize Medicare claims data to help identity segments of a patient population that are at real risk, and need to get to the ER or hospital soon. There are tools that help journalists identify easily public health issues, like healthcare outcomes disparities by race, gender and ethnicity. There are tools that help country commissioners and mayors understand whats going on in a community, from a health standpoint, and make better policy decisions, like showing them food desserts. Theres just a whole fleet of rapidly growing services for consumers, for doctors, nurses, journalists, employers, public policy makers, that help them make decisions, help them deliver improved health and healthcare, and create jobs, all at the same time. \n  Thats very exciting. If you look at all of those products and services \u2014 and a subset of them are the ones that self-identify to us, to actually be exhibited at the Health Datapaloozas. Look at the 20 healthcare apps that were at the first Datapalooza or the 50 that were at the second. This year, there are 230 companies that are being narrowed down to about a total of 100 that will be at the Datapalooza. They collectively serve millions of people today, either through brand new products and services or through new features on existing platforms. They help people in ways that we would never have thought of, let alone build. \n  The taxpayer dollars expended here were zero. We basically just took our data, made it available in machine-readable format, educated entrepreneurs that it was there, and they did the rest. Think about these other sectors, and think about whats possible in those sectors. \n  In education, through making the data that weve made available, you can imagine much better tools to help you shop for the college that will deliver the biggest bang for your buck and is the best fit for your situation. \n  Weve actually made available a bunch of data about college outcomes and are making more data available in machine-readable form so it can feed college search tools much better. We are going to be enabling students to download machine-readable copies of their own financial aid application, student loan data and school records. That will really turbo charge smart scholarship and school search capabilities for those students. You can actually mash that up with college outcomes in a really powerful, personalized college and scholarship search engine that is enabled by your personal data plus machine-readable data. Tools that help kids and their parents pick the right college for their education and get the right financial aid, thats something government is going to facilitate. \n  In the energy space, there are apps and services that help you leverage your Green Button data and other data to really assess your electricity usage compared to that of others and get concrete tips on how you can actually save yourself money. Were already seeing very clever, very cool efforts to integrate gamification and social networking into that kind of app, to make it a lot more fun and engaging \u2014 and make yourself money. \n  One dataset thats particularly spectacular that were making a lot more usable is the EnergyStar database. Its got 40,000 different appliances, everything from washing machines to servers that consumers and businesses use. We are creating a much, much easier to use public, downloadable NSTAR database. Its got really detailed information on the energy use profiles and performance of each of these 40,000 appliances and devices. Imagine that actually integrated into much smarter services. \n  On safety, the kinds of ideas that people are bringing together are awesome. Theyre everything from using publicly available safety data to plot the optimal route for your kid to walk home or for a first responder to travel through a city and get to a place most expeditiously. \n  Theres this super awesome resource on Data.gov called the Safer Products API, which is published by the Consumer Products Safety Commission (CPSC). Consumers send in safety reports to CPSC, but until March of last year, you had to FOIA [Freedom of Information Act] CPSC to get these. So what theyve now done is actually publish an API which not only makes the entire database of these reports public, without you having to FOIA them, but also makes it available through an API. \n  One of the ideas that came up is that, when people buy products on eBay, Craiglist, etc, all the time, some huge percentage of Americans never get to know about a recall \u2014 a recall of a crib, a recall of a toy. And even when a company recalls new products, old products are in circulation. What if someone built the ability to integrate the recall data and attach it to all the stuff in the eBays and Craigslists of the world? \n  Former CIO Vivek Kundra often touted government recall apps based upon government data during his tenure. Is this API the same thing, shared again, or something new?\n  Park: I think the smartest thing the government can do with data like product recalls data is not build our own shopping sites, or our own product information sites: its to get the information out there in machine-readable form, so that lots and lots of other platforms that have audiences with millions of people already, and who are really good at creating shopping experiences or product comparison experiences, get the data into their hands, so that they can integrate it seamlessly into what they do. I feel that thats really the core play that the government should be engaged in. \n  I dont know if the Safer Products API was included in the recall app. What I do know is that before 2011, you had to FOIA to get the data. I think that even if the government included it in some app the government built, that its important for it to get used by lots and lots of other apps that have a collective audience thats massively greater than any app the government could itself build. \n  Another example of this is the Hospital Compare website. The Hospital Compare website has been around for a long time. Nobody knows about it. There was a survey done that found 94% of Americans didnt know that there was hospital quality data that was available, let alone that there was a hospital compare website. So, the notion of A) making the hospital care data downloadable and B), we actually deployed it a year and a half ago in API form at Medicare.gov. \n  That then makes the data much easier for lots of other platforms to incorporate it, that are far more likely than HospitalCompare.gov to be able to present the information in actionable forms for citizens. Even if we build our own apps, we have to get this data out to lots of other people that can help people with it. To do that, we have to make it machine-readable, we have to put it into RESTFUL APIs \u2014 or at least make it downloadable \u2014 and get the word out to entrepreneurs that its something they can use. \n  This is a stunning arbitrage opportunity. Even if you take all this data and you API-ize it, its not automatic that entrepreneurs are going to know its there. \n  Lets assume that the hospital quality data is good \u2014 which it is \u2014 and that you build it, and put it into an API. If nobody knows about it, youve delivered no value to the American people. People dont care whether you API a bunch of data. What they care about is that when they need to find a hospital, like I did, for my baby, I can get that information. \n  The private sector, in the places where we have pushed the pedal to the medal on this, has just demonstrated the incredible ability to make this data a lot more relevant and help a lot more people with it than we could have by ourselves. \n  White House photo used on associated home and category pages: white house by dcJohn, on Flickr","item_date":"May 29 2012 18:09:53","display_item_date":"05-29-2012","url":"http:\/\/radar.oreilly.com\/2012\/05\/us-cto-seeks-to-scale-agile-te.html","source":"radar.oreilly.com"},{"title":"REST definition and its place within Enterprise Integration","details":"In a previous post I explained why REST is useless when it comes to Enterprise Integration. Even though at the very beginning I explicitly stated that\n Roy Fielding wrote his dissertation entirely in the context of Web\n REST has absolutely no business benefits whatsoever with regards to Enterprise Integration\n I got surprised to say the least by comments in various channels, most from professionals, even vendor representatives. Tech people, but nonetheless\n The most commonly heared argument was that I misrepresented REST or even clearly showed my lack of understanding of REST, upon which my simple response was: can you please point me to a few articles or posts that respresent your understanding? but surprise you or not, the responses to that were either absent, or absurdly far from being to the point (e.g. \u201cyour post doesn\u2019t even make me want to help you\u201d)\n So, I show you what REST is, according to Roy\u2019s dissertation, and Wikipedia, and then will disect that with regards to Enterprise Integration, and see where the twain meet. Does that seem fair? I most certainly think it does. If it doesn\u2019t, point me to definitions that better suit your understanding of either, please. This is not meant as a debate, but a dialogue (as usual). If you don\u2019t want to participate in the dialogue, fine \u2013 don\u2019t even bother to react or comment\n The definition of REST is kind of hard to come by. Yet, according to the magnificent Wikipedia, here are the constraints of REST (shortened yet literally quoted by me for brevity):\n Client\u2013server: A uniform interface separates clients from servers\n Stateless: The client\u2013server communication is further constrained by no client context being stored on the server between requests\n Cacheable: (\u2026) Responses must therefore, implicitly or explicitly, define themselves as cacheable, or not, to prevent clients reusing stale or inappropriate data in response to further requests\n Layered system: Intermediary servers may improve system scalability by enabling load-balancing and by providing shared caches. They may also enforce security policies\n Code on demand (optional): Servers are able temporarily to extend or customize the functionality of a client by the transfer of executable code. Examples of this may include compiled components such as Java applets and client-side scripts such as JavaScript\n Uniform interface\n The uniform interface between clients and servers, discussed below, simplifies and decouples the architecture, which enables each part to evolve independently. The four guiding principles of this interface are detailed below\n \n Identification of resources: (\u2026) For example, the server does not send its database, but rather, perhaps, some HTML, XML or JSON that represents some database records expressed, for instance, in Finnish and encoded in UTF-8\n Manipulation of resources through these representations: When a client holds a representation of a resource, including any metadata attached, it has enough information to modify or delete the resource on the server, provided it has permission to do so\n Self-descriptive messages: Each message includes enough information to describe how to process the message. For example, which parser to invoke may be specified by an Internet media type (previously known as a MIME type). Responses also explicitly indicate their cacheability\n Hypermedia as the engine of application state: Clients make state transitions only through actions that are dynamically identified within hypermedia by the server (e.g. by hyperlinks within hypertext). Except for simple fixed entry points to the application, a client does not assume that any particular actions will be available for any particular resources beyond those described in representations previously received from the server.\n \nYou find very little of all of that when you search the Web. What you do find, is an insistency on CRUD to use with HTTP. Which is odd, because CRUD isn\u2019t even part of REST. It\u2019s unclear where it got added, how and by whom, but here\u2019s what the common misunderstanding is:\n REST identifies four verbs or HTTP methods: POST, GET, PUT and DELETE \u2013 identical to C(reate), R(ead), U(pdate) and D(elete), the basic functions of a database\n \nRoy explicitly disagrees with that in his March 2009 post:\n Some people think that REST suggests not to use POST for updates. Search my dissertation and you won\u2019t find any mention of CRUD or POST\n So, CRUD\u2019s out the door, leaving the 6 constraints and the 4 guiding principles\nLet me comment on REST\u2019s constraints first, from an Enterprise Integration point of view\n Client\u2013server: A uniform interface separates clients from servers. No, what Roy said was \u201cBy separating the user interface concerns from the data storage concerns, we improve the portability of the user interface across multiple platforms and improve scalability by simplifying the server components\u201d. And that makes perfect sense in EI. But uniform interface? That is a lock-in no one can afford \u2013 functionally uniform, yes I agree. But technically?\n Stateless: The client\u2013server communication is further constrained by no client context being stored on the server between requests. I certainly hope so. Cache prevents any means of scale\n Cacheable: (\u2026) Responses must therefore, implicitly or explicitly, define themselves as cacheable, or not, to prevent clients reusing stale or inappropriate data in response to further requests. I never got around this one. You can\u2019t be stateless and allow for cache at the same time, client-side nor server-side.\u00a0This is a void in Roy\u2019s plan\n Layered system: Intermediary servers may improve system scalability by enabling load-balancing and by providing shared caches. They may also enforce security policies. A clear deeply infrastructural technical addition, again focussing on speeding up the Web. Enforcing security policies? Only when those have been agreed upon beforehand, I hope \u2013 it would be devastating if your request bounces somewhere off intermediary server 5 or 6. Shared caches? Again, this defies the notion of statelessness\n Code on demand (optional): Servers are able temporarily to extend or customize the functionality of a client by the transfer of executable code. Examples of this may include compiled components such as Java applets and client-side scripts such as JavaScript. In a Web context, this may make sense. In an EI context, it won\u2019t: you offer back-end services, and that\u2019s it. Can\u2019t temporarily offer more to some clients, and then take away those privileges again\n Uniform interface: The uniform interface between clients and servers, discussed below, simplifies and decouples the architecture, which enables each part to evolve independently. The four guiding principles of this interface are detailed below\n \nSo, out of 6 (or is it 5) constraints, only one might work for Enterprise Integration. The constraints are mostly focused on caching, but that doesn\u2019t come as a surprise:\nRoy Fieldings\u2019s dissertation\u00a0is 12 years old now.\u00a0I read the dissertation, from beginning to end. It taught me among others that\n REST enables the caching and reuse of interactions, dynamic substitutability of components, and processing of actions by intermediaries, thereby meeting the needs of an Internet-scale distributed hypermedia system. (\u2026) REST elaborates only those portions of the architecture that are considered essential for Internet-scale distributed hypermedia interaction\n Roy was mainly concerned about Web performance and thus wrote his dissertation. Did he mean it to be used as a great paradigm for Enterprise Integration? If any, he certainly didn\u2019t state so. I\u2019ll take my chances and state that he most certainly didn\u2019t, and still doesn\u2019t.\n Anyway, there are 4 guiding principles left\n Let me comment on REST\u2019s guiding principles, from an Enterprise Integration point of view\n Identification of resources: (\u2026) For example, the server does not send its database, but rather, perhaps, some HTML, XML or JSON that represents some database records expressed, for instance, in Finnish and encoded in UTF-8. Absolutely agreed. Decoupling between data in the database and the form it is requested in or even responded in, is without questioning\n Manipulation of resources through these representations: When a client holds a representation of a resource, including any metadata attached, it has enough information to modify or delete the resource on the server, provided it has permission to do so. Roy says \u201cInformation hiding is one of the key software engineering principles that motivates the uniform interface of REST. Because a client is restricted to the manipulation of representations rather than directly accessing the implementation of a resource, the implementation can be constructed in whatever form is desired by the naming authority without impacting the clients that may use its representations\u201d which makes sense, and relates back to decoupling, and the use of an interface\n Self-descriptive messages: Each message includes enough information to describe how to process the message. For example, which parser to invoke may be specified by an Internet media type (previously known as a MIME type). Responses also explicitly indicate their cacheability. This is again strictly Web tech related, and touches on the fundamental differences between REST and Enterprise Integration: REST is written for the Web, where unknowns interact with unknowns. Enterprise Integration is all about predefined agreements between knowns\n Hypermedia as the engine of application state: Clients make state transitions only through actions that are dynamically identified within hypermedia by the server (e.g. by hyperlinks within hypertext). Except for simple fixed entry points to the application, a client does not assume that any particular actions will be available for any particular resources beyond those described in representations previously received from the server. Again, strictly Web context. It is difficult to follow which parts of REST are about conversations, and which are about request-reply during those conversations\n \nIt\u2019s not my intention to invalidate REST. It is my intention to really get behind its definitions, and its benefits, and see where those can be useful for Enterprise Integration. Call me narrow-minded but that\u2019s all I\u2019m interested in at the moment. The use of REST for the web? Not my area of expertise, but I believe REST is already widely implemented there.\n I think I have done a fair amount of research in order to find out what REST really is about \u2013 I even found that one if its pillars, CRUD, isn\u2019t part of REST. It got added to it at some point, certainly not by Roy Fielding himself\n Is REST useful for Enterprise Integration? Certainly not on the wide basis that Enterprise integration must operate on: multi-channel, multi-message, multi-device, multi-platform, multi-industry. In short: no, absolutely not.\n REST focuses on caching, layering, dsitributed hypermedia interaction for an Internet-scale use. That dwarfs Enterprise Integration in size, of course.\n But there is also a fundamental difference between the Web and Enterprise Integration. A2A, B2B, B2C: these all operate on predefined, fixed agreements. Can\u2019t just plug into the Matrix and try to figure out for yourself what the functionality is of the return of a function \u2013 that\u2019s the epitome of making assumptions\n Where ever I look, REST is aimed to increase performance within an Internet-architecture by enabling caching, layering, intermediate services. Its second aim is to diminish or even avoid rework if parts change their definition or functional form, by using representations rather than the real deal.\nThe first goal is absolutely unrelated to any form of Business Integration. If there is any latency within Enterprise Integration, this is not the way to solve it \u2013 I\u2019d first start to use a better tool.\nThe second goal is outright forbidden to use within Enterprise Integration. I explicitly want to subscribe to service A version 35 that provides me with information X.29a. I absolutely do not want that to change without my prior knowledge\n The main difference between REST and Enterprise Integration is that the former runs on the Web, serving humans. The latter runs inside companies, serving machines. The first must be highly flexible to cope with the inherent dynamics, the second must be extremely rigid to enforce the agreed starics.\nTheir common ground? Basically, little to nothing.","item_date":"May 29 2012 17:57:54","display_item_date":"05-29-2012","url":"http:\/\/www.cloudave.com\/19982\/rest-definition-and-its-place-within-enterprise-integration\/#comment-25954","source":"www.cloudave.com"},{"title":"API Software Testing","details":"An API (Application Programming Interface) is a collection of software functions and procedures, called API calls, that can be executed by other software applications. API testing is mostly used for the system which has collection of API that needs to be tested. The system could be system software, application software or libraries. Each API has its different functionality therefore it provides different results on different input. The errors or exception returns may also vary depending upon the API. However once integrated within a product, the common functionality covers a very minimal code path of the API and the functionality testing \/ integration testing may cover only those paths.\n  As companies extend software applications by opening up and sharing data via APIs this puts additional demands on software testing teams. This type of testing is technical and requires an understanding of protocols and data formats that go well beyond the usual GUI stuff you run.\n  API Testing is slightly differ from GUI Testing:\n  Testing APIs requires understanding of Inner workings: It may possible that some of API interconnected with other to share their functionality. So, there is requirement to understand the internal working properly.\n  Lack of Domain Knowledge: It is possible that tester may not well versed in using API then requirement of exploring the interface and their usage is needed. This problem can be solved out by involving the tester from starting of programming an application.\n  Sufficient Programming Skills: \u00a0Generally, APIs Tests connected with sequence of calls called program that\u2019s why programming knowledge is required for tester to understand each API functionality.\n  No Available Documentation: This is usually seen, there is no documentation available with API\u2019s that create problem in understanding the purpose of calls, the parameter types and possible valid\/invalid values and their return values. Hence, proper documentation will help test designer to test faster.\n  Access to Source code: The availablity of source code will help test designer in understanding and analysing the mechanism is followed. In the absence of source code, it is problematic for test designer to find out analmolies exist in code.\n  Time Constraints: It is really time consuming to test APIs thoroughly as there is requirement of resources to develop tools and design tests.\n  Finally it is concluded that API testing, like other activities owned by the QA team, must consider the full functionality of the system, as it will be used by the end user (in this case, another program). This means that API tests must be far more extensive than unit tests, and take into consideration the sorts of scenarios that the API will be used for, which typically involve interactions between several different modules within the application.\n\t  \n \t\t\t\t \t\tTo provide high-end, affordable, and the best of its class software testing services and QA consulting across all domains, business segments, and technologies aimed at benefiting clients and enhancing their business prospects and profitability. More...\t\t\n     \n \t\t\t\t \t\tCommitment to the best, passion to excel, integrity, innovation, timeliness, mutual benefit, and transparent and flexible approach.","item_date":"May 29 2012 17:51:14","display_item_date":"05-29-2012","url":"http:\/\/www.360logica.com\/360logica-social\/blog\/item\/108-api-software-testing","source":"www.360logica.com"},{"title":"Now Even Property Management Has An API","details":"ActiveBuilding, the makers of a property management portal, helps managers keep in touch with residents. From a single interface, property management can communicate with residents regarding maintenance, services and announcements. Now, ActiveBuilding has launched the ActiveBuilding API which brings ActiveBuilding\u2019s traditional functionality to endless integration possibilities.\n\n\n\n\n\nSuppose a developer wants to collect rent and track packages from a single application. With the ActiveBuilding API, the developer can collect rent and track packages without leaving the original ActiveBuilding property management platform. Or, if the property management provider is not currently utilizing ActiveBuilding, the developer can create the application to track packages and collect rent, and add all the ActiveBuilding platform data by simply adding the ActiveBuilding API to the application.\n\n\nThe ActiveBuilding API uses REST and formats data as XML. It uses an API key for authentication. Premier customers representing both the broad industry (e.g. National Apartment Association, California Apartment Association, South Coast Apartment Association) and premier management companies (e.g. Berkshire Property Advisors, Blanton Turner, Riverstone Residential Group), present a hotbed of developers to launch the API to.\n\n\nFor access to the API click here. Developers can register for a key and also gain access to eight different sections depending on their application needs (announcements API, community API, key log API, packages API, units API, residents API, events API, service requests API).","item_date":"May 29 2012 17:49:15","display_item_date":"05-29-2012","url":"http:\/\/blog.programmableweb.com\/2012\/05\/29\/now-even-property-management-has-an-api\/","source":"blog.programmableweb.com"},{"title":"Business Challenges Solved by APIs: What Every Executive Should Know | Elastic Path Software","details":"It seems like everyone either has or needs an API these days. Analysts are touting how critical it is for businesses to engage their customers across every touchpoint. APIs are supposed to fuel innovation for these new applications, new channels, and, ultimately, new ways to generate revenue.\n  But getting your API strategy off the ground could be more difficult than it sounds. Even with the right strategy and architecture, there are very real technical challenges in your way.\n  Join API experts from Elastic Path Software as well as API Evangelist Kin Lane for a frank discussion on the current state of APIs.\n  Webinar takeaways:\n \nLearn how APIs are critical to the way consumers engage with businesses today\n \tUnderstand how your current challenges can be solved by a smart API strategy\n \tAppreciate what technical roadblocks await your team, and how to solve them\n \n Elastic Path is the leader in digital commerce technology and expertise for enterprises selling digital goods and content. Major global brands such as Google, Time Inc, and Virgin Media rely on Elastic Path to monetize digital relationships with their customers in ways that are frictionless, social, and everywhere.","item_date":"May 29 2012 17:44:43","display_item_date":"05-29-2012","url":"http:\/\/www.elasticpath.com\/webinar\/api-for-execs-resource#.T8UFRntG1eY.twitter","source":"www.elasticpath.com"},{"title":"The API-ificiation of software \u2013 and LEGOs \u2014 Tech News and Analysis","details":"Today everything has an API. Facebook has hundreds of APIs across such social areas as friends, photos, likes and events. Google has thousands of APIs across search\/AdWords, Web analytics, YouTube, maps, email and many more. Amazon has APIs that cover the spectrum from Alexa Web traffic rankings to e-commerce product and pricing information and even the ability to start and stop individual machines. I spent a decade architecting and building component and services based software, and another decade after that evaluating and investing in infrastructure software, I believe this mobile and cloud influenced wave of RESTful service-oriented software may finally live up to its initial promise.\n\n\nAlthough the majority of API attention has centered on consumer Web services, an emerging cadre of startups are focused on infrastructure and business processes. These newcomers are providing a broad range of critical services neatly packaged as frameworks or APIs. Some of these companies, such as Salesforce and Google Maps, are next generation SaaS providers that have built solutions to serve both end users and developers. Others, including Mailchimp and Twilio, are pure play offerings that solely target developers as customers.\n\n\nOpen source helped to reignite the open systems movement in the late 1990s, which popularized the idea of creating public projects and actively soliciting community feedback and involvement. Tens of thousands of open source projects have been created, but only those projects that built sizable communities have thrived. Most of the large infrastructure software categories were eventually filled by strong open source projects and some spawned successful commercial software companies, including RedHat, XenSource, Sourcefire, MySQL, JBoss, Talend and Alfresco. These companies span a broad range \u2014 from operating systems\/hypervisors to security to middleware and database\/content management.\n\n\nDevelopers now expect the same instant gratification as end users. Instead of having to download, configure and manage all the associated software components, more and more of these capabilities need to be packaged \u201cas-a-service\u201d \u2014 hence, the move to cloud services. It is also important to remember that software development is an art, and programmers want a very simple and elegant programming interface.\n\n\nThese developer-focused startups provide simply packaged programming interfaces for a wide range of cloud-enabled services \u2014 from such basic infrastructure capabilities as storing and retrieving files to much more complex business processes, including invoicing, billing and payment processing. In some instances, startups\u00a0begin by offering a constrained solution around a key feature\/function, but then leverage that starting point to broaden the service into a more complete offering. This is the approach that\u00a0CloudFlare\u00a0and\u00a0Urban Airship\u00a0took, for example.\n\n\nTargeting developers is not the only thing these startups are doing differently. Leveraging the inherent distribution of the app economy gives these new companies another intriguing opportunity to disrupt the incumbents. The explosive growth in mobile applications and the associated re-platforming means that developers are rebuilding from the ground up and evaluating new technologies at all layers. In addition, the emergence of so many new languages even further complicates the problems for incumbents. Originally written for the C++ and Smalltalk audiences, one of my favorite programming books of all time, \u201cDesign Patterns: Elements of Reusable Object-Oriented Software ,\u201d by the famous Gang of Four, has certainly influenced software architects on clean and reusable design abstractions.\n\n\nHere is a broad, but by no means exhaustive, list of some of the potential infrastructure categories and disruptive companies. (If you want an exhaustive list of APIs check out ProgrammableWeb.) Some of the categories are unfilled by potential disruptors, and other categories include pure cloud services as well as software products. (In a follow-up post, I will share another list that includes a range of higher level business process type services.)\n\n\nIn some ways, this isn\u2019t an entirely new approach. During the Web and client server technology cycles, some successful software companies used a similar model of providing libraries (and even DLLs). Crystal Reports made it extremely easy for developers to embed reporting within their applications; Hummingbird and Wollongong offered (TCP\/IP) networking stacks; Visigenic provided (ODBC) database libraries\/drivers; and Neuron Data and PowerBuilder provided platforms for user interfaces. These examples show that it was possible to build successful businesses at all levels of the software stack.\n\n\nConsequently, APIs are only the latest packaging as developers transition from objects and class libraries to RESTful services. However, it seems that the promise of service oriented architectures is finally being realized, and it is creating a strong opportunity for innovative business models.\n\n\nSo what does all this mean? Client-server software was replaced by on-premise Web-based apps, which are in turn being replaced by cloud\/SaaS solutions. Perhaps the next generation of software will be solutions composed from these APIs\/services. All of these exciting startups are proving that developers need \u2014 and want \u2014\u00a0these services. But ultimately, building complete solutions might require a LEGO-like construction kit. Is the future of software about Lightweight Enterprise Gadget Orchestration? Perhaps, there is even an opportunity for someone to provide an Interface Builder (remember NextStep?) that can natively access the myriad of emerging SaaS\/APIs.\n\n\nRobin Vasan, managing director at Mayfield,\u00a0invests in the cloud, SaaS and mobile themes. Some of his current investments include Alfresco, Couchbase, Marketo and Webroot. Past successes include Akimbi, Trigo and webMethods. Mayfield has also been involved in such other leading companies as 3Com, 3PAR, Citrix, Concur, Legato, Nuance, Tibco and Vantive.\n\n\nSome rights reserved\u00a0by\u00a0fdecomite.","item_date":"May 29 2012 17:10:01","display_item_date":"05-29-2012","url":"http:\/\/gigaom.com\/2012\/05\/28\/the-api-ificiation-of-software-and-legos\/","source":"gigaom.com"},{"title":"81 New APIs: Associated Press, Nike, eBay, Rdio and SendGrid","details":"This week we had 81 new APIs added to our API directory including a customer referral service, news content metadata service, corporate sustainability initiative, cloud based email service, eBay notification management service and social music playback integration. In addition we covered WorldMate\u2019s launch of their e-mail parsing API. Below are more details on each of these new APIs.\n\n\n About.me API: About.me is a service that allows users to create one-page websites about themselves. Users can customize their page, upload a photo, include biographies, interests, and links to Twitter, Facebook, LinkedIn, and other sites.\n\n\nThe About.me API allows developers to access and integrate the About.me functionality with other applications and to create new applications. Some example API methods include searching and retrieving profiles and information, adding and editing About.me pages, and connecting with social media sites.\n\n\n APPELLO API: The APPELLO API was developed to connect to sets of EpiDoc encoded documents and to extract information from them. EpiDoc is a set of guidelines for the structured markup of epigraphic documents in TEI XML. This API is provided by the Centre for the Study of Ancient Documents and The British Museum. It can be accessed using REST calls in XML format.\n\n\n Aramex API: The service provides parcel, package, and freight delivery via a global network of shipping providers. Applications can integrate package shipping and rate calculations functions to generate shipments along with tracking to report on delivery progress while completed shipments are en route.\n\n\nAPI methods allow designation of sender and recipient address, shipment content, expedited delivery options, and related parameters. Methods return shipping rates and process shipments on agreed-upon terms, including label printing, pickup scheduling, and cancellation of scheduled shipments. Methods then allow shipment tracking and delivery status reporting.\n\n\n Associated Press Metadata API: The Associated Press is the worlds oldest and largest newsgathering organization. The AP Metadata service is a set of APIs that gives developers access to the metadata associated with the organizations news content. The metadata includes both the AP news taxonomy and the AP metadata tags. Data can be returned in XML, JSON, HTML or TTL.\n\n\n Babyworld API: Babyworld.net is a site where parents can share pictures and stories about their children with family and friends around the world. The site also supports a community of parents for sharing of advice, experiences and more. The API provides admin functionality for users wishing to integrate the site with their own application. Functionality includes data storage, available domain lookup, messaging, notification and more. The API in available in REST, SOAP and JavaScript forms and returns are formatted in XML and JSON.\n\n\n BigTable API: BigTable is a sparse, distributed, scalable database created by Google and used by many of their popular services. Andrew Hitchcock exposes the BigTable interface for public use via a pseudo-RESTful API. Anyone can register for an account and gain free access to the service. This is an unofficial API and is not endorsed, supported, or maintained by Google.\n\n\n Biosemantics JANE API: The service queries a repository of scholarly journals and author names across biological sciences to identify those related to a particular topic or text sample. In response to a title, abstract, name, or subject keyword, the service looks for matches across records in the Medline database. It is intended to suggest likely publishing venues for articles on a particular topic as well as authors to follow and specific articles to review.\n\n\nAPI methods support submission of a query terms, including names, keywords, and text selections. Requests can limit returned records to journal names, author names, or published papers, if desired.\n\n\n Bugzilla API: Bugzilla is the bug tracking and reporting system created and used by Mozilla. It is also available for use by other projects and organizations. The life cycle of a bug, also known as workflow, is customizable to match the users needs. The Bugzilla REST API provides users with programmatic access Bugzillas service using either JSON or JSONP.\n\n\n Clover API: Clover is a one-tap payment app for mobile purchases. It offers two layers of security by locking the account to a phone number\/mobile device and requiring PIN authentication. The user enters payment and shipping information just once, then can make mobile purchases with a click on the Clover icon. \n\n\nThe Clover REST API is available for merchants to list, accept, reject and refund orders using RESTful calls.\n\n\n CO2Stats API: CO2Stats is a service that aims to make websites carbon neutral and energy-efficient. CO2Stats offers a suite of software that monitors a sites energy usage, offers tips on how to make a site more energy-efficient and purchases the appropriate amount of audited renewable energy from wind and solar farms. It also provides certification when a user takes steps to make their site more environmentally friendly. The API allows users to create and delete CO2Stats accounts for their customers.\n\n\n Commzgate Gateway API: The service provides outbound and inbound SMS text messaging capabilities focused on the communication needs of business organizations. Applications can generate messages to specified recipients or poll for and retrieve messages from others via the service.\n\n\nAPI methods support mobile originate (MO) and mobile terminate (MT) messaging for both inbound communication from phones and mobile devices and outbound communication to devices. The API also supports a proprietary push-messaging and announcement service (MACH) that establishes a continuous communication channel via a mobile app installed by users.\n\n\n CTA Train Tracker API: The CTA Train Tracker API allows for querying near-real-time arrival times of Chicago Transit bus trains. The API is in beta and receives arrival time predictions via the QuikTrak rail system monitoring software (trains not currently GPS enabled). Each query responds with predictions for all platforms at a given train station in an XML document.\n\n\n Devot:ee Upload API: Devot:ee is a collaborative community website for fostering growth and knowledge-sharing about ExpressionEngine. ExpressionEngine is a budding content management system. On Devot:ee, developers can plans or information about new plug-ins, extensions, modules, and more that they are developing. The Upload API allows developers upload files and release data about their add-ons. It is a RESTful API that returns JSON formatted data. \n\n\n Donor Tools API: Donor Tools is a non-profit software for managing and tracking donors using a variety of tracking, reporting and communication features. The API offers the user complete access to same data and features that the web interface provides. This allows developers to create their own custom applications or simply access their Donor Tools data from anywhere. \n\n\n eBay Client Alerts API: The eBay Client Alerts API allows developers to get near real-time updated data on eBay. Data includes seller and buyer events such as price changes, items ending, end of auction, item won and more. Developers can use this data to build updated and lightweight alerting and notification applications. The API uses RESTful calls and responses are formatted in JSON.\n\n\n eBay Large Merchant Services API: The eBay Large Merchant Services (LMS) provides a way to process the large numbers of transactions within the eBay Trading Platform. LMS includes features such as SKU-based inventory management, Orders with OrderLineItems for all transactions, and OrderAck state for handoff between eBay and Merchant applications. The API accesses this functionality to aid the integration needs of Large Merchants and ISVs. The API can be used to enable SKU management, inventory management, and order fulfillment. The API uses SOAP calls and responses are formatted in XML.\n\n\n eBay Platform Notifications API: The eBay Notification API makes it possible for applications to receive notifications for events that occur on eBay. Notifications are triggered by events such as when a listing ends or the creation of a transaction. The API lets users manage their notification preferences for applications and other users. It uses SOAP protocol and responses are formatted in XML.\n\n\n eBay Product Services API: Parts Compatibility is an eBay feature that uses structured data to associate compatible assemblies with parts listed on eBay. It is meant to improve search relevancy allow item titles and descriptions be more detailed and infromative. The eBay Product Services APIs allow users to get information about eBay products and list their items with Parts Compatibility. It uses REST and SOAP calls and responses are formatted in XML, JSON and Name Value Pairs.\n\n\n Employrium API: Employrium is a resource for both employers and job seekers in most major metropolitan areas. For job seekers, it offers realtime job search via web, mobile device and Twitter stream. Employers can pay for an account to make job postings. The API is made specifically for employers so they can easily track activity on their active postings (number of applicants, expiration date, etc.).\n\n\n ESMA MiFID API: The ESMA (European Securities and Markets Authority) MiFID (Markets in Financial Instruments Directive) API provides access to a database housing a variety of financial information. Topics covered include shares admitted to trading on EU regulated markets, systematic internalizers, multilateral trading facilities, regulated markets, and central counterparties.\n\n\n Field Force Manager API: The service provides tracking and oversight tools for mobile workers. It helps with measurement and management of remote employees to ensure effective mobile service. It integrates with third-party systems to exchange information about staff locations and activities, with reporting intervals from 5 minutes to once per day or week.\n\n\nAPI methods support worker account management, including location, status, and activities underway. Location can be tracked by worker report or automated GPS reporting. Methods support highly details route and timing reports, with recording of navigation milestones passed and stop times. Methods also support time management and reporting, with job assignments, completion time estimates, and timecard functions for payroll.\n\n\n Gtdagenda API: Gtdagenda is a mobile app that helps with personal task management. Users can track their tasks, goals, projects and schedules. It can sync with Evernote and Google Calendar. The API gives users access to the functionality of the app. Included is the ability to view, create, edit and delete any of the listed services. The API uses RESTful calls and responses are formatted in XML.\n\n\n http.net DNS API: http.net is an ICANN-accredited domain name registrar, providing domain services, housing, DNS, SSL certificates and more for resellers since 1999.\n\n\nThe http.net DNS API supplies resellers with an interface for automated realtime DNS object management. The service makes different SOAP versions and a RESTful API on multiple endpoints available.\n\n\n Illocution Upzilla API: Illocution Inc. is a big data consortium devoted to large-scale text and document analysis. They provide several applications for keyword tracking, trend tracking, lexical analysis, and more. They have also built some corpora and lexicons. The Upzilla API exposes the functionality of their raw trend data feed. Users can define parameters for the span and amount of data they want returned. It is a RESTful API that returns JSON results.\n\n\n Inapub API: The Inapub API provides a variety of information on what local  pubs have to offer. Examples include which pubs serve food, televise football matches, serve a specific kind of beer, or host live music and events. GET and POST data can be pulled from several categories including Venues, Events, Beer, User Info and a few others.\n\n\n IndExs API: IndExs is an index that contains information on the titles, abbreviations, and bibliographies of mycological exsiccatae. Exsiccatae are published, uniform, numbered sets of preserved specimens distributed with printed labels. IndExs can be accessed through a web console, or by using a SOAP or REST API.\n\n\n Instamanager API: Instamanager is a cloud-based vacation rental software provided by Bookt. The platform includes a public rental website and back-end reservation management software, among other services. The API allows for platform customization, creation of widgets and apps, and full interaction with user data on the cloud-based platform.\n\n\n Integrify API: Integrify is a provider of Cloud based lean business process management software. The software offers process definition, workflow automation and visibility for areas such as Information Technology, Human Resources, Finance, Sales, Marketing and other types of services. The API allows developers to integrate Integrify into their own existing systems allowing for customized processes, forms and workflows. The API is available as both REST and JavaScript.\n\n\n Interactive Data API: Interactive Data is a provider of financial market data, analytics and other solutions to financial institutions and individual investors. They supply real-time market data, time-sensitive pricing, evaluations and reference data. The API allows developers to access this data to create monitoring, analysis or trading applications. Full documentation is not publicly available.\n\n\n Jango SMTP Event API: Jango SMTP is a transactional email delivery service that adds tracking and authentication to email. Using the service allows for open tracking, click tracking, DomainKeys\/DKIM signing, grouping, logging, and extreme email delivery. The Event API can make an HTTP call to any web service or web page upon a specified event without requiring polling. Events include sending and opening of email, a click of a URL in an email, an unsubscribe or bounce and compalints.\n\n\n Kawet API: Kawet is a mobile app creation platform. Its main prodcut is Cashew, a CMS dedicated to the design and management of mobile apps. The platform is based on a CMS and aims to reduce the time and const to create, manage and devlier business apps across multiple devices. An API is available that exposes much of the functionality of the CMS allowing developer to create native apps. Documentation is not publicly available.\n\n\n Koemei API: Koemei provides video and audio asset transcription services. Koemei transcribes video and audio files, indexes the transcripts, making the video and audio files searchable on the web.\n\n\nThe Koemei API allows developers to access and integrate the functionality of Koemei with other applications. Some example API methods include retrieving media files, retrieving transcript files, and uploading media.\n\n\n Lauerfac.es API: Lauerfac.es is an avatar-hosting platform for images of Christopher Lauer, a prominent member of Germany\u2019s Pirate Party. Users can upload expressive images of Christopher Lauer\u2019s face for browsers to copy and paste around the web. The API allows developers to access images and their metadata by making requests by ID, category, or tags. It is a RESTful API that provides JSON, JSONP, and XML responses. \n\n\n LDSTech Mormon Channel API: LDSTech is a community site where users can contribute to the technological work of the LDS (Mormon) church. The Mormon Channel is the official radio station of The Church of Jesus Christ of Latter-day Saints and broadcasts 24 hours a day, seven days a week. The API allows developers to build apps that query content currently available as audio streams on LDS.org. It uses RESTful calls and responses are formatted in XML and JSON.\n\n\n MapIt API: The service maps U.K. postcodes and specific location parameters, defining map polygons and current or past administrative areas for the locations. It accepts either complete or partial postcode values, SRID values for specific points, or area identifiers. Mapping parameters returned are based on data from the Ordinance Survey, the Royal Mail, and the Office for National Statistics.\n\n\nAPI methods support submission of one or more postcodes, complete or partial, to be mapped. Methods also allow submission of one or more identifiers (SRID values or values from the British National Grid or Irish National Grid) for specific points or areas. Returned data provide map points or polygons, depending on the values input.\n\n\n MediaFire API: MediaFire is a cloud-based service for storing and sharing files. Users can host and share any file type including documents, presentations, videos, and images. All accounts can upload and share an unlimited number of files, with the maximum file size being 200MB for free accounts. Users can access and manage their MediaFire cloud using a RESTful API.\n\n\n Meteostone Weather API: Meteostone is a crowd-sourced network of local weather data and weather media information from around the world. All content is provided by users. The Meteostone API allows users to create their own weather applications or services using Meteostone weather data. \n\n\n Mimer SQL Validator API: The Mimer SQL Validator API is a freely accessible public web service that validates SQL statements against the ISO SQL99 Standard. SQL statements are stored anonymously to be used by the ISO SQL Standards Committee. This API can be accessed via SOAP calls using the XML data format.\n\n\n MongoHQ API: MongoHQ is a database hosting platform for MongoDB, a scalable NoSQL database. The MongoHQ API allows users to create MongoDB databases on the MongoHQ platform and to interact with them programmatically. The API, currently in its Beta phase, is RESTful and employs the JSON data format.\n\n\n MutualMind API: MutualMind is an enterprise level social media management service. It is a web app that can monitor and promote brands across social networks. The API gives users access to the data behind the service including campaigns, content and statistics. An API key is required and it uses RESTful calls. Responses are formatted in XML and JSON.\n\n\n Nike Materials Sustainability API: Nikes Better World initiative looks at how Nike promotes sustainability practices as well as the benefits of sports around the world. As part of this initiative, Nike has a Materials Sustainability Index (MSI). The MSI is a tool used to help product teams select environmentally better footwear and apparel materials from better suppliers. The indexs scoring system evaluates the environmental impacts of materials used in products using a combination of materials-specific data. It tracks impacts such as Chemistry, Energy and Greenhouse Gas Intensity, Water and Land Use Intensity, and Waste. The API provides access to data such as listing of materials, matching materials, material search via keyword and more. It uses RESTful calls and responses are formatted in JSON or HTML.\n\n\n Offerpop Contest Integration API: Offerpop is a social media marketing service. Their software allows brands to run white-label marketing programs on Facebook and Twitter. The Offerpop platform can be used to create, manage and track campaigns and deliver promotions like interactive contests, games, polls and exclusive offers. The Contest Integration API can automatically upload content users are creating with a developers app or website into an Offerpop Photo Contest campaign.\n\n\n Open Dems Polling Place API: The Polling Place API allows developers to create applications that can look up the nearest polling places based on an individuals street address and zip code. It is offered by Democrats.org through their Open.Dems project: Opening up the Party to innovation while sharing our technology and ideas.\n\n\n Open Dems Voter Registration API: The Voter Registration API allows developers to create applications for easy and streamlined voter registration without worrying about varying state laws and the need to follow up with voters. Users of the API must also sign up as a Raise Your Vote partner. It is offered by Democrats.org through their Open.Dems project: Opening up the Party to innovation while sharing our technology and ideas.\n\n\n Plivo Framework API: Plivo Framework is a communications framework for developers to build telephony apps from. It has libraries from Java, Ruby, Python, and PHP so that developers do not need to change their web developments for integration. The API is REST-based and returns XML responses. It exposes all of the framework\u2019s functionality toward developing voice apps including, making or receiving calls, recording calls, setting up group calls, and more. It also supports broad integration into social networks, databases, and external APIs.\n\n\n Ponyfac.es API: Ponyfac.es is a gallery of My Little Pony avatars. Users submit images or animated GIFs from the My Little Pony cartoon show that others can copy and paste as expressive avatars. The API exposes the image access functionality of the collection. Developers can make requests by tags, ID, or categories (defined as the Ponies\u2019 names). It is a RESTful API that returns JSON, JSONP, and XML results. \n\n\n Pushingbox API: PushingBox is cloud notification service. It connects an array of users\u2019 digital notification services, such as email, Twitter, Toasty, and more to receive alerts for predetermined actions. These actions are called scenarios, and can linked to anything from the doorbell ringing to receiving emails. Their API exposes the integrative functionality of the PushingBox service. \n\n\n QR4 QR Code API: QR4 specializes in QR Code generation in all contexts of QR code use. The API is a simple QR code generating tool that uses simple URL calls to return ready-to-use QR codes. It currently supports four types of code: text, direct call, SMS messages, and direct Wifi access.\n\n\n Ragefac.es API: Ragefac.es is a gallery of facial meme avatars. Users can submit images of popular expressive memes to the homepage repository. Browsers can select the image they want to copy and paste it around the web. The API provides access to the image retrieval functionality of the website. Requests can be made by ID, tags, categories, or for all of the images. It is a RESTful API that returns JSON formatted data and UTF-8. \n\n\n Random House Insight API: The Random House Insight API is a set of programming tools that allow internet applications to view and search digitized book content. It provides keyword searches that find matches against the full text of the book. The API can then serve up book pages in different formats depending on the needs of the developer.\n\n\n RateBeer API: Ratebeer.com is member driven beer rating database that offers detailed profiles on beer and breweries around the world. The JSON API allows users to retrieve and post information to the site, along with some other perks. Users can search for bars and beer stores by location, get a sample list of beers offered at a location, retrieve beer scores and info on any beer in the ratebeer database, and search by UPC code, brewer, season, style, etc. The API also allows users to check in to a place and tick a beer as consumed for future reference.\n\n\n Rdio Web Playback API: Rdio is a subscription music service that helps users discover new music, follow friends and people with great taste in music, and listen to their playlists. The Web Playback API is based on an Adobe Flash file (SWF) and Javascript, and allows developers to incorporate the playback of licensed Rdio music into their web applications. Full length songs are available to Rdio subscribers and trial users; non-users will be able to hear previews of the songs.\n\n\n ReferralCandy API: ReferralCandy is an ecommerce referral tool that businesses can use to encourage current customers to refer friends back to their business or products. Customers are emailed a unique coupon link to share with friends. The coupon\/discount is determined by the business. Sales can then be tracked based on customer referrals. The API offers e-store integration with the ReferralCandy platform using RESTful GET and POST calls.\n\n\n Ryzom API: The service provides information from the MMORPG game server, including basics like server and account status, but also game time statistics or summaries of character profiles. In addition to character information, the service provides access to information and images about in-game guilds and other details of game play.\n\n\nAPI methods support retrieval of server status and time, version release notes, and related detail. Methods also access to give titles and profiles of game characters as well as banner images, profiles, and membership of player guilds.\n\n\n Saygent API: Saygent is an in-application feedback service. Saygent can be integrated into mobile applications, websites, and in-store POS to gather customer feedback. \n\n\nThe Saygent API allows developers to access and integrate the functionality of Saygent with other applications. Some example API methods include making calls, retrieving feedback, accessing responses and respondents, and managing account information. Provider can be contacted at answers@saygent.com\n\n\n SendGrid Event API: SendGrids cloud-based email helps keep emails out of spam folders, provides a cloud service that scales with email demands, and provides email analytics to track opens and clicks.\n\n\nThe Event API uses hooks to send real-time notifications of email events such as click, open, bounce, delivered, dropped, spam report, etc. to a specified URL. It can be integrated with the SMTP API for notifications on other custom parameters used on an email campaign. The information collected on the log server can then be pulled and manipulated by the user. Data can also be received via HTTP POST for each event or a batch of events in JSON format.\n\n\n SharpDevelop Code Converter API: The Code Converter API allows users to convert a selection of code from one programming language to another. Users may convert from C# to VB.NET, Boo, Python, or Ruby. They may also convert from VB.NET to C#, Boo, Python, or Ruby. This API is available via SOAP calls using the XML data format.\n\n\n ShopWiki API: The service consolidates online catalog listings from an unlimited selection of products and sellers, aiming to provide a single listing of all products available from any online provider. Its search interface returns keyword matches from product names and descriptions with links to obtain more information and complete purchase. The service also provides product buying guide content and discount coupon offerings for selected products.\n\n\nAPI methods support submission of a query string and return product listings with matching names and description text. Methods also register the service provider to any query to allow revenue sharing for completed purchases. Search parameters can limit results to particular sellers, price ranges, brands, and colors.\n\n\n Sketchfab API: SketchFab is media platform that hosts interactive 3D models. Artists can publish their 3D modeling work to share with friends, family, or perspective employers. Others can browse the site or look up specific kinds of models, like planes, trains, or automobiles. The SketchFab API exposes the website\u2019s upload functionality. Users can upload their images and include descriptive metadata. It is RESTful API that returns JSON results. \n\n\n Slatemind  API: SlateMind is a mind-mapping tool that runs off of the Slatebox application platform. It allows teams to collaboratively map out ideas visually on a blank slate. It is marketing toward teachers and businesses as an easy-to-use tool for conveying ideas. The SlateMind API is a simple querying tools for retrieving publically available slates. \n\n\n SourceSMS API: SourceSMS is a service that provides bulk SMS\/text messaging, both sending and receiving. SourceSMS allows users to connect the functionality of SourceSMS with other CRM applications.\n\n\nThe SourceSMS API allows developers to access and integrate the functionality of SourceSMS with other applications. The main API methods are sending SMS\/text messages and receiving SMS\/text messages.\n\n\n SSW Australian Postcode API: The SSW Australian Postcode API is a public web service that provides a quick and centralized method of checking that suburb name and postcode details are correct for addresses located in Australia. This API can be accessed using a web form or via SOAP calls using the XML data format.\n\n\n StructuredRetailProducts API: StructuredRetailProducts is a database that provides information and data on companies, market trends, sales, revenue, and market news and analysis. \n\n\nThe StructuredRetailProducts API allows developers to access and integrate the functionality of StructuredRetailProducts with other applications and to create new applications. Some example API methods include searching and querying the data, retrieving information given certain parameters, and managing account information. \n\n\n TalentBin API: TalentBin is big data social recruiting tool for web development jobs. They crawl sites for personal information, reduce it to distinction identities, and store it to maintain a passive candidate database of implicit web resumes. Their API gives developers access to personal information of cataloged identities, such as their location, skills, and company. It is a RESTful API that can return data in JSON, JSONP, and XML. \n\n\n The Cancer Genome Atlas API: The Cancer Genome Atlas (TCGA) is a project to improve the understanding of the molecular basis of cancer through the application of genome analysis technologies, including large-scale genome sequencing. TCGA offers a collection of APIs that provide REST-based, programmatic access to its information resources. Available information includes annotations, data matrices, data reports, project metadata, and biospecimen metadata.\n\n\n The Internet Chuck Norris Database API: The Internet Chuck Norris Database provides a large collection of Chuck Norris jokes for public entertainment. Using a RESTful API, users can retrieve one or several random jokes, or fetch a specific joke. Users may also retrieve a joke, but replace Chuck Norris with a name of their choosing.\n\n\n UBC ePayment API: ePayment provides University of British Columbia (UBC) departments with secure, real-time authorization and processing services for credit card and Interac Online payments. The ePayment API is suitable for merchants who have their own website or shopping cart and require online payment processing. ePayment will generate JVs for all authorized payments to record the purchases in UBCs Financial Management System (FMS).\n\n\n uBoost API: The service provides customized motivational information to promote engagement and sharing in education, healthcare, and online settings. It encourages members to login and interact online, improving engagement and attachment by providing incentives and rewards for member activities.\n\n\nAPI methods support user account management (creating, updating, and inactivating) and definition of user groups populated by individual accounts. Methods also support management of user award points and badges as part of the motivation function.\n\n\n University of Washington Decision Support API: The Enterprise Data Warehouse is a central repository of the University of Washingtons electronically stored institutional data. This repository is organized in a way that is meaningful for business analysis and reporting. The Decision Support API provides programmatic access to information about the data and reports contained in the Enterprise Data Warehouse.\n\n\n University of Washington Financial API: The University of Washington Financial API provides programmatic access to the University of Washingtons budget and organization information from the FIN mainframe database, vendor information from the PAS mainframe database, and metadata from the MyFD glossary. All resources are private, that is, they are available only with approval.\n\n\n University of Washington HR Payroll API: The University of Washington Human Resources (HR) Payroll Web Service provides programmatic access to Job Class data and, in the future, much more HR-related data. At this time, all resources are private and therefore available only with approval. Full documentation of this API requires the user to log in.\n\n\n University of Washington Person API: The University of Washington Person API provides programmatic access to information on UW personnel, including employees, students, and alumni. This information includes names, university ID numbers, employee IDs, student numbers, e-mail addresses, phone numbers, and other relevant data. All resources are private, and access requires approval.\n\n\n University of Washington R25 Classroom Scheduling API: The University of Washington R25 Classroom Scheduling API provides programmatic access to schedule information for UW classrooms. Users may search for this information by event, location, or reservation. For example, an application could query the API for details on room availability for KANE 110. This API can be accessed via REST calls using the XML data format.\n\n\n University of Washington Student API: The University of Washington Student API provides programmatic access to information in the student database. This database contains information on UW students including their course data, registration data, section data, personal data, and general academic data. This service includes both public and private functions.\n\n\n University of Washington Study Abroad API: The University of Washingtons Study Abroad program offers hundreds of opportunities around the globe for UW credit throughout the year. The Study Abroad API provides a public interface to program information that can be used to display the most current offerings for a particular region, country, city, or term on the users web site.\n\n\n Vitrue Agency API: Vitrue is a provider of a cloud-based social marketing solution. Their solution, the Social Relationship Management (SRM) Platform gives agencies and brands a way to manage, measure and maximize value on social networks and across the web. The Agency API allows agencies to create, deliver and manage custom brand experiences on their Facebook wall. With the API, an agency can create a custom Wall app and deliver it to the news feeds of their clients fans. Documentation for the API is not publicly available.\n\n\n Vocaroo API: Vocaroo is an online voice-recording web tool. Users can record messages through their computers onto an app on the Vocaroo homepage. Vocaroo will save the user\u2019s message for a limited time and provide a link where it can be played back. The site is still in beta. The API exposes Vocaroo\u2019s uploading and link sharing functionality. It is a RESTful API that returns JSON formatted data. \n\n\n Watch API: Watch! is a service that lets fans search for a place to watch their favorite sports team and then book a table at that locale. Restaurants and bars register with the service to participate. Listed games are primarily from La Liga (Spain), the Euroleague, and the NBA. Listed bars are primarily in Europe and major US cities. The API offers users the same functionality as the web interface with outputs in JSON format.\n\n\n WebReserv API: WebReserv.com is an online booking system. Users can search for local businesses and products, view pricing and availability details and make reservations. Businesses can get their products and prices set up in addition to managing the bookings. The WebReserv API gives developers access to the WebReserv booking platform and enables them to build applications that interact with the service. Some of the calls include GetBusinessInformation, GetProductList, SetAvailability and more. The API uses HTTP calls and responses are formatted in XML.\n\n\n Wild West Domains Reseller API: Wild West Domains (WWD) is Godaddys reseller company. They do not sell domain names directly but instead enables domain resellers to sell rebranded Godaddy services. These services including Godaddy tech support can be offered at lower prices than if purchased directly through Godaddy.\n\n\nThe API gives resellers a way to integrate WWD services into their Web site or applications and allowing them to use their own style and branding. Services offered through the API include domain name registration, domain name transfers, web based email, for sale parked pages and more. The API uses SOAP calls and responses are in XML.\n\n\n WorldMate API: WorldMate is a mobile travel itinerary management platform. The platform collates structured itineraries by recognizing and then parsing confirmation emails from travel providers worldwide. The itineraries are then accessible via the WorldMate web application and a mobile application.\n\n\nThe WorldMate API allows developers to access and integrate the functionality of WorldMate with other applications and to create new applications. Some example API methods include parsing emails, retrieving traveller information, and retrieving travel provider information.","item_date":"May 27 2012 19:44:03","display_item_date":"05-27-2012","url":"http:\/\/blog.programmableweb.com\/2012\/05\/27\/81-new-apis-associated-press-nike-ebay-rdio-and-sendgrid\/","source":"blog.programmableweb.com"},{"title":"A VC: Twilio's Nine Things","details":"It helps a lot to have a one pager that outlines the core values of the company. I just saw our portfolio company Twilios version of that. They call it Our 9 Things. I wish I could publish it here but I dont have permission from Jeff and so I will resist the urge. It has things like think at scale and be frugal on it. You get the idea I hope. This guiding light is a framework for the culture and values of the organization and each new hire should be assessed against the framework to make sure the fit is good.\n    Well it turns out that Twilio published their 9 things on their website this week and so I can now publish them here.\n  \n  I like that they published them in the form of a telephone dialpad. For those that dont know Twilio makes telephony work easily in web and mobile apps. Putting the 9 things in this format makes a statement in itself about their culture.\n  These need not and should not be your companys values, although it is likely that you may share a number of these values with Twilio. The point is to articulate what your culture is about and put it front and center so that everyone knows what they are.\n  Nicely done Twilio.","item_date":"May 27 2012 19:37:33","display_item_date":"05-27-2012","url":"http:\/\/www.avc.com\/a_vc\/2012\/05\/twilios-nine-things.html","source":"www.avc.com"},{"title":"David Linsin - Developers and Documentation","details":"The relationship between developers and documentation can be described like the bond between cats and dogs. They are on totally different levels of the food chain, so sometimes the relationship works out, but most of the time it doesn\u2019t. If the cat and dog get along well you\u2019ll have a happy home, but if they don\u2019t you\u2019ll live in a state of toleration - nothing more and nothing less.\n  It is said that the best way of forming a healthy bond between a cat and a dog is to bring them up together. They\u2019ll get to know each other from the beginning and learn to respect each other. I think that works well for the relationship of developers and documentation as well.\n  When talking about documentation it\u2019s important to differentiate. There are lots of different kinds of documentation. Here\u2019s an excerpt:","item_date":"May 27 2012 19:21:07","display_item_date":"05-27-2012","url":"http:\/\/dlinsin.github.com\/2012\/05\/27\/Developers-and-Documentation.html","source":"dlinsin.github.com"},{"title":"The Top 5 Ways to Access World Bank Data","details":"Our most popular open data destination - the main World Bank Data site gives you an overview of the data we have on a country, region or topic. I like it because you can quickly browse and filter through many years of indicator data, make some basic charts and even embed them into your own web page.\u00a0\n  In the past, most of the data on this site came from the World Development Indicators (WDI) but now, if you look at the country pages, youll see weve started to integrate open data from other World Bank databases.\n \nFor example, on this page for Bangladesh you can see development indicators from the WDI, a map with details for our projects and operations in the country, a summary of our lending activities, details of the surveys we have covering Bangladesh and data on climate change.\n \nWatch out for a blog post soon from Tim explaining the technical details of how this all works - well be applying the same approach to other parts of the site.\n \nDataBank is the most awesome World Bank data tool youve probably never heard of. Its an analysis and visualisation platform that contains collections of time series data from a variety of sources, including the WDI.\n \nYou can create your own advanced queries, generate tables, charts and maps and easily save, embed, share and download them. Theres a video tutorial and some FAQs here, but the best thing to do is just have a play - theres a chart I made below and you can see other charts and reports users have shared.\n \nIm a huge nerd and still surprised how often Im away from a computer (read: triva night) with a sudden thirst for data. Mobile apps are great for this and we produce Data Finder for the iPhone, iPad, Android phones & tablets and Blackberry devices too.\n \nAll the content is available in English, French, Spanish and Chinese so you can tell the 3.5 billion people that collectively understand those langauges to download them now!\n \nThe best bit for me is that these apps mostly work offline, so if you dont have a data plan, are on a low-bandwidth internet connection, or are truly off grid you can still easily find out that GDP growth for Bangladesh in 2010 was a hearty 6.1%.\n \nYou know what an API is, right? It stands for Application Programming Interface, and it gives anyone online, direct access to live data, straight off our databases. There\u2019s no need to manually download things: every time we update something, you\u2019ll always have the latest version if you access it via the API.\n \nIts easist if you see for yourself - stick this link in (a new tab in..) your web browsers address bar:\u00a0\n \nThat not-so-scary looking XML is a structured representation of the last 10 years of GDP growth figures for Bangladesh - the key bit of the URL is the indicator code for GDP Growth, in this case \u201cNY.GDP.MKTP.KD.ZG\u201d . Can you see the unrounded figure of 6.1% for 2010 at the top?\n \nThis is what mobile apps like Data Finder suck in and display to you in a prettier format. In fact, our entire website is built off these same public APIs - all were doing is transforming this machine readable data into a World Bank branded data site. Doesn\u2019t it sound easy when you say it like that?\n \nIf youre interested, weve got documentation on the APIs that we provide for various types of of data. The development data API comes with a query builder to let you easily generate data like the above - mostly on time series indicators. There are also APIs to access our projects and operations data, our financial information and, most recently, data on climate change.\n \nOh, and if youre a STATA user, theres already a module that connects to the development data API so you can access live World Bank data directly from STATA.\n \n OK, this should really be at the top - the majority of people who see World Bank data these days get it via Google Search. If you havent seen it (where have you been?), go and type in GDP Growth Bangladesh into Google search and see what happens. Go ahead - Ill wait for you.\n \nPretty cool, huh? Google takes all our open data from the WDI and translates it into 34 languages (try searching for poblaci\u00f3n de espa\u00f1a in http:\/\/www.google.es) they then display it in your own language when they spot you typing in certain keywords.\n \nIt gets really exciting when you click on the little graph and arrive at the Public Data Explorer - another great free charting and visualisation tool which you can use to create charts and embed them like this:\n \nRemember, open data at the World Bank is free to use, re-use and redistribute for commercial and non-commercial purposes (check our terms of use) - so\u00a0 youre just as likely to find our data through other platforms, interfaces and tools and I\u2019d love to hear about them.\n \n Wait! I almost forgot about the Data Catalog. Its a central index of almost a hundred different databases from across the World Bank and its growing every month. \nSome are big, some are small, not all of them are accessible via the API, but almost all of them fall under our open data terms of use which as above means you can basically do what you like with the data as long as you attribute the source.\n \n No, this isnt just a smaller version of the data catalog - microdata is a term the statisticians Im surrounded by use to describe raw data from surveys, censuses, and administrative systems. This is really powerful stuff if youre a researcher and the World Bank Microdata Catalog is one of the best systems of its kind in the world - do check it out.\n Want to know where the World Bank\u2019s money goes? Check out finances.worlbank.org to get the latest data on our financial activities - you can easily create your own charts and visualisations and download data for your own use too.\n The best bit for me is the mobile app - I have it on my iPad and iPhone and I can easily see which projects we have in a country when I\u2019m travelling.\n With more organisations publishing their financial and operational data in the IATI format, I\u2019m looking forward to seeing more tools that show me where all money from all development organisations is being spent.","item_date":"May 25 2012 17:01:23","display_item_date":"05-25-2012","url":"http:\/\/blogs.worldbank.org\/opendata\/node\/526","source":"blogs.worldbank.org"},{"title":"Moldova Re-Invents the Hackathon","details":"The IC4D blog explores the impact of information and communication technologies (ICT) on development, both directly and as an enabler for other sectors of the economy. It is a space to share ideas and experiences, and to inspire dialogue on new and innovative ways in which ICT can create opportunities in developing countries.\n   \n This last weekend saw the first hackathon ever to occur in the country of Moldova, and they definitely made it their own. For a country that is just starting to build its civic hacker community, the results were nothing short of remarkable: 85 participants set to work almost immediately within impressive, self-organized team structures to produce 18 functional apps.\n In traditional terms, this hackathon could be described as more of a code sprint for the final three days of a prize challenge, but even that description understates the innovation and complexity of the event format. In addition, it doesn\u2019t do justice to all of the accomplishments that were achieved by the organizers as well as the participants.\n  Moldova is a small country in Eastern Europe, between Romania and Ukraine. It is one of the poorest countries in Europe, and is striving to begin the process of joining the EU. It is also the recipient of a $20 million line of credit from the World Bank to fund its e-government efforts, as led by the e-Government Center (an official part of the Moldovan government). Moldova has an active developer community, but they are only beginning to connect with civil society and government. The country also just joined the Open Government Partnership, and has approved a National Action Plan that commits the government to self-improvement in multiple areas, one of which is the release of government data on the Moldovan Data Portal.\n This hackathon, called the Open Innovation Challenge \u201cApps for Moldova,\u201d was the culmination of Innovation Week, which was designed to engage the private sector and civil society in the launch of this National Action Plan. Putting on this event was truly a multi-lateral effort, including the e-Government Center and the World Bank along with local non-profits, developer organizations, and private sector companies.\n  The desires to (a) develop a civic hacker community and (b) create public usage of government data in order to ensure the sustainability of Moldova\u2019s open government efforts led to the unique format of this three day hackathon.\n Prizes were awarded to teams in part based on the perceived sustainability of the app and the implementation plan. The prizes were provided by the e-Government Center, MoldCell, Endava, and Fusionworks (local companies) and while some were traditional give-aways (iPads, 3G modems, relevant books\u2026), most were truly innovative and designed to encourage the long-term sustainability of the projects and the developers. These included:\n \u2022 $5000 service delivery contracts with the government of Moldova for completion of the app.\n \u2022 Employment and internship opportunities with some of the biggest companies in the country.\n \u2022 Workspace including all of the required office amenities.\n To encourage teams to form early and come prepared, there was a submission deadline to compete for these prizes that was 8 days prior to the beginning of the event.\n Day one of the actual event began with a half day TechCamp that brought international experts from Africa, Europe, and North and South America to educate the participants on the value and skills necessary to create great apps that leverage government data (for a complete list of who attended, see here).\n The morning began with a series of very short speeches from the organizers and prize presenters and quickly moved into presentations that would tangibly benefit all participants. This included overviews of what is happening in other parts of the world, the potential benefit to Moldova of the work they are about to embark upon, and practical advise on how to win the hackathon.\n At this point, all participants were split into 10 groups, each of which speed-geeked their way through 10 different presenters who provided them with additional informational overviews on topics that may benefit them during the weekend. This included data vizualization, mapping, crowd sourcing, data scraping, and how to access government data. In addition, this introduced all of the participants to many of the international experts that were available to assist them over the weekend.\n The TechCamp closed with an opportunity for the participants to return to the presenter that they most wanted to learn from and spend a half hour diving deeper into the topic. Everyone went to a networking lunch at which the experts mingled amongst the participants to further discuss relevant concepts and begin to form working relationships.\n While teams had to submit proposals early in order to be eligible for the prizes, developers were invited to attend and join in the activities even if they didn\u2019t yet have a team. In addition, people were encouraged to come up with new ideas during the course of the TechCamp, and information was provided to them about unrelated prize competitions that they may be able to enter.\n In order to let everyone know the diversity of work that was occurring and to connect teams with needs to available developers, the code sprint began with a series of 1 minute presentations by each team of all 26 submitted applications. Following on this, new ideas were presented. While some teams did not ask for additional help, most did and these were given a number and distributed around the room so that all of the free developers could talk to them and determine which team they would like to work with.\n Once the teams were established, everyone was given free run of the space and spread out to get to work. The experts began circulating between individual teams to provide advice and assistance and determine what needs they may have. The organizers of the event turned their attention to fulfilling these needs. In two projects, required government data that was not yet available was released in the open data portal based upon requests from the developers during the course of the weekend.\n As I said before, this was truly a multi-lateral effort which gave the developers the sense that they sit at a critical point for the future of Moldova and that their effective engagement can make a significant difference in the future of their country.\n 2pm on Sunday was the deadline for all work to be completed. Teams had to upload two screenshots and a paragraph explanation to http:\/\/appsformoldova.tumblr.com\/ and also give a five minute presentation to all participants. During these presentations, each team was awarded with Certificates of Participation and flash drives. The judges also began their scoring.\n Following the presentations, each team set up a laptop for the apps showcase, thus enabling all participants (and judges) to see firsthand how the apps work ask additional questions.\n The event finished with a formal awards ceremony and a celebratory networking reception.\n As with any hackathon, the most exciting outcomes from this event are the apps that were created during it. A full list of the developed apps is available at http:\/\/appsformoldova.tumblr.com\/ but the winners of the government-sponsored prizes were Social Tools, OpenMed, MoldBizPedia, and E-Ticket. Click here for more information from the original blog post \u00bb\n Beyond the apps however, there are several additional levels at which the results of this event may be judged \u2014 and from my perspective, it would appear that it was a success on all of them.\n Globally speaking, significant advice was drawn on from all over the world to educate and inform this young civic hacker community. The participants seemed to gain a great deal from these interactions \u2014 as did the experts. In addition, while the follow-up analysis is just beginning, one goal for this event was to provide a case study for other countries to learn from. So much worked correctly in Moldova, but there are also lessons that can be drawn from this experience to improve on it for next time. Look for more posts on this topic over the next month as this analysis continues.\n For Moldova, significant progress was made in terms of both encouraging use of government data from the Moldovan Data Portal and laying the groundwork for a civic hacker community. Admittedly, there is still much to be done to fully achieve both of these goals, but a solid foundation has been laid. Furthermore, several of the teams are on track to become self-sustaining as small businesses, thus supporting economic development in the country.\n An unexpected benefit from this event was an upsurge in the release of data to the portal by Ministries of the government. While I don\u2019t have enough evidence to prove that this was the cause of the sudden release of numerous additional datasets, release of datasets had stagnated until the event drew near.\n Finally, given all of the above success and the fact that everyone involved \u2013 the experts, the organizers, and the participants all had a great time, it is very likely that this is only the first of many \u201chackathons\u201d in Moldova. This fact will most likely go the farthest towards creating sustainable use of government data and a vibrant civic hacker community.\n You can reach the author at wayne@open4m.org.","item_date":"May 25 2012 16:59:24","display_item_date":"05-25-2012","url":"http:\/\/blogs.worldbank.org\/ic4d\/node\/555","source":"blogs.worldbank.org"},{"title":"What Exactly Happens At A Facebook Hackathon?","details":"David Cohen on May 25, 2012 11:48 AM\t \tHave you ever wondered what goes on at a Facebook hackathon? Pedram Keyani offered the lowdown in a note on the Facebook Engineering page.\n Keyani said the first official hackathon took place in 2007, adding that they have expanded from 20 people at the first one he attended to more than 500 at the most recent one last Thursday.\n He added that 60 percent of the projects from the hackathons last December and this past February and March were shipped, either internally or to Facebook users, including:\n Full-screen photos\n Photo filters for Facebook for Every Phone\n A 42-foot QR code on the roof of Facebook headquarters in Menlo Park, Calif.\n An internal timeline application that rewards engineers with a Pokemon each time they fix a bug\n \n At around 7 p.m. on the day of the hackathon, everyone gathers around the crane from our old 1601 California office \u2014 the kickoff place for more than one0half of our hackathons \u2014 and Roddy Lindsay, Blaise DiPersia, Bubba Murarka, and I get up to give the official charge. There are only two rules: You have to work on something outside your day job, and if it\u2019s your first hackathon, you have to hack.\n From there, everyone gets together with their group and starts hacking. Around 10 p.m., the group gets back together to eat the traditional hackathon Chinese food feast from Jing Jings, a small place we\u2019ve been getting food from since we were in our first offices in downtown Palo Alto. Then everyone keeps working until around 6 a.m. or when they pass out \u2014 whichever comes first.\n After each hackathon, we keep the momentum going by holding a prototype forum where everyone who built a project can present it to the company. Prototype forum usually happens a week after the hackathon, so it gives people a chance to fine-tune their projects and prepare them for live demos. Everyone gets the same amount of time at the forum \u2014 two minutes \u2014 to convince their peers that their idea should ship.\n At my first hackathon in 2007, about 20 people showed up. At our most recent hack, last Thursday night, more than 500 employees came together to hack on apps for nonprofits, new internal tools, and major upcoming features. The camaraderie, productivity, and occasional insanity of hackathons have helped make Facebook what it is, and I have no doubt that they\u2019ll stay core to how we build for years to come.","item_date":"May 25 2012 16:57:53","display_item_date":"05-25-2012","url":"http:\/\/allfacebook.com\/hackathon-primer_b90003","source":"allfacebook.com"},{"title":"Data Visualizations Skills Needed at PDF:Applied Hackathon","details":"The Personal Democracy Forum (PDF) will be held in NYC on June 11 \u2013 12. This conference examines technology\u2019s impact on government, politics, and society. The theme of the 2012 conference is \u201cInternet is the New Political Power.\u201d This year they are adding PDF:Applied, a two-day hackathon to develop real-world software applications that support the theme of the conference. The hackathon, PDF:Applied, will precede the conference on June 9 -10.\n  PDF:Applied, brought to you by Personal Democracy Media with support from sponsors Microsoft, AT&T, ThoughtWorks, ChallengePost, GitHub and General Assembly, will bring leading civic \u201chackers\u201d together with practitioners in government and NGOs to build tools that enhance civic life through technology-driven innovation.\n  Why am I mentioning this on an effective graphs blog? They need data visualization skills and are hoping that some of the data viz community join the hackathon. There are many benefits to participating. Besides the satisfaction of helping, winners get to present at the main conference and get exposure to prominent judges and leaders in the community. It\u2019s a great project with interesting work and the chance to build something and make connections with civic leaders and other hackers.\n  More information is available at http:\/\/personaldemocracy.com\/pdf-applied and registration is at http:\/\/pdfapplied-es1.eventbrite.com\/","item_date":"May 25 2012 16:57:34","display_item_date":"05-25-2012","url":"http:\/\/www.forbes.com\/sites\/naomirobbins\/2012\/05\/25\/data-visualizations-skills-needed-at-pdfapplied-hackathon\/","source":"www.forbes.com"},{"title":"The Wonderful World of API's","details":"Did you know that API stands for Application Programming Interface?\u00a0 You hear that acronym thrown around all the time these days, but what does it stand for? Why should you care? And most important, how do they impact your business? \u00a0These questions and more were discussed during the Social Media Club of San Francisco\u2019s panel discussion at @Mashery HQ in San Francisco, Tuesday the 15th of May.\u00a0\n Moderating was Christopher Saad, the Chief Strategy Officer and Co Founder of Echo. @Chrissaads Echo works with sites such as Lady Gaga, Discovery Channel and Universal Music Groups. Raj Kadam, Co-founder and CEO of @ViralHeat and works extensively with sentiment analysis, analytics and engagement for social media and real time web. Roland Smart, VP of Marketing at Involver which powers more pages on Facebook than any other platform. Muray Mckerlie, Co-Founder and VP of Sales & Marketing of Whit.li, an API that creates \u201cRelevancy Ratings.\u201d And to round out the panel, Tyler Singletary, the Developer Evangelist for @Klout and manages the API platform. He also has a brother at Twitter!\n API\u2019s are open data systems that allow anyone to create and customize rules to build upon the original data and create what might be needed for your own business or organization. If your not sure what an API is, s simple, fun explanation of APIs is offered by BBYOPEN on YouTube here: http:\/\/youtu.be\/7r7QpIDEI_o\n It was an open discussion also in the way that the seating was set up for the event. Rather than have the 5 panelists that face the audience as if lecturing, the chairs were a bit \u201chelter skelter \u201coff to the side. It set the stage for an engaging and open conversation about how API\u2019s influence us today especially in the world of Social Media.\n Kadam of ViralHeat points out, \u201cAPI\u2019s are universally seen as a way to provide brand awareness that then allows Viral Heat to understand more about their customers and then figure out the proper business model to monetize them.\u201d The Viralheat sentiment API is used by platforms including Klout,  Seesmic, and Zuberance.\n They all seek to monetize their API\u2019s by enabling anyone with an interest in it to have it. If it\u2019s a bigger customer such as an enterprise company, they \u201creel\u201d in the opportunity and negotiate a deal. @Whit_li,\u00a0 focuses on providing a \u201csandbox\u201d environment to those that want to play with their data and they can learn from them. They have partnered with Tech Crunch for their Disrupt NYC\u00a0 providing the API that will power TechRecs,\u00a0an app that will examine each conference attendees personality traits, demographics and rare interests, and provide relevant recommendations and suggest whom they should meet during the event.\n @Involver\u2019s API\u2019s makes their own products more robust and they recognize it\u2019s critical to their future as their success lies in providing content to their customer\u2019s through API\u2019s. They offer free subscriptions but after a certain threshold of queries and calls that triggers the red light to reach out to those users and do a deal.\n Klout measures one\u2019s influence on Twitter, Facebook, LinkedIn, YouTube, Foursquare, Google+ and other social apps. It relies on their API\u2019s to provide content to other partners and customers. In fact they offer a threshold of up to 60,000 calls a day! Singletary feels their success is due to them offering a personalized experience through social media. Their perks are pretty cool too \u2013 if you download their iphone app and you have a score of over 40, you will have access to the Cathay Pacific Lounge to any visitor traveling through the \u201cA\u201d boarding area at SFO\u2019s international terminal, even if you aren\u2019t a Cathay Pacific passenger.\n There are numerous sources on the Internet to learn more about API\u2019s. You can start with Mashery and download their Mashery\u2019s API Kit. Most sites have a .dev page where you can download their API developer kit, they showcase their apps and white papers usually. Although it still takes someone techie to integrate an API, marketers can easily partner with companies who offer APIs to enhance their site with content or data that will enrich their product or users experience. That is the brilliance of todays open apis - it expands a companys offerings and many times at this stage of the game, APIs are free!\n Programmable Web is the place to be part of the community of API\u2019s. It has an amazing directory of over 6,000 API\u2019s and 6,635 Mashups. Each entry provides stats, graphs, social media and all types of information you might be seeking when seeking to know if an app is worth bothering with or if its already out in the market.\u00a0\n The best way to learn about APIs is to go in person and meet other API  enthusiasts by attending a Hackathon. This is an organized event where  there are usually contests to create the best mashed up API\u2019s. Mashery  is well known for their Circus Mashimus at SXSW and you can find others popping up almost weekly these days. As you can see, APIs are here to stay and found all over this wonderful, connected world of ours.\n Another great way to meet like minded API geeks, join the Social Media Club #SMCSFO","item_date":"May 25 2012 16:53:31","display_item_date":"05-25-2012","url":"http:\/\/socialmediaclub.org\/chapter\/san-francisco\/blog\/wonderful-world-apis-1","source":"socialmediaclub.org"},{"title":"Do Developers Love Legalese?","details":"At this year\u2019s SXSW, Mashery (a ProgrammableWeb sponsor) conducted a survey of attending developers and discovered something interesting: \u00a0They care more about getting Commercial Terms of Service (TOS) than code samples from their APIs. \u00a0Does this mean a new rash of coder\/lawyers skewed the survey sample? \u00a0No, it\u2019s just an indication that developers are growing up\u2013and they expect their favorite APIs to grow with them.\n\n\nBelow you can see the 2012 survey results for yourself, and then compare it to Mashery\u2019s 2010 developer survey (see slide #3), wherein code samples rated as most important to API developers, followed by \u201cClear API License terms in plain English.\u201d \u00a0It\u2019s not exactly apples-to-apples, since this year\u2019s survey asked specifically about \u201cCommercial Terms of Service,\u201d but over a third of the developers surveyed rated Commercial TOS as being a top priority\u2013more important than code samples or documentation. \u00a0So what\u2019s changed in the last two years?\n\n\nMashery\u2019s 2010 developer survey:\n\n\n\n\n\n2012 survey results:\n\n\n\n\n\nIt\u2019s not just one thing, but a whole slew of them. \u00a0ProgrammableWeb readers can see from our API Directory that several new services are added every single day. \u00a0With that diversity comes competition, best practices, and eventually maturity: there\u2019s been a lot of progress in just the last year, since a 2011 Trove survey recounted a litany of API developer complaints\u2013including poor documentation and lack of sample code\u2013and bestowed the dubious honor of \u201cWorst API\u201d upon Facebook. \u00a0(Mashery also notes that API Explorers are increasing in importance, as developers want to get \u201chands-on\u201d with an API as quickly as possible.)\n\n\nAPIs are getting more reliable and providing more useful data all the time, and becoming essential to many businesses (and business models) along the way. \u00a0As Mashery notes, developers are starting to ask: \u201c[W]hy would I waste my valuable time integrating with an API if [I] can never monetize my app using that data?\u201d \u00a0Service providers, take note.","item_date":"May 25 2012 16:34:36","display_item_date":"05-25-2012","url":"http:\/\/blog.programmableweb.com\/2012\/05\/25\/developers-love-legalese\/","source":"blog.programmableweb.com"},{"title":"Big Data and Data Inequality: Research Is Just The Beginning","details":"There was a recent hoo-ha at a scientific conference in France, when Bernardo Huberman was furious when researchers from Google and a contributing university presenting results of social data analysis declined to share the data.\n The issue came to a boil last month at a scientific conference in Lyon, France, when three scientists from Google and the University of Cambridge declined to release data they had compiled for a paper on the popularity of YouTube videos in different countries.\n The chairman of the conference panel \u2014 Bernardo A. Huberman, a physicist who directs the social computing group at HP Labs here \u2014 responded angrily. In the future, he said, the conference should not accept papers from authors who did not make their data public. He was greeted by applause from the audience.\n In February, Dr. Huberman had published a letter in the journal Nature warning that privately held data was threatening the very basis of scientific research. \u201cIf another set of data does not validate results obtained with private data,\u201d he asked, \u201chow do we know if it is because they are not universal or the authors made a mistake?\u201d\n He added that corporate control of data could give preferential access to an elite group of scientists at the largest corporations. \u201cIf this trend continues,\u201d he wrote, \u201cwe\u2019ll see a small group of scientists with access to private data repositories enjoy an unfair amount of attention in the community at the expense of equally talented researchers whose only flaw is the lack of right \u2018connections\u2019 to private data.\u201d\n Facebook and Microsoft declined to comment on the issue. Hal Varian, Google\u2019s chief economist, said he sympathized with the idea of open data but added that the privacy issues were significant.\n \u201cThis is one of the reasons the general pattern at Google is to try to release data to everyone or no one,\u201d he said. \u201cI have been working to get companies to release more data about their industries. The idea is that you can provide proprietary data aggregated in a way that poses no threats to privacy.\u201d\n The debate will only intensify as large companies with deep pockets do more research about their users. \u201cIn the Internet era,\u201d said Andreas Weigend, a physicist and former chief scientist at Amazon, \u201cresearch has moved out of the universities to the Googles, Amazons and Facebooks of the world.\u201d\n  And of course, big data is worth big money \u2014 leaving aside the privacy concerns \u2014 and controlling access to that data is central to the aspirations of companies like Google, Facebook, and others.\n Research is just the first place where the latent data inequality of the post normal world will come to light. We will each of us \u2014 as individuals \u2014 be divided from the inherent value of information about our activities and the inferences that can be made about them. As a society, we will find corporations that do not have our interests at heart working to exploit the potential value of our aggregated data exhaust. We are an exploitable resource \u2014 like the oceans of fish or the oil beneath the ground \u2014 and these companies plan to harvest all the value without our involvement.\n We will find that we don\u2019t own the information about ourselves anymore than we own our DNA. (Yes, others can patent your genes: see The Tissue-Industrial Complex.)","item_date":"May 25 2012 16:21:19","display_item_date":"05-25-2012","url":"http:\/\/www.stoweboyd.com\/post\/23726184976\/big-data-and-data-inequality-research-is-just-the","source":"www.stoweboyd.com"},{"title":"Half of Enterprises Adopt APIs For New Business Channels","details":"A survey conducted by Vordel during one of the firms webinars found that about 50 percent of business are adopting APIs in general, and another 25 percent are dedicated to mobile applications, while 15 percent are used to build a developer community. \n  \n  There was no information, however, on which and how many users responded to the survey.\n  \n  \u201cOur research shows enterprises are increasingly leveraging APIs to build out new business channels,\u201d said Hugh Carroll, VP Marketing for Vordel. \u201cWith today\u2019s tight budgets, Vordel recommends those responsible for overseeing the use of APIs in their businesses, should fully understand how to leverage their existing investments in SOA Application Gateways to ensure their organization efficiently delivers on its enterprise API strategy.\u201d\n  \n  The company noted that three quarters of the organizations polled appointed IT managers and enterprise architects to oversee their API strategy.\n  \n  Vordel, a provider of application gateways for cloud, mobile and SOA computing, is currently offering a webinar that explains API best practices for IT managers. These practices include:","item_date":"May 25 2012 01:12:33","display_item_date":"05-24-2012","url":"http:\/\/www.tomsitpro.com\/articles\/api-cloud_computing-mobile-soa-it_managers,1-235.html","source":"www.tomsitpro.com"},{"title":"Data is the currency of democracy","details":"\u201cData is the currency of democracy\u201d, explained an open data advocate who encouraged co-operatives to open up access to its data.\n  \tChris Taggart, creator of OpenCorporates \u2014 a website that lists 43 million companies from around the world, told a group of co-operative and data enthusiasts:\u00a0\u201cIt\u2019s all about data these days \u2014 it\u2019s incredibly powerful, but only if you have access to it.\u201d\n  \tThe Im Open campaign, launched by Co-operative News, Co-operatives UK and Open Data Manchester, is engaging the sector to open up its data for the movements benefit and the first of many seminars started the conversation at the Co-operative Insurance Tower, Manchester.\n  \tChris Taggart said open data is a way of opening up all the data streams of different entities and allowing people to reuse it for their own means.\n  \tMany groups of people have started to open data including companies, governments, local authorities and even the United Nations. Discussions are now taking place between co-operatives to see how the sector can embrace open data.\n  \tAdded Mr Taggart:\u00a0\u201cData has always been important to our lives and now our lives are data. If democracy is anything, its about the ability to take information, pamphlets, what the parties say and to take a different view to someone who has seen the same thing.\n  \t\u201cIf you can\u2019t have access to the data then you don\u2019t get to have an opinion. There is no misuse of data.\u201d\n  \tHe went on to say that many governments and local authorities are weak in the data world, because they have too much data they cannot use.\u00a0He said that co-operatives can use open data to show how they are different from other types of companies, using the Co-operative Group as an example, he noted: \u201cThe Co-operative Bank is open about what holdings it has, but it is locked away on a report or PDF.\n  \tIf you put that out as open data it would highlight the point of difference from them and if nobody follows suit that makes a significant point of difference between them and other banks.\u201d","item_date":"May 24 2012 23:23:58","display_item_date":"05-24-2012","url":"http:\/\/www.thenews.coop\/node\/8333","source":"www.thenews.coop"},{"title":"Gluecon Hackathon And The First Tiggzi App Builder Plug-in: AT&T SMS API | SYS-CON MEDIA","details":"Today and tomorrow I\u2019m Gluecon for the mobile hackathon being run during the conference. Tiggzi app builder is already an amazing tool for hackathons but now we are making it even faster to build apps. We just released our first public Plugin for AT&T SMS API. With a few clicks, you can import the plugin that has pre-packaged REST services configured to send SMS messages using AT&T API. There is even a sample page from which you can test the service.\n There are two ways to import the plugin. You can add the plugin when a new app is created or import into already existing app.\n Adding to a new app\n     From Apps page, click Start Now\n Click App Builder > Go\n Click Mobile App > Start Now\n Select Blank Mobile App template, click Next\n Enter app name and click Add Plugins\n This information can be found in AT&T app settings. If you don\u2019t enter the information during this step, you can enter it by going to Project > ServiceSettings after the app has been created\n That\u2019s it, the app is created with the AT&T SMS plugin:","item_date":"May 24 2012 22:40:54","display_item_date":"05-24-2012","url":"http:\/\/www.sys-con.com\/node\/2283860","source":"www.sys-con.com"},{"title":"Facebook To Allow Advertisers To Purchase Premium Ads Via Power Editor, API","details":"David Cohen on May 23, 2012 9:54 PM\t \tFacebook is working with developers and ad agencies on giving advertisers more ways to purchase premium ads .\n The social network said that \u201cin the coming weeks,\u201d advertisers will be able to purchase premium ad inventory via the power editor, or via the ads application-programming interface.\n Facebook officially introduced premium ads at its Facebook Marketing Conference in New York Feb. 29. The power editor debuted last year.","item_date":"May 24 2012 22:40:43","display_item_date":"05-24-2012","url":"http:\/\/allfacebook.com\/premium-ads-power-editor-api_b89851","source":"allfacebook.com"},{"title":"Platform Updates: Operation Developer Love","details":"Since last Wednesdays update, we published a developer Spotlight on Tumblr, explained how to drive mobile app installs with the App Center, announced that we will be hacking and presenting at Over the Air and explained how to use Insights to get to know your unique users.\n  User restrictions for Open Graph stories\n  Developers can now create a better user experience in their Open Graph App by only showing stories to users that meet certain restrictions such as age, country or certain kinds of content. For example, a video app can specify that stories published by their app may only appear in countries in which they have launched. These settings can be applied against all content from the app or on a per object basis. For example, a video app can restrict stories from movies rated PG13 to viewers age 13+, and restrict movies rated NC17 to viewers age 17+. If there are multiple restrictions applied to a story, we choose the most restrictive option. To learn more about how to implement these new settings, please see the documentation.\n   \n  As announced on the Roadmap, on June 6, 2012, the following changes will be in effect:\n     Removal of FBML\n   FBML apps will no longer work on Platform. The FBML Removal migration will appear and will be enabled for all apps. It will be possible to disable the migration, thereby re-enabling FBML, until July 5, 2012 when the migration and all FBML endpoints will be removed completely. If your app is still utilizing FBML, please update your code to use non-FBML endpoints before June 6 to avoid any issues.\n      XMPP Connections must be done over TLS\n   Apps connecting to Facebooks XMPP service will be required to use STARTTLS for all connections. We will start rejecting unencrypted connections.\n   Bugs activity between Tuesday, May 15 and Tuesday, May 22\n  199 bugs were reported\n 40 bugs were reproducible and accepted (after duplicates removed)","item_date":"May 24 2012 22:40:24","display_item_date":"05-24-2012","url":"http:\/\/developers.facebook.com\/blog\/post\/2012\/05\/23\/platform-updates--operation-developer-love\/","source":"developers.facebook.com"},{"title":"The ReferralCandy API: Tell Your Friends About It","details":"ReferralCandy is an online marketing company that specializes in referral marketing. The company provides a plug-and-play platform that makes referral marketing easier. Techinasia.com notes that ReferralCandy modeled their structure after Dropbox\u2019s two-sided incentive referral program. Dropbox\u2019s program rewarded both the user that signed up through a referral link and the referrer with additional storage space. This tactic has proven successful for ReferralCandy who recently released their ReferralCandy API.\n\n\n\n\n\nWhenever a customer makes a purchase from a company that uses ReferralCandy\u2019s reward program an email is immediately sent to that customer giving them a link to a promotion. The customer can then send this promotion to friends or share it via facebook or twitter. If a friend follows that link and makes a purchase the original customer is given points that can be used towards several reward options.\n\n\nThe ReferralCandy API allows developers to integrate this reward program into existing e-stores. The API implements RESTful GET and POST calls. You can find full documentation for this API here.\n\n\nThe ReferralCandy API is one of 349 reference APIs and 318 shopping APIs in the ProgrammableWeb directory.","item_date":"May 24 2012 22:28:40","display_item_date":"05-24-2012","url":"http:\/\/blog.programmableweb.com\/2012\/05\/24\/the-referralcandy-api-tell-your-friends-about-it\/","source":"blog.programmableweb.com"},{"title":"API Design - Matt Gemmell","details":"One of the development tasks I do most often is designing the API for a reusable component. The components are usually for iOS (though sometimes they\u2019re for OS X), and are invariably GUI controls or views of some kind.\n  I\u2019ve designed literally dozens of component APIs over the years, including for clients like Apple, and I\u2019ve learned quite a bit about the process. I periodically release open source components too, and the feedback I\u2019ve had has helped me put together a set of guidelines for API design that I\u2019d like to share with you.\n     This is an important topic, whether you\u2019re an open source contributor, or working as part of a team on a large app, or just creating your own software. Just like the first launch experience of an app, your API is part of the first impression that a developer will have with your code, and will have a huge impact on whether they use it or throw it away.\n  APIs are UX for developers. I\u2019ve always been surprised that there isn\u2019t more material written about this aspect of our work, in a way that\u2019s specific to the popular platforms.\n  As we go through some guidelines, I\u2019m going to use my most recently released open source GUI component, MGTileMenu, as an example where necessary. You can read all about MGTileMenu here first, if you like.\n  Desirable qualities\n  API design is very much like user interface and user experience design. Your target audience has different needs and characteristics, but they\u2019re still humans who are looking to get a job done. As with a friendly, usable app UI, you\u2019re still trying to make your API:\n \nAs with any piece of software designed for use by humans, we have to think about the use cases. We have to make the most commonly-needed stuff easy, without undue configuration. Default behaviours should be useful as-is, and should be sensibly chosen. The software should be discoverable, and should allow the user to generalise from known paradigms. It\u2019s all exactly the same set of principles as when we\u2019re creating UIs.\n  The developer interface\n  Components are interacted with by the developer using four primary explicit means:\n  The class interface: its exposed properties and methods.\n The delegate protocol, where relevant.\n The data-source protocol, where appropriate.\n Any provided notifications.\n \nWe have to design each of those, judiciously and deliberately, for human use. There are two key questions when you\u2019re thinking about the API:\n  What is the control?\n This influences the interface and convenience methods. Is it a button? A slider? Your interface is obvious. Your convenience methods will follow the standard semantics of the control.\n What is the control like?\n This influences the delegate and\/or data-source model and notifications. If it\u2019s a new type of control, is it essentially very similar to something else? An outline view is a linear table. A calendar widget is a date-picker. A collection of commands presented with a unified presentation is a menu.\n \nOur core goal is consistency with existing components and models, so that we can turn an unfamiliar control into something the developer already understands. Use standard APIs, models, and patterns wherever possible (and that\u2019s almost always). Familiarity and intuitiveness are just as important at code level as they are for the end user.\n  Let\u2019s look at the four components of the component API mentioned above.\n  Class interface\n  Here\u2019s the interface file for MGTileMenu.\n  Before we even start talking about the specifics of the interface, we have a couple of over-arching rules:\n  Rule 1: Use the local dialect\n  One the most common mistakes I see in API design is the use of foreign conventions. APIs belong to a platform and a developer ecosystem. You simply can\u2019t use whatever idioms and architectures you\u2019re used to from a different platform; to do so is to pollute your current codebase and to damage the productivity of your fellow developers.\n  Learn your target platform\u2019s conventions before coding. For example, on iOS or OS X, don\u2019t use exceptions for control flow. Name your methods in an appropriate manner (which usually means sufficiently verbose, but should also of course be sufficiently succinct).\n  Learn what a protocol is, and a delegate, and a category. Use that terminology throughout your code. Learn the relevant naming schemes for constructors and destructors. Obey native memory management rules. The vocabulary and the grammar are indivisible, and you\u2019re either developing for a given platform or you\u2019re not.\n  Rule 2: Design decoupled\n  Any component should be designed such that it\u2019s not coupled to the project you created it for, and if it\u2019s a GUI control or view, it should at least display something by default. Use the existing framework classes as a guide, and maintain loose coupling with delegate protocols, well-designed\/named API methods and notifications where appropriate.\n  An obvious but very effective way to do this is to create a new project for each component, and develop the component literally in isolation. Force yourself to use your own API. Stay away from the temptation of tying unrelated classes together. Start as you mean to go on.\n  With that said, let\u2019s talk about the class interface proper. Initialisation methods are one of the most important parts of the interface, because they\u2019re how people get started with your component. Your class will have certain required settings for initial configuration. So, an obvious rule:\n  Rule 3: Required settings should be initializer parameters\n  If something needs to be set, don\u2019t wait for it - require it up-front, immediately, and return nil if you don\u2019t get something acceptable.\n - ( id ) initWithDelegate: ( id < MGTileMenuDelegate > ) theDelegate ; \/\/ required parameter; cannot be nil.  \n \n  This is a corollary to the previous rule: remember not to just swallow those parameters. Give access to them via properties, and note if they might have been massaged in any way (sanitised, or otherwise modified).\n @property ( nonatomic , weak , readonly ) id < MGTileMenuDelegate > delegate ; \/\/ must be specified via initializer method.  \n \nThese previous two examples raise a further general point.\n  Rule 5: Comment your header files (including defaults)\n  Realistically, you won\u2019t always provide separate, standalone documentation for a component. If you don\u2019t provide documentation, your .h files (and demo app) are your docs. They should be suitably written, and by \u2018suitably\u2019 I mean:\n  Sufficiently detailed, but no more so. Be succinct.\n For professionals. Assume things that are safe to assume. Don\u2019t waffle.\n \nParticularly, you should briefly note default values beside properties or accessors; it\u2019s much easier to scan those in the header file than to try to locate your initialisation code in the implementation.\n @property ( nonatomic ) CGGradientRef tileGradient ; \/\/ gradient to apply to tile backgrounds (default: a lovely blue)  @property ( nonatomic ) NSInteger selectionBorderWidth ; \/\/ default: 5 pixels  @property ( nonatomic ) CGGradientRef selectionGradient ; \/\/ default: a subtle white (top) to grey (bottom) gradient  \n \n  Your class should be designed so that it requires minimal code to integrate (delegate\/data-source protocol included, about which more later). Excluding delegate methods, you should aim to make it usable at least for testing purposes with only 3 lines of code.\n  Those lines are:\n  Instantiate it.\n Basically configure, so it will show and\/or do something.\n Display or otherwise activate it.\n \nThat should be it. Anything substantially more onerous is a code smell. Here are the relevant lines from MGTileMenu\u2019s demo app:\n  1 2 3 4 5 6 7 8 \n \/\/ Instantiate.  tileController = [[ MGTileMenuController alloc ] initWithDelegate: self ];    \/\/ Configure.  tileController . dismissAfterTileActivated = NO ; \/\/ to make it easier to play with in the demo app.    \/\/ Display.  [ tileController displayMenuCenteredOnPoint: loc inView: self . view ];  \n \nRule 7: A fat demo usually means a broken component\n  Another corollary: the size of your demo harness is a quality metric for your component, where smaller is better. Demo harnesses\/code should be as small and thin as possible (making suitable allowances for demos that aim to explore all of a component\u2019s customisation or functionality).\n  The core required code to turn an empty Xcode app template into a demo of your app should be minimised. It\u2019s not OK to required copy-pasted boilerplate to get your component working, and having an example of it in your demo isn\u2019t an excuse.\n  Rule 8: Anticipate customisation scenarios\n  My standard rule for apps is don\u2019t give the user options. Choose sensible defaults to fit the majority, and skip the Preferences window. Good software, after all, is opinionated.\n  The situation is a bit different with components, because the scenarios of use aren\u2019t as clear-cut. You can certainly make a component that only fits one specific situation, but usually we want some flexibility. You never know exactly how another developer is going to use your component, so you have to build in some generality.\n  It\u2019s important to choose your customisation points carefully. It\u2019s particularly important to consider dependencies - not in the compiling\/linking sense, but rather the logical relationships between types of customisation. I approach this by trying not to think of customisation at the instance-variable level, but rather at the \u201caspect\u201d level. What aspects of your component do you want to allow customisation of? Then you work out what specific properties to expose.\n  It\u2019s easy to cripple a certain type of customisation by not exposing sufficient configuration points. Some examples:\n  Don\u2019t expose width and height without considering corner radius too.\n Don\u2019t expose background colour without highlighted background colour.\n Don\u2019t expose size without spacing.\n \nThe specifics depend on the component, but just try to consider the relationships between properties, from the point of view of either appearance or functionality. Empathise with the developer. Be flexible, without abandoning the identity of the component.\n  1 2 3 4 5 6 \n @property ( nonatomic ) BOOL dismissAfterTileActivated ; \/\/ automatically dismiss menu after a tile is activated (YES; default)  @property ( nonatomic ) BOOL rightHanded ; \/\/ leave gap for right-handed finger (YES; default) or left-handed (NO)    @property ( nonatomic ) NSInteger tileSide ; \/\/ width and height of each tile, in pixels (default 72 pixels)  @property ( nonatomic ) NSInteger tileGap ; \/\/ horizontal and vertical gaps between tiles, in pixels (default: 20 pixels)  @property ( nonatomic ) CGFloat cornerRadius ; \/\/ corner radius for bezel and all tiles, in pixels (default: 12.0 pixels)  \n \nLet common sense be your guide. Decide what options will serve 70% or so of the usage situations you can think of, and provide those options. Let your delegate methods and code structure serve the rest.\n  There\u2019s a particular pattern that keeps cropping up in components that I like - some of which are from standard frameworks, some open source from third parties, and some even my own. It\u2019s a ratio of the number of properties (or accessors, or customisation points) on a component, to the number of \u201cdo stuff\u201d methods (i.e. all the other stuff, from initializers to state-updating).\n  It\u2019s pretty much always more properties, and fewer \u2018actions\u2019 (again, that\u2019s not actions in the Interface Builder sense). MGTileMenu has an initializer, and four actual for-public-use methods (one of which is a convenience that calls another). In terms of customisation points, it has four times as many. I think that\u2019s a good ratio, and leads to components that are both concise in actual functionality, but also flexible in customisation.\n - ( id ) initWithDelegate: ( id < MGTileMenuDelegate > ) theDelegate ; \/\/ required parameter; cannot be nil.  - ( CGPoint ) displayMenuPage: ( NSInteger ) pageNum centeredOnPoint: ( CGPoint ) centerPt inView: ( UIView * ) parentView ; \/\/ zero-based pageNum  - ( void ) dismissMenu ;  - ( void ) switchToPage: ( NSInteger ) pageNum ; \/\/ zero-based pageNum  \n \n  A great way to simplify both the API and implementation of your component is to use existing controls in your implementation. A unified presentation doesn\u2019t meant that you can\u2019t build something out of pre-existing components (indeed, that\u2019s one of the basic principles of good software engineering).\n  Consider how UITableViewCell and UIButton have simple APIs because they use sub-controls such as UIImageViews and UILabels. You can, and should, do that too - and if appropriate, expose the corresponding sub-controls to keep your class interface concise and consistent.\n  In MGTileMenu, for example, the tiles are regular UIButtons (not even subclasses). This drastically simplified the implementation compared to drawing the tiles within a single custom view, tracking input events, and supporting accessibility.\n  Rule 11: Convenient for you is convenient for me\n  You\u2019ll naturally add convenience methods during implementation, and the instinct is to keep them private. Instead, consider whether you can expose them for use by those who integrate your component into their own apps.\n  Whatever made it more convenient for you to add a method or function may apply to those developers too.\n  For example, in MGTileMenu I created these convenience functions:\n CGRect MGMinimallyOverlapRects ( CGRect inner , CGRect outer , CGFloat padding );    CGGradientRef MGCreateGradientWithColors ( UIColor * topColorRGB , UIColor * bottomColorRGB ); \/\/ assumes colors in RGB colorspace  \n \nThe first helps me shift a tile menu so that it\u2019s fully visible within its parent view (which might be handy for another developer, if they\u2019re providing ancillary UI related to the menu), and the second returns a Core Graphics gradient from two UIColors, which I used when setting a default background for the tiles (and another developer may find handy when implementing MGTileMenu\u2019s delegate protocol, to give tiles custom gradients).\n  Rule 12: Magic is OK. Numbers aren\u2019t.\n  Sooner or later, you\u2019ll put magic into your component. Hopefully there\u2019ll be plenty of the Steve Jobs type of intuitive, delightful, empowering magic, but what I\u2019m talking about is things like numbers and other values that have special meaning in your code. A common example is -1, to indicate a unique thing in a set, or a special situation.\n  It\u2019s fine. It\u2019s genuinely OK to do that. What\u2019s not OK, though, is needlessly putting mysterious raw values throughout your code, and it\u2019s especially not OK to expose that in the API. If you\u2019re exposing magic, dress them up for consumption. Use #defines or a constant or something. Just make them presentable and understandable.\n \/\/ Used for the page-switching tile in methods expecting a tile-number.  #define MG_PAGE_SWITCHING_TILE_INDEX -1  \n \n  There are classic delegate and data-source protocols that we can draw on for almost any component. If you\u2019re displaying data, the One True Data-Source Protocol is likely to be something very close to:\n  How many things do I have?\n What\u2019s the value for property Y of thing X?\n \nSimilarly, in almost any situation, the One True Delegate Protocol is likely to take the form:\n  Should this thing do that?\n This thing is about to do that.\n This thing just did that.\n \nThis is also known as the Should, Will, Did protocol pattern, and it ties neatly in with the Will-Did notification pattern too, about which more later.\n  Let me mention something you might find controversial: I find it perfectly acceptable to conflate the delegate with the data-source (i.e. combine them into a single protocol). I do it with MGTileMenu and several other components, for example.\n  I fully accept the principle of separating them, and I can think of many cases where you\u2019d want to keep them separate. Apple keeps them separate too, generally. That\u2019s fine.\n  In my experience, though, in most cases it\u2019s fine to combine them. Most people handle data-source methods and delegate methods in the same place. I\u2019ve never had a complaint about unifying those protocols, and I can scarcely remember a situation where even existing separate protocols were handled in different places.\n  If you care about purity, or have a need to separate delegate from data-source, then obviously you should do so. I just don\u2019t think you need to feel bad if you combine them.\n  Rule 13: Limit \u2018required\u2019 delegate methods\n  Be very careful when choosing which of your delegate methods are required. Too many required methods tends to indicate:\n  Poor choice of default behaviour.\n Too much of your own politics are in your code.\n \nA well-designed component should need very, very few required delegate methods - just the bare minimum to do whatever it does. Choose carefully. Equally, remember that it\u2019s easy to add optional methods later, but it\u2019s hard to turn optional ones into required ones (people will complain, and rightly so).\n  MGTileMenu has five required methods, four of which are data-source methods:\n - ( NSInteger ) numberOfTilesInMenu: ( MGTileMenuController * ) tileMenu ; \/\/ in total (will be shown in groups of up to 5 per page)  - ( UIImage * ) imageForTile: ( NSInteger ) tileNumber inMenu: ( MGTileMenuController * ) tileMenu ; \/\/ zero-based tileNumber  - ( NSString * ) labelForTile: ( NSInteger ) tileNumber inMenu: ( MGTileMenuController * ) tileMenu ; \/\/ zero-based tileNumber  - ( NSString * ) descriptionForTile: ( NSInteger ) tileNumber inMenu: ( MGTileMenuController * ) tileMenu ; \/\/ zero-based tileNumber  \n \nThe first two follow the One True Data-Source Protocol. The third and fourth do too, but they also expose my politics: I think that software should be accessible, and I\u2019m forcing you to supply a label and hint for each tile for VoiceOver to read. I\u2019m comfortable with it.\n  There\u2019s also one delegate method proper:\n \nThat one is required because it\u2019s how you find out that a tile was activated. If you\u2019re not willing to pay attention to that, MGTileMenu will do nothing useful, and you might as well not be using it at all. So, it\u2019s required.\n  Following immediately on from the last rule: make things accessible. Don\u2019t tack it on at the end, either: design for accessibility from the start. If you follow the \u201cuse controls in your controls\u201d rule, you probably get this almost for free.\n  Delegate (or rather, data-source) methods, as shown above, are a great place to twist the arm of another developer to make them at least provide something for VoiceOver. And if you can automatically repurpose something visual (like a displayed text label) as a VoiceOver label, so much the better (again, in most cases VoiceOver already handles this for you).\n  Be socially conscious. Make it hard not to support accessibility. I also wrote an article about supporting VoiceOver in iOS apps, which Apple recommends to companies who contact them about accessibility programming. I recommend it too, but then I wrote it, so you\u2019d expect that.\n  Rule 15: Use semantic objects for parameters\n  This doesn\u2019t just apply to protocols, but protocols are where it\u2019s particularly important. Use actual, first-class, semantically-appropriate objects for data, even if it\u2019s more hassle for you to work with in your implementation.\n  If you\u2019re asking for a date, don\u2019t accept numbers - get an actual NSDate object. There are objects or structures for just about everything, and you should use them as intended. Create a class if you need to (you probably won\u2019t need to).\n  The one standard exception, of course, is indices - there\u2019s no reason for them to be anything but primitives, since NSNumber adds nothing that\u2019s semantically important enough to offset the bundling\/unbundling inconvenience.\n  Rule 16: Enhance the API if semantics don\u2019t fit\n  I see this all the time. I mentioned earlier how you can think of almost any new, custom control as being substantially like something that already exists (often, it\u2019s like the already-existing thing that you\u2019re using behind the scenes for your implementation).\n  That\u2019s great, and you\u2019re very clever, but semantics trump similarity. It\u2019s absolutely fine (and wonderful) to layer a new API on top of an existing one, in order to make the semantics fit. For example:\n  A contact list implemented with a table should have a contacts-related API\n A month-view calendar implemented with a grid should have a date-related API\n \nAnd so forth. Don\u2019t force yourself (or other developers) to constantly be mentally converting between an abstract implementation API and the actual semantics of the component - make the API reflect the actual purpose of the component instead.\n  MGTileMenu\u2019s delegate protocol does that by treating the menu not as a collection of UIButtons (the implementation), but rather as a unified menu, with numbered tiles each of which have relevant display properties.\n  Rule 17: Highlighting is interesting\n  I learned this one by having to go back and add new delegate methods and notifications to APIs I thought were finished. For interactive controls, highlighting is interesting. By \u2018interesting\u2019, I mean of potential significance to the surrounding app.\n  Any control will inform the app (in one sense or another, perhaps just by calling an action method) when it has been fully triggered, but comparatively few will notify when they\u2019ve been visually highlighted (selected, pressed) or unhighlighted without being triggered. It turns out that that\u2019s actually pretty important. The app might want to:\n  Add, remove or reposition ancillary UI.\n Update some other part of its display.\n Offer some contextual help.\n Some other thing you can\u2019t possibly foresee.\n \nHighlighting is certainly an example of an optional set of delegate methods, but they\u2019re important to have, and almost always trivial to implement.\n - ( void ) tileMenu: ( MGTileMenuController * ) tileMenu didSelectTile: ( NSInteger ) tileNumber ; \/\/ zero-based tileNumber  - ( void ) tileMenu: ( MGTileMenuController * ) tileMenu didDeselectTile: ( NSInteger ) tileNumber ; \/\/ zero-based tileNumber  \n \n  Many of us approach optional delegate methods as an either-or situation: if you don\u2019t implement them, you get the default behaviour, and if you do, then you\u2019re totally responsible for what happens. That\u2019s not ideal.\n  In any implementation which provides an optional delegate method, you should still fall back on the default behaviour even if the method is implemented, but doesn\u2019t return something sensible. It sounds obvious, but it\u2019s amazing how many components will blithely let delegate objects return any kind of craziness without sanity-checking, just because the delegate has somehow promised to behave itself by implementing the method.\n  I\u2019m talking particularly about visual customisations, such as background colours or images. Consider very, very carefully whether you shouldn\u2019t intervene in that case, and fall back upon your default appearance. Did they really want to show nothing? Does that even make sense? Will it make the control look broken? If so, step in, and serve up the default just as if the delegate method was never implemented in the first place.\n  Relatedly, have a documented, standard, unsurprising way to deliberately invoke the default behaviour via returning something like nil from each optional delegate method.\n  MGTileMenu, for example, has a relatively complex hierarchy of ways you can customise tile backgrounds. You can implement any (or all, or none) of three optional delegate methods to provide a background image, gradient or colour for each tile, in that priority order. You can also opt into the default behaviour for any tile at any time, by returning nil or NULL as is appropriate to the type.\n  You\u2019ll have to try fairly hard (by returning clearColor\n, or an empty UIImage object) to really, really make a tile\u2019s background completely transparent.\n  Rule 19: Always say who\u2019s talking\n  This is a simple rule, and an equally simple mistake to make. In your delegate methods, always pass the sender as a parameter. Always. Even for singletons. Even for things you cannot conceive would ever be used more than once simultaneously. No exceptions.\n \n  The One True Data-Source Protocol should always have query methods such that the most interesting thing goes first. The specific quality or property you\u2019re requesting a value for. Like this:\n \nThe return type should flow naturally into the first part of the method name, without causing surprise. Data-source protocols often have many similarly-named methods, so keep the unique and interesting parts at the very start. Easier to read, and easier to autocomplete.\n  Some people have pointed out that Apple\u2019s UITableViewDataSource protocol doesn\u2019t do it that way, and instead puts the sender first, for example:\n \n  The One True Delegate Protocol, however, isn\u2019t for queries but rather for notifications. In this situation, you put the sender first (following our \u201csay who\u2019s talking\u201d rule above).\n \nThis follows how an interaction would go between two people having a conversation. You wouldn\u2019t just jump in and say \u201cShe\u2019s going to be late,\u201d because the other person would have to ask \u201cWho?\u201d\n  Instead, you start by saying who\u2019s talking. It\u2019s a convention, and handily distinguishes query (data-source) from notification (delegate) methods.\n  Rule 22: If a convention is broken, throw it away\n  Having said all of the above, remember that convention and consistency must at some point bow to superior judgement - in this case, yours. If a convention is broken, skip it without worrying. Rename things, if yours is truly better.\n  As an example, there\u2019s a pre-existing convention for menu controls whereby you can enable or disable menu-items via the delegate, using a method called validateMenuItem:\n. For the sake of consistency, I was tempted to use that same method name as part of my delegate protocol. I decided not to, because:\n  It has a horrible, horrible name. \u201cValidate\u201d? That doesn\u2019t say \u201cenable\u201d to me.\n It\u2019s imperative, where in my case I\u2019m really asking a question.\n It broke the naming scheme of my other delegate methods.\n \nInstead, I went for something simpler and more understandable, if unconventionally-named:\n \nWe can debate the specific wording, but if you encountered that method you\u2019d know what it was for and how to use it right away. To me, that\u2019s better.\n  Notifications\n  Notifications are the other half of delegate protocols. My position is that, if you\u2019re using a delegate protocol (you should, if it\u2019s at all appropriate), then it\u2019s incomplete until you add the notifications that naturally follow from it.\n  In MGTileMenu, you can find the notifications in the interface file for MGTileMenuController.\n  Rule 23: Notifications follow delegate methods\n  There\u2019s a natural correspondence between delegate methods (proper; not data-source methods) and notifications. You use them in the same places in your code, and for exactly the same purpose.\n  If you have a delegate method that tells the delegate about something happening, you should usually provide a notification for that same purpose. Take your notification-like delegate methods, remove the interrogatory ones (the should\n methods), and you have your list of notifications to implement.\n  The delegate methods\u2019 parameters should match up with the notifications\u2019 userInfo contents, with the obvious exception that you pass the sender as the notification\u2019s object\n, rather than bundled up in the info dictionary.\n  Delegate methods:\n \n  Give a notification the information it requires in order to be useful. Remember that notification receivers may (and almost always will) not have anything to do with the delegate or data-source chain for your component.\n  Ask yourself what would be useful, and provide that information. At the very least, you must ensure that all arguments provided to the corresponding delegate method are wrapped up in the userInfo object.\n  Delegate methods:\n - ( void ) tileMenu: ( MGTileMenuController * ) tileMenu willSwitchToPage: ( NSInteger ) pageNumber ; \/\/ zero-based pageNumber  - ( void ) tileMenu: ( MGTileMenuController * ) tileMenu didSwitchToPage: ( NSInteger ) pageNumber ; \/\/ zero-based pageNumber  \n \n \/\/ The following notifications have a user info key MGPageNumber with an NSNumber (integer, zero-based) value.  #define MGPageNumberKey @MGPageNumber  extern NSString * MGTileMenuWillSwitchToPageNotification ; \/\/ menu will switch to the given page  extern NSString * MGTileMenuDidSwitchToPageNotification ; \/\/ menu did switch to the given page  \n \n  Whether testing means formal TDD is up to you, but testing itself isn\u2019t optional. Every optional delegate method. Every posted notification. Every point of customisation, in every possible combination. Components provide a thousand opportunities for subtle issues.\n  There will be bugs. Find them and fix them first. If you\u2019re pushed for time, cut a feature and debug instead. Thou shalt suffer no bugs to ship.\n  Final thoughts\n  I\u2019ve formulated the above rules by learning the hard way, through years of making mistakes while creating components and their APIs. I do try to practise what I preach, though inevitably there will be a hundred examples of where I haven\u2019t.\n  Whilst not all rules apply to all situations, and no rule applies in every case, following as many of these as you can will give you a better chance of producing flexible, well-designed, reusable components for yourself and others to enjoy.\n  You may want to grab a quick summary of the rules, as shown below; I have the full-size version hosted on Flickr.\n    \n   If you\u2019re interested in releasing your components for others to use, as I did with MGTileMenu, you may also want to read my article on releasing open source code, which touches on some of these points and also talks about your README file, license choice, and related matters.\n  I hope you\u2019ve enjoyed this article. If you did, perhaps you\u2019d consider buying a non-attribution license for some of my open source components, or supporting future articles on this blog (and future source code releases) via a donation.\n     \n  For more about software and user experience design, along with a multitude of other topics, you should follow me (@mattgemmell) on Twitter. You can also hire me for your own projects.\n  Now go and make great software.","item_date":"May 24 2012 22:20:56","display_item_date":"05-24-2012","url":"http:\/\/mattgemmell.com\/2012\/05\/24\/api-design\/","source":"mattgemmell.com"},{"title":"PeopleBrowsr Chooses 3scale SaaS API Management Solution to Launch PeopleBrowsr Kred API for Influence Marketing From Social Data and Analytics","details":"SAN FRANCISCO, CA--(Marketwire - May 24, 2012) -  PeopleBrowsr, the creator of community influence measure Kred and the global leader in social analytics, selected 3scale (www.3scale.net), the creator of Out-of-the-Box API management infrastructure, to launch, manage and productize the PeopleBrowsr Kred Application Programming Interface (API).\n         Depending on development requirements, the PeopleBrowsr Kred API powered by 3scale is available in four different classes, including Find Influencers, Deep Analytics, Action Analytics and Global Kred Score, and either 60 or 1,200 days of data access. The API grants access to developers to over 1,200 days of indexed and filtered social data, including the full Twitter firehose since 2008, public Facebook posts, 40 million blogs and forums, and other sources. \n         3scale helps customers like PeopleBrowsr unleash the power of their API. 3scale provided the expertise and breadth of vision that helped us to easily shift and totally optimize the PeopleBrowsr Kred API into a tiered offering like no other, said Travis Wallis, PeopleBrowsrs Director, API. Its been incredibly easy for us to integrate 3scales unique plug and play platform to manage access, authenticate users, gain business insights and deliver the very best tools for marketers and developers who want to make great apps and identify influencers with the PeopleBrowsr Kred API. \n         3scale powers 100+ APIs and has over 60,000 developers writing applications using these APIs. 3scale customers (http:\/\/www.3scale.net\/our-customers) are taking advantage of 3scale API Management platform and infrastructure that includes the following:\n         \nWorld-class innovators like PeopleBrowsr choose 3scale to open, launch and manage their API because we offer years of experience in productizing our customers API capabilities, said Guillaume Balas, 3scale CMO. With our years of API management experience 3scale brings the expertise and solution to help API providers develop new business models and monetize distribution models. \n         3scale works closely with customer tech- and business teams to provide optimal solutions for their needs by maximizing the impact and reach of their API among developers and business partners. For more information about 3scale, please visit www.3scale.net or contact sales@3scale.net.\n         About PeopleBrowsr:\nPeopleBrowsrs end-to-end social media marketing solutions assemble the collective intelligence, identify its most influential people, and make them accessible for analysis and engagement. Kred spotlights the most influential people by interest-based community and location. Its 100 terabyte Datamine of 1,200 days of social media posts is accessible through Playground, a comprehensive social analytics platform, or via API for custom development. \n         PeopleBrowsrs advanced metadata API enables brands and agencies to easily develop custom applications for browsing, filtering and analyzing PeopleBrowsrs Datamine. The APIs were expressly developed for marketers to answer three crucial questions about the people they engage with on social networks: Who are they, What are they saying, and How are they connected to others.\n         For more information, please email contact@peoplebrowsr.com or tweet @PeopleBrowsr.\n3scale is one of the worlds leading cloud-based API infrastructure providers and enables developers, and enterprises to securely open, control, manage and monetize their APIs. 3scale unique suite of infrastructure services brings API providers unprecedented control, visibility on API activity and peace of mind.\n         Access 3scale API Management platform is via instant signup at www.3scale.net or contact sales@3scale.net","item_date":"May 24 2012 22:19:54","display_item_date":"05-24-2012","url":"http:\/\/www.marketwire.com\/press-release\/PeopleBrowsr-Chooses-3scale-SaaS-API-Management-Solution-Launch-PeopleBrowsr-Kred-1661279.htm","source":"www.marketwire.com"},{"title":"Building Streaming REST APIs with Ruby","details":"Twitter popularized the term firehose API, to mean a realtime stream of data sent through a persistent connection. But even if youre not a realtime service, streaming APIs are great for pushing data from the backend to clients. They reduce resource usage because the server can decide when its a good time to send a incremental chunk of data. They can also improve the responsiveness of your user experience.  The same HTTP API can be reused to power multiple different apps. For example, you could write your web frontend with a Javascript frameworks like Backbone.js, but reuse the same API to power a native iOS application. Follow the jump to read about how streaming APIs work, and how you can write one with Rack::Stream.\n  Rack::Stream is rack middleware that lets you write streaming API endpoints that understand HTTP, WebSockets, and EventSource. It comes with a DSL and can be used alongside other rackable web frameworks such as Sinatra and Grape.\n  Whats Streaming HTTP?\n  Normally, when an HTTP request is made, the server closes the connection when its done processing the request. For streaming HTTP, also known as Comet), the main difference is that the server doesnt close the connection and can continue sending data to the client at a later time.\n  \n  To prevent the connection from closing, rack-stream uses Thins async.callback to defer closing the connection until either the server decides to close the connection, or the client disconnects.\n  Rack::Stream\n  Rack::Stream is rack middleware that lets you write streaming HTTP endpoints that can understand multiple protocols. Multiple protocols means that you can write an API endpoint that works with curl, but that same endpoint would also works with WebSockets in the browser. The simplest streaming API you can make is:\n  # config.ru # run with `thin start -p 9292` require rack-stream  class App   def call(env)     [200, {Content-Type => text\/plain}, [Hello,  , World]]   end end  use Rack::Stream run App \n  If you ran this basic rack app, you could then use curl to stream its response:\n  > curl -i -N http:\/\/localhost:9292\/  HTTP\/1.1 200 OK Content-Type: text\/plain Transfer-Encoding: chunked Connection: close Server: thin 1.3.1 codename Triple Espresso  Hello World \n  This isnt very exciting, but youll notice that the Transfer-Encoding\n for the response is set to chunked\n. By default, rack-stream will take any downstream applications response bodies and stream them over in chunks. You can read more about chunked transfer encoding on Wikipedia.\n  Lets spice it up a bit and build an actual firehose. This next application will keep sending data to the client until the client disconnects:\n  require rack-stream  class Firehose   include Rack::Stream::DSL    def call(env)     EM.add_periodic_timer(0.1) {       chunk \\nChunky Monkey     }     [200, {Content-Type => text\/plain}, [Hello]]   end end  use Rack::Stream run Firehose \n  The first thing to notice is the Firehose rack endpoint includes Rack::Stream::DSL\n. This are convenience methods that allow you to access env[rack.stream]\n, which is injected into env\n whenever you use Rack::Stream\n. When a request comes in, the #call\n method schedules a timer that runs every 0.1 seconds and uses the #chunk\n method to stream data. If you run curl, you would see:\n  > curl -i -N http:\/\/localhost:9292\/  HTTP\/1.1 200 OK Transfer-Encoding: chunked Connection: close Server: thin 1.3.1 codename Triple Espresso  Hello Chunky Monkey Chunky Monkey Chunky Monkey # ... more monkeys \n  rack-stream also allows you to register callbacks for manipulating response chunks, and controlling when something is sent with different callbacks. Heres a more advanced example with callbacks added:\n  require rack-stream  class Firehose   include Rack::Stream::DSL    def call(env)     after_open do       chunk \\nChunky Monkey       close  # start closing the connection     end      before_chunk do |chunks|       chunks.map(&:upcase)  # manipulate chunks     end      before_close do       chunk \\nGoodbye!  # send something before we close     end      [200, {Content-Type => text\/plain}, [Hello]]   end end  use Rack::Stream run Firehose \n  If you ran curl now, you would see:\n  > curl -i -N http:\/\/localhost:9292\/  HTTP\/1.1 200 OK Transfer-Encoding: chunked Connection: close Server: thin 1.3.1 codename Triple Espresso  HELLO CHUNKY MONKEY GOODBYE! \n  For details about the callbacks, see the project page.\n  Up until this point, Ive only used curl to demonstrate hitting the rack endpoint, but one of the big benefits of rack-stream is that itll automatically recognize WebSocket and EventSource requests and stream through those as well. For example, you could write an html file that accesses that same endpoint:\n  Whether you access the endpoint with curl, ajax, or WebSockets, your backend API logic doesnt have to change.\n  For the last example, Ill show a basic chat application using Grape and Rails. The full runnable source is included in the examples\/rails\n directory.\n  require grape require rack\/stream require redis require redis\/connection\/synchrony  class API < Grape::API   default_format :txt    helpers do     include Rack::Stream::DSL      def redis       @redis ||= Redis.new     end      def build_message(text)       redis.rpush messages, text       redis.ltrim messages, 0, 50       redis.publish messages, text       text     end   end    resources :messages do     get do       after_open do         # subscribe after_open b\/c this runs until the connection is closed         redis.subscribe messages do |on|           on.message do |channel, msg|             chunk msg           end         end       end        status 200       header Content-Type, application\/json       chunk *redis.lrange(messages, 0, 50)            end      post do       status 201       build_message(params[:text])     end   end end \n  This example uses redis pubsub to push out messages that are created from #post\n. Thanks to em-synchrony, requests are not blocked when no messages are being sent. Its important do the redis subscribe after the connection has been opened. Otherwise, the initial response wont be sent.\n  What about socket.io?\n  socket.io is great because it provides many transport fallbacks to give maximum compatibility with many different browsers, but its pubsub interface is too low level for capturing common app semantics. The application developer doesnt have nice REST features like HTTP verbs, resource URIs, parameter and response encoding, and request headers.\n  The goal of rack-stream is to provide clean REST-like semantics when youre developing, but allow you to swap out different transport protocols. Currently, it supports normal HTTP, WebSockets, and EventSource. But the goal is to support more protocols over time and allow custom protocols. This architecture allows socket.io to become another protocol handler that can be plugged into rack-stream. If you wanted to use Pusher as a protocol, that could also be written as a handler for rack-stream.\n  rack-stream aims to be a thin abstraction that lets Ruby developers write streaming APIs with their preferred frameworks. I plan to broaden support and test against common use cases and popular frameworks like Sinatra and Rails. If you have any questions or comments, feel free to submit an issue or leave a comment below!","item_date":"May 24 2012 22:19:33","display_item_date":"05-24-2012","url":"http:\/\/intridea.com\/blog\/2012\/5\/24\/building-streaming-rest-apis-with-ruby","source":"intridea.com"},{"title":"What does a digital asset management API do?","details":"You may have heard the term \u201cAPI\u201d before, and you may know that it stands for Application Programming Interface\u2014but what does that mean?\nJust like a user interface that \u00a0lets people access your digital asset management system, an API lets other computer systems access your DAM.\nIf you\u2019re not in IT you may be thinking \u201cSo what? How does that help me?\u201d. Well, let me give you an example of how letting a website access your DAM via an API can help generate revenue:\nA stock photography company uses the Portfolio API to synchronize their local image management workflow with a cloud-based ecommerce solution. Previously, images were tagged twice\u2014once during initial ingestion into the DAM from contributing photographers, and once again after uploading to the ecommerce website. Instead of manually uploading and re-tagging images a second time in the ecommerce platform, synchronization between the in-house workflow and ecommerce website now takes place automatically via a custom Portfolio API script.\nThe images and metadata managed within Portfolio Server are automatically transferred to an ecommerce website, enabling efficient monetization of the digital asset collection. In addition, the API provides a programming toolkit to web developers that offers robust image processing tools like batch format conversions, image previewing, and video streaming.\nFor more examples like this, along with more information about the Portfolio API and other enterprise DAM\u00a0technologies,\u00a0check out our Enterprise DAM Technology whitepaper.","item_date":"May 24 2012 22:19:22","display_item_date":"05-24-2012","url":"http:\/\/blog.extensis.com\/dam\/what-does-a-digital-asset-management-api-do.php","source":"blog.extensis.com"},{"title":"API Development, XML, and JSON Connectivity","details":"Web Service Application Programming Interface also known as API is essentially a set of programming standards and instructions that is used to access a Web-based software application. It is an interface which allows software companies to design application that can be used by other developers to design their own products. \n  An Application Programming Interface (API) is a software to software application and not a user interface as some people may want to believe. APIs are used more commonly on ecommerce solutions and applications especially with Payment gateway integration processes. It is the application that allows or facilitates the safety transfer of monies through the verification and processing of credit card information.\n  At Software Inc, we have used our technical expertise to severally optimize APIs in web application development. APIs has in fact been used to create applications for many clients who want to have an interface with several other social media sites like Facebook or Flickr. API application has facilitated web interaction and data exchanges between communities and applications.\n  We have developed online solutions for a variety of clients using API, travel companies who want to build a very functional and modern have benefited mostly from our incorporation of APIs on their website which affords them the services of Google Maps.\n  Also, APIs are profoundly better than the more average links which many businesses have been wasting their time and effort using, Using Google Map API on your website will allow your most esteemed clients to not only see your business location but also have access to location description services that will allow them to find their way to you.\n  XML is one of the most used computer languages in these times, and it basically means Extensible Markup language. It is mostly used for documents as it allows it to be delivered in a flexible text format; it is a set of codes that structures documents into pictures, words or data.\n  The work of XML is closely related to that of HTML but the two still retain very different characteristics and task delivery. While HTML is basically to format and display data, XML, on the other hand is used to transport data.\n  XML is therefore a lot more broad and useful than HTML. XML has a wide array and an unlimited possession of tags which makes it to support several types of web applications.\n  At Software Inc, we have a deep knowledge of all this web application tools and we know how to use them to optimize services.  Applications like XML give our versatile professionals the technical leverage to accomplish many things with more ease, which means we will deliver to you with speed and excellence.\n  Our professionalism is widely extended to mobile phone applications like iphone and blackberry and we can create JSON web services in C# with data base connectivity. Likewise we can create JSON web   \n  \nAfter trying 3 different products in the market, I was hopeless to start my own business. Then I found Gripsell, there is no looking back since then. One of the finest and best product, I have seen. Everything is so organized and the control panel is very friendly.\nDavid Kremenetsky\n Highly recommended. One thing that separates out the already best product of Clone Portal, is their support. If you are setting up a new business, go with Clone Portal. \nMike Rooney\n I am glad that I havent made a wrong choice by selecting Gripsell as our technology enabler.\n James Jr.\nAm a fan of Gripsell. You exhibit such a high level of professionalism that I just tell what I need and relax. Keep up the good job.","item_date":"May 24 2012 22:18:59","display_item_date":"05-24-2012","url":"http:\/\/gripsell.com\/api-development-xml-and-json-connectivity","source":"gripsell.com"},{"title":"Microsoft holds hackathon before its Windows 8 launch","details":"With Microsoft geared to launch Windows 8 later this year, developers with an idea in mind and with a dream to see their app on the Windows Store made their way to the event. \n  \u201cWe were asked to design applications from twenty categories such as education, science, games, etc. I created a game application called Brainvita,\u201d said Ms Ahuja, who was given 36 hours to create the app. There are certain certification requirements on the basis of which applications for the store will be selected. \n  Things such as alignment, look and feel are some factors which will be taken into consideration, say participating developers. \n  \u201cThis is the fourth hackathon that I have participated in. The USP of the Windows 8 applications is their user friendliness,\u201d said Mr Ravi Raj, a Bangalore-based app designer who aids other developers to create apps. \n  \u201cAlmost 74 of the applications that I have designed have been selected for the Store,\u201d he said, referring to the apps selected from his participation in the Hyderabad, Bangalore and Pune. \n  Ballmers pat for India \n  Commenting on the rich talent pool of developers in India, Mr Steve Ballmer, CEO of Microsoft Corporation, said, \u201cIndia is an incredible bastion of software developers. On a global scale, 25 to 30 per cent of all the software is written by people in India.\u201d \n  The new version of the Windows will be cloud-connected and the users will be able to access it on a variety of devices. \n  The Windows Store will be similar to the Apple store where users will be able to buy and download apps from Microsoft and app developers.","item_date":"May 24 2012 22:15:38","display_item_date":"05-24-2012","url":"http:\/\/www.thehindubusinessline.com\/industry-and-economy\/info-tech\/article3452956.ece?homepage=true&ref=wl_home","source":"www.thehindubusinessline.com"},{"title":"The Next SoundCloud Powers A Sonic Hackathon","details":"Last week, music aficionados gathered around the world, aiding SoundCloud\u2019s mission to unmute the web by celebrating sound (and creating lots of it).\n           \n                   In Oxford UK last week, a group of audiophiles gathered around musician Neil C. Smith, and watched him make music. Not exactly unusual behavior, but the circumstances were. This wasn\u2019t a concert, or a busking, or even an impromptu street jam, but a well coordinated event celebrating the tool that made the music possible at all. It was also an event that was being replicated, more or less, in 160 locations around the world.\n  SoundCloud\u2019s Global Meetup Day on May 17th stretched from Boston to Berlin, and everywhere in between, with over 1,300 registered attendees. It\u2019s the kind of event that\u2019s becoming more common in the age of Twitter and Facebook--they encourage well executed and often unexpected uses of a tool that drive that tool\u2019s innovation further. In other words, they\u2019re like massive hackathons, meant to stretch the boundaries of the tool, inspire people to use it to innovate, while also testing the adaptability and usability of the tool to guide internal future innovations. \n  The Global Meetup Day was organized internally, socially, and by a handful of designated SoundCloud Heroes, ambassadors chosen to help organize events at the local level. Fans of the music sharing site connected over their shared love of sound, made collaborative recordings, and tested the limits of what\u2019s possible with the private beta version of the more social Next SoundCloud. By doing so in person, they also proved that these sounds have echoes that reach beyond the web.\n  The biggest surprise was how large of a movement Global Meetup Day has become. It all started two and half years ago with approximately 4-5 meetups, but SoundCloud now has one of the top 10 local communities on Meetup.com, says David No\u00ebl, Community Evangelist for SoundCloud. \n  The social aspect of the event extended into the sounds accumulated during it and curated for it. At the Oxford meetup, after composer Malcolm Atkins spoke about his experience scoring films, Neil C. Smith used his own music software called Praxis, which can be downloaded and used for free on Google Code, to mix various shared tracks stored on the SoundCloud group dropbox. These tracks included birdsong, footsteps, actual beats, and other ambient noises. Eventually Malcolm joined Smith with some live improvisation too.\n  \n  Elsewhere in the world, Jeremy Johnston gave the Syracuse group a tour of SubCat Music Studios, a world-class recording studio in downtown Syracuse, and home to all manner of top-dollar vintage gear. Chicago SoundCloud Hero Jaime Black recorded his meetup as a podcast and livestreamed the event.\n  \n  In Berlin, the over 200 attendees glommed onto a surprise demonstration of Next SoundCloud by CEO Alexander Ljung. Although all participants were given passwords to try out the beta version of Next SoundCloud, they didn\u2019t have to wait until they got home to use the new version, which can record anything and is instantly shareable.\n  The feedback from our first Community private beta users has been amazing, says No\u00ebl. Even with this being only a first step of the next phase, SoundClouders have been excited about the repost and sets feature, as well as, the continuous playback and keyboard shortcuts. We\u2019ve also seen a lot of positive response about the new user interface and design. We can\u2019t wait to roll out more features and updates.\n  Although it\u2019s always interesting to see what sort of neat results materialize from an event like this--or, rather, hear those results--perhaps the most important thing is to simply put the people who love a tool the most in a room together and watch what happens. Eventually, they\u2019ll show the creators how their tool is even more useful than they imagined.","item_date":"May 24 2012 22:15:21","display_item_date":"05-24-2012","url":"http:\/\/www.fastcocreate.com\/1680841\/the-next-soundcloud-powers-a-sonic-hackathon","source":"www.fastcocreate.com"},{"title":"Official Google Blog: Transparency for copyright removals in search","details":"We believe that openness is crucial for the future of the Internet. When something gets in the way of the free flow of information, we believe there should be transparency around what that block might be. \n\nSo two years ago we launched the Transparency Report, showing when and what information is accessible on Google services around the world. We started off by sharing data about the government requests we receive to remove content from our services or for information about our users. Then we began showing traffic patterns to our services, highlighting when they\u2019ve been disrupted.","item_date":"May 24 2012 19:58:35","display_item_date":"05-24-2012","url":"http:\/\/googleblog.blogspot.com\/2012\/05\/transparency-for-copyright-removals-in.html","source":"googleblog.blogspot.com"},{"title":"Groups: A new view, new API methods, and add from Uploadr","details":"The community you build is what makes Flickr truly  unique. And Flickr Groups are the pulse of the community on Flickr. So  we\u2019re very excited to share three Group announcements with you.\n        Justified View\n        Since the Justified View appeared on Photos from your Contacts and on your Favorites page you have been asking us to bring the same experience to groups. So starting today,  you can browse the 1.2 billion photos in Flickr groups in our new Justified  View.\n    \n        But there\u2019s more: not only are Group pools available in Justified View,  we\u2019ve also created a sidebar that makes the  entire Groups experience more interactive and simpler to navigate. As you  scroll through the photos in a Group Pool, the new sidebar persistently gives you  context on what the group is about. It also surfaces the most recent  discussions happening in the group and highlights the top tags as well as the top  contributors for members who want to dig a little deeper.\n        For your viewing pleasure, we\u2019ve picked  out some amazing Groups that we thought you might want to check out (and  join!). Take a look at Less Is More,  panoramas, and U.S. National  Parks. \n    \n        Lots of you have  asked us to make it easier to get your photos into a group and we\u2019ve just added  the ability to upload photos directly to your groups within the new Flickr  Uploadr.\u00a0 Just click on Add to  Group in the editing panel to access your existing groups and presto! Your photos will be sent to the group at the same time that they\u2019re  published on your photostream. \n        New Groups API methods, new possibilities\n        We are also very excited to release several new API methods that enable our members to interact with group discussions  such as reading topics and replies, and posting to  group discussions. The  release also includes methods to join and leave a group (that may be of special interest  to you mobile developers out there).  You can find more information  regarding these new methods on our API documentation page. Or you can head over to the code.flickr blog to read Jamal\u2019s post on the new API methods.\n        We continue to work hard on the new Flickr. Stay tuned! We created a new thread for bugs and feedback for justified group pools and the new sidebar; head over to the existing thread if you encounter issues with the new Add to Groups option in Uploadr.","item_date":"May 24 2012 19:58:34","display_item_date":"05-24-2012","url":"http:\/\/blog.flickr.net\/en\/2012\/05\/24\/groups-a-new-view-new-api-methods-and-add-from-uploadr\/","source":"blog.flickr.net"},{"title":"NPR slideshows reach iPad browsing, API : Inside NPR.org : NPR","details":"Were making a major upgrade today to all of NPRs slideshows. Our slideshows are now iOS-friendly and available in our API.\n                           Previously, NPR.org slideshows played in Flash, which iPhones and iPads couldnt display. The lack of API distribution also made these slideshows unavailable to our mobile apps and member station sites. In late fall, we began to change our production. We created a new slideshow experience in our NPR Music live event pages, which included a new player and distribution process.\n                           In todays move, we expand the use of that technology to all of NPR.org. We are upgrading more than 2,000 slideshows: those from news stories, music stories and our Picture Show blog.\n                           These slideshows include more than 30,000 images. If you browse NPR.org on your iPad, you can now view all of these images, swipe between images and tap for captions. Weve improved the slideshows buttons and behaviors for a better experience \u2014 wherever you seeing them. We plan to display slideshows across many more NPR platforms in the future.\n                           If you use the NPR API, you can access many of these images.\n                           More than 13,000 images, including more than 3,000 NPR images, are available to local station sites immediately. NPRs Digital Services division, which works with local stations, already has NPR.org slideshows flowing into the Core Publisher platform and plans to optimize the experience further in the future.\n                           For all API users, more than 8,000 images are now available.\n                           To find details about using slideshows from the API, read the second half of this previous Inside NPR.org blog post, under Collections. Every story that has a slideshow should have a collection marked as type slideshow. An update to the earlier post is that the output is available in NPRML and now JSON.\n                           Keep in mind NPR slideshows use images from a wide variety of sources, and we dont have rights to distribute all of our images. But we continue to work toward as much distribution as possible.","item_date":"May 24 2012 17:48:09","display_item_date":"05-24-2012","url":"http:\/\/www.npr.org\/blogs\/inside\/2012\/05\/23\/152828200\/npr-slideshows-reach-ipad-browsing-api?ft=1&f=91000411","source":"www.npr.org"},{"title":"Enterprise APIs: Where to Now?","details":"This guest post comes from\u00a0Mark   O\u2019Neill who is CTO with Vordel, a provider of Application Gateways to link all enterprise applications, users, and devices across Cloud, mobile, and on-premise environments.\n\n\n  Cloud, mobile and social media are now mainstream computing concepts.  Today, consumer and business users all demand access to applications and data from multiple devices, inside and outside of enterprises, on a continuous basis.  As a result, users interact with an enterprise through many different interfaces which all converge at the  API layer. A well-executed enterprise API strategy will ensure API usage is tracked and security is achieved, enabling enterprises to create more selling channels, improve engagement with customers and prospects and offer more value to partners.\n\n\nWhile API adoption has snowballed in recent times and the API economy is thriving API management as both an IT and business discipline is still playing catch-up, especially around the area of enterprise API management. The huge popularity of consumer-oriented APIs, which are typically used to transmit information for public consumption has meant that most related API management practices focus on developer enablement portals which provide self-service options for an open community of developers consuming public-facing APIs.\n\n\nHowever, change is rapidly happening within the area of enterprise APIs as organizations increasingly adopt enterprise APIs for their ability to transmit sensitive information and execute business transactions.\n\n\nEnterprise APIs\n\n\nA key difference between consumer oriented and enterprise APIs, is that enterprise APIs typically handle confidential information, with higher value business transactions and demand a stricter level of regulatory compliance. This means that enterprise APIs require a robust management strategy to monitor for inappropriate usage, as breaches could result in monetary loss and\/or raise compliance issues. As such, designing an API delivery strategy for enterprise APIs demands a higher level of operational requirements than designing a more consumer focused one. Furthermore, the development of an enterprise class API management platform introduces new technology parts to the existing business application infrastructure \u2013 which can cause potential integration issues. Additionally, the secure and scalable delivery of enterprise APIs requires advanced security, integration and runtime middleware.\n\n\nAPI Management Lifecycle\n\n\nTo effectively design an enterprise API delivery strategy, IT managers need to understand the API management lifecycle. Within this context, API management could be considered the next evolution of Service Oriented Architecture (SOA), but extended beyond the enterprise with web-centric architecture. Figure 1 below shows the 5 phases of API management and the technologies required at each stage to build out a comprehensive API management platform.\n\n\n\n\n\nReference Architectures\n\n\nAn enterprise\u2019s choice of API management platform architecture will be driven by three factors; the type of APIs the enterprise needs to deliver, the readiness of its source APIs and integration requirements. It is worth noting that integration into existing systems can be the most challenging and complex aspect of enterprise API delivery.\n\n\nTypically, enterprises will require a flexible API management platform offering a high level of security, integrity and integration capabilities.  While there are several architectural patterns to choose from, the two-tier API delivery platform detailed below is perhaps the most common for the scalable delivery of enterprise APIs.\n\n\n\n\n\nTwo-Tier API Delivery Platform\n\n\nThe two-tier API delivery platform provides flexible and scalable solution by separating the portal tier from an additional API gateway tier. The two-tier API delivery platform provides an enterprise with the flexibility to manage enterprise APIs, backend APIs requiring non-trivial transformation and orchestration and situations where a standalone identity repository for API management is not acceptable. It is also useful in scenarios where support is required for existing trusted relationships, security protocols and certificates, as well as non-trivial access federation scenarios.\n\n\n\n\n\nDepending on the extent of the integrations, many scenarios will require the API gateway to be situated on-premise. The two-tier architecture offers the options of being deployed locally, in the cloud or in a hybrid model. The portals can be deployed either on-premise or in the cloud, independent of the API gateway deployment location. While there are strong reasons to deploy partner and provider portals in the Cloud, an internal developer portal is probably best suited to an on-site deployment.\n\n\nConclusion\n\n\nAn enterprise needs to understand the type of APIs it will deliver and consume as part of its overall API management strategy and plan its architectural approach accordingly. A successful enterprise API management strategy is developed as part of an overall integration strategy that incorporates existing on-premise infrastructure and new Cloud and Mobile environments. To achieve success, enterprises should consider an API container to control the administration, monitoring, security and transformation of all API traffic.","item_date":"May 24 2012 17:45:03","display_item_date":"05-24-2012","url":"http:\/\/blog.programmableweb.com\/2012\/05\/23\/enterprise-apis-where-to-now\/","source":"blog.programmableweb.com"},{"title":"Opalesque Exclusive: Hedge funds leverage social media for trading decisions - Opalesque","details":"Bailey McCann, Opalesque New York:  According to data from Twitter, when last years Washington D.C. earthquake hit, users in New York read about it on Twitter 30 seconds before they felt under their feet. Ever since Twitter started breaking news through user networks, a variety of social media monitoring firms have been working to cash in on this edge by providing monitoring and analysis of social media services for busy company executives. Now, that trend is moving into hedge funds. US-based firm Gnip is providing hedge funds with a real time data feed that they are using to influence trading decisions.\n  Gnip was founded in 2008 when social media was making a splash in the consumer advertising space. Now, it is the largest provider of social data to the enterprise and delivers 90 billion public social data activities each month. In recent years other, less product focused industries have started to leverage social media monitoring to understand public moods and capture breaking news, avoiding the lag between when the news actually breaks and when it gets picked up by traditional media - if it gets picked up at all.\n  In essence, Gnip provides an aggregate data feed of public social media posts around a given topic. Depending on the agreement with the client, the firm may provide an additional layer of information such as sentiment or textual analysis. So far, a dozen funds amounting to $30bn in assets under management are leveraging this data feed to impact their trading choices.\n  Funds that choose us are typically $1bn to $5bn AuM, with a heavy quant slant, says Seth McGuire Director of Assets and Financial Technology at Gnip in an interview with Opalesque. The real time data can be a valuable factor in high frequency or algorithmic based trading.\n  According to McGuire, the data feed can provide some intelligence into trading signals happening throughout the trading day or during significant world or market events. Funds that already leverage more traditional real time news feeds such as Bloomberg or Thompson Reuters can add in social media data and often see the signal before it breaks on other feeds.\n  Academic researchers are also studying the effects of social media on trading decisions. Last year, the University of Munich launched TweetTrader to predict stock trends based on tweets. University researchers also published a paper examining how social media might be telegraphing trading signals. Since then, similar studies have been published by the University of Utah and the University of California at Riverside confirming results from the University of Munich work that social media can, in fact, impact trading choices.\n  \u0093These findings confirm the value of social data as an important source for financial companies looking for value-adding applications for their firms,\u0094 Mcguire says.\n  For its part, Gnip recently partnered with StockTwits a popular, curated equities twitter feed that provides real time chatter on market movements. Mcquire notes that they were specifically interested in StockTwits because the feed is curated. The team at StockTwits filters the stream so that there is less noise.\n  The noise Mcguire speaks of can be anything from spam bots to false or irrelevant information. When asked how Gnip and its partners control for bad information in the feed Mcguire explained that the network itself typically stamps out false threads. If you look at the data in aggregate, if users start tweeting something that is false it might get sent around early on but there is a greater push back from the network itself showing that the piece of information is false.\n  People are starting to understand now that social media works like other news sources, you can cull information from it but you have to look at the whole picture, he said.","item_date":"May 24 2012 00:57:27","display_item_date":"05-23-2012","url":"http:\/\/www.opalesque.com\/industry-updates\/2596\/hedge-funds-leverage-social-media-for-trading.html","source":"www.opalesque.com"},{"title":"Federal CTO and CIO Stress Importance of APIs and Open Data","details":"Federal CTO Todd Park and CIO Steven VanRoekel announced at TechCrunch Disrupt NYC this week the governments initiative to open up data to developers to help drive innovation of new applications and services based on that data. What was once chained up in government documents and mysterious backend systems, will now be pushed towards making government data more social -- ultimately changing the meaning of social participation -- and transforming the the new default to Open Data, where everything should be an API to allow for data, applications and services to be made available anywhere, anytime and on any device. \n The U.S. Governments push for this new digital roadmap is designed to decrease the inefficiencies in government and to allow developers to build applications that the government would never have thought of. In order to improve upon the governments efficiency levels, the federal government plans to ensure that all agency websites have a \/DEVELOPERS page. The governments campaign for innovation has also been powered by meet up and hackathon sponsorships through which developers get the opportunity to demo new apps built on government APIs. \n Here are the first five projects that were announced as part of the governments Open Data campaign: \n *Launching Blue Button for America: creating apps to allow U.S. citizens to access their own health data\n *Introduce RFP-EZ: allows startups to have access to compete for government projects\n *Launching MyGov portal: user-friendly website for government services\n *Launching 20% Campaign: initiative to move from cash payments to mobile payments overseas\n *Open access to data from industries such as energy, education, non-profits and safety.","item_date":"May 23 2012 23:14:25","display_item_date":"05-23-2012","url":"http:\/\/blog.mashery.com\/content\/federal-cto-and-cio-stress-importance-apis-and-open-data","source":"blog.mashery.com"},{"title":"Confessions of a First-Time Hacker: From The Trenches of the TechCrunch Disrupt NY 2012 Hackathon | Mashery Blog","details":"It was 10am on a bright NY morning.\u00a0 Joggers, cyclists, and brunch goers were out and about, and here I was inside a hanger-like pier on the Westside of Manhattan. This would be my first Hackathon. 24 hours to build a music app.\u00a0 This was my goal. To create a simple location-based app that polled free and accessible data.\u00a0\n My concept was called BarTunes.\u00a0 I wanted to have a way to display an interactive map of restaurants\/bars where a user can quickly see what\u2019s \u201cplaying now\u201d.\u00a0 Quickly - a user could see that MGMT is playing at Caliente Cab or Miles Davis is playing Kettle of Fish.\u00a0 The type of music would drive user behavior in a fun, and \u2018on-the-go\u2019 way.\n This turned out to be impossible, or rather not possible in the 24 hours.\u00a0 So we had to pivot.\u00a0 Instead we crafted a new music discovery app called CityVibes. This new approach would allow users to discover what music is popular in US cities from Shazam tag data.\u00a0 What made the project more unique was that it created a real-time playlist that included a mood rating (e.g., is your city hot or not?).\u00a0 The app also allowed you to play the tunes instantly on your iPhone.\u00a0\u00a0 I won\u2019t give you all the gory details but we utilized Rdio\u2019s APIs (on the Mashery Platform), Twitter, Shazam data, and capped off with a magical API from The Echo Nest, which allowed for song analysis.\n Thankfully, the TechCrunch organizers made it easy to find a team.\u00a0 A Google Docs spreadsheet listed what folks had to offer, and what they were looking for. \u00a0I was fortunate to meet a wizard of a coder, Pavan.\u00a0 After describing my concept, Pavan began thinking through what\u2019s feasible, and what platform it should reside on.\u00a0 An iPhone application was his desired platform (I\u2019m on Android), and this type of hacking was out of my league.\u00a0 We ultimately figured out roles: I would create the upfront conceptual API architecture, flows and GUI, and Pavan would handle the iPhone development and API connectivity.\u00a0 \u00a0\n At 4am, Pavan was troubleshooting -- and I was crashing.\u00a0 We were surrounded by hackers in a \u201cpost-apocalyptic control room\u201d meets \u201cindustrial space\u201d meets \u201cjunk food bar.\u201d At this point I found a comfortable spot on the concrete and closed my eyes for a bit.\u00a0\n At 8am, we had 90 minutes until the app was due.\u00a0 I was frantically cutting and resizing assets in Photoshop, and each new build was looking better and better.\u00a0 \n There were about 100 teams and each had only 60 seconds to present.  Fearing that I would miss any detail, I decided to script the whole pitch. Pavan would do the demo on camera, and I would do the talking.\u00a0 \n At our turn, I was filled to the brim with 5 hour energy, feeling dizzy, and about to present in front of the  crowd on the main stage.\u00a0 When our number was called, I don\u2019t know if it was the caffeine or the lack of R.E.M., but I belted out the pitch like a car salesman in 40 seconds, and barely touched upon the app.\u00a0 There was applause, then we were 86\u2019d off the stage.\u00a0 Argh! I realized I could have slowed it down, and followed my partners demo and cues more closely and kept thinking, Can we please go back in time?\u00a0 This one-minute pitch is rough!\n  I left the presentation in a fog, and crashed out feeling ill - missing the award ceremony.\u00a0 My mind was gone, and my body was broken into a pulp.\u00a0 A pillow was all I wanted.\u00a0 The clock radio said 6pm\u2026.Have I been sleeping all day?\n Lying there exhausted ,I reached for my mobile trying to focus my eyes on a new message I received 3 hours ago.\u00a0 It was from my hacking partner.\u00a0 \u201cWe won\u201d\u00a0 Our app was awarded for best use of the Echo Nest API.\u00a0 I couldn\u2019t believe it.\u00a0 I did a couple of fist pumps and laid back down with a smile, ready to do it all over again.","item_date":"May 23 2012 23:14:03","display_item_date":"05-23-2012","url":"http:\/\/blog.mashery.com\/content\/confessions-first-time-hacker-trenches-techcrunch-disrupt-ny-2012-hackathon","source":"blog.mashery.com"},{"title":"The Perfect Ratio: New Hackathon Brings Ladies and Gents Together to Build Cool Products | Betabeat","details":"If you\u2019ve ever been to a hackathon, you\u2019re probably well-acquainted with the fact that the majority of attendees and participants are usually dudes. The paucity of gender diversity in tech is a controversial issue, but one that probably won\u2019t be solved by responding to the\u00a0over-saturation\u00a0of Y chromosomes with an all-female hackathon. That\u2019s where Hack\u2019n Jill comes in.\n Sponsored by Appnexus and hosted at the company\u2019s headquarters, Hack\u2019n Jill is a weekend-long summer hackathon that will bring together 50 men and 50 women in an effort to \u201ccreate an environment where both genders feel welcome to build cool things together.\u201d\n The theme of the event, which will take place June 15th and 16th, is #HackYourSummer: \u201cThink about what makes summer great \u2014 from going out and being active to chilling at home with friends \u2014 and be inspired to make it even better,\u201d explains the Hack\u2019n Jill blog.\n \u201cWe went through a couple iterations of the event, and we ended up settling on a 50\/50 split of guys and girls because we wanted to expose guys and girls working together, as opposed to just have women working on their own,\u201d Kara Silverman, one of the event\u2019s organizers, told Betabeat by phone. \u201cWe also wanted to create female advocates through experience.\n \u201cWe were looking to demonstrate that it\u2019s your skillset that should be emphasized,\u201d added Eugenia Koo, another Hack\u2019n Jill cofounder. \u201cWe really wanted to concentrate on promoting this diversity and really the type of positive outcomes that can come about through an event like this.\u201d\n Ms. Silverman said she had been to many tech events where the dearth of women was clearly visible. \u201cIn the line to pitch at Startup Weekend Mobile, there were maybe 50 people and 5-10 of them were women,\u201d she said.\n But in a city with far fewer female devs than male, won\u2019t it be difficult to achieve that perfect male\/female ratio?\n \u201cSo far the signups are really, really close to 50\/50,\u201d said Ms. Koo. \u201cWe might have maybe three or four more guys than girls, but it\u2019s right on target.\u201d\n The Hack\u2019n Jill team is hoping that with the success of their inaugural event, they can expand the event to an annual experience.","item_date":"May 23 2012 23:11:57","display_item_date":"05-23-2012","url":"http:\/\/betabeat.com\/2012\/05\/the-perfect-ratio-new-hackathon-brings-ladies-and-gents-together-to-build-cool-products\/","source":"betabeat.com"},{"title":"ChowNow Extends Functionality with API Release","details":"ChowNow originally changed ordering takeout by allowing customers to place an order directly from a restaurant\u2019s Facebook page. Recently, ChowNow has launched the ChowNow API which takes ordering to another level. The API allows developers to deliver the same functionality of the Facebook application with any application a developer desires. \u00a0For instance, customers can now browse a restaurant\u2019s menu from the particular restaurant\u2019s webpage, place an order, the payment method is processed, and the restaurant\u2019s bank account is credited without leaving the restaurant\u2019s webpage.\n\n\n\n\n\nThe integration potential of the API stretches as far as a developer\u2019s imagination will reach. Developers can integrate the API with a POS system and CRM package to track order history and make future suggestions based on a customers previous actions. The integration capabilities of robust Contact Center applications that have revolutionized the hospitality industry over the past decade has now leaped from telephone conversations to a web experience.\n\n\nChowNow CEO, Christopher Webb, targeted Facebook functionality as a launchpad because over seventy percent of US restaurants have a Facebook presence, but cannot always afford a desirable web presence. ChowNow\u2019s Facebook and mobile success has led to more investors and encouraged expansion to new platforms. The API is currently in private beta. Interested developers can e-mail info@chownow.com or click here.","item_date":"May 23 2012 20:29:15","display_item_date":"05-23-2012","url":"http:\/\/blog.programmableweb.com\/2012\/05\/23\/chownow-extends-functionality-with-api-release\/","source":"blog.programmableweb.com"},{"title":"Billions of API calls traversing Web, redefining \u201csoftware\u201d","details":"Online services such as Google, Facebook, Netflix and eBay are handling billions of application programming interface (API) calls per day and some companies are accounting for billions of dollars in revenue per year via API links to their services, according to John Musser, founder of ProgrammableWeb.\n\n\nMusser, speaking Wednesday at the annual Glue Conference in Broomfield, Colo., highlighted ballooning statistics his company has collected and laid out the 10 hottest trends in the API market, including growth rates, funding, protocols, and business models. The Programmable Web maintains a database of open APIs.\n\n\n\u201cAPIs are how we are going to build software in the future,\u201d he said. \u201cWe are just going to glue it together.\u201d\n\n\nMusser presented his list of the Top 10 API trends, in no particular order:\n\n\nAPIs are quickly becoming a necessity for online services and for enterprises as users demand application access from anywhere with any device. That demand has fueled API growth.\n\n\nAn API is a set of functions that allow computer programs to talk and share data.\n\n\nThe Programmable Web now has 6,000 open APIs listed in its directory. \u00a0That is up from 5,000 just three months ago. In contrast, it took eight years for the directory to count its first 1,000 APIs. Those numbers do not include countless private APIs that are mostly used to support mobile apps.\n\n\nNearly 15% of the 359 enterprise APIs listed in the ProgrammableWeb directory were added in the past three months.\n\n\nThe major difference between enterprise and consumer focused APIs is that enterprise APIs typically handle more sensitive data and transactions. In addition, enterprises need to manage and secure access to those APIs, using protocols such as OAuth.\n\n\nMusser noted the \u201cbillionaires club\u201d of API calls with Twitter handling 13 billion per day, up from three billion in 2010. During this month, Netflix is handling 1.4 billion per day and Klout an even one billion. eBay was handling one billion per day for the first three months of 2012.\n\n\nAnd bigger numbers are on the horizon. He noted that Amazon Web Services next month will hit one trillion objects in its Simple Storage Service (S3).\n\n\nMusser noted that Expedia\u2019s affiliate network counts $2 billion worth of business a year via APIs.\u00a0 Musser quoted Expedia executives saying that 90% of what they do is business through APIs.\n\n\nDevelopers are demanding programmable access to the most functional parts of applications. And end-users are doing the same, although they don\u2019t really know it, as they devour Twitter-based or Facebook-based applications or, closer to the enterprise, SalesForce.com-based apps. More than half of Salesforce.com\u2019s traffic comes through APIs.\n\n\nAnd APIs make those applications device-agnostic - smartphone, tablet, PC, DVR, kiosk, in-car computer, gaming console and other platforms.\n\n\nMusser said companies are using money to incent users to build on top of their APIs.\n\n\n\u201cOnce you have an API, how do you get people to build on it,\u201d he said. He pointed to Twilio and Box as companies giving developers money for building on top of their APIs.\n\n\nHe said REST and JavaScript Object Notation (JSON) are the dominant protocols for APIs, saying that 96% of the protocols in social APIs are REST. JSON is used as the data format in 60% of REST APIs.\n\n\n\u201cJSON is a trend, but most people did not see that one coming,\u201d he said. \u201cNearly one in three APIs this year is JSON only.\u201d\n\n\nHe also noted that Hackathons are a popular method for rallying developers behind an API. There were 160 Hackathons in the first quarter of 2012 and last year the top prize taken home from a Hackathon was $100,000. Musser said companies such as Twilio and Stripe (online payments) think of their API as a product for developers and that companies such as SupermarketAPI  (grocery industry) use the API as a brand.\n\n\nIn addition, Musser said multiple business models are emerging to go with pay-as-you-go (Amazon Web Services), \u00a0and unit-based (Google AdWords).\n\n\n\u201cThe biggest trend may be the indirect model,\u201d he said, including one-time sign-up (Jigsaw), content syndication (New York Times), and internal uses such as mobile devices (Netflix).\n\n\nFinally, Musser said invisible mashups are trending, such as the inclusion of the Twitter API in Apple\u2019s iOS operating system.\n\n\nThe Glue Conference runs through Thursday. Follow it on Twitter at #gluecon","item_date":"May 23 2012 20:26:22","display_item_date":"05-23-2012","url":"http:\/\/www.zdnet.com\/blog\/identity\/billions-of-api-calls-traversing-web-redefining-8220software-8221\/493","source":"www.zdnet.com"},{"title":"Old SOA versus new SOA? Open APIs change the game","details":"As the high-profile Oracle-Google Android trial has continued, it has cast light on the changing nature of the Application Programming Interface (API). Oracle Corp., Redwood City, Calif., contends that its Java APIs can be copyrighted. Palo Alto, Calif.-based Google demurs.\n  The first view recalls the days when big companies only shared APIs as part of complex \u2013 and expensive \u2013 corporate alliances. The latter view reflects the API ethos of the Internet era, where Web APIs are open and freely shared.\n  So, the API landscape is changing. This evolution of the API is front and center in an ongoing shift in the SOA world, one increasingly driven by emphasis on wider use of light-weight JSON and REST services.\n  The move is reflected further in new API management products that supplement established SOA governance and management tool sets. Observers note that, although RESTful strategies and APIs seem to be staging a revolution, it is a revolution based on SOA principles and one where there is still room for some more traditional SOA-based approaches.\n  It may be said that today\u2019s Web APIs more truly reflect the nature of the Web, compared to earlier so-called Web services. But the Web services and SOA experiences were important ones. James Governor, an industry analyst with RedMonk, says the hard work done by organizations to isolate and encapsulate services is \u201cterribly valuable.\u201d While he admits the original Web services stack has come under considerable scrutiny, he says \u201cit does have value, but not for everybody.\u201d \u00a0\n  In his view, the SOA work will maintain its worth because it is always amenable to use in the newer Web technologies, for both development and integration. \u201cThe hard work has been done; now we can see how the Web does this.\u201d\n  Learning from the API management approaches we are taking from the Web will have a significant impact on enterprises,\u201d he notes. \u201cEveryone is retooling for API management; it is like API management is the new SOA,\u201d he says.\n  Governor says recently announced API management capabilities from IBM Corp., Armonk, N.Y., means APIs have arrived. [The company discussed IBM Cast Iron Live Web API services earlier this month at IBM Impact 2012.] But other SOA stalwarts such as Layer 7 Technologies, Vancouver, B.C., and SOA Software, Inc., Los Angeles, Calif., had been chanting the API management mantra ahead of Big Blue.\n  Scott Morrison, CTO and chief architect at Layer 7 Technologies, has seen and participated in the SOA community. Today, he says his company is focused on management and security for APIs and SOA based architectures. \u201cWe have been through the whole SOA revolution and what it meant to the new revolution for APIs,\u201d he says. He describes that revolution as \u201ca move toward agility and simplicity and away from complexity and formalization.\u201d The company has added SecureSpan API Proxy, the Layer 7 and the API Portal OAuth Toolkit to its lines of SOA governance and gateway tools.\n  Another established SOA hand sees a similar trend. \u201cThe term API being bandied around the industry today is quite different from the traditional developer API,\u201d said Ian Goldsmith, vice president of product marketing, SOA Software. \u201cAs used today it refers to an interface exposed by an application for developers to consume over the network, not a proprietary interface. Today, it is standards based.\u201d Goldsmith\u2019s firm has added Atmosphere API management tools to lines that include its mainstay SOA governance software suite.\n  Open, public APIs\n  A key shift has been the culture of openness. \u201cOne reason the Web took off was that anybody could be a programmer. You could click `view source\u2019 and see the source code and then you could copy and paste from that model of success,\u201d says Morrison.\n  \u201cThat was a profound shift; the browser world is much more open, it is all about delivering the code as is to the browser and literally allowing people to see how it works,\u201d he adds.\n  Without that openness, Morrison says computing can be difficult. \u201cWhen I look at the API revolution it reminds me that when you start from something you can then build more sophisticated capabilities that you could never do in classic SOA.\u201d Morrison says the beauty of the API world is it leverages what\u2019s already been done and doesn\u2019t require you to sit down and install special infrastructure or libraries to do machine-to-machine communication. \u201cIn a lot of ways, you can think of it as the web for machine-to-machine communication,\u201d he says.\n  However, Morrison admits nothing is perfect. In jettisoning more formal SOAP-based and WS file approaches to distributed systems as opposed to RESTful system approach, there can be dangers as the functional problems get more sophisticated.\n  Security is a good example. Morrison says a great thing that APIs have achieved is they have moved away from a complex approach to security embedded in SOA, which tried to push security down to the messaging level. \u201cThey were powerful but the truth is most people didn\u2019t need that. It was elegant from a design perspective but it was overkill,\u201d says Morrison.\n  Indeed, he says, sometimes the solution was so big and complex it became unwieldy and easy to misconfigure, so it created security problems in its own right.\n  \u201cThe API security model says \u2018good old SSL is good enough.\u2019 Simple is best for security because there is less to screw up. So, you see a lot of the same benefits around security with API revolutions, he adds.\n  Another weakness is that right now, in the RESTful world, there is no commonly accepted approach to describe how an API should be defined. Some observers might counter that SOAP, so closely connected to established SOA, was perhaps too thoroughly defined.\n                                      \n  \t\t\t\t\t\t\t  \t\t\t\t\t\t\t\tHere at CamelOne 2012, FuseSource is releasing two new products \u2013 Fuse ESB Enterprise 7.0 and Fuse MQ Enterprise 7.0.\n  \t\t\t\t\t\t\n  \t\t\t\t\t\t\t  \t\t\t\t\t\t\t\tAutomotive rescue and recovery services in the UK gained significant messaging advantages over the past few years. The new Automotive Network Services (ANS) system, built and maintained by Apex Networks, connects automotive clubs and insurance agencies with the mechanics, riggers, and first responders that provide emergency services to accidents and disabled vehicles.\n  \t\t\t\t\t\t\n  \t\t\t\t\t\t  \t\t\t  \t\t\t\t\t\t  \t\t\t\t\t\t  \t\t\t\t\t\t\t  \t\t\t\t\t\t\t\t  \t\t\t\t\t\t\t\t\t\t\tScaling up and scaling out to meet new business demands  \t\t\t\t\t\t\t\t\t\n  \t\t\t\t\t\t\t  \t\t\t\t\t\t\t\tScaling up and scaling out are two ways for growing businesses to increase productivity, but doing so blindly may be more costly than its worth.\n  \t\t\t\t\t\t\n  \t\t\t\t\t\t\t  \t\t\t\t\t\t\t\tAccording to a recent Gartner survey, mobile technology is the second most important priority for CIOs in 2012, but building the business case is easier said than done.\n  \t\t\t\t\t\t\n  \t\t\t\t\t\t  \t\t\t  \t\t\t\t\t\t  \t\t\t\t\t\t  \t\t\t\t\t\t\t  \t\t\t\t\t\t\t\t  \t\t\t\t\t\t\t\t\t\t\tBasic vs. complex: Companies walk the line on analytics best practices  \t\t\t\t\t\t\t\t\t\n  \t\t\t\t\t\t\t  \t\t\t\t\t\t\t\tBusinesses looking to build effective business intelligence and analytics programs have to toe the line between focusing on the basics and using new technology to build more complex BI systems.\n  \t\t\t\t\t\t\n  \t\t\t\t\t\t\t  \t\t\t\t\t\t\t\tAn Oracle Hyperion system implementation in three weeks? Yes, it can be done. Communications firm Neustar implemented Hyperion in less than a month and gained efficiencies in the process.\n  \t\t\t\t\t\t\n  \t\t\t\t\t\t\t  \t\t\t\t\t\t\t\tAt some point, your JD Edwards shop will have to decide whether to switch to a different product, upgrade or stay where you are. Learn what two other organizations did when working through JD Edwards upgrades.","item_date":"May 23 2012 20:24:38","display_item_date":"05-23-2012","url":"http:\/\/searchsoa.techtarget.com\/feature\/Old-SOA-versus-new-SOA-Open-APIs-change-the-game","source":"searchsoa.techtarget.com"},{"title":"123 Database APIs: GeoNames, Freebase and Yahoo Query Language","details":"Our API directory now includes 123 database APIs. The newest is the BigTable API. The most popular, in terms of mashups, is the GeoNames API. We list 81 GeoNames mashups. Below you\u2019ll find some more stats from the directory, including the entire list of database APIs.\n\n\n\n\n\nIn terms of the technical details, REST and XML lead the way. There are 80 database REST APIs and 36 database SOAP APIs. Our directory lists 77 database XML APIs and 62 database JSON APIs.\n\n\n\n\n\nThe most common tags within database are 30 science database APIs, 20 bioinformatics database APIs and 14 reference database APIs.\n\n\n\n\n\nOn the mashup side, we list 9 database mashups. We named New Dance Tracks as mashup of the day in October.\n\n\nFor reference, here is a list of all 123 database APIs.\n\n\n\u00a0\u00a0ADL 3D Repository (3DR) API: 3D model database\n\n\n\u00a0\u00a0Amazon DynamoDB API: NoSQL database service\n\n\n\u00a0\u00a0Amazon RDS Relational Database Service API: Relational database service in the cloud\n\n\n\u00a0\u00a0Amazon SimpleDB API: Online database service\n\n\n\u00a0\u00a0Analytical Path API: Application data vizualization service\n\n\n\u00a0\u00a0appto.us API: Mobile and web application database\n\n\n\u00a0\u00a0Archimedes Project Donatus API: Morphological analysis data service\n\n\n\u00a0\u00a0AtlasCT Location-Based Services API: Location-based services\n\n\n\u00a0\u00a0Beatport API: Electronic music downloads\n\n\n\u00a0\u00a0BigTable API: Online database service\n\n\n\u00a0\u00a0Bing Maps Spatial Data Services API: Mapping geospatial data service\n\n\n\u00a0\u00a0BioMart API: Scientific database service\n\n\n\u00a0\u00a0Boliven Patents API: 100 million Patent and Scientific documents\n\n\n\u00a0\u00a0Boliven Publications API: Patent and scientific document portal\n\n\n\u00a0\u00a0Budget Your Trip API: Travel planning service\n\n\n\u00a0\u00a0BuzzData API: Data hosting and management service\n\n\n\u00a0\u00a0Cambridge University Library  API: Library catalog and services\n\n\n\u00a0\u00a0CartoDB API: Geospatial data and map creation service\n\n\n\u00a0\u00a0Caspio Bridge API: cloud computing integration service\n\n\n\u00a0\u00a0Charities Commission API: charity database service\n\n\n\u00a0\u00a0CICC gNova API: Chemical database search\n\n\n\u00a0\u00a0CKAN API: Data portal platform service\n\n\n\u00a0\u00a0ClearDB API: Cloud based database\n\n\n\u00a0\u00a0Comprehensive Knowledge Archive Network API: Community driven open knowledge database\n\n\n\u00a0\u00a0cPath API: Medical database lookup\n\n\n\u00a0\u00a0DabbleDB API: Online database service\n\n\n\u00a0\u00a0Dance-Tunes API: Electronic music downloads\n\n\n\u00a0\u00a0Database.com API: Cloud Computing Database Service\n\n\n\u00a0\u00a0Datafiniti API: Search engine for data\n\n\n\u00a0\u00a0DbApi API: Database management service\n\n\n\u00a0\u00a0DKFZ SoapDB API: Genetics and protein sequence databases\n\n\n\u00a0\u00a0Duplicated Genes Database API: Database of co-located and duplicated genes\n\n\n\u00a0\u00a0Dydra API: Cloud-based RDF store\n\n\n\u00a0\u00a0e-Fungi API: Fungi genomics database and analysis service\n\n\n\u00a0\u00a0EBI WSDbfetch API: Biological database search service\n\n\n\u00a0\u00a0EBI WU-BLAST API: Database search for biological sequence similarities\n\n\n\u00a0\u00a0Edmunds.com Dealer API: Vehicle dealer information\n\n\n\u00a0\u00a0Edmunds.com Vehicle API: Automobile information\n\n\n\u00a0\u00a0Evrythng API: Online profiles for physical objects\n\n\n\u00a0\u00a0Factolex API: Fact\/Lexicon Database Service\n\n\n\u00a0\u00a0Factual API: Open Data for sharing and reference tools\n\n\n\u00a0\u00a0Factual Places API: Open Data for sharing and reference tools\n\n\n\u00a0\u00a0FixYa API: Question and answer site\n\n\n\u00a0\u00a0FluidDB API: Information Architecture & Database Service\n\n\n\u00a0\u00a0Fragmento API: Business compliance management service\n\n\n\u00a0\u00a0Freebase API: Community driven open database\n\n\n\u00a0\u00a0FreeCovers.net API: Album cover archive\n\n\n\u00a0\u00a0Game Jolt API: Independent video game directory\n\n\n\u00a0\u00a0GeoNames API: Geographic name and postal code lookup\n\n\n\u00a0\u00a0GIS Cloud API: Geospatial data cloud hosting & map creation service\n\n\n\u00a0\u00a0Golm Metabalome Database API: Metabolic molecule database\n\n\n\u00a0\u00a0Google Base API: Platform for structure and semi-structured data\n\n\n\u00a0\u00a0Google Fusion Tables API: Data sharing and visualization platform\n\n\n\u00a0\u00a0gubb API: List creation and management\n\n\n\u00a0\u00a0Health Indicators Warehouse API: Health indicators database\n\n\n\u00a0\u00a0HMMER API: Protein sequence search and alignment service\n\n\n\u00a0\u00a0Hoovers API: Corporate data services\n\n\n\u00a0\u00a0InfiniteGraph API: Distributed graph database\n\n\n\u00a0\u00a0Institute of Development Studies API: Research and data repository \n\n\n\u00a0\u00a0Intelius Search API: Public records database search\n\n\n\u00a0\u00a0iRefIndex API: Protein interaction databases\n\n\n\u00a0\u00a0Lexaurus API: Vocabulary management service\n\n\n\u00a0\u00a0Location Labs Spatial Storage API: Access and Search Geo-Tagged Content\n\n\n\u00a0\u00a0LongJump API: Platform as a Service (PaaS)\n\n\n\u00a0\u00a0LyrDB API: Lyrics Database\n\n\n\u00a0\u00a0M\/DB API: Database service\n\n\n\u00a0\u00a0Maiana API: Topic Maps Browser and Editor\n\n\n\u00a0\u00a0Martindale-Hubbell API: Legal directory\n\n\n\u00a0\u00a0MetNet API: Biological database access service\n\n\n\u00a0\u00a0miRMaid API: miRNA database access service\n\n\n\u00a0\u00a0Missatsamtal.se API: Swedish Telemarketer\/Unknown Caller Phone Number Database\n\n\n\u00a0\u00a0MongoLab API: Hosted MongoDB database\n\n\n\u00a0\u00a0MRS API: Biological and medical database search service\n\n\n\u00a0\u00a0MyAnimeList API: Anime collection referene list\n\n\n\u00a0\u00a0MyHits API: Protein domain database\n\n\n\u00a0\u00a0MyTaskHelper API: Database management service\n\n\n\u00a0\u00a0NASA Coordinated Data Analysis System (CDAS) API: Space physics instrument data\n\n\n\u00a0\u00a0NCSU Libraries CatalogWS API: Library catalog and services\n\n\n\u00a0\u00a0NetProspex API: Contact data management\n\n\n\u00a0\u00a0NextDB  API: Hosted database accessible from JavaScript\n\n\n\u00a0\u00a0Norwegian Bioinformatics Platform API: Databases for significant sites in biological sequences\n\n\n\u00a0\u00a0Nuclear Protein Database API: Database of nuclear vertebrate proteins\n\n\n\u00a0\u00a0Nutritionix API: Nutrition data search and database\n\n\n\u00a0\u00a0Octopart Electronics API: Electronics parts search services\n\n\n\u00a0\u00a0Open EAN\/GTIN Database API: Barcode database\n\n\n\u00a0\u00a0OpenLigaDB API: Community run sports database\n\n\n\u00a0\u00a0Pathbase API: Mouse pathology imagery database\n\n\n\u00a0\u00a0Phish.Net API: Phish fan site\n\n\n\u00a0\u00a0Phospho.ELM API: Protein phosphorylation site database\n\n\n\u00a0\u00a0PlantTFDB API: Database of plant transcription factors\n\n\n\u00a0\u00a0Plings API: Places and Activities Database\n\n\n\u00a0\u00a0ProDom API: Protein domain family database\n\n\n\u00a0\u00a0PSLC DataShop API: Educational research repository\n\n\n\u00a0\u00a0Qirina API: Keyword analysis database\n\n\n\u00a0\u00a0Qrimp API: Developer platform with database capabilities\n\n\n\u00a0\u00a0RCSB Protein Data Bank API: 3D protein database\n\n\n\u00a0\u00a0RdbHost API: SQL database web service\n\n\n\u00a0\u00a0Reactome API: Biological pathway database\n\n\n\u00a0\u00a0Realtors Property Resource API: Real estate data service\n\n\n\u00a0\u00a0Recreation.gov API: Federal recreation database\n\n\n\u00a0\u00a0SABIO-RK API: Biochemical reaction database\n\n\n\u00a0\u00a0SeqHound API: Bioinformatics research database\n\n\n\u00a0\u00a0ServerCyde API: Web application building service\n\n\n\u00a0\u00a0SimpleGeo API: Location services platform\n\n\n\u00a0\u00a0SimpleGeo Places API: Location Search Service\n\n\n\u00a0\u00a0Sindice API: Semantic data search and extraction service\n\n\n\u00a0\u00a0StructuredRetailProducts API: Financial and business database\n\n\n\u00a0\u00a0Sunlight Foundations Party Time! API: Political fundraiser invitation database\n\n\n\u00a0\u00a0Swoogle API: Semantic data search\n\n\n\u00a0\u00a0TeamDesk API: Web based database application service\n\n\n\u00a0\u00a0TempoDB API: Time-series data database and analytics platform\n\n\n\u00a0\u00a0TFmodeller API: DNA-binding protein modeling service\n\n\n\u00a0\u00a0The SEED API: Genomic information database\n\n\n\u00a0\u00a0Trackingo API: Personal Data Tracking Service\n\n\n\u00a0\u00a0TreeBASE API: Phylogenetic tree database\n\n\n\u00a0\u00a0TuneFind API: Movie & TV Show Music Database Service\n\n\n\u00a0\u00a0UN Comtrade API: United Nations Commodity Trade Statistics Database\n\n\n\u00a0\u00a0Unfuddle API: Bug and issue tracking\n\n\n\u00a0\u00a0Urbarama API: Architecture atlas\n\n\n\u00a0\u00a0USA Today Sports Salaries API: Sports information service\n\n\n\u00a0\u00a0WeoGeo API: Geo-data storage and sharing service\n\n\n\u00a0\u00a0Xeround Cloud Database API: Cloud Database Service\n\n\n\u00a0\u00a0Yahoo Query Language API: Query language platform for Yahoo services","item_date":"May 23 2012 19:34:16","display_item_date":"05-23-2012","url":"http:\/\/blog.programmableweb.com\/2012\/05\/23\/123-database-apis-geonames-freebase-and-yahoo-query-language\/","source":"blog.programmableweb.com"},{"title":"Moodstocks \u00bb The new Moodstocks SDK is out!","details":"We have great news for you today. We\u2019re happy to be releasing a brand new version of our SDK as well as an awesome Help Center!\n Moodstocks SDK, v3.4\n It features loads of cool improvements:\n \u2022 we\u2019ve released the hardware constraints on Android: the SDK now supports\u00a0ARMv6\u00a0(e.g. HTC Wildfire S) and\u00a0ARMv7 w\/o NEON\u00a0devices (e.g. all devices including a NVIDIA Tegra 2 CPU)\n \u2022 we\u2019ve introduced a\u00a0synchronization progress\u00a0callback: with it you\u2019re now able to display a shiny progress bar on the UI side\n \u2022 we\u2019ve switched to\u00a0Clang\u00a0for the iOS library build resulting in better runtime performance and smaller library size\n \n We\u2019ve upgraded our former documentation into a handy Developer\u2019s Guide as well as an up-to-date FAQ section. You should find everything you\u2019ve always wanted to know right there!\n We hope you enjoy it and we\u2019re looking forward to experiencing your applications powered by this brand new SDK version!","item_date":"May 23 2012 19:34:14","display_item_date":"05-23-2012","url":"http:\/\/www.moodstocks.com\/2012\/05\/23\/the-new-moodstocks-sdk-is-out\/","source":"www.moodstocks.com"},{"title":"Why APIs? On doing Business in the Full Context of Customers","details":"We received some strong interest in our recent webcast Why APIs in which we explored why APIs are  important tto successful businesses and the different API strategies (Internal, Partners, Customers or Open) we see employed. So I thought wed drill down into the core ideas in a few blog posts.\n What challenges are businesses facing in 2012? \n The three major trends of social, mobile, and cloud in the market today are driving huge changes in how individuals connect, how businesses connect, how businesses engage with their customers and employees, and therefore how Information Technology (IT) works.\n According to analysts at Canalys, at the end of 2011, smart phone shipments outpaced PCs (including iPads) for the first time - smart phone units shipped was up 63% from 2010. Gartner estimated that 18 billion apps were downloaded in 2011, up 114.5% from 2010 and will rise to 31 billion in 2012.\n According to this IDC study in 2011, the use of consumer-inspired social media such as Facebook and Twitter is also growing fast: almost twice as many information workers said they were using these technologies in the workplace in 2011 as the previous year. IDC also predicts that the market for Big Data technology and services will reach $16.9 billion by 2015, up from $3.2 billion in 2010. That\u2019s a 40% growth rate year-over-year.\n The \u201cconsumerization\u201d of IT is happening fast thanks to each of these trends individually but even more because of how these trends are playing out in concert in the marketplace. The effects multiply to drive ubiquity and an explosion in consumption.\n \n Think about a person sitting watching TV, while using his iPad to keep up with friends on Facebook, while browsing coupons on his Groupon email alias. Or consider the businessperson waiting for a flight using her Android tablet to check her social network on LinkedIn while accessing salesforce.com to forecast her sales pipeline.\n \n These scenarios, and dozens of others like them, represent a fundamental shift in how customers are interacting with businesses. It\u2019s no longer interaction through a Web site but through apps on numerous devices and platforms, social media, and cloud services.\n The question on the provider side becomes whether businesses have a strategy for the explosion of consumption and whether they are doing business in the full context of customers and consumers. It\u2019s always an imperative that businesses align investments with their customers.\n Doing business in the full context of customers\n How does a business effectively target and support the consumers in our scenarios? Let\u2019s look at how apps are built in a typical enterprise. It\u2019s common for the initial requirement for an iPad app to come from the executive office or from a marketing team. A business might start by developing an app with one of their products, a partner, a social network (say, Facebook), and a cloud service (say PayPal). The app is built and all is well.\u00a0\n \n Then they realize that there\u2019s also a user base on Android so they build  it again on the Android platform. Then they realize that they should  target another social network. So they build again. And again and again  to target more social networks, products, partners, and cloud services.  Quickly, there\u2019s a plethora of apps targeting different platforms,  different services, different partners, and different products.\n \n The order of magnitude increase in resources to support different browser types in the mid 1990s (to facilitate exposing internal systems via the Web) is happening again. Today, support is needed for different app platforms \u2013 different versions of Android and iOS, and so on.\n \n Clearly, important changes need to happen in how we build our apps so that we can effectively and efficiently do business in the full context of customers.\n Introducing APIs\n The cloud, social, and mobile app phenomenon is stimulating an explosion in the API economy. Frequently when a mobile app user clicks a button to do a transaction (send messages, make trades, get credit information, and so on) or sends or retrieves data, an API \u2013 either transactional or data - is called.\u00a0 My colleague Anant describes transactional and data APIs as the Ying and Yang of APIs. He posits that while the API conversation is dominated by transactional APIs today, a revolution is underway in the world of Data APIs because of the requirements for easy consumption, flow, and interaction of data in a world of big data.\n Next time, we\u2019ll dig deeper into how APIs are the answer to handling mobile and social ubiquity and how they bridge the gap between IT requirements and those of the new app economy.","item_date":"May 23 2012 17:08:36","display_item_date":"05-23-2012","url":"http:\/\/blog.apigee.com\/detail\/why_apis_on_doing_business_in_the_full_context_of_customers\/","source":"blog.apigee.com"},{"title":"Join the Facebook Platform team at Over The Air","details":"On Friday 1st & Saturday 2nd June 2012, 600 mobile developers and designers will converge on Bletchley Park - home of the British wartime efforts to break the Enigma and Lorenz codes - for Over The Air 2012, a chance to meet, learn, and hack on the latest mobile platforms and technologies.\n  The Facebook team will be there help teams who want to take on the Facebook Open Graph Challenge - to build the best mobile, social Open Graph app in 24 hours. Each winner will receive a Samsung Galaxy SII.\n  Well also be giving two talks. The first is an introduction to Open Graph and will show you how to build an end-to-end Open Graph Mobile app in 60 minutes. The second is a look into how we build Facebook itself, our tools and processes that allow us to ship thousands of lines of code to over 900 million people every day.\n  Over The Air is a free event, but youll need to register via Eventbrite to attend. If youre interested in hacking on the Facebook Open Graph Challenge or have questions or other ideas you want to hack on, join our Over the Air Facebook group. The Facebook team will be hanging out in the group before, during and after the event.\n  We look forward to seeing you there!","item_date":"May 23 2012 16:58:39","display_item_date":"05-23-2012","url":"http:\/\/developers.facebook.com\/blog\/post\/2012\/05\/23\/join-the-facebook-platform-team-at-over-the-air\/","source":"developers.facebook.com"},{"title":"White House launches new digital government strategy ","details":"Theres a long history of people who have tried to transform the United States federal government through better use of information technology and data. It extends back to the early days of Alexander Hamiltons ledgers of financial transaction, continues through information transmitted through telegraph, radio, telephone, and comes up to the introduction of the Internet, which has been driving dreams of better e-government for decades.\n  Vivek Kundra, the first U.S. chief information officer, and Aneesh Chopra, the nations first chief technology officer, were chosen by President Barack Obama to try to bring the federal governments IT infrastructure and process into the 21st century, closing the IT gap that had opened between the private sector and public sector. \n  Today, President Obama issued a presidential memorandum on building a 21st century digital government.\n  In this memorandum, the president directs each major federal agency in the United States to make two key services that American citizens depend upon available on mobile devices within the next 12 months and to make applicable government information open and machine-readable by default. President Obama directed federal agencies to do two specific things: comply with the elements of the strategy by May 23, 2013 and to create a \/developer page on ever major federal agencys website. Heres an excerpt from President Obamas memo:\n  The innovative use of technology is fundamentally transforming how the American people do business and live their daily lives. Exponential increases in computing power, the rise of high-speed networks, and the growing mobile revolution have put the Internet at our fingertips, encouraging innovations that are giving rise to new industries and reshaping existing ones.\n  Innovators in the private sector and the Federal Government have used these technological advances to fundamentally change how they serve their customers. However, it is time for the Federal Government to do more. For far too long, the American people have been forced to navigate a labyrinth of information across different Government programs in order to find the services they need. In addition, at a time when Americans increasingly pay bills and buy tickets on mobile devices, Government services often are not optimized for smartphones or tablets, assuming the services are even available online.\n  On April 27, 2011, I issued Executive Order 13571 (Streamlining Service Delivery and Improving Customer Service), requiring executive departments and agencies (agencies) to, among other things, identify ways to use innovative technologies to streamline their delivery of services to lower costs, decrease service delivery times, and improve the customer experience. As the next step toward modernizing the way Government works, I charged my Federal Chief Information Officer (CIO) with developing a comprehensive Government-wide strategy to build a 21st century digital Government that delivers better digital services to the American people.\n  Today, the CIO is releasing that strategy, entitled Digital Government: Building a 21st Century Platform to Better Serve the American People (Strategy), which provides agencies with a 12-month roadmap that focuses on several priority areas.The Strategy will enable more efficient and coordinated digital service delivery by requiring agencies to establish specific, measurable goals for delivering better digital services; encouraging agencies to deliver information in new ways that fully utilize the power and potential of mobile and web-based technologies; ensuring the safe and secure delivery and use of digital services to protect information and privacy; requiring agencies to establish central online resources for outside developers and to adopt new standards for making applicable Government information open and machine-readable by default;\n  aggregating agencies online resource pages for developers in a centralized catalogue on www.Data.gov; and requiring agencies to use web performance analytics and customer satisfaction measurement tools on all .gov websites.\n  Ultimately, this Strategy will ensure that agencies use emerging technologies to serve the public as effectively as possible. As a Government, and as a trusted provider of services, we must never forget who our customers are \u2014 the American people.\n  While Kundra and Chopra set in a motion of series of reforms, from more transparency on IT spending and waste to an ambitious open government data program to adoption of cloud computing to improved IT security and a modern approach to open innovation, they left an immense portfolio and set of challenges for Steven VanRoekel and Todd Park to take on and implement against. Many of those challenges remain the same, including attracting talent, reforming procurement, data quality, and the reality of agency mainframes that are still running on COBOL.\n  There are many things the federal government should do to improve IT performance and efficiency, said Darrell West, vice president for government studies and director of the Center for Tech Innovation at the Brookings Institute, when asked for comment. It can quit adopting expensive legacy systems that are obsolete from the moment they are purchased and move towards more nimble strategies.\u00a0 There are many new apps that are available through the federal Apps Store and agencies can use them to improve performance and cut costs.\u00a0 In the medical area, there are 40,000 mobile health applications.\u00a0 Technology should be a money saver, not a money waster.\u00a0 The key problem federal officials face is overcoming economic interests that are vested in the past, not the future of technology innovation.\n  Big visions matter, in terms of inspiring the country to action or a historic course, from building transcontinental railroads to sending men to the Moon to starting up a new government agency. Implementing against that vision, however, in a time of great budget pressure, increased demands for government services online and falling trust in institutions, is just as important.\n  Today, VanRoekel and Park have put their own stamp on the future of digital government in the United States with the introduction of a digital strategy for  21st century government, which went online this morning. (In a notable upgrade to the way such policies have been released, this digital strategy was coded in HTML5.)\n  They introduced it to the nation in an decidedly non-Washingtonian sort of way, traveling north to New York City, a city that has become one the global epicenters for data-centric digital government under Mayor Michael Bloomberg, to directly engage Gotham Citys community of tech entrepreneurs, venture capitalists and (civic) developers at TechCrunchs Disrupt Conference. A video clip of their presentation is embedded below:\n    Whether their pitch for smart government gets funded and supported is an open question. One measure will be whether this trip results in new recruits: Park and VanRoekel urged the developers and entrepreneurs in the room to join a startup called the U.S. government as one of 15 Presidential Innovation Fellows. Park said in New York that the fellowships will focus on five areas: a USAID campaign, open data across the federal government, releasing more health information, an online system theyre calling MyGov, a request for proposals program to help startups with procurement.\n  Park and VanRoekel took questions from the press following their presentation. Molly Walker published audio of their discussion of the digital strategy, which you can listen to in the player below:\n  \n  Whats next for open data?\n  There have been longstanding questions about what constitute high quality datasets for years. VanRoekel, in past conversation, has identified an activity that many Americans engage in \u2014 looking for real estate. Buying a house is the single biggest purchase many citizens will ever make. \n  Now, VanRoekel and Park are talking about releasing more data that could actually inform that purchase. The challenge is that the third parties that they want to have use it need to have it be high quality, regularly updated, standardized and accessible. For instance, developers making energy apps dont want data published every quarter: they want it every week, every day or, if possible, every hour. \n  To learn more about how theyre thinking about these challenges, I interviewed Park and VanRoekel yesterday about the components and thinking behind this strategy. My discussion with VanRoekel follows. Look for an extended discussion with Park and video from TechCrunch Disrupt later today. \n  This digital strategy has been coming for a long time. Whats in it?\n  VanRoekel: Over the last 10 months or so weve been hard at work on a bunch of divergent strategies, thinking about our web presence as a federal government, managing the 1,800 .gov domain names we have out there, and tens of thousands of websites and millions of pages. Weve been focused on mobile and the consumerization of technology and that impact on government. And as you know, I come storming in with a passion around open data, making data machine-readable and accessible, unlocking the potential of that data, as you and I talked about last time, and many times at the FCC. We had a moment this winter when we realized these divergent strategies were really converging into one thing, where they were all parts of a bigger strategy, which breaks down into several areas. \n  This first is an information-centric approach. That pillar is about making data openly available as the new default across government. \n  The second is around a shared platform, where we can share resources across government, removing duplication and leveraging existing projects that were doing. \n  The third layer is all about customer centric-government. Typically in the government, we very tightly couple the presentation of stuff with the stuff itself. Finding that stuff is nearly impossible, if you have to navigate a quagmire. The nature of this is embracing the new application delivery model, looking at how we deliver content and data more effectively to Americans. Also, how do we create, across the customer-centric platform, a way of creating government as a platform, where we have a common approach to building applications on top of government data. Our goal is to bring government to Americans where they live, versus expect Americans to come to government, where we are. We saw this in the past, with weather and GPS. Combined, theres over a $100=billion dollar industry, if not more.  We now, as Americans, probably take that for granted, in the ability to just connect in and use that data from the government. We have unending examples of places where this government data can be unleashed to the private sector and to American citizens to provide better e-services.\n  The last pillar, which I wont go into depth now but that its very important that were doing, is that were building consistent methodology around security and privacy. Were calling on a bunch of groups in this strategy to help us on that front. \n  How will government become more data-centric?\n  VanRoekel: Yesterday was the three-year anniversary of Data.gov. It was an important first step to raise the awareness of people around data. It largely was a bulk upload, bulk download system. It didnt manifest in data being digital by default through the entire lifecycle of that data. It was sort of an add-on, to take the step of getting it to Data.gov. The key difference in what were doing here and evolving Data.gov \u2014 which will be involved, as part of this \u2014 is architecting data for openness.\n   There are some very specific deliverables in the strategy itself, including a government-wide open data content and web API policy and then work with NIST and others to identify standards and best practices for interoperability to scale that across government. We will have, from its creation to its dissemination, open data as the new normal. Were going to, as part of this, inspire investing in IT systems that respect open data and think about open data. \n  This also bridges into big data and that roadmap as well. The next step of that is thinking about web APIs, how we build for modularity. Its about how we deliver our content and manifest systems in a way that allow APIs to do that. Part of this deliverable will also be about expanding Data.gov to also be the metadata catalog of all data in the federal government. It wont be a place where you use data but it will be a place where you find data, discover data or discover the relatedness of data. We also intend to take Data.gov and add some API key management and some other technology there, so it is the developer platform of all this stuff. \n  As you remember from the Open Government Directive, where we directed all agencies to create a \/open page. Part of this deliverable will be all agencies creating a \/developer page, to start to build the catalog of citizen-accessible web APIs for their systems. \n  This may all be a bit abstract to a non-technical citizen. Why is open data important to the American people? Why is this a strategy thats important enough to elevate to this level? \n  VanRoekel: For citizen-facing discussions and the conversation Ill have with consumer publications, I start with the customer-centric element and think about what are those applications that will be delivered by government and go drive a new set of normals and value systems. I always harken back to examples like GPS, like weather, but we can also look at early agencies that have done a better job, in the realm of health, tax, or travel, including that MyTSA app on your smartphone giving you wait times and security gates at the airport nearest you, or travel tips. Those are possible from unlocking government data and making that government data available to application providers. \n  To the TechCrunch crowd, Im going to talk about real estate. When youre buying a home, why doesnt it manifest to you the myriad of data that the government has locked up about school quality, healthcare quality, infrastructure investments, broadband, everything else that people really care about when theyre picking a place to live? We dont do that \u2014 we do roof composition and the number of bathrooms, and thats typically the extent of it. Some services are doing a better job with other government data but largely its pretty siloed and not very specific to what Americans really care about. \n  How will this strategy result in those releases happening in the format and timeliness that making that kind of data useful would require? How do you drive the timely release of data in a consumable format from agencies, given the technical challenges they have?\n   VanRoekel: Thats why were launching the strategy now. It couldnt more important to manifest those things across all of the opportunities we have. From declaring that open data is now the new default  \u2014 that you have to, when you create datasets, do so in an open way \u2014 when we say that when you buy IT systems that interface with data, they need to be purchased in a way that respect  the aspects of open data, when you talk about dissemination of data, were going to inspire in the strategy different aspects of agencies building and delivering open data solutions. The first tranche of this is that each agency has to do two of them. Thats a crawl, walk, run approach. You have to have the magic mix of all of these elements: great policy, great technology, and great people leading the way. Thats what were trying to bring to bear to get there. Weve been kind of haphazard. Theres been no one at the top saying thou shalt do this. Now we have that with this strategy. \n  President Obama has talked about information technology with some frequency, in the context of the importance of American innovation, improved government services and job creation. How much have you talked with the president about this strategy? How closely has he been involved in helping to shape it? How much support do you have from him?\n  VanRoekel: We have an incredible amount of support, both from the president and his staff. He was excited enough about this strategy that he insisted in actually issuing a memo as a cover page for it. This presidential memorandum is basically telling agencies go do this stuff. Its important, talking about the necessity of unlocking this government technology on behalf of citizens. That doesnt happen very often, with him weighing in on the importance of whats happening here. \n  If you look at how well agencies have complied with the \/open requirements, in terms of simply updating their open government plans, youll see some laggards. What policy hooks will you have, with respect to making sure the deliverables in this strategy come through?\n   VanRoekel: The important part of this is that we have one policy out there. Policy doesnt go all the way. Policy should be a place that gives framing and direction, and inspires people to act. It explains the art of the possible. Part of this is just education on what is the possible and how we do this. I think the important part that will make this stick long term includes three things. \n  One is oversight. Part of this strategy will be working with agencies to create reporting mechanisms \u2014 and those reporting mechanisms will be built in open, standard ways that allow people to create easily accessible dashboards that track agency progress on these deliverables, so we can put some pressure on people from an oversight standpoint. The new normal for dashboards needs to be open data as the feeds, and allowing people to build on top of them. \n  The second part is regular accountability in meetings with us. Weve got other mechanisms to touch base with people, too. I care a lot about this. At the FCC, I mandated this stuff and watched and saw it through to some really successful endpoints. \n  The third \u2014 and this is probably one of the most important elements \u2014 is injecting this into the different parts of doing business within government agencies. Its getting the permit team to do new things. Its getting the IT team to buy and deploy things in new ways. \n  Luckily, were not starting from scratch. I think agencies generally \u2014- and IT professional in those agencies \u2014 want to deliver against mission. They also care a lot about this stuff. They know this is the new way of doing things. They just need to be empowered in a way that gets them to that end point. Were not going to get a huge tide of resistance. Its going to bring them measurable value. \n  Part of this strategy is around mobile consolidation, including contracts and other things which are going to save them a lot of money in this fiscal environment. Weve got a nice forcing function there. \n  How much is the bring your own device (BYOD) wave in government and enterprise driving this data-centricism of this digital strategy? CIOs are stressed about not being able to get mobile app development talent to create multiple applications for multiple platforms.\n  VanRoekel: It definitely is part of it. We have to walk that careful walk of being device and vendor-agnostic, in terms of the strategies we lay out. We dont want to be creating a marketplace winner through policy. We also have to be cognizant that we really have to open up the doors here for us to utilize these government systems in new ways. I know thats not going to be a sweeping phenomenon right away. We need to get stuff moving in the right direction. I think this is the way we do it. We do it by getting agencies to deliver solutions that are decoupled from the underlying data and ride government like a platform. We rapidly prototype and create single websites presence for the government. We get the private sector to step up and deliver solutions as well. \n  So you want agencies to be dogfooding the same data and web services that will be consumed by third parties?\n  VanRoekel: I think there have to be places where, when we unlock the data, it makes sense for us to actively deliver solutions against that data. Thats either proof-of-concept work or actually deliver solutions against it.\n  This post has been updated to add audio and video when it became available online.","item_date":"May 23 2012 15:44:58","display_item_date":"05-23-2012","url":"http:\/\/radar.oreilly.com\/2012\/05\/white-house-launches-new-digit.html","source":"radar.oreilly.com"},{"title":"Hackathon aims to transform business","details":"Sponsored by Frog and LRN, the San Francisco hack will take place June 9 and 10 at Frog\u2019s downtown office with a $5,000 prize awarded to the winning idea.\n The goal is to develop practical solutions that hold companies accountable to their ideals and technological solutions developed at the hackathon may focus on transparency, fairness, inclusiveness, empowerment, day-to-day decision-making or behaviors, sponsors said.\n \u201cThere\u2019s a lot out there about how to embed values into business,\u201d said Katy Brennan, corporate communications manager for LRN. \u201cWe wanted to move that beyond just talking to creating real concepts, products and solutions that could be realized.\u201d\n Partners of the event include BSR, Carnegie Mellon University, Dachis Group, Fast Company, Net Impact, Silicon Valley Bank, and the World Economic Forums Global Agenda Council in Decision-Making. Reinvent Business aims to bring together software developers, designers, gamers, filmmakers, storytellers, and business leaders \u201cto design and build innovative products and services that have the capacity to change corporate behavior from within,\u201d sponsors said.\n Judges will include venture capitalists, product designers, journalists, marketers and others. Silicon Valley Bank is supplying the $5,000 cash prize.\n \u201cWe do not expect to come up with a silver bullet,\u201d said Tim Leberecht, chief marketing officer at Frog. \u201cWe don\u2019t believe there is one software application that will resolve the ethical and decision-making problems the enterprise faces these days. But we think this is a valuable approach because it forces you to really delve into technology challenges rather than staying at this very high level.\u201d\n    \tLindsay Riddell covers energy and cleantech for the San Francisco Business Times.                                                                     Related:\n                               Startups","item_date":"May 23 2012 02:34:12","display_item_date":"05-22-2012","url":"http:\/\/www.bizjournals.com\/sanfrancisco\/blog\/2012\/05\/hackathon-aims-to-transform-business.html","source":"www.bizjournals.com"},{"title":"Hackathon follows the political money at Stanford","details":"The weather was spectacular in the Bay Area this past weekend, but a group of scientists, engineers and journalists sat hunched over their computers in a basement room on the Stanford University campus, blissfully ignorant of the beautiful climate outside.\u00a0 They were hot on the trail of money, lots of money, and how it flowed weekly, daily, even hourly through the backrooms and boardrooms of the American political system.\n The two-day \u201chackathon\u201d was organized by Teresa Bouza, a Knight Fellow at Stanford, who received a grant to develop new tools that journalists and others could use to make sense out of the huge amounts of online data available today, drawing from multiple sources such as the FEC\u2019s own website to information collected by independent groups like MapLight.\u00a0 She pulled together a number of sponsors including O\u2019Reilly Media, Knight-Mozilla Open News, the Sunlight Foundation, Sony, Revolution Analytics, and mongoDB.\u00a0 With a Presidential election looming, Bouza thought a focus on campaign financing would be especially timely.\n   \n  \u201cPeople were really amazed,\u201d said Bouza.\u00a0 \u201cWhat this group was able to accomplish in a short period of time was remarkable.\u201d\n Called \u201cDatafest,\u201d the event attracted a diverse mix of approximately 60 students, statisticians, bloggers and journalists who organized into teams.\u00a0\u00a0 Each group took a piece of the political funding process and built tools that would shine new light on the flow of money in politics.\n What they found was eye-opening.\u00a0 One group used Lockheed Martin, the giant defense contractor, as a case study and was able to chart publicly available data to visually show how the millions spent on contributions tracked closely with billions the company was awarded in government contracts.\u00a0 \u201cThere\u2019s a heightened awareness of how individual companies work with politicians,\u201d said Ludi Rehak, a Stanford student and one of the team members.\u00a0 \u201cWhat we\u2019re doing here this weekend is taking our citizen activism and applying it to our background in tech.\u201d\n Another group examined the percentage of campaign contributions that flow into races for governor from out of state.\u00a0 Their data was startling. In 2012 alone, the group found a state like Montana where 100 percent of the contributions were from outside versus North Carolina where the percentage was a paltry 8 percent.\n For journalists like Coulter Jones of the Center for Investigative Reporting, examining common problems in the complicated and shadowy world of campaign finance was worth sacrificing his weekend.\u00a0 \u201cIt was a good opportunity to work with people I haven\u2019t met before,\u201d said Jones.\n To help facilitate the exercise, Google donated unlimited access to its App Engine service and there were multiple databases the teams could draw from for the financial data they needed.\u00a0 But there was one major group of lawmakers for whom no online data detailing current political contributions exists today:\u00a0 the august members of the United States Senate.\n Unlike their counterparts in the House of Representatives (who must file their reports electronically with the Federal Election Commission), Presidential contenders, and a vast majority of the statewide and local legislative candidates in the country today, current or prospective members of the Senate still file their campaign contributions by paper to the chamber\u2019s Secretary.\u00a0 Access to these reports, when they are finally posted online after a lengthy and cumbersome process, is usually after Election Day.\n According to Kim Alexander of the California Voter Foundation, there is a bill \u2013 S219 \u2013 that is designed to bring campaign filing for Senate candidates into the 21st century.\u00a0 But, as Alexander ruefully admits, \u201cit\u2019s not going anywhere.\u201d\n On Sunday afternoon, after the last database had been downloaded and the final graphics rendered, the tired but satisfied teams gathered to hear presentations of each other\u2019s work.\u00a0 Prizes were handed out, including a brand-new 46 inch Sony television.\u00a0 After spending two days building tools to shine new light on the darker corners of campaign finance in the U.S., the teams saved their data, unplugged their laptops, and climbed a long set of stairs into the bright sunlight.\n       \n Stanford University hosted a hackathon this past weekend and participants took dead aim on campaign finance data.","item_date":"May 23 2012 02:34:05","display_item_date":"05-22-2012","url":"http:\/\/www.examiner.com\/article\/hackathon-follows-the-political-money-at-stanford","source":"www.examiner.com"},{"title":"Innovator Spotlight: TradeKing","details":"TradeKing is a nationally licensed online broker\/dealer dedicated to empowering the independent, self-directed investor. The TradeKing platform features powerful online equity, options, ETF, mutual fund and fixed-income trading tools, along with a rich set of news, research and analysis capabilities. \n Dan Raju, TradeKings CIO, talked to us about how the TradeKing API is serving as a core element of the firms strategy for delivering the next-generation trading experience for traders and fueling the companys growth.\n \nHow are you using APIs today?\n When we launched our API last year, we were one of the first brokerage firms to offer a simple REST-based API. Our API is an absolutely critical piece of our infrastructure now, promoting efficiencies and innovation across every channel: from our website and other internal systems development to our external partners and the broader app developer community.\n Weve made our API available so that the development community can do any possible mash-up of our services\u2013 and they are. This level of integration and innovation we are seeing is something we could never have accomplished by ourselves this quickly.\u00a0\u00a0\n What kind of business benefits have you experienced as a result of your APIs?\n We get a million plus calls every day on our API, and a growing number of partners have built their platforms on our API. We also have individual traders who write apps or trading algorithms for their own personal use.\n Everyone has his or her own personal way of trading, and the API opens up endless possibilities for traders to define their own experience.\u00a0 In a couple of months, well be launching our own app gallery to showcase the apps developed by business partners and developers with our APIs. \u00a0We are just scratching the surface now. We think in the next 12 months, well have a much larger set of partners on top of the API. All this is benefiting our customers by offering them a greater set of choices they never had before.\n What has helped you promote developer adoption?\n To attract a wide array of developers, it is crucial to offer a simple yet rich API, so that\u2019s what we have done.\u00a0 We couple that with good developer engagement - extremely good and continuous engagement.\u00a0 Developers then grow into partners and traders or create trading experiences that benefit our customers.\u00a0 We believe TradeKing offers the most secure yet simple API in the marketplace. Weve also conducted incentivized virtual hackathons and crowdsourcing initiatives, which have been very successful. This past fall, we conducted an extremely successful API Campus Challenge program, in which we encouraged undergraduate students to create platforms on our API. Young brains from colleges and universities all across the nation participated to create amazing apps.\n As a part of our overall API growth strategy, we also provide special pricing discounts for our API Partners.\n How do you work with Apigee?\n We built our API platform on Apigee Enterprise, and then we customized it to meet our requirements.\u00a0 Initially, we considered developing the entire API ourselves, but we decided to keep our primary focus on the core of our business, delivering a great brokerage experience for our customers, and draw on Apigee\u2019s expertise to help develop our API platform.\n Because we are in the financial sector, security is of the utmost importance. Apigees platform delivers excellent security features and is very customizable. In addition, Apigees staff worked with us in many, many ways, helping us think through APIs as a technology platform and helping us build the delivery platform itself.\n In our view, it comes down to three things: response, understanding customer needs, and relationship. Apigee delivered on these and has been a very valuable partner.\n What is your vision for your API program?\n We are one of the top-ranked online brokerages in the country, growing very rapidly. We manage peoples assets, give them a set of capabilities to trade, and offer an extremely transparent pricing structure. However, we recognized that our customers\u2019 demands for next-generation interfaces are rapidly changing, and our API helps us stay in front of that.\n We anticipate that in the next phase, the API will be a meaningful revenue driver for us. It is already opening us up to new markets.\n We will continue to develop our own platforms on the API and incentivize and engage our development partners.\u00a0 At the end of the day, it\u2019s all about delivering value to our customers, and the API will continue to help us drive increased value at an accelerated pace.","item_date":"May 22 2012 19:31:20","display_item_date":"05-22-2012","url":"http:\/\/blog.apigee.com\/detail\/innovator_spotlight_tradeking\/","source":"blog.apigee.com"},{"title":"7 Wicked Ecommerce Applications of Shopping APIs","details":"Ever wonder how Facebook Timeline and Twitter apps like Hootsuite and Klout are made possible? These social networks have made their APIs public (application programming interfaces), enabling developers to access their content and data in order to build their own unique experiences. In turn, these apps drive more traffic and usage to the social platforms, ensuring their long-term success (at least for the foreseeable future).\nSome APIs are even monetized. Google Maps is a \u201cfreemium\u201d API, API access is free if the application built will be free for all users, or licensed if the end-product will be itself monetized by the developer.\nThen there are ecommerce APIs, or shopping APIs.\nAccording to Brian Walker, VP and Principal Analyst of eCommerce and Multichannel Technology at Forrester Research:\nA Shopping API is \u201ca way of describing the exposing of core content and commerce functions in a programmatic way, over the Internet, for use either internally or by exposing to third parties. A Shopping API includes the ability to place orders.\u201d\nPut another way, typical commerce web services like customer information, merchandising systems, order history, search tools, product catalogs, etc. can be exposed to different consumer touch points, connecting through the application programming interface.\n7 Examples of Shopping APIs in the Wild\nFoodie.fm, a personalized grocery shopping app, uses Tesco\u2019s API to build its Apple and Facebook apps \u2014 an example of a 3rd party leveraging a shopping API.\nAnd of course, Tesco\u2019s South Korean subway experience. Using mobile phones and QR code readers, commuters can grocery-grab on their way home without ever visiting the supermarket. An example of an internal use of the API.\nSears augments its merchandising and creates a truly entertaining, interactive catalog for baby items.\nShoppers can answer quiz questions, listen to lullabies and download their lyrics, hover over hotspots and access customer reviews and social sharing tools. This is not your grandmother\u2019s baby catalog.\nYou can now conduct voice searches with nouveau-search engine Wolfram Alpha, Siri and Best Buy.\nSearch and compare prices, and access detailed information in or out of store.\nMarketplace APIs\nOutright is a freemium small business accounting software program that connects easily with PayPal, Etsy, Amazon and eBay.\nUsing APIs, Outright can call in sales, tax, shipping and other expense data directly from marketplaces and PayPal.\nVerizon Wireless\nVerizon is planning to allow both developers and consumers to take advantage of its upcoming \u201cturbo charge\u201d API. Network customers can tap into extra bandwidth to power their smartphone apps during times of high network congestion through a microtransaction API. For example, if a Skype call starts breaking up, the user can make a small payment to improve it.\nThe catch is the app must also use the API to enable the turbo option.\nNike\nNike\u2019s FuelBand records biometrical data while you exercise and syncs it to your smartphone app to keep you on top of your fitness goals. Nike opened up its FuelBand API to developers to hack and mash up with music (announced at SXSW this year) \u2014 we\u2019ll see what kind of wicked augmentations they come up with.\nPath and Nike have also teamed up to create features for sharing your favorite runs with friends.\nNot only is opening up the FuelBand API to other developers great for branding, it creates demand for the $150 gadget.\nBut the possibilities are endless.\nWhy think about APIs now?\nFuelBand is just one example of new consumer products that will be increasingly connected, social and even commerce enabled. According to Forrester Research in 2010, 8 million American adults owned 8 or more connected devices in their home.\nIt\u2019s undoubtedly even higher now.\nThe Web is key channel to service and sell to customers, but it\u2019s a maturing one.\nConsumer expectations for how to leverage their connected toys continue to increase. As consumers engage across wide variety of touchpoints, it becomes more complex for businesses to deliver great or even viral experiences, to market and extend the business leveraging that consumer\u2019s network.\nAnd innovators are creating these high expectations. Amazon is the perfect example of how you can leverage APIs to deliver consistent, relevant experience across touchpoints including the Kindle. A \u201csingle view of the customer\u201d allows account holders to see the same personalization and site content across channels touchpoints.\nHow different industries use commerce APIs\nDigital goods\nAPIs support in-app purchases and upgrades, subscription management, and entitlements management. For example, TapTap Revenge allows users to buy more tracks within the game. In many cases, a shopping API can be used to do much the same thing, instead of using the iTunes platform.\nSports Illustrated and many others provide access to subscription content across multiple platforms, recognizing entitlements to view content. But as new platforms are continuously emerging, without a scalable solution like a commerce API, keeping up will be very difficult.\nRetail\nAPIs make it easier and cheaper to deliver consistent pricing, offers, features and content across form factors (screen sizes), without the need to support each system with its own resources, and to integrate multiple touchpoints (mobile, mPOS, kiosks, digital signage, IPTV) into a single commerce engine more efficiently \u2014 with your own existing teams. Marketing and customer service can be supported more effectively, reducing the total cost of ownership for development and integration.\nFor businesses selling through marketplaces, using APIs can be way to scale across multiple properties. 3rd party tools may be more costly than using your own shopping APIs, and don\u2019t offer you the flexibility to innovate that using your own does. Ditto for social networks.\nBrick and mortar stores can use APIs to enable offline orders that are fulfilled in a traditional way to take place in the digital environment. Fulfillment systems can connect through APIs to enable drop-shipping direct from both warehouses and retail stores, increasing efficiency and availability of products.\nYour API questions answered by our expert panel\nOur webinar Shopping APIs and How They Future-proof Your Business (available on-demand for a limited time) included some very lively discussion around audience-submitted questions.\nWhat makes shopping APIs different from traditional content APIs?\nSal Visca: We have seen application programming interfaces for many years. Essentially, what we\u2019ve done in the past is take a technology platform, its user interface, the business and data logic, and rip off the user interface from the technology stack, and say the services underneath is really what the APIs are. Then you would put different technology layers on top of that that \u2013 a SOAP interface, or web services, REST, and say \u201clet\u2019s just manifest the services that are in the platform through these different technologies\u201d and have this multi-channel access in. So in a mobile web interface, you\u2019d have a mobile application calling those APIs.\nAt Elastic Path, we tried to look at this by turning it on its head. Instead of looking at the services we offer in the platform, and manifesting those, we turn it around and ask \u201chow are these services going to be consumed by the end user?\u201d The reason we\u2019re excited about this at Elastic Path is it enables what we refer to as \u201cElastic Commerce\u201d \u2013  the frictionless, social, everywhere type of commerce that looks at the Internet of things, and the ability to weave commerce transactions into the context of life.\nBrian Walker: The ecosystem has evolved to the point that APIs are core to business strategy. A business leader needs to think about APIs as a way to drive and extend their business. It\u2019s not just about integrating 2 technologies together and leveraging APIs to do that in a programmatic and efficient way. What makes a shopping API different is it\u2019s a productization of what APIs can do, and it becomes part of a business strategy moving forward. Think about a shopping API as an investment, and in itself a channel that can be utilized in a wide variety of ways to drive the business forward.\nWhich consumer devices are best positioned to benefit from a Shopping API?\nBrian Walker: Those that are best positioned are conventional Web as well as touchpoints like tablets and smartphones. Whether building out versions of your site or developing apps for them, the shopping API can enable a business to extend and embed commerce into wide variety of devices more easily. Digital display advertising, interactive TV and more futuristic applications, like appliances and cars represent what\u2019s perhaps coming. It will change rapidly, but it\u2019s hard to predict what will resonate the most. Commerce will become more increasingly embedded within the product itself. In order to \u201cfuture proof your business,\u201d here\u2019s a good opportunity to think about how laying this foundation can enable you to adjust and adapt more quickly because you can expose more quickly and embed commerce within products and services.\nFrom the consumer POV, how will they perceive the benefits of a Shopping API?\nSal Visca: Consumers themselves won\u2019t care so much that there\u2019s a shopping API, they care about the end-user experience, and the fact that a shopping API can enable new forms of innovation. In my role, I talk about some of these silly examples like being able to order from a watch using near-field communication. The end-user doesn\u2019t have to think too hard about going into a shopping experience, they\u2019re just going about their day-to-day business and using different devices to interact with life. Whether they\u2019re playing a game, or in a social context, it\u2019s what\u2019s enabled by underlying technology like a shopping API that can really support those new models.\nBrian Walker: The consumer is looking for it to work, and for it to work on the next device that they own. They can\u2019t understand why it wouldn\u2019t work, or why there would be a problem transacting when they can elsewhere, and the bar is set higher and higher by their experiences with other companies. The shopping API is an important tool for the business to react to how things change, and enable them to work in a consistent and scalable way. You don\u2019t want an offer on your tablet app that is no longer available because it is sold out. If you can\u2019t keep your different consumer touchpoints in sync, you\u2019re going to cause frustration and increase your customer service cost. Although customer doesn\u2019t care, they do care about the quality of the end experience.\nCan you describe how as a publishing company with multiple websites, we might capitalize on shopping APIs?\nSal Visca: A shopping API enables new forms of consumption. So, if you have multiple sites with content,  you can break up that content into more fine-grained components \u2014 maybe a chapter, maybe a complete book or volumes of books \u2014 whatever the granularity is, an API can now enable you to allow those to be consumed. E.g. if you\u2019re reading a book, and you reach the end, you now have a promotion to buy the next version of it. Or, if you\u2019re a law firm reading an article, and you need other case files, the recommendations we\u2019re all familiar with in an ecommerce experience. An API allows the elasticity and flexibility to take your digital content and intellectual property, and package it up in different ways to be consumed in different ways.\nHow should we get started with a shopping API?\nBrian Walker: It\u2019s easier to begin with product catalog as a way to begin working with APIs, and begin understanding how to establish that. Focus primarily on an existing project. For example, developing an app or mobile site is a great opportunity to take a step back and recognize that investing in APIs could establish that connection programmatically. It may take more time and money up front, but beginning that work now, leveraging an existing project, is going to pay dividends in the future.\nAnother important issue to mention, APIs are not a project, they are a program. One mistake clients may make is thinking of the API as a project to launch, and moving on to the next project. They need to recognize that having someone from the business side as well as developers working on APIs as an ongoing program is critical to further developing them, leveraging them and supporting them, and providing a platform to grow the business on. It\u2019s not  a situation where you launch it and walk away, and sometime come back to it to do an upgrade.\nSal Visca: At Elastic Path, we\u2019re all about agile development, learning, evolving and continually refining your usage. And in terms of getting started, we always recommend to start with a mobile app or web interface and try to call some of the different APIs, and see what you get back. It\u2019s amazing, once you get started, it begins to enable new forms of innovation. This needs to be an ongoing process that you incorporate into your overall process. As we get new form factors and new devices, it\u2019s important the API doesn\u2019t require a large amount of understanding of the underlying system and all the objects that get touched during that process. It\u2019s easy to slap on a REST interface or Java services, anyone can do that.\nYou talked about using shopping APIs to enable 3rd parties to integrate with our ecommerce platform. How about using shopping APIs for internal expansion, such as splitting the front-end from the back-end, putting the front-end on the cloud, and so on?\nSal Visca: I was working at SAP, a very large software company, and we found we had to build APIs between internal components to ensure that we were building nicely architected, layered solutions. It\u2019s amazing, if you put an API on a core set of services, it enforces a certain discipline on how internal development is done. And you become very componentized in your internal development, which is very key for not building old, monolithic systems, and enabling high adaptability to innovate and move quickly in the future, whether it\u2019s a public or private API.\nBrian Walker: Internal flexibility, and separating the front-end from the back-end is probably the most important value of an API strategy today. Exposing through 3rd parties is an interesting way that shopping APIs will continue to evolve, but the value proposition for businesses is about the increased flexibility internally.\nWhat business models would be most effective at monetizing an API?\nSal Visca: Obviously, you can get into transaction-based models where you\u2019re providing Commerce-as-a-Service, which allows a vendor to put a product into a catalog and sell it to end consumers. You have a transaction price, and some percentage of that price. Those are models that we\u2019re fairly familiar with. As we start to embed commerce in other applications, like a game \u2014 I have credits, I want to buy more ammunition, these kinds of things \u2014 there\u2019s lots of micro-payment type models that come out of that.\nBrian Walker: A lot of it does depend on what types of products a company is selling, and what\u2019s happening in their industry as a whole. For example, if you\u2019re a software publisher, you\u2019re probably feeling a lot of pressure at that $40-$50 price point. We\u2019re increasingly seeing, whether it be a game or an application being sold at a lower initial price point, the carving up of different levels or features \/ capabilities of the application, which might increasingly be delivered as a service itself, and allow users to pay as you go or pay for increased capability or features within the product. Media companies have the opportunity to deliver cross-platform content, and take a direct role in the commerce that they want to do, not relying always on third party marketplaces for distribution.\nBottom line, there will need to be a lot of experimentation, embedded and as devices continue to evolve, and integrated solutions within stores that can leverage ecommerce capabilities for \u201cendless aisle\u201d etc. Shopping API can help that occur much more efficiently than in the past.\nWhat are some use cases of the Shopping API that couldn\u2019t be done as easily with existing shopping methods?\nSal Visca: If you\u2019re in the retail space, when a customer is in-store and looking at a camera, using NFC and a touch pad can bring up generic information, reviews etc. A flexible API in the mobile shopping app can support that. In digital goods, it\u2019s more around I\u2019m watching video, and at the end I\u2019m offered a promotion to buy the next episode \u2014 to get people while they\u2019re consuming content. Having a flexible API that can be embedded in consumer applications would have been extremely difficult in the past, because you have to know so much about the underlying system, get back information in the mobile app and parse it, and deal with it. Now, you need to know a lot less about the underlying systems.\nBrian Walker: Look at the cost to integrate to a marketplace when Amazon began, or the early days of eBay, and even now. Those projects were in some cases hundreds of thousands of dollars. Shopping APIs streamline costs to launch a mobile site or mobile application, and really, it\u2019s about lowering the cost to drive and expand the business. Not that you couldn\u2019t do these things before, but it\u2019s about making it more efficient, more readily accessible and a core part of the business strategy. An area of investment and focus, rather than a project.\nInterested in learning more about APIs? Please view our on-demand webinar Shopping APIs and How They Future Proof Your Business, and stay tuned for our next post.","item_date":"May 22 2012 18:04:44","display_item_date":"05-22-2012","url":"http:\/\/www.getelastic.com\/7-wicked-ecommerce-applications-of-shopping-apis\/","source":"www.getelastic.com"},{"title":"The Magento REST API: A Better Way to Integrate Business Applications - eCommerce Software for Growth","details":"Merchants have been asking for a fast and secure way to integrate more business applications within Magento. We\u2019ve met this request by introducing the Magento REST API as part of the Magento Enterprise 1.12 and Community 1.7 releases.\n Noteworthy benefits of the REST API include simplicity, ease of testing and troubleshooting, and better performance. It allows you to manage customers, customer addresses, sales orders, inventories and products using HTTP verbs such as GET, POST, PUT and DELETE. Data requests and responses can be in XML or JSON format.\n  REST Resources\n  REST resources are simply the entities or identities that are exposed to the developer. REST defines the identity of the resource via the URI (uniform resource identifier). Each resource has a unique URL address and any interaction with a resource takes place at its URI. The following resources are supported in CE 1.7.0.0.\n  \tProducts: Allows you to retrieve the list of products, create a simple product, and update or delete a product.\n \tProduct Categories: Allows you to retrieve the list of categories assigned to a product and assign or unassign a category to a product.\n \tProduct Websites: Allows you to retrieve the list of websites assigned to a product and assign or unassign a website to a product\n \tCustomers: Allows you to retrieve the list of customers and create, update, or delete a customer.\n \tCustomer Addresses: Allows you to retrieve the list of customer addresses, and create, update, or delete an address.\n \tInventory: Allows you to retrieve the list of stock items and update a stock item.\n \tSales Orders: Allows you to retrieve the list of sales orders and specific order information.\n \tSales Order Items: Allows you to retrieve the items for a specific order.\n \tSales Order Addresses: Allows you to retrieve billing and shipping addresses for an order.\n \tSales Order Comments: Allows you to retrieve comments for a specific order.\n \n  Set up permissions to operate with resources for the three different user types: admin, customer, and guest. The admin is the backend logged-in user, the customer is the frontend logged-in user, and the guest is a non-logged-in frontend user.\n Configure which attributes will be allowed to retrieve or update for the different user types\n Register the third-party application (setting up consumer) and provide the information to the third-party application.\n \nFor a more detailed explanation with sample data, check out our wiki page. As always, we welcome your feedback and are eager to help with any issues you may encounter. Please use our bug tracker and choose the Webservices API from the Category selection.\n \t\t\t\t\t \t\t\t\t\t                                        \t\t\t\t\t\n@Bargento is the event to attend in France for anyone in the #Magento community,  you won\u2019t want to miss it May 29! http:\/\/t.co\/XbYibk8Z\nUpdated 11 months ago  \nHear a real true life testimony of how @smileycookie revamped its store, in a #webinar 5\/23 by @lyonsconsulting\nUpdated 23 hours ago  \nLearn how @Lyonscg created a customer friendly site for @smileycookie in a #Webinar 5\/23! Register Now: http:\/\/t.co\/KwLWHGOc\nUpdated 1 day ago  \nFollow Magento on Twitter","item_date":"May 22 2012 17:32:16","display_item_date":"05-22-2012","url":"http:\/\/www.magentocommerce.com\/blog\/the-magento-rest-api-a-better-way-to-integrate-business-applications\/","source":"www.magentocommerce.com"},{"title":"6,000 APIs: It\u2019s Business, It\u2019s Social and It\u2019s Happening Quickly","details":"The last 1,000 APIs in our directory were added in the shortest time ever. It was just over three months ago that we reached the 5,000 API milestone. What\u2019s new? The trends are actually getting harder to spot, like trying to explain the difference in the weather between now and five minutes ago. Previous trends, such as social and mobile are certainly continuing. And business uses of APIs are increasing, both in terms of those listed in the directory and our discussions with developers and providers.\n\n\n\n\n\nIt\u2019s not to say that APIs have been fun and games until now. Early adopters, such as the Salesforce API, have been around since before ProgrammableWeb, using APIs to extend their businesses. But until recently, many saw an API as a technical nice-to-have, not a necessity. The growth in APIs in general supports that this is no longer the case. Also, almost 15% of all 359 enterprise APIs were added in the last three months, with almost one per day added in May.\n\n\nLast month I shared some of the lessons we\u2019ve learned about APIs at The Next Web Conference (video embedded above). I talked about \u201cthe new API:\u201d apps, partners and income, three trends that show APIs mean business.\n\n\nApps are a big part of the private API iceberg. The growth in APIs that we don\u2019t track has to do with those that support mobile apps, but aren\u2019t made available to general developers. We know from talking to companies that there are a lot of these. Plus, any mobile app that does anything worthwhile needs to be able to send data back home.\n\n\nMany developers, who once simply used APIs, are now themselves becoming API providers, at least for private APIs. There is a whole backend-as-a-service ecosystem, which includes 24 backend APIs. Of these, two-thirds are part of the last 1,000 APIs. Talk about a growing sector.\n\n\n\n\n\nLooking at public APIs that support both developers and a company\u2019s apps, you\u2019ll find that most of the usage is internal. For example, the Guardian API sees seven times the requests from its internal apps as external developers. And most of Evernote\u2019s billions of API requests per month originate in its own software.\n\n\nRemember that catchy new acronym, \u201cSoLoMoClo?\u201d It stands for Social, Local, Mobile, Cloud, a handy way to describe the trends in greater tech. It also all is API-driven.\n\n\nOne in three social APIs is less than a year old. Already in 2012 we\u2019ve seen well over 100 of the 691 social APIs added to our directory. That puts us on pace for 325 added by the end of the year, which would be about 50% more than were added in 2011.\n\n\nMobile we\u2019ve already written about and there are many that are private that we may never know about. We also specifically track over 350 mobile APIs.\n\n\nThough we\u2019re a bit foggy on the definition of \u201ccloud,\u201d we do list 139 cloud APIs, which include many of the storage APIs, including the Google Drive API, which we said is kind of a big deal.\n\n\nFor what it\u2019s worth, we also track local APIs, but that\u2019s a bit more nebulous than cloud, even. For example, many of the 334 mapping APIs could be considered local.\n\n\nWe can never be sure where APIs are headed, but they\u2019re certainly getting there quickly. There are many more than 6,000 companies in the world, so if every company will have an API, they\u2019ll need to get crackin\u2019.\n\n\nFor further analysis of API milestones, see our coverage of 5,000, 4,000, 3,000 and 1,000 APIs.","item_date":"May 22 2012 17:27:45","display_item_date":"05-22-2012","url":"http:\/\/blog.programmableweb.com\/2012\/05\/22\/6000-apis-its-business-its-social-and-its-happening-quickly\/","source":"blog.programmableweb.com"},{"title":"Vordel Survey Finds Half of Enterprises Are Adopting APIs to Build Out New Business Channels","details":"BOSTON, May 22, 2012 (BUSINESS WIRE) -- Vordel, a provider of Application Gateways for Cloud, Mobile and SOA        computing, today announced the findings of its recent Application        Programming Interface (API) poll. The poll conducted during Vordels        recent webinar        (   http:\/\/www.vordel.com\/37    )        on architecting the delivery platform for the API economy, revealed        that half of the enterprises polled are adopting APIs to build out new        business channels. Additionally, the survey revealed a quarter of the        respondents polled are adopting APIs to roll out mobile applications;        with the remaining quarter of respondents leveraging APIs for other        unidentified purposes or to build a developer community.                                     \n                                          Vordel        survey reveals half of enterprises are adopting APIs to build out new        business channels (   http:\/\/www.vordel.com\/36    )                                     \n                                          The research also revealed that three quarters of the organizations        polled appointed IT managers and enterprise architects to oversee their        API strategy. Companies and consumers around the world are being wired        together via APIs and according to Forrester Research, Every enterprise        with an Internet presence has opportunities to expose web-based        application programming interfaces (APIs) to third-party access,        unlocking value in data and services that would otherwise be hidden        behind monolithic legacy systems. (1)                                     \n                                          Our research shows enterprises are increasingly leveraging APIs to        build out new business channels, said Hugh Carroll, VP Marketing for        Vordel. With todays tight budgets, Vordel recommends those responsible        for overseeing the use of APIs in their businesses, should fully        understand how to leverage their existing investments in SOA Application        Gateways to ensure their organization efficiently delivers on its        enterprise API strategy.                                     \n                                          Recommendations:                                     \n                                          The webinar        (   http:\/\/www.vordel.com\/37    )        with Randy Heffner identified several simple practices that enterprise        architects and IT managers can follow to ensure they are meeting the        enterprise API delivery schedule. These practices include:                                     \n                                          --         Identifying a best practice reference architecture for API Delivery                                     \n                                          --         Learning how to leverage existing SOA infrastructure to manage API          delivery                                     \n                                          --         Identifying challenges for securing and managing API delivery                                     \n                                          About Vordel                                     \n                                          Vordel links all enterprise applications, users, and devices across        Cloud, mobile, and on-premise environments. Vordel Application Gateway        provides integration, security, governance, and acceleration for        enterprise applications and APIs. Fortune 5000 enterprises and        government agencies use Vordel to extend their enterprise applications        and SOA infrastructure beyond the perimeter to enable Cloud based        services and mobile computing. Vordel makes it possible to deliver and        consume Applications Anywhere with ITs existing applications and        infrastructure, without costly upgrades and rewrites.                                     \n                                          About Vordel API Poll                                     \n                                          The webinar focusing on the evolution from SOA to API outlines the        foundation delivery platform for the rapidly emerging enterprise API        economy. The webinar was attended by more than 100 participants from        Fortune 500 companies who were polled for their response to several        questions.                                     \n                                          (1) Forrester Report: Protecting Enterprise APIs With A Light        Touch July 2011","item_date":"May 22 2012 17:16:33","display_item_date":"05-22-2012","url":"http:\/\/www.marketwatch.com\/story\/vordel-survey-finds-half-of-enterprises-are-adopting-apis-to-build-out-new-business-channels-2012-05-22","source":"www.marketwatch.com"},{"title":"Data journalism research at Columbia aims to close data science skills gap - O'Reilly Radar","details":"Successfully applying data science to the practice of journalism requires more than providing context and finding clarity in vasts amount of unstructured data: it will require media organizations to think differently about how they work and who they venerate. It will mean evolving towards a multidisciplinary approach to delivering stories, where reporters, videographers, news application developers, interactive designers, editors and community moderators collaborate on storytelling, instead of being segregated by departments or buildings. \n  The role models for this emerging practice of data journalism wont be found on broadcast television or on the lists of the top journalists over the past century. Theyre drawn from the increasing pool of people who are building new breeds of newsrooms and extending the practice of computational journalism. They see the reporting that provisions their journalism as data, a body of work that can itself can be collected, analyzed, shared and used to create longitudinal insights about the ways that society, industry or government are changing. (Or not, as the case may be.) \n  In a recent interview, Emily Bell (@EmilyBell), director of the Tow Center for Digital Journalism at the Columbia University School of Journalism, offered her perspective about whats needed to train the data journalists of the future and the changes that still need to occur in media organizations to maximize their potential. In this context, while the role of institutions and journalism education are themselves evolving, they both will still fundamentally matter for whats next, as practitioners adapt to changing newsonomics. \n  Our discussion took place in the context of a notable investment in the future of data journalism: a $2 million research grant to Columbia University from the Knight Foundation to research and distribute best practices for digital reportage, data visualizations and measuring impact.  Bell explained more about what how the research effort will help newsrooms determine whats next on the Knight Foundations blog: \nThe knowledge gap that exists between the cutting edge of data science, how information spreads, its effects on people who consume information and the average newsroom is wide.  We want to encourage those with the skills in these fields and an interest and knowledge in journalism to produce research projects and ideas that will both help explain this world and also provide guidance for journalism in the tricky area of \u2018what next\u2019.  It is an aim to produce work which is widely accessible and immediately relevant to both those producing journalism and also those learning the skills of journalism.\n  We are focusing on funding research projects which relate to the transparency of public information and its intersection with journalism,  research into what might broadly be termed data journalism, and the third area of  \u2018impact\u2019 or, more simply put, what works and what doesn\u2019t.\n  Our interview, lightly edited for content and clarity, follows. \n  What did you do before you became director of the Tow Center for Digital Journalism?\n  I spent ten years where I was editor-in-chief of The Guardian website. During the last four of those, I was also overall director of digital content for all The Guardian properties. That included things like mobile applications, et cetera, but from the editorial side. \n  Over the course of that decade, you saw one or two things change online, in terms of what journalists could do, the tools available to them and the news consumption habits of people. You also saw the media industry change, in terms of the business models and institutions that support journalism as we think of it. What are the biggest challenges and opportunities for the future journalism? \n  For newspapers, there was an early warning system: that newspaper circulation has not really consistently risen since the early 1980s.  We had a long trajectory of increased production and actually, an overall systemic decline which has been masked by a very, very healthy advertising market, which really went on an incredible bull run with a more static pictures, and just widen the pipe, which I think fooled a lot of journalism outlets and publishers into thinking that that was the real disruption. \n  And, of course, it wasn\u2019t. \n  The real disruption was the ability of anybody anywhere to upload multimedia content and share it with anybody else who was on a connected device.  That was the thing that really hit hard, when you look at 2004 onwards. \n  What journalism has to do is reinvent its processes, its business models and its skillsets to function in a world where human capital does not scale well, in terms of sifting, presenting and explaining all of this information.  That\u2019s really the key to it. \n  The skills that journalists need to do that -- including identifying a story, knowing why something is important and putting it in context -- are incredibly important.  But how you do that, which particular elements you now use to tell that story are changing. \n  Those now include the skills of understanding the platform that you\u2019re operating on and the technologies which are shaping your audiences\u2019 behaviors and the world of data. \n  By data, I don\u2019t just mean large caches of numbers you might be given or might be released by institutions: I mean that the data thrown off by all of our activity, all the time, is simply transforming the speed and the scope of what can be explained and reported on and identified as stories at a really astonishing speed. If you don\u2019t have the fundamental tools to understand why that change is important and you don\u2019t have the tools to help you interpret and get those stories out to a wide public, then you\u2019re going to struggle to be a sustainable journalist. \n  The challenge for sustainable journalism going forward is not so different from what exists in other industries: theres a skills gap. Data scientists and data journalists use almost the exact same tools.  What are the tools and skills that are needed to make sense of all of this data that you talked about? What will you do to catalog and educate students about them? \n  Its interesting when you say that the skills of these clients are very similar, which is absolutely right. First of all, you have a basic level of numeracy needed - and maybe not just a basic level, but a more sophisticated understanding of statistical analysis.  That\u2019s not something which is routinely taught in journalism schools but that I think will increasingly have to be. \n  The second thing is having some coding skills or some computer science understanding to help with identifying the best, most efficient tools and the various ways that data is manipulated. \n  The third thing is that when you\u2019re talking about data scientists, it\u2019s really a combination of those skills.  Adding data doesn\u2019t mean you dont have to have other journalism skills which do not change: understanding context, understanding what the story might be, and knowing how to derive that from the data that you\u2019re given or the data that exists.  If it\u2019s straightforward, how do you collect it?  How do you analyze it?  How do you interpret them and present it? \n  It\u2019s easy to say, but it\u2019s difficult to do. It\u2019s particularly difficult to reorient the skillsets of an industry which have very much resided around the idea of a written story and an ability with editing. Even in the places where I would say there\u2019s sophisticated use of data in journalism, it\u2019s still a minority sport. \n  I\u2019ve talked to several heads of data in large news organizations and they\u2019ve said, \u201cWe have this huge skills gap because we can find plenty of people who can do the math; we can find plenty of people who are data scientists; we can\u2019t find enough people who have those skills but also have a passion or an interest in telling stories in a journalistic context and making those relatable.\u201d \n  You need a mindset which is about putting this in the context of the story and spotting stories, as well having creative and interesting ideas about how you can actually collect this material for your own stories. It\u2019s not a passive kind of processing function if you\u2019re a data journalist: it\u2019s an active speaking, inquiring and discovery process. I think that that\u2019s something which is actually available to all journalists. \n  Think about just local information and how local reporters go out and speak to people every day on the beat, collect information, et cetera. At the moment, most get from those entities don\u2019t structure the information in a way that will help them find patterns and build new stories in the future. \n  This is not just about an amazing graphic that the New York Times does with census data over the past 150 years.  This is about almost every story.  Almost every story has some component of reusability or a component where you can collect the data in a way that helps your reporting in the future.   To do that requires a level of knowledge about the tools that you\u2019re using, like coding, Google Refine or Fusion Tables.  There are lots of freely available tools out there that are making this easier.  But, if you don\u2019t have the mindset that approaches, understands and knows why this is going to help you and make you a better reporter, then it\u2019s sometimes hard to motivate journalists to see why they might want to grab on.    The other thing to say, which is really important, is there is currently a lack of both jobs and role models for people to point to and say, \u201cI want to be that person.\u201d   I think the final thing I would say to the industry is we\u2019re getting a lot of smart journalists now. We are one of the schools where all of our digital concentrations from students this year include a basic grounding in data journalism. Every single one of them.  We have an advanced course taught by Susan McGregor in data visualization. But we\u2019re producing people from the school now, who are being hired to do these jobs, and the people who are hiring them are saying, \u201cWrite your own job description because we know we want you to do something, we just don\u2019t quite know what it is.  Can you tell us?\u201d    You can\u2019t cookie-cutter these people out of schools and drop them into existing roles in news trends because those are still developing.  What we\u2019re seeing are some very smart reporters with data-centric mindsets and also the ability to do these stories -- but they want to be out reporting.  They don\u2019t want to be confined to a desk and a spreadsheet.  Some editors usually find that very hard to understand, \u201cWell, what does that job look like?\u201d    I think that this is where working with the industry, we can start to figure some of these things out, produce some experimental work or stories, and do some of the thinking in the classroom that helps people figure out what this whole new world is going to look like.  \nWhat do journalism schools need to do to close this skills gap?  How do they need to respond to changing business models?  What combination of education, training and hands-on experience must they provide?\n  One of the first things they need to do is identify the problem clearly and be honest about it. I like to think that we\u2019ve done that at Columbia, although I\u2019m not a data journalist.  I don\u2019t have a background in it.  I\u2019m a writer.  I am, if you like, completely the old school. \n  But one of the things I did do at The Guardian was helped people who early on said to me, \u201cSome of this transformation means that we have to think about data as being a core part of what we do.\u201d  Because of the political context and the position I was in, I was able to recognize that that was an important thing that they were saying and we could push through changes and adoption in those areas of the newsroom. \n  That\u2019s how The Guardian became interested in data.  It\u2019s the same in journalism school. One of the early things that we talked about [at Columbia] was how we needed to shift some of what the school did on its axis and acknowledge that this was going to be key part of what we do in the future. Once we acknowledged that that is something we had to work towards, [we hired] Susan McGregor from the Wall Street Journal\u2019s Interactive Team.  She\u2019s an expert in data journalism and has an MA in technology in education. \n  If you say to me, \u201cWell, what\u2019s the ground vision here?\u201d  I would say the same thing I would say to anybody: over time, and hopefully not too long a course of time, we want to attract a type of student that is interested and capable in this approach. That means getting out and motivating and talking to people. It means producing attractive examples which high school children and undergraduate programs think about [in their studies]. It means talking to the CS [computer science] programs -- and, in fact, more about talking to those programs and math majors than you would be talking to the liberal arts professors or the historians or the lawyers or the people who have traditionally been involved. \n  I think that has an effect:  it starts to show people who are oriented towards storytelling but have capabilities which are align more with data science skill sets that there\u2019s a real task for them. We can\u2019t message that early enough as an industry. We can\u2019t message it early enough as an educator to get people into those tracks.  We have to really make sure that the teaching is high quality and that we\u2019re not just carried away with the idea of the new thing, we need to think pretty deeply about how we get those skills. \n  What sort of basic sort of statistical teaching do you need?  What are the skills you need for data visualization?  How do you need to introduce design as well as computer science skills into the classroom, in a way which makes sense for stories? How do you tier that understanding? \n  Youre always going to produce superstars. Hopefully, we\u2019ll be producing superstars in this arena soon as well. \n  We need to take the mission seriously. Then we need to build resources around it.  And that\u2019s difficult for educational organizations because it takes time to introduce new courses.  It takes time to signal that this is something you think is important. \n  I think we\u2019ve done a reasonable job of that so far at Columbia, but we\u2019ve got a lot further to go. Its important that institutions like Columbia do take the lead and demonstrate that we think this is something that has to be a core curriculum component. \n That\u2019s hard, because journalism schools are known for producing writers.  They\u2019re known for different types of narratives.  They are not necessarily lauded for producing math or computer science majors. That has to change.","item_date":"May 22 2012 17:14:26","display_item_date":"05-22-2012","url":"http:\/\/radar.oreilly.com\/2012\/05\/data-journalism-research-at-co-1.html","source":"radar.oreilly.com"},{"title":"Box upgrades API to make it simpler, more powerful","details":"IDG News Service - Box has given its API (application programming interface) a makeover so that developers will have an easier time building and integrating applications with its software, and have those applications do more than is currently possible.\n Version 2 of the API for the Box cloud-hosted enterprise collaboration and content management software is now in beta, but is expected to reach final release form soon, according to company officials.\n Box plans to keep the original API active at least through the end of this year, and maybe longer if it deems it necessary to avoid disrupting the third-party applications that rely on it. Eventually, existing applications will need to be migrated over to version 2 of the API.\n There are more than 170 applications that integrate with the Box software, making about 300 million API calls per month. Box is used by 120,000 businesses -- including 82% of the Fortune 500 -- and 10 million end users.\n The new API is based on more modern technologies, such as JSON (JavaScript Object Notation), whereas the original API used XML, for example. The end result should be that developers find it easier and more powerful to use, according to Box.\n In terms of new capabilities, version 2 of the API allows for the creation of applications that can access more deeply collaborator discussions occurring in Box among end users. A new feature called Instant Mode streamlines and simplifies the process for granting end user-access to third-party applications tied to Box.\n Box also announced that its recently launched OneCloud central console for third-party mobile applications now also has an improved software development kit with new documentation and libraries intended to simplify the application integration process.\n In addition, 15 new mobile application vendors have built links for their applications and OneCloud, including CloudOn, iDesk and Handshake, bringing the total to almost 50 iOS-based apps. OneCloud will support Android applications later.\n Juan Carlos Perez covers enterprise communication\/collaboration suites, operating systems, browsers and general technology breaking news for The IDG News Service. Follow Juan on Twitter at @JuanCPerezIDG.","item_date":"May 22 2012 16:58:02","display_item_date":"05-22-2012","url":"http:\/\/www.computerworld.com\/s\/article\/9226570\/Box_upgrades_API_to_make_it_simpler_more_powerful","source":"www.computerworld.com"},{"title":"The Guardian creates an API for n0tice, its open news platform","details":"The Guardian is taking its commitment to open journalism further today, releasing an open API for n0tice, its community messaging platform. Launched last fall, n0tice is an amalgam of several things, a community bulletin board, a classifieds service and a local news wire. \n In the same way Twitter asks \u201cwhat\u2019s happening,\u201d n0tice poses the question \u201cwhat\u2019s happening near you,\u201d and on any given day that could include updates on Olympics-related road closures, public meeting notices or a recipe for a cocktail to celebrate the queen\u2019s jubilee.\n In releasing an API for n0tice, the Guardian is inviting businesses, journalists, and others to find new uses for all of the information residents are searching for and sharing every day. \u201cIt feels like we\u2019re sitting on this huge bundle of potential and it\u2019s just a matter of continuing to execute,\u201dMatt McAllister, the Guardian\u2019s director of digital strategy, said in an interview.\n It\u2019s probably not a coincidence n0tice asks a similar question to Twitter, both offer a deceptively simple service that has potential to offer more in return. n0tice is your newsfeed, but it\u2019s also your CraigsList. And like CraigsList n0tice is inherently local. If you come to the site via desktop it asks you where you are, on the phone it uses GPS to determine your location. Though most of the n0tice activity takes place in the UK, you can find noticeboards internationally. \u201cWe\u2019re certainly working with a new paradigm for users experience, where location is navigation,\u201d he said. \n The Guardian sees that as a kind of utility that has value to readers, but also to a broader network of developers who can build new tools, visualizations and applications. They\u2019re launching the API now because they\u2019ve built up a small but active user base and are looking to grow further, McAllister said. \n The API can help with that in some ways by exposing n0tice to more people in new incarnations. The Guardian has been using the n0tice API internally for crowdmapping projects, including their coverage of the summer Olympics. As the the games\u2019 symbolic flame makes its way to London, the Guardian is using n0tice to help map the torch route. Using an automated feed of photos and text updates from local n0tice users, those submissions supplement stories and multimedia the Guardian is already producing for the Olympics, McAllister said. \u201cThe torch route is quite fun because it just ticks a lot of our boxes,\u201d McAllister said.\n At least several of those boxes are marked \u201copen\u201d or \u201copen source,\u201d which is no surprise given the Guardian\u2019s stated commitment to what they call open journalism. n0tice serves many needs, one for the locals as a means of connecting through news or activities, but also a need for the Guardian to continue to show what open journalism means. \n The API is a clear invitation for people to wrench on the data coming through n0tice and explore what an open journalism project could look like. The Guardian is using projects like the torch map as an example for what the platform is capable of. McAllister said a company in the UK is planning to launch a branded campaign using the API soon. More than 700 noticeboards have been launched since the project started in late 2011, and most people are still discovering new uses for the platform. Though people are finding new uses for n0tice, McAllister said they\u2019re mirroring habits from other services, essentially as another place to link to a blog or Tweet. \n McAllister said the hope is for people to use the boards to cultivate public spaces they share with others. The more boards created, the richer the API becomes. \u201cIt comes down to an openness question,\u201d he said. \u201cWhen you can let go a bit and let a community run with the space you created then amazing things can start to happen.\u201d\n This is why the boards offer users a lot of ownership, they can customize their own branding and subdomain. They also can moderate the activity on the board. Another benefit to owners is the potential to make money with their noticeboards through ads, specifically having advertisers pay for prominent placement on boards. \n With the n0tice site running and the addition of the API, McAllister said the next part of the trilogy is rolling out an iPhone app. While the site works well in mobile browsers, he said they wanted to create a unique app experience that can take advantage of the relationship between where you are and what\u2019s happening around you. \u201cWhen you\u2019re holding a device location has much more meaning,\u201d he said. \n \n     \t\t\t  \t\t\t\t \t\t\t\tThe Guardian is a foundation-owned national British newspaper known for its global reach and liberal editorial stance. The Guardian was founded in 1821 as the Manchester Guardian (it was moved to London in 1964). In 1936, ownership of the paper\u2026\n   \t\t\t     \t\t\t\n   Ken Doctor\u00a0\u00a0\u00a0\u00a0May 17, 2012 The newsonomics of News U. \nJournalism and education are both about knowledge. Could their post-disruption business models start to blur?","item_date":"May 22 2012 16:57:08","display_item_date":"05-22-2012","url":"http:\/\/www.niemanlab.org\/2012\/05\/the-guardian-creates-an-api-for-n0tice-its-open-news-platform\/","source":"www.niemanlab.org"},{"title":"Open Legislation Hackathon, Victoria, BC","details":"The goal is to share the experience, findings, and difficulties of creating online apps based on legislative data published by the different Queen\u2019s Printers of Canada.  Present your apps and experience to representatives of the annual QPAC conference from across Canada! Your app and presentation may change the way other provinces look at legislative data, online licensing and open data policies!\n What are we building?\n Anything you like!  It would be great if it uses legislative data. We want to see what you can come up with! Focus on multiple jurisdictions, or just one.\n For more information, please see the announcement.\n HT @kimnayyer and @ColinLaChance.\n Click here for other upcoming legal hacking events.","item_date":"May 22 2012 02:51:23","display_item_date":"05-21-2012","url":"http:\/\/legalinformatics.wordpress.com\/2012\/05\/21\/june-2-open-legislation-hackathon-victoria-bc\/","source":"legalinformatics.wordpress.com"},{"title":"Buddy Media Unifies Social Marketing Data with New API","details":"With Facebook\u2019s IPO scheduled for the 18th this month, many are questioning the nearly $100 billion valuation. No one argues that individuals and companies of many shapes and sizes currently use facebook as a marketing tool. The argument boiling around Facebook is whether advertising on Facebook works. \u00a0Most social websites that drive revenue through selling ad space have proprietary tools that measure ROI. However, Buddy Media hopes to remove the \u201canarchy\u201d of social media marketing and give deeper insight to advertising across multiple social media sites with its new unified social media API.\n\n\nThe social media API allows marketers to track conversions and campaigns across multiple social media sites (e.g. Facebook, Pinterest, Google+, Youtube, etc.) from a single enterprise social media suite. CEO, Michael Lazerow, commented that Buddy Media alone can \u201cprovide a unified solution that helps our customers organize their teams, optimize their social programs to provide real business results, and repeat.\u201d Buddy Media goes far beyond tracking \u201cshares\u201d and \u201clikes.\u201d With the API, developers can integrate data gathered (e.g. actions taken, sales conversions, etc.) with existing email and CRM packages.\n\n\n\n\n\nBuddy Media boasts that 8 of the world\u2019s top 10 brands already utilize its social media suite. Key clients include L\u2019Oreal, Virgin Mobile Live, Carnival Cruise Lines, Mattel, HP, UNFPA, Lenovo, and the NFL. The NFL\u2019s General Manager of Digital Media, George Scott, confidently backed the new API: \u201cThe unified social media API will allow us to collect social data across the league and use it to better inform all of our social marketing decisions.\u201d\n\n\nBuddy Media expects a unified approach to social media will attract greater use of their suite because of the ever-expanding use of social media marketing. During the release, Buddy Media reported that studies show \u201ccompanies now average 178 social corporate media accounts, and that number will likely increase.\u201d","item_date":"May 22 2012 02:45:38","display_item_date":"05-21-2012","url":"http:\/\/blog.programmableweb.com\/2012\/05\/21\/buddy-media-unifies-social-marketing-data-with-new-api\/","source":"blog.programmableweb.com"},{"title":"Can Developers Really Be Bought Off To Build At Hackathons?","details":"I was at TC Disrupt this past weekend and was talking to a ton of developers asking them what they were working on. It seemed like every other person I spoke with was building off of Mobli\u2019s API. I found that a bit odd as Mobli is not super popular. I soon found out that Mobli was offering a $10,000 prize to the best use of the Mobli API. $10k is a lot of money and it all made sense- Mobli was buying off developers to use their API.\n I\u2019m singling out Mobli, but there were tons of companies giving out incentives to hack on their platform. I haven\u2019t seen a single company give out quite as much money before, but offering prizes to use an API is nothing new. It\u2019s a great thing for both companies and developers.\n I think it will be interesting to see: 1) Will people continue to use the Mobli API once this hackathon is over? Does the big cash prize carry over post-hackathon? 2) Will anyone build anything meaningful on Mobli that lives on for more than a week or two?\n Who knows, my gut tells me no, but you never know.\n I guess the real question is whether developers can be bought off to build at hackathons and if this is a worthwhile route for companies to take or just a waste of money?\n  \t\t\t\t\t\n  \t\t                                                                                                                                                                                                                                beautyinbreakdown said:                                                                                                                              I think this\u2019ll depend a lot more on the overall experience of using the API \u2014 is it easy to integrate, are the Mobli people responsive to questions\/concerns, etc.","item_date":"May 22 2012 02:44:13","display_item_date":"05-21-2012","url":"http:\/\/alexstechthoughts.com\/post\/23477309234\/can-developers-really-be-bought-off-to-build-at?372dc640","source":"alexstechthoughts.com"},{"title":"78 New APIs: MasterCard, Travelocity and AddShoppers","details":"This week we had 78 new APIs added to our API directory including a data integration service, travel booking service, user social information and data service, wallet checkout service and social eCommerce sharing platform. In addition we covered the release of the Seamless Food Delivery API. Below are more details on each of these new APIs.\n\n\n Accthub API: Accthub is a service that acts as an account management system for app builders. It is used to store and mange users account information, credentials, addresses and meta-data. Among the information it can store includes users email address, hashed password, full name, langage, timezone, gender, company, website URL. The API gives users access to the full functionality of the service. The API uses RESTful calls and responses are formatted in JSON.\n\n\n ActiveBuilding API: ActiveBuilding is a platform for property management and resident communications. Some features included in ActiveBuilding include maintenance request management and tracking, package tracking, and mass mailing to residents for announcements.\n\n\nThe ActiveBuilding API allows developers to access and integrate the functionality of ActiveBuilding with other applications and to create new applications. Some example API methods include retrieving community information, managing package tracking information, and adding and editing content.\n\n\n AddShoppers Social Product API: AddShoppers is a social commerce sharing platform. They aim to help retailers increase sharing by attaching their Social Promo Callouts to sharing buttons. The platform is fully integrated with Facebook\u2019s Want\/Own actions. With the API users will have the ability to sort products by most wanted, Tweeted, etc., setup triggers that will email customers automatically when a Wanted product is released, measure the ROI of social sharing at the product level. The API uses RESTful calls and responses are formatted in JSON.\n\n\n Betfair Games API: The Betfair Games Exchange allows players to log in to online versions of Blackjack, Hi Lo, Bullseye Roulette, Hold Em, Baccarat, Card Derby Racing and Omaha Hi. The Betfair Games API uses REST protocol that allows developers to integrate with the Games Exchange and retrieve data on the games market and their personal account. It also offers transactional capabilities so players can manage and place bets through the Games API.\n\n\n Buddy Media API: Buddy Media is a provider of enterprise level social marketing software. Used by eight of the worlds top ten global advertisers, Buddy Media helps brands build connections from initial point of contact through point of purchase. The social media API allows marketers to track conversions and campaigns across multiple social media sites (e.g. Facebook, Pinterest, Google+, Youtube, etc.) from a single enterprise social media suite. The API allows developers to integrate data gathered such as actions taken, sales conversions, etc. with existing email and CRM packages. Full documentation is not publicly available.\n\n\n Budget Your Trip API: Budget Your Trip is a service that determines travel costs based on users\u2019 information. Users register their trips and include expenses by category and location. The information the input will be shown against average costs computed from other users\u2019 similar travels. Their RESTful API exposes the website\u2019s essential functionality. Results about trip location, currency, and average costs are returned in XML, JSON, or text. \n\n\n Buyvite Transaction API: Buyvite is a group purchasing platform available via web, mobile and Facebook apps. It allows the user to organize group purchases like event tickets, gifts, flights, etc. and collect money easily through the secure payment platform. The API uses POST requests to submit transactions and retrieve account information.\n\n\n ChowNow API: ChowNow is a mobile and Facebook application for restaurants to manage food orders. Restaurants can use ChowNow to create their own applications for their customers to order food from, either from mobiles or Facebook.\n\n\nThe ChowNow API allows developers to access and integrate the functionality of ChowNow with other applications and to create new applications. Public documentation is not available; interested developers should sign up for more information here: http:\/\/www.chownow.com\/#!\/api.\n\n\n Coupz API: Coupz is an online deal aggregator. Coupz sends out daily deal emails by city from a variety of retail stores, restaurants, and other services.\n\n\nThe Coupz API allows developers to access and integrate the functionality of Coupz with other applications. Public documentation is not available; interested developers should sign up here: http:\/\/www.coupz.com\/api\/index.html.\n\n\n Crisply API: Crisply is a time-tracking service that connects with other systems used to automatically track a users time. Some features of Crisply include timesheets, project systems, and billing systems. \n\n\nThe Crisply API allows developers to access and integrate the functionality of Crisply with other applications. Some example API methods include accessing timesheets, retrieving times logged by activity and client, and managing user information and account information.\n\n\n DeliveryWatch API: The service monitors European email traffic and providers to report statistics for delivery rates, uptime, spam filtering in place, and related measures. It can provide notice of email delivery via a large selection of internet service providers and blacklist monitoring, with updates when a sender is tagged as a sender of spam.\n\n\nAPI methods support retrieval of email delivery performance for ISPs worldwide along with updated reports on spam filtering configurations and blocked sender lists. Methods allow targeting of reports for specific recipient domains and filtering by leading spam monitors.\n\n\n Digitalsmiths API: Digitalsmiths is a video search and recommendation service that creates and leverages data around video content\n including TV, film and live events across any connected device. Digitalsmiths solutions help users find relevant, personalized entertainment across multiple channels and devices. The API allows for metadata integration. Documentation is not available.\n\n\n DRCOG API: The Denver Regional Council of Governments (DRCOG) has a goal of fostering regional cooperation among county and municipal governments in the Denver metropolitan area. The DRCOG database contains spatial and tabular data that is used for regional planning and policy decisions. Users can search this database by keyword or browse it by subject. The API allows users to look up data including geotable attributes, list geotable fields, retrieve features from a specified geotable and more. The API uses RESTful calls and responses are formatted in XML, JSON and KML.\n\n\n Easy Projects API: Easy Projects is a web-based project management platform for businesses seeking to offer a collaborative online environment for team and individual projects. It offers team management tools for assigning tasks, reporting tools for monitoring business efficiency, and messaging boards for collaborating. The API uses GET, POST, PUT and DELETE calls to interact with projects, activities, timelogs, messages, users and portfolios.\n\n\n eCoComa Convert API: The eCoComa Convert API allows users to convert text between a variety of types and styles. Users may convert text between Unicode and utf8 characters, or between Unicode and Chinese text. Chinese characters can be converted between their traditional and simplified forms. Users may also convert between C# and VB.NET code, or between a UNIX timestamp and System.DateTime.\n\n\n eCoComa Domain API: The eCoComa Domain API allows users to retrieve information about a specific host from the Internet Domain Name System (DNS) or to find sites that are hosted by a specific IP address. Another function provides access to the whois service to check domain name availability or to find out who owns a particular domain name.\n\n\n eCoComa Email API: The eCoComa Email API allows users to perform two separate e-mail-related functions. One function allows users to receive e-mail from a POP3 server, while the other is used to chat someones mail server to determine whether their address is valid. This API is available via SOAP calls using the XML data format.\n\n\n eCoComa Geo API: The eCoComa Geo API allows users to perform a variety of geolocation tasks. Users may retrieve accurate city and state information for a U.S. ZIP Code, or find the matching ZIP Code for a given address, city, and state. Users may also employ a function to find the distance between two U.S. addresses. Another function determines an internet visitors country based on their IP address.\n\n\n eCoComa Marketing API: The eCoComa Marketing API provides users with access to methods for search engine marketing (both search engine optimization and pay per click advertising), banner advertising, e-mail marketing, affiliate marketing, interactive advertising, and email advertising. This API is accessible via SOAP calls using the XML data format.\n\n\n eCoComa RSS API: The eCoComa RSS API allows users to create custom RSS (Really Short Syndication) feeds that they can subscribe to in order to receive the latest updates on news or other regularly updated information sources. Users may use this API to combine and filter multiple RSS feeds or create entirely new RSS feeds. The API can be accessed via SOAP calls using the XML data format.\n\n\n eCoComa Video API: The eCoComa Video API allows users to search for videos from a variety of online sources. Given a keyword to search for, the API retrieves a list of videos that are associated with the specified keyword. Calls exist to retrieve lists from MSN, AOL, Google, and Yahoo. This API operates via SOAP calls using the XML data format.\n\n\n eFreightline API: The service aggregates shipping rates and freight ratings along with available capacity from over 50 carriers serving both regional and national destinations. This single data source can deliver rate quotes for full-truckload or less-than-truckload (LTL) shipments, creating a central source where shippers can plan for transportation needs.\n\n\nAPI methods support submission of load size, items to be shipped, desired ship date, and points of pickup and destination, along with other specifications. The service returns rate quotes and current capacity for potential shippers. Methods also allow completion of the shipping transaction.\n\n\n eHealth Technology API: The service provides health insurance carriers and general agents with integrated access to exchanges that support their underwriting of consumer applications for coverage. Underwriting functions are supplemented by membership and enrollment tools to accept and automatically process applications, return notifications and status information, and enable instant underwriting (eApproval) upon submission of an application.\n\n\nAPI methods support submission of customer application information, underwriting checks to match appropriate coverage, and instant approval with notification of completion. Extension of API access to independent insurance agents and brokers is pending.\n\n\n Euro42 Euro to Gulden API: The Euro42 Euro to Gulden API is a currency conversion service that allows users to convert between Euros and Guldens. Gulden is the German word for a gold penny, equivalent to the Dutch term Guilder. Guldens have been used in a number of European countries, though which country the Guldens in this API are meant to be from is unspecified.\n\n\nAll API documentation is in English, but the rest of the website is in Dutch.\n\n\n FAROO API: FAROO is a web search engine. FAROOs search functionality is based on peer-to-peer (P2P) searching, user-generated reviews and attention, and webpage popularity based on users.\n\n\nThe FAROO API allows developers to access and integrate the search functionality of FAROO with other applications. The available API method is displaying search results, news results, and trending topics. \n\n\n FixYa API: FixYa.com is a community-based question and answer site. Users log in and post any questions, specifically about technical problems they have. Other community members and subject experts provide solutions. Their API is offered through business-to-business partnership. It exposes their large repair and support solutions database to be integrated in customer service operations. \n\n\n FleetMon API: The service provides worldwide position, tracking, and traffic information for maritime vessels. It locates ocean-going ships, with indications of last port visited and destination port, times of arrival and departure, and related information. Applications can search by vessel identifier or registered name. Port-by-port summaries also are available. Information can be consumed as a live data feed in raw NMEA AIS format.\n\n\nAPI methods support submission of vessel-specific queries by identifier (IMO, MMSI) or name or port-specific queries. Returned data include vessel registration information, photo, current location by latitude\/longitude and port, last destination with arrival and departure times, ultimate destination, current speed, and other details.\n\n\n For Wanted API: For Wanted is an online platform for selling stuff. Users can list their stuff that they would like to sell and at the price they want to sell it at. Other users can browse and purchase the merchandise.\n\n\nThe For Wanted API allows developers to access and integrate the search functionality of For Wanted with other applications and to create new applications. The available API method returns a list of items that match search criteria.\n\n\n GuteGutscheine API: GuteGutscheine is a German coupons and daily deals website offering rebates for over 8,100 online stores. Users can conduct a search for the latest coupons or have them sent to their email as they become available. The API allows access to all of the coupons and offers listed on the site. Functionality includes returning coupons, discounts and more details about a specific provider and delivering the most popular coupons and rebates within the specified period. Full documentation is not publicly available.\n\n\n HelloFax API: HelloFax is a faxing and electronic signature service. It allows multiple devices in an office to send documents to designated fax machines. The developers have also recently rolled electronic signature functionality into HelloFax. The API wrapper is exposed on GitHub. It is a RESTful API that returns JSON responses that can be converted into Ruby. The methods exposed are document uploading and status monitoring. \n\n\n Inbox25 API: The service provides hosted email marketing campaign management for bulk message delivery with message creation and formatting, hosting of file attachments and images, and performance tracking. It provides tools for managing subscriber lists and addresses, bounced message notification, and mailing list groupings and subsets. It also allows campaign definition, list tracking, and success monitoring.\n\n\nAPI methods support adding and removing subscribers and assigning subscribers to lists according to campaign and messaging strategies. Methods also support tracking of messages sent, delivery and open rates, click rates, bounced addresses, unsubscribe requests, and more.\n\n\n Index Fungorum Fungus API: The Index Fungorum is a global fungal nomenclator. It contains the names of fungi (including yeasts, lichens, chromistan fungal analogues, protozoan fungal analogues, and fossil forms) at all ranks. The Index Fungorum Fungus API provides a range of SOAP calls that users may employ to look up the names of fungal organisms.\n\n\n Informatica Cloud API: Informatica is a provider of data integration software and services. Informatica Cloud addresses specific business processes (customer\/product master synchronization, opportunity to order, etc.) and point-to-point data integration requirements (e.g. Salesforce.com to on premise or cloud-to-cloud end-points).\n\n\nInformatica Cloud allows users to integrate data across cloud-based applications such as Salesforce CRM as well as on-premise databases and applications. In addition to data integration it can handle business processes such as customer\/product master synchronization, opportunity to order, and more. Key capabilities include data quality, data replication and data loading. The API allows for custom integrations with the platform. Public documentation is not available.\n\n\n iOpenAt.com API: iOpenAt.com is a search engine that users can search and return the opening hours of retail locations closest to them. Currently, iOpenAt.com only includes retail locations of stores in the UK.\n\n\nThe iOpenAt.com API allows developers to integrate the functionality of iOpenAt.com with other applications. The API method returns opening hours of locations within the set postal code searched by.\n\n\n ISO New England API: ISO-NE oversees the operation of New Englands bulk electric power system. It seeks to ensure the day-to-day reliable operation of New Englands bulk power generation and transmission system, by overseeing and ensuring the fair administration of the regions wholesale electricity markets, and by managing comprehensive, regional planning processes. The ISO-NE API gives users access to energy and market data. The API uses RESTful calls and responses are formatted in XML and JSON.\n\n\n Journeys Travel Insurance API: The UK service provides travel insurance, either directly or via resellers on commission. It offers add-on trip insurance to travel booking products, with automated rate quotes and completion of policy issuing and management. \n\n\nAPI methods support submission of a quote request specifying travel dates, destination, and trip duration, along with issuance of a policy based on the quote. Methods also support retrieving information about a previously issued policy and cancellation of a policy.\n\n\n Knod.es API: Knodes is a service that provides data about users of websites and applications based on data-driven insights their networks. Knodes offers information about users of applications and websites based on their profiles from their social networks.\n\n\nThe Knodes API allows developers to access and integrate the functionality of Knodes with other applications and to create new applications. Public documentation is not available; interested developers should sign up here: http:\/\/knod.es\/index\/signup.\n\n\n Live Departure Boards API: The Live Departure Boards API allows users to obtain the same live train information presented on the LDB web pages for use on their own sites. Users can choose to employ a board listing departures only, a board listing arrivals only, or a board that lists both arrivals and departures. A license must be requested from National Rail Enquiries prior to using this API.\n\n\n MasterCard PayPass Online API: PayPass Online allows checkout from a site or app using any wallet that belongs to the Paypass Online network. This speeds up the checkout process by allowing customers to use the payment and shipping information already associated with their wallet of choice on the network. PayPass Online supports full and mobile sites and both Android and iOS apps. This service is free for both consumers and merchants.\n\n\n MESSAGEmanager API: The service handles messaging via email or SMS, configuring and sending messages to specified recipient contact lists. It also monitors outbound messaging traffic and reports on success or failure  by message and mode.\n\n\nAPI methods support submitting a message, including message body, delivery mode, recipient address, and other specifications. Methods also support designation of a file folder location where the service will poll for new files containing message sending and configuration information. Query methods retrieve delivery performance information to confirm receipt of messages, bounce rates, and blocked addresses.\n\n\n Mgnet.me shortener API: Mgnet.me is a shortener for magnet: URI scheme. Magnet links are primarily used for referencing resources available for download via peer-to-peer networks. The API provides the same functionality in XML, JSON and text output formats.\n\n\n MODIS Land Subsets API: The MODIS Land Subsets API allows users to retrieve images taken of Earth by the Moderate Resolution Imaging Spectroradiometers (MODIS) aboard the Aqua and Terra satellites. Users can retrieve information about available imagery and then choose their desired image data products by wavelength, date, and latitude and longitude location recorded.\n\n\n Music Xray S2O Partner API: vMusic Xray is a music-sharing platform intended to connect musicians and music industry representatives. Musicians post tracks to be analyzed for acoustical properties by Music Xray\u2019s Song to Opportunity (S2O) technology. Music Xray will then email the user when an industry professional is looking for music that is comparable to S2O\u2019s analysis. The S2O Partner API exposes Music Xray\u2019s song uploading and comparison functionality to business partners. Users who register through a partner\u2019s API account become affiliates who generate commission for the partner. The API is RESTful and returns XML responses. A Ruby SDK is also available. \n\n\n New Dawn ACM API: New Dawn Technologies is a provider of web-based, case management solutions for governments. Solutions include case management software, data sharing, e-filing, e-payment, e-discovery, and public access for government agencies including criminal, civil and municipal courts, attorney generals, county prosecutors, public defenders, corrections departments, child, family and health services. New Dawn offers and API for their Adaptive Case Management (ACM) solution. It allows customers to integrate the software with their existing applications. Full documentation is not publicly available.\n\n\n Nextgen Mobile CardBoardFish API: The service provides a computer application interface for sending SMS text messages. It allows both individual messages and bulk messaging to specified lists of recipient numbers. Tools available include message definition, contact list management, campaign management, and administrative functions like billing and service reporting.\n\n\nAPI methods support submission of a message with sender, recipient, message text, along with optional parameters like data encoding scheme and delivery receipt request. Methods also support retrieval of incoming messages, scheduling of messages, and service billing functions.\n\n\n Nimbuzz API: Nimbuzz is a mobile application that allows users to make and receive calls, message with contacts, and share files for free. Nimbuzz is a mobile social connection application.\n\n\nThe Nimbuzz API allows developers to access and integrate the functionality of Nimbuzz with other applications. Public documentation is not available; interested developers should sign up for access and more information.\n\n\n Observu API: Observu is a site monitoring service. Observu monitors sites and servers and sends alerts by email, text, and calls, as well as a dashboard. \n\n\nThe Observu API allows developers to access and integrate the functionality of Observu with other applications. Some example API methods include adding data, retrieving lists of messages, and adding monitors.\n\n\n Online Courier Quotes API: The service provides Australian postage, courier, and shipping rate calculation functions and modules for content management systems (DotNetNuke, Umbraco, Drupal, Joomla). The services integrate with existing shopping carts or provide standalone calculations. Shipping cost estimates cover almost any delivery address in Australia. \n\n\nAPI methods support submission of a package type and size (including weight), point of origin and destination, and shipping date. Methods return shipping options matching the request with rate quotes and shipper details. \n\n\n Online Whois Lookup API: Online Whois Lookup is a service that allows users to input domain name and see a domains whois record. The Whois API can be used to check if a domain name is available for registration. It can also lookup information about when a domain was created, changed and its expiration date. The API uses HTTP calls and responses are formatted in XML and JSON.\n\n\n Openwave Network Address Book API: Openwave Messaging is a provider of messaging solutions with cloud and social integration capabilities. The Network Address Book (NAB) synchronizes contact information across multiple platforms allowing providers to both personalize and simplify their messaging service offerings. Users of NAB can access and manage their contacts, events tasks and more. This can be accomplished on platforms including webmail, PC email clients, mobile phone address books and voice interfaces. An API is offered that lets developers integrate functionality such as messaging, billing and portal. Public documentation is not available.\n\n\n Papirus API: The service replaces ad hoc email collaboration with a templated workflow and task-management platform. It includes messaging and communications functions tied to work activities. It also provides reporting functions to monitor assignments and process compliance, including overdue activities and completion statistics.\n\n\nAPI methods support retrieval of task lists and individual task reports, providing the person responsible, person assigning the task, start date, due date, completion date, and current status. Methods also support creating new tasks, linking tasks in process sequence, and adding comments visible to the person assigned and other task stakeholders.\n\n\n Plivo API: Plivo is a telephony platform for integration with other apps. It is built to be integrative and take desired call making and receiving functions off of developers\u2019 hands. It provides capacity for conferencing, call tracking, recording, and more. This API is HTTP-based with SSL support for exposing all of the services essential functionality. It returns JSON responses. Pricing details are forthcoming from the developers. \n\n\n Qumu API: Qumu is a business video platform provider. Qumus video platform lets businesses capture, manage, and distribute live and on-demand content. The Qumu Video Control Center gives businesses control over the entire video life cycle from acquiring video content, building and managing a library, delivering on-demand video and providing a secure portal for employees to view content. An API is available via an SDK that allows developers to access the platforms functionality and add it to iOS, Android platforms or desktop and web applications. Public documentation is not available.\n\n\n Raven Slingshot API: Raven Slingshot offers a series of products for the agriculture industry, including Field Hub and Slingshot RDK. These services collect precise data on a variety of field operations that help streamline business procedures. The Raven Slingshot API provides a platform that allows inter-operability of the previously independent Slingshot products for an enhanced field tracking experience.\n\n\n SalesBoom API: The service provides a platform for enterprise resource planning (ERP) and customer relationship management (CRM). Functionality includes customer account creation and updating, contact tracking, customer service, and related activities. Sales support functions include lead tracking, quote management, sales forecasting, and other sales force automation functions.\n\n\nAPI methods support creation and updating of customer accounts, retrieval of customer transaction and contact history, and case\/incident management. Methods also support creation and modification of sales lead information, review of quotations provided, generation of sales forecasts, and other sales support functionality\n\n\n Seamless API: Seamless is an online and mobile food ordering and delivery service. Users can use Seamless to order food and get it delivered to where they are.\n\n\nThe Seamless API allows developers to access and integrate the functionality of Seamless with other applications and to create new applications. Public documentation is not available; interested developers should sign up for an API key here: http:\/\/www.seamless.com\/business-development\/developers\/.\n\n\n SirsiDynix Symphony API: SirsiDynix provides library automation systems for more than 23,000 libraries in more than 70 countries around the world. SirsiDynix offers APIs that allow libraries to gain full access to all information held within their system. The Symphony API includes tools for performing batch transactions and gives users the ability to gather, edit, load, export and format data within the system. Functionality includes access to the library catalog, ability to verify accounts and logins, access to user account information, retrieval of system information such as policies and much more. Interested developers should contact the provider for more information.\n\n\n SmartPea Grocery  API: SmartPea Grocery is a web service that users can search for local grocery deals and sales. Users can build a shopping list on the website.\n\n\nThe SmartPea Grocery API allows developers to access and integrate the functionality of SmartPea Grocery with other applications and to create new applications. Some example API methods include searching and returning grocery store deals by zip code and associated information including image, name, retailer, price, savings, and nutritional facts for any searched grocery item.\n\n\n SpazioDati API: SpazioDati is a service that parses, extracts, and links text and data. SpazioDati currently works in English and Italian.\n\n\nThe SpazioDati API allows developers to access and integrate the functionality of SpazioDati with other applications and to create new applications. Some example API methods include searching and retrieving information, adding and editing text and information, and account management.\n\n\n Spearman Correlation Coefficient API: In math the Spearman correlation measures the statistical dependence between two variables X and Y. The simple API at spearmancorrelation.com can be used to calculate the Spearman correlation coefficient for any set of data. The API uses RESTful calls and responses are formatted in XML and JSON.\n\n\n TauP API: TauP is a seismic travel time calculator. In addition, it can calculate derivative information such as ray paths through the earth, as well as pierce and turning points. It handles many types of velocity models and can calculate times for virtually any seismic phase with a phase parser. TauP is available for download, or it can be accessed programmatically via SOAP API.\n\n\n TauYou T-Image API: TauYou T-Image is a translation technology. T-Image allows users  to read in a variety of languages any text they want to understand. T-Image is available in Chinese, Dutch, English, French, German, Italian, Japanese, Korean, Portuguese, Spanish, and Swedish. \n\n\nThe TauYou T-Image API allows developers to access and integrate the functionality of T-Image with other applications and to create new applications. The main API method is uploading files and retrieving the translated versions.\n\n\n Teleportd API: Teleportd is a service that aggregates, organizes, and makes photos taken and shared from smartphones searchable. Teleportd allows customers to search photos and contact the owner for permissions.\n\n\nThe Teleportd API allows developers to access and integrate the functionality of Teleportd with other applications. Some example API methods include searching and retrieving photos by location, time periods, and users, as well as user account management.\n\n\n Travelocity API: Travelocity is an online service for travel shopping. Users can search, browse, and book travel, such as airline tickets, hotels, and rental cars.\n\n\nThe Travelocity API allows developers to access and integrate the functionality and data of Travelocity with other applications and to create new applications. Documentation is available with login; interested developers can sign up for their API key and access here: http:\/\/connect.travelocity.com\/.\n\n\n UN Comtrade API: UN COMTRADE is the name for the United Nations Commodity Trade Statistics Database. The database contains annual international trade statistics from over 170 countries, detailed by commodities and partner countries. Currently the database contains over 1.7 billion data records for 45 years. The API allows users to query the database filtering on parameters including Commodity Classification, Reporter, Year, Commodity Code, Partner Country and Trade Flow. The API uses RESTful calls and responses are formatted in XML and SDMX.\n\n\n University of Nottingham Get Entry Profile API: The University of Nottingham Get Entry Profile API allows users to obtain a UCAS Entry Profile for a higher education course. The UCAS (Universities and Colleges Admissions Service) is the British admission service for students applying to university and college. This API is part of the e-Portfolio for Lifelong Learning Reference Model Project.\n\n\n VINquery API: The service provides real-time decoding of automobile vehicle identifier numbers (VINs). It accepts HTTP requests with VIN numbers and returns detailed specifications for the make, model, and year plus optional equipment implied by the submitted digits.\n\n\nAPI methods support submission of a valid character string representing a VIN for an individual car. Returned data break down the manufacturer and model, plus the year, engine and transmission installed, and some other configuration details encoded in the VIN.\n\n\n Virtual Observatory Services Distance API: The Virtual Observatory Services Distance API calculates the distance of celestial objects from Earth. Several methods of distance calculation are available including angular diameter, co-moving line of sight, co-moving transverse, and luminosity. This API can also calculate the distance modulus of an object or return the current distance of the Hubble telescope.\n\n\n Virtuoso Facets API: The Virtuoso Facets API is a general purpose RDF query facility for facet-based browsing. Faceted browsing is a technique for accessing information organized according to a faceted classification system, which classifies each information element along multiple explicit dimensions. This API takes an XML description of the desired view and generates the reply as an XML tree containing the requested data. The user agent or a local web page can use XSLT to render this for the end user.\n\n\n Visual DataFlex Country Information API: The Visual DataFlex Country Information API allows users to retrieve basic information on countries, such as capital city, currency, flag, phone code, and languages used. These pieces of information may be requested individually or all together for a given country. This API can be accessed via SOAP calls using the XML data format.\n\n\n Visual DataFlex Football Pool API: The Visual DataFlex Football Pool API provides access to information on European football (soccer) teams, players, and games. Users can discover the names of all players on a team, who plays which position, and who has red or yellow cards. Users may also retrieve information on game results, as well as which cities and stadiums games are played in.\n\n\n W3 TempConvert API: The W3 TempConvert API allows users to convert temperatures from Celsius to Fahrenheit and vice versa. This API was created as an example for W3 Schools, but is nonetheless fully functional. The API can be accessed via SOAP calls using the XML data format.\n\n\n Wrike  API: Wrike.com offers a social project management software that allows businesses to manage over 50 projects in one workspace with the goal to increase team collaboration and productivity and overall efficiency. The Wrike API platform allows clients to build custom applications on top of existing Wrike software features. It offers seven callable API methods and endpoints: Profile, Contacts, Folders, Tasks, Comments, Attachments, Time Logs.\n\n\n XML ME Get Custom News API: The XML ME Get Custom News API allows users to retrieve news articles on a topic of their choosing. The user inputs a desired news topic, and the API returns a list of articles from Moreovers news service. This API is available via SOAP calls using the XML data format.\n\n\n XML ME Video Games Finder API: The XML ME Video Games Finder API allows users to search for video games programmatically. The API accepts a search string and then returns a URL pointing to the video games search results. This API operates via SOAP calls using the XML data format.\n\n\n YackTrack API: YackTrack is a service that allows users to search for, track, monitor, and access social media conversations about them, their products or companies, and other.\n\n\nThe YackTrack API allows developers to access and integrate the functionality of YackTrack with other applications and to create new applications. Some example API methods include searching and retrieving comments and accessing information about the comments, such as date and person.\n\n\n Yi.tl API: Yi.tl allows users to make long URLs into shorter yitl urls. The service provides shorter URLs than bit.ly or Tinyurl and also allows users to tag their short URLs. The service also allows users to track their URLs in order to see how many clicks each link gets, where they come from, when they clicked and more.\n\n\nThe yi.tl API allows developers to integrate the service with their own applications and sites. The API offers much of the same functionality including the ability to generate or get existing short URLs and retrieving statistics such as top clicked links, least clicked links, newest links. The API uses RESTful calls and responses are formatted in XML and JSON.\n\n\n Zip Tax API: Zip Tax is a service that provides sales tax data of the United States. Users can provide a zip code and retrieve the sales tax information for that zip code. \n\n\nThe Zip Tax API allows developers to access and integrate the functionality of Zip Tax with other applications and to create new applications. Some example API methods include looking up and retrieving sales tax information by zip code.","item_date":"May 22 2012 02:43:24","display_item_date":"05-21-2012","url":"http:\/\/blog.programmableweb.com\/2012\/05\/20\/78-new-apis-mastercard-travelocity-and-addshoppers\/","source":"blog.programmableweb.com"},{"title":"How To Increase The Quality Of Your APIs","details":"SmartBear Software has released a new product designed to test and monitor the quality of Application Programming Interfaces and web services. API Complete is intended to help development shops increase the quality of the APIs that they do select and deploy.  \n  \nHeres the rationale: More Agile in active usage can mean that interconnections between developers and operations staff become increasingly fragmented \u2014 a problem which is further compounded by increasingly disparate, disconnected, geographically displaced teams. Put simply, we need to drill down into more granular analysis of application components wherever possible. \n \nTo this end, Gartner analyst Eric Knipp recommends treating a public web API as a key component of your web strategy, not merely as a bolt-on to an existing project. Thus the API gets managed with the same care a firm would typically afford to its enterprise web presence. \n \nAccording to SmartBears Ian McLeod, the speed and availability of web services are a crucial component of the larger end-user experience delivered by web applications \u2014 but, until now, development and operations teams had to use disparate toolsets for testing and monitoring.  \n \nAPI Complete combines the soapUI API testing tool, loadUI for load testing, and AlertSite, SmartBears web performance monitoring solution and global monitoring network into an integrated framework for API lifecycle quality management. Using common test scripts and validation assets, API Complete helps development, IT operations, and e-commerce teams ensure that APIs are thoroughly tested pre-deployment and performing well for end-users or business partners around the world once in production, said McLeod. \n \nNOTE: Web APIs are generally agreed to be growing exponentially and are required for mobile and social applications, as well as for new forms of e-commerce that combine web services and contextual information. To ensure their quality, says SmartBear, it is now critical to conduct meticulous functional and load testing during pre-deployment to identify and resolve problems early, as well as continuous monitoring and regression testing post-deployment to ensure ongoing quality of service and availability.","item_date":"May 22 2012 01:45:27","display_item_date":"05-21-2012","url":"http:\/\/www.drdobbs.com\/testing\/240000707","source":"www.drdobbs.com"},{"title":"Opening up government data: a practical guide","details":"No government can afford to become isolated from the society it serves, otherwise it risks becoming distant and clumsy, trapped by its own, self-referential routines.\nThat was the clear message in last Septembers report, entitled The civiclong tail by UK thinktank Demos. This report on the the willingness of governments to communicate more openly with their citizens, is part of an emerging trend that shows governments all over the world are starting to open up their data vaults for access by all interested parties.\nThe call for open data is not an entirely new phenomenon. There have been many previous efforts and initiatives in the western world to make governments more accessible for business and public alike, but most of them have been buried under administrative bureaucracy and very seldom really caught the attention of the general public, let alone become successful. \nYet, as the Demos report states relationships between government and its citizens (as voters, service users and taxpayers) should become more open, transparent and so more accountable. Government should be able to share much more information with citizens, who should be able to see in much finer detail what decisions government is taking and why. Citizens should in turn be able to contribute their views, ideas and feedback. \nIs this a utopian view?\nNot really. \nTthings got moving in the right direction when the US government launched its Data.gov initiative in May 2009 as part of president Barack Obamas Open Government Directive [http:\/\/www.whitehouse.gov\/open] , which instructs all federal agencies to use technology that makes their activities more transparent and enables them to engage more actively with citizens. \nData.gov has become a collective data repository for government data from all agencies with the primary goal of improving access to federal data and expanding the creative use of those data beyond the walls of government. It enables the public to participate in government by providing downloadable federal datasets to build applications, conduct analyses, and perform research. The site provides access to more than 390,000 datasets, over 1,100 government applications and close to 240 citizen-developed apps.\nTwenty-eight other countries have followed the US example, including Belgium. At data.gov.be, you can already find a list of some 80 datasets with topics ranging from air quality to real estate sales and population by municipality. The site is managed by Fedict, the Federal Public Service for Information and Communication Technology, responsible for defining and implementing the federal e-government strategy. A number of applications have been uploaded as well, and there is also a considerable list of various ideas and requests for opening up specific data sets. In short: still fairly modest in size but its a promising start.\nIn Europe lots of work is done by Europeana , a digital platform that rolls multimedia library, museum and archives into one digital website combined with Web 2.0 features. It offers direct access to digitised books, audio and film material, photos, paintings, maps, manuscripts, newspapers and archival documents that are Europes cultural heritage. Visitors to www.europeana.eu can search and explore different collections in Europes cultural institutions in their own language in virtual form, without having to visit multiple sites or countries. Europeana was launched by the European Commission and the EUs culture ministers in Brussels on 20 November 2008. There is also the Europeana Foundation that promotes collaboration between museums, archives, audiovisual collections and libraries so that users can have integrated access to their content through Europeana and other services. Currently, Europeana gives access to over 20 million objects from more than 1,500 institutions in 32 countries. In September 2011, it had 3 million direct visitors but the actual number of users goes far beyond that because the content from Europeana is available on various Websites and applications through API services.\nSAS BeLux absolutely applauds this trend. While the US and the UK have been leading examples in putting government data online, Belgium and Flanders are now really following suit. \nTake, for example, the Federal Public Service Economy. For a number of years now, it has been leading the way by giving citizens access to its publications and the underlying datasets. We applaud  the fact that in this department, data is not just dumped online in bulk, but is supplemented with documentation and metadata.\nWe feel  open data can really have a long tail effect on our society, instigating a shift from a relatively small number of hits at the top of the demand curve towards a huge number of niches in the tail as the cost of production and distribution continues to drop. \nIn this context, big data is a major resource of knowledge. When you are able to correctly understand, mine and exploit big data using efficient analytics, you can turn data into knowledge that brings huge value. \nMoving data out of its historic vaults and relate it to other sources of information from the connected world like social media, blogs or web content, can bring completely new insights and breakthroughs - and just remember that analytics is no longer the exclusive domain of some wizards with a PHD in statistics. SAS solutions can be used by anyone nowadays. \nI believe an innovation strategy for open data in the public domain should meet five requirements:  \n\u2022 Simplicity: Processes tend to be tedious, and we need ways to cut through all the red tape;\n\u2022 Risk: Being too risk averse will kill innovation;  is launching a beta really that risky?\n\u2022 Speed: the crisis is a valid argument for more urgency and  the open data long tail effect can be one of the engines to energise society;\n\u2022 Low cost: many of the building blocks already exist: good people, technology, data, etc. By prioritising based on impact and creating the right organisational environment, one can achieve strong results through shifting budgets rather than being hampered by big budget hurdles;\n\u2022 Openness: innovation strategies are often closed and producer driven, while great examples emerge through a consumer\/citizen-driven approach like crowd sourcing or open innovation\nHowever, all this would would require a totally different mindset. First of all, you will have to focus on issues that matter. People dont care about open data or the web. All they are interested in is whether their neighbourhood is safe, their school is good, and their grandparents are well cared for. \nYou would also have to embrace open innovation, to solicit ideas from citizens and application developers, and make prototypes available so that people can add to them. \nThen you literally need to create a new space for innovation where people with needs and people with potential solutions can meet. The US City Camp and the UK Social Innovation Camp are promising models, as they bring together seb developers and civic entrepreneurs. \nAnd we need to learn how to disinvest from older, less effective services while investing in new, cost effective, distributed self-service solutions.\nI am convinced that SAS can play a major role here as it is committed to assist the different governments in making their open data ambitions come true. In many countries there are concerns that releasing such a deluge of data to the public at large would not really lead to a meaningful use. But every day, public services all over the world are already using SAS software to recognise patterns and discover trends in their structured and unstructured data in order to understand the past and predict what is likely to happen next. \nBelgium is particularly lucky to have so many Federal Public Services that are littered with innovative people, including social security, finance, the Flemish government (Vlaamse Overheid), public health, and economy. All of them use SAS analytics to open that treasure chest of data. Lets continue to facilitate this for the general public in the years to come.\nJeroen Van Godtsenhoven is head of business development & pre-sales at SAS Belgium and Luxembourg.\nContent on this page is produced and controlled by SAS.","item_date":"May 22 2012 01:13:52","display_item_date":"05-21-2012","url":"http:\/\/www.guardian.co.uk\/public-leaders-network-sas-partner-zone\/governments-respond-opening-data?CMP=twt_gu","source":"www.guardian.co.uk"},{"title":"WorldMate Launches Email Parsing API","details":"WorldMate, the world\u2019s largest mobile itinerary management and booking service, launched the WorldMate API this week. The e-mail\u00a0parsing API extracts travel data (e.g. confirmation e-mails, key travel information, airport codes, etc.) and sends the information back to the developer\u2019s platform. Providing such information opens WorldMate\u2019s data to a new realm of applications. WorldMate has already seen adoption from developers outside the itinerary management and booking space. Early examples include expense reporting, flight status, and compliance apps. WorldMate CEO, Jean Tripier, commented: \u201cWe are overwhelmed with the immediate popularity of the API across a wide spectrum of developers.\u201d\n\n\n\n\n\nDuring the pre-release, over fifty developers licensed the WorldMate API. Notably pleased beta customers include Conducive Technologies, ExpenseCloud, and Shorts Travel Management. WorldMate\u2019s API parses e-mails from 1300 sources: including 155 airlines, 240 hotel chains, 22 car rental agencies, 620 travel agencies, 12 rail systems, TicketMaster, and Opentable.\n\n\nNo industry standard exists for travel e-mail confirmations; thus, airlines, hotels, and travel agencies all use their own format for e-mail confirmation. The WorldMate API parses the tens of thousands of e-mail formats and returns the formatted data in XML or a return URL. The free developer\u2019s license parses 50,000 e-mails per year. A developer\u2019s license can be obtained here.","item_date":"May 21 2012 18:43:24","display_item_date":"05-21-2012","url":"http:\/\/blog.programmableweb.com\/2012\/05\/21\/worldmate-launches-email-parsing-api\/","source":"blog.programmableweb.com"},{"title":"About.me Releases Public API, SDK At Disrupt, Now Integrates With Reputation, Smarterer, Forkly, Kred And Showyou","details":"Google Chrome overtakes Internet Explorer as the Webs most used browser\u00a0 \u2014\u00a0 Google Chrome has been long expected to leapfrog Microsofts Internet Explorer (IE) to take its position as the Webs most used browser and, according to data from Statcounter, the momentous change of leadership happened last week.\n  \n On Day Two, Facebook Shares Open More Than 10 Percent Off\u00a0 \u2014\u00a0 After opening at a share price of $36.53, shares of Facebook stock dipped by more than 10 percent below the companys initial public offering price of $38, trading as low as $33 at one point on Monday morning.\n   More: TechCrunch, CNET, CNET, VentureBeat, Fortune, The Tech Trade, Wall Street Journal, Reuters, BBC, DealBook, ABCNEWS, Deal Journal, Pulse2 Technology \u2026, Forbes, Mashable!, MarketBeat and Business Insider\n  \n How The Media (Including TechCrunch) Is Wrong About Facebooks IPO\u00a0 \u2014\u00a0 Judging by many of the headlines on Friday, you might think that Facebooks IPO was a miserable failure.\u00a0 The Wall Street Journal declared, \u201cFacebooks IPO Sputters,\u201d and our very own TechCrunch declared that bankers were \u201cstruggling\u201d to keep the share price up.\n  \n EU offers Google last chance in antitrust case\u00a0 \u2014\u00a0 (Reuters) - The European Unions antitrust chief on Monday ratcheted up the pressure on Google, giving it a matter of weeks to settle an investigation into allegations of anti-competitive behavior and avoid formal charges and a possible fine.\n   More: VentureBeat, 9to5Google, Search Engine Land, Hillicon Valley, TechCrunch, EU Press Room, GigaOM, Wall Street Journal, InfoWorld, CNET, The Verge, Engadget, The Register, PC World, Washington Post, Ars Technica, New York Times, ZDNet and Pulse2 Technology \u2026\n  \n Leap Motion: 3D hands-free motion control, unbound\u00a0 \u2014\u00a0 The startups technology puts sub-millimeter accuracy at user fingertips, offers control gestures like pinch-to-zoom, and promises new applications that make the Kinect and its kin look like yesterdays news.\n  \n Nasdaq CEO Blames Software Design For Delayed Facebook Trading\u00a0 \u2014\u00a0 Nasdaq OMX Group Inc. (NDAQ), under scrutiny after shares of Facebook Inc. were plagued by delays and mishandled orders on its first day of trading, blamed \u201cpoor design\u201d in the software it uses for driving auctions in initial public offerings.\n   More: GigaOM, WebProNews, Business Insider, Firstpost, Reuters and Computerworld\n  \n    Wall Street Journal:   \nNasdaq Acknowledges Troubles With Facebook Deal\n   More: Fox Business, Business Insider, The Register, AllFacebook, Betabeat, Seeking Alpha, USA Today, Business Insider, CNET, Pulse2 Technology \u2026, VentureBeat, CNBC, Gizmodo, The Verge, DealBook and 24\/7 Wall St.\n  \n Fat Lady Finally Sings: Yahoo and Alibaba Officially Shake on $7 Billion Stock Sale Deal [UPDATED]\u00a0 \u2014\u00a0 As AllThingsD.com reported several days ago they would, Yahoo and Alibaba Group have finally reached an agreement for the Silicon Valley Internet giant to sell back half its stake in the Chinese Web company in a $7 billion deal.\n \n Cable companies expand free Wi-Fi\u00a0 \u2014\u00a0 BOSTON\u2014The nations biggest cable operators are banding together to offer free Wi-Fi access to their broadband customers in more than 50,000 hotspots around the country.\u00a0 \u2014\u00a0 On Monday, Bright House Networks, Cablevision, Comcast, Cox Communications \u2026 \n   More: Business Wire, VentureBeat, The Verge, GigaOM, ZDNet and The Next Web\n  \n Google Missed The Boat On Buying Twitter.\u00a0 \u201cHasnt Been Interested Since They Committed To Google+\u201d -Fred Wilson\u00a0 \u2014\u00a0 Michael Arrington says Dick Costolo gave Google the chance to buy Twitter, but the search giant passed.\u00a0 Fred Wilson, Union Square Ventures founder and former Twitter board member \u2026 \n   More: VentureBeat and Business Insider\n  \n Hulus summer plans: 3 original and 7 exclusive TV shows to premiere starting June 4\u00a0 \u2014\u00a0 Hulu this morning announced a slate of original and exclusively licensed TV shows that will begin premiering on June 4 on the online video streaming site and premium subscription service Hulu Plus.\n   More: Gizmodo, VatorNews, VentureBeat, CNET, TechCrunch, GadgeTell, Wall Street Journal, Business Wire and Wired.\u00a0 See also Mediagazer\n  \n How Pixars Toy Story 2 was deleted twice, once by technology and again for its own good\u00a0 \u2014\u00a0 \u201cThats when we first noticed it, with Woody.\u201d\u00a0 \u2014\u00a0 \u201c[Larry Cutler] was in that directory and happened to be talking about installing a fix to Woody or Woodys hat.\n \n WiFis future: faster, smarter, and fewer cables\u00a0 \u2014\u00a0 And youll be buying a bunch of new devices.\u00a0 \u2014\u00a0 Photo illustration by Aurich Lawson\u00a0 \u2014\u00a0 WiFi has easily been one of the most useful technologies of the past decade\u2014so many of our daily tasks and the devices we use rely on it.\n \n Samsung begins blocking unofficial S-Voice requests ahead of Galaxy S III launch\u00a0 \u2014\u00a0 After features underpinning its soon-to-be-released Galaxy S III smartphone leaked, Samsung is reported to have begun blocking unofficial requests generated by its S-Voice service from unsupported devices.\n   More: 9to5Google, Gotta Be Mobile, Android Community, Engadget, Softpedia News, TechnoBuffalo, Phones Review, SlashGear and The Verge\n  \n After being ousted from Yahoo, Scott Thompson resigns from Splunk board\u00a0 \u2014\u00a0 As you probably know, former PayPal president Scott Thompson was recently forced out of Yahoo, mere months after joining the beleaguered Internet company as chief executive officer.\u00a0 Now hes also losing board seats left and right.\n   More: CNET, Market Wire, Business Insider, The Tech Trade, WebProNews, Pulse2 Technology \u2026 and New York Times\n  \n  Qualcomm Spark:  Book Excerpt: The Creative Destruction of Medicine\u00a0 \u2014\u00a0 The emergence of powerful tools to digitize human beings creates an unparalleled opportunity to inevitably and forever change the face of how health care is delivered.\n \n  Cloud Foundry:  Building a Real Time Activity Stream on Cloud Foundry with Node.js, Redis and MongoDB - Part I\u00a0 \u2014\u00a0 Cloud Foundry provides developers with several choices of frameworks, application infrastructure services and deployment clouds.\n \n  Channel 9:  Kinect for Windows SDK 1.5 - Face Tracking, Seated Skeletal Tracking, Kinect Studio, & More\u00a0 \u2014\u00a0 Rob Relyea, a Principal Program Manager on the Kinect for Windows team joins us again on Channel 9 to discuss \u2026 \n \n  Atlassian Blogs:  Between a rock and a hard place - our decision to abandon the Mac App Store\u00a0 \u2014\u00a0 On June 1st, Apple will change the rules of the Mac App Store to require all applications to run inside of a \u2018sandbox\u2019.\n \n  SoundCloud:  SoundCloud Heroes at Global Meetup Day\u00a0 \u2014\u00a0 Global Meetup Day is coming up on Thursday, May 17.\u00a0 Weve checked in with our SoundCloud Heroes to hear how they will be connecting around sound.\n \n  Sponsor Techmeme \n \n  Whos Hiring In Tech?\n  Google:  The end to your job search.\n  Ask.com:  Want a new gig? You just have to ask.\n  Tumblr: Like websites? This is a website.\n  Shape Security: Who let the bots out?\n  DoubleDutch: Mobile like a fox.\n  Postmates: Get ship done.\n  @ClimateCorp: Big data tackles climate change.\n  Amazon: Work hard. Have fun. Make history.\n  Facebook: Best place to build & make an impact.\n  Zynga: Its fun over here.  Lets play.\n  Twitter: Less characters; more fulfilling.\n  foursquare: Making the real world easier to use.\n  Square: Come simplify the complex.\n  Add your company here \n \n This is a Techmeme archive page. It shows how the site appeared at 12:15\u00a0PM\u00a0ET, May\u00a021,\u00a02012. \n The most current version of the site as always is available at our home page. To view an earlier snapshot click here and then modify the date indicated. \n  From Mediagazer\n Steve Myers \/ Poynter: NPR creates news applications team as part of strategy for \u2018multimedia audio\u2019 \nPress Gazette: Tessa Jowell suggests Daily Mail, Evening Standard and Sunday Times involved in hacking \nErik Wemple: New York Times public editor to leave in September","item_date":"May 21 2012 18:43:20","display_item_date":"05-21-2012","url":"http:\/\/www.techmeme.com\/120521\/p27#a120521p27","source":"www.techmeme.com"},{"title":"Cloud API Standardization \u2013 It\u2019s Time to Get Serious","details":"Lots of people continue to perpetuate the idea that the AWS APIs are a de facto standard, so we should just all move on about it. \u00a0At the same time, everybody seems to acknowledge the fact that Amazon has never ever indicated that they want to be a true standard. \u00a0Are we reallyIn fact, they have played quite the coy game and kept silent luring potential competitors into a false sense of complacency.\n Amazon has licensed their APIs to Eucalyptus under what I and others broadly assume to be a a hard and fast restriction to the enterprise private cloud market. I would not be surprised to learn that the restrictions went further \u2013 perhaps prohibiting Eucalyptus from offering any other API or claiming compatibility with other clouds.\n Amazon Has ZERO Interest in Making This Easy\n Make no mistake \u2013 Amazon cares deeply about who uses their APIs and for what purpose. \u00a0They use silence as a way to freeze the entire market. \u00a0If they licensed it freely and put the API into an independent governance body, we\u2019d be done. \u00a0But why would they ever do this and enable easy portability to other public cloud providers? \u00a0You\u2019re right \u2013 they wouldn\u2019t. If Amazon came out and told everybody to bugger off, we\u2019d also be done \u2013 or at least unstuck from the current stupidly wishful thinking that permeates this discussion. \u00a0Amazon likes us acting like the deer-in-the-headlights losers we all seem to be. Why? Because this waiting robs us of our will and initiative.\n It\u2019s Time to Create A Cloud API Standard\n Do I know what this is or should be? Nope. Could be OpenStack API. It won\u2019t be vCloud API. It doesn\u2019t freaking matter. Some group of smart cloud platform providers out there should just define, publish, freely licence and fully implement a new standard cloud API.\n DO NOT CREATE A CLOUD API STANDARDS ORG OR COMMITTEE. Just go do it, publish it under Creative Commons, commit to it and go. License it under Apache. And AFTER it gets adopted and there\u2019s some need for governance going forward, then create a governance model (or just throw it under Apache). Then every tool or system that needs to access APIs has to only do it twice. Once for Amazon and once for the true standard.\n Even give it a branding value like Intel Inside and make it an evaluation criteria in bids and RFPs. I don\u2019t care \u2013 just stop treating AWS API as anything other than a tightly controlled proprietary API by the dominant cloud provider that you should NOT USE EVER (once there is a standard).\n Take it one step forward \u2013 publish a library to translate the Standard API to AWS under an Apache license and get people to not even code AWS API into their tools. \u00a0We need to isolate AWS API behind a standard API wall. \u00a0Forever.\n Then, and only then, perhaps we can get customers together and get them to force Amazon to change to the standard (which they will do if they are losing enough business but only then).","item_date":"May 21 2012 18:12:53","display_item_date":"05-21-2012","url":"http:\/\/www.cloudbzz.com\/cloud-api-standardization-the-aws-question\/","source":"www.cloudbzz.com"},{"title":"Hack weekends treat coders like monkeys in a cage","details":"Serial entrepreneur Ryan Carson, who has built a series of conferences and education services for web designers and developers in the U.K., says he has had enough of hackathons. \n\n\nAccording to him, they\u2019re run by opportunists looking to cash in on the eagerness of young talent for their own purposes:\n\n\nDoes he have a point?\n\n\nI\u2019ve certainly seen the number of them multiply massively over the past few years, which means the quality and intent now varies wildly. Some are good, some are bad. But the general underlying cultural trend definitely seems to be there \u2014 the idea that you can (and should) convince a few hackers to develop things in their spare time using a mixture of Red Bull, pizza and some vague dream about building The Next Big Thing. \n\n\nSometimes these are great community events that bring people together to do fun stuff; sometimes they feel exploitative \u2014 and involvement can certainly be tricky for those who don\u2019t want to buy into macho coder culture.\n\n\nOver on Hacker News the post has generated some pushback (as you might expect, given that it\u2019s a hangout for people who generally subscribe to this sort of ethos). Butit\u2019s important, of course, to remember that there is no concerted effort here to undermine, and plenty of people happily engage in hack events of all stripes. There is no great Hackathon Authority arranging all of these events, depriving ordinary working stiffs of their weekends. It is simply a confluence of different people all seeing benefits in the same thing. \n\n\nIs it just a case of calling out the bad seeds?\n\n\nPhotograph of Ryan Carson used under Creative Commons license courtesy of Flickr user jeffkward","item_date":"May 21 2012 15:07:03","display_item_date":"05-21-2012","url":"http:\/\/gigaom.com\/europe\/hack-weekends-ryan-carson\/","source":"gigaom.com"},{"title":"Introducing Our 2012 Disrupt NYC Hackathon Winners: Thingscription, PoachBase, And Practikhan! | TechCrunch","details":"After nearly 24 hours of fighting fatigue and crafting code, our Disrupt NY 2012 Hackathon is finally drawing to a close. Not a moment too soon \u2014 I think some of our hackers are about ready to keel over at this point.\n\n\nNevertheless, we just got an eyeful of 92 projects that our wonderful hackers have been slaving away on through the night, but only three teams will be able to show off their work on the main Disrupt stage this Wednesday afternoon. In addition to that, our API sponsors \u2014 about.me, CityGrid, Microsoft BizSpark, Mobli and OpenTok \u2014 have offered up prizes of their own to the groups that made best use of their services, so there will be plenty of winners here today.\n\n\nOur panel of judges have carefully sifted through these 92 submissions, but who exactly decided the fates of these hackers? The panel of judges include Tarikh Korula of Mahaya, Christina Cacioppo of Union Square Ventures, Kip Voytek of MDC Partners, David Tisch of TechStars, Dinesh Moorjani of Hatch Labs, Nora Abousteit of Kollabora, and Dave Jagoda of Andreessen Horowitz.\n\n\nSo, without further ado, meet our newest Hackathon winners!\n\n\n\t\n A service that tracks user impressions to figure out what products to offer as a recurring subscription to customers.\n\n\n\t\n A recruiting service that uses Crunchbase data to determine which startups may soon fold, and who\u2019s worth swiping away from them.\n\n\n\t\n A platform that lets teachers create their own online quizzes to share with their students.\n\n\nCan\u2019t get enough of the Hackathon? I don\u2019t blame you \u2014 here are a few more photos of the closing festivities to tide you over until next time.\n\n\nThis slideshow requires JavaScript.\n\n\n\n\n\nDisrupt NYC is set to be one of our biggest shows yet, with returns from Michael Arrington and MG Siegler, along with a variety of big names like Marissa Mayer, Sarah Tavel, Fred Wilson, and David Lee and more. It\u2019s going to be huge.\n\n\nIf you\u2019re interested in checking out Disrupt and\/or the Hackathon yourself, tickets are still on sale here and info on the Hackathon can be found here. Companies who want to join the Battleground can apply for the last remaining spots in Startup Alley. You can find the full agenda here.","item_date":"May 21 2012 00:03:20","display_item_date":"05-20-2012","url":"http:\/\/www.google.com\/url?sa=X&q=http:\/\/techcrunch.com\/2012\/05\/20\/introducing-our-2012-disrupt-nyc-hackathon-winners-thingscription-poachbase-and-practikhan\/&ct=ga&cad=CAcQARgAIAAoATAAOABAyvDk_QRIAlgBYgVlbi1VUw&cd=byOFHGcXFhM&usg=AFQjCNHg0Ghqv_m-41cnTLCIP4w4DcRpTA","source":"www.google.com"},{"title":"BLS Library Blog: Hackathon Highlights","details":"To learn more about the First Annual BLIP Legal Hackathon, read the BLS Advocate article by Alexander Goldman and Boyan Toshkoff. Also see last month\u2019s post on this blog. In addition, get a sense of the Legal Hackathon by viewing the video below which has a selection of clips from the morning session and an interview with #HackTheAct competition winners.","item_date":"May 21 2012 00:03:17","display_item_date":"05-20-2012","url":"http:\/\/www.google.com\/url?sa=X&q=http:\/\/blslibraryblog.blogspot.com\/2012\/05\/hackathon-highlights.html&ct=ga&cad=CAcQARgAIAEoATAAOABAk5fl_QRIAlgAYgVlbi1VUw&cd=03-3XUF-3_Q&usg=AFQjCNHTOb448iLt_H8tVJkzlrTbRg-hNA","source":"www.google.com"},{"title":"Off to php|tek, and favoring protocols over APIs | Mashery Blog","details":"Scanning through my news feeds before getting ready for php|tek this year, I found this article about favoring protocols over APIs and it raises some good points about API design and interoperability. The main focus of the article is that an API is really an agreed-upon combination of a protocol and an endpoint, and that if more standardized protocols are developed and used, changing endpoints becomes trivial.\n As an example, take XMPP. There are thousands of messaging services that are built on top of the XMPP protocol, from well-known public services like Google Talk to private internal chat services run on company intranets. Because the protocol is standardized, there are also many different clients that support the protocol. Because the services all speak XMPP, the clients can interact with any of those services by changing their endpoints.\n All of the players in that interaction gain value by using the protocol. The clients get a bigger audience since they can connect to more services, and the endpoint providers get to rely on standard libraries for building and documenting their service, so there service code can be more DRY.\n Its not just simple messaging protocols that can benefit from this idea. Any industry or business case has the potential for a standard protocol. For example, the OpenTravel spec allows for transfer of flight information, hotel reservations, car rentals, and several other pieces of travel-related information in a standardized way. There are many other standard protocols too, for example weather data or news. The more companies that adopt a specification, the less friction there is transferring data in that industry. Imagine how much easier it would be to build a news aggregator app if you could connect to 20+ news outlets and parse all of the responses exactly the same way.\n If youre building an API, consider implementing a standardized protocol if one already exists in your business space. If youre consuming an API that is not built upon a standard protocol, see if the API provider is willing to work with an existing standard, or propose their own.\n \n If youd like to talk about this or other API geekery, come see us at php|tek in Chicago this week. Well be mingling around in different sessions or hanging out at the Mashery booth. Look for us at the unconference too!","item_date":"May 21 2012 00:03:14","display_item_date":"05-20-2012","url":"http:\/\/blog.mashery.com\/content\/phptek-and-favoring-protocols-over-apis","source":"blog.mashery.com"},{"title":"A year as a Developer Evangelist","details":"I can\u2019t believe it, but I\u2019ve now been working for Pusher for a year. It\u2019s been an exciting year both in life and in business \u2013 and it\u2019s great when you spend a lot of the time working on something you are really interested in. So, I thought I\u2019d push out a post covering the things that I\u2019ve done this year along with a few thoughts.\n A Developer Evangelist?\n I\u2019m a strong believer that we are much more than a job title and that each person has their own strengths, weaknesses and interests so they should get to work on the things that will most benefit them and through that benefit the company they work for.\n \n Write blog posts about technology. This could be your product or just interesting technology in general.\n Attend events: tech meetups, hack days, hack weekends and conferences.\n Talk at events: as above.\n Generally use social media to interact with other developers. Blogging is part of this but also Twitter, Google+, irc, StackOverflow and various other channels. It\u2019s not an all or nothing thing though.\n Keep up to speed with relevant technologies.\n \nThere will be other responsibilities based on the role and the person. I spend quite a bit of time handling support queries. This is a great way of understanding how people are using Pusher and ways in which we could make our service better.\n For me the job covers customer support (communication and tech support), marketing, a bit of product strategy and of course development. And probably a bunch of other things from time to time.\n Interestingly enough I\u2019ve never been very keen on public speaking. Yet, it\u2019s actually pretty core to this job. So, I\u2019m having to do it even though I\u2019m not sure if I really want to do it or not, I just understand the importance of doing it. In any job you have to try new things and test yourself \u2013 this is the biggest test for me.\n If you like the sound of this job, we are looking to hire another Developer Evangelist \n Finally, I highly recommend Christian Heilmann\u2019s Developer Evangelist handbook. It\u2019s one of the things that made me realised that this is what I wanted to do.\n Here\u2019s a list of things that I\u2019ve done over the past year. But for faster consumption, my highlights have been:\n Tutorial published in .net magazine. It was really cool so see an article I\u2019d written in print on the shelves of places like WHSmith.\n Being on a panel at the Local Social Summit. It was really fulfilling to think about social and business aspects of the realtime web.\n Talking at Tech Meetup Edinburgh. I\u2019ve been to techmeetup a few times so it felt really good, after the event, to have spoken and my talk seemed to have gone down well.\n Getting involved in Code Africa at News International. It was great to be part of a team that produced something really usable and with a real application in such a short space of time.\n \nRead on for more details and additional links.","item_date":"May 20 2012 01:45:38","display_item_date":"05-19-2012","url":"http:\/\/www.leggetter.co.uk\/2012\/05\/20\/a-year-as-a-developer-evangelist.html","source":"www.leggetter.co.uk"},{"title":"The Secret Lives of REST APIs","details":"The recent enterprise acceptance of lightweight REST-based protocols for exposing data and application assets as APIs has been due, in large part, to the simplicity of the resulting interfaces. This simplicity means there is little barrier to entry for developers wishing to consume these APIs in applications built for mobile, Web, desktop, Cloud and gaming platforms. However, as this article from Netflix\u2019s Daniel Jacobson reveals, simplicity can\u2019t be the only goal when designing an API. Flexibility, scalability, optimization, orchestration and adaptation are just a few of the features required in a successful API infrastructure.\n At Layer 7, our enterprise customers build incredibly elegant API platforms using our API management technology. Our solutions recognize that one size does not fit all and we provide the tools to adapt to changing requirements without re-architecting new APIs from scratch. Though we certainly support the simple \u201clarge number of known and unknown developers\u201d use case Jacobson describes \u2013 with robust, scalable technology deployed on a wide variety of hardware, virtual, software and Cloud platforms \u2013 we can also address the specific concerns raised by the variety of devices and environments in Netflix\u2019s ecosystem.\n Message size, structure and delivery constraints due to device variation represent a large part of the problem. Layer 7 Gateways support the relevant formats and transports and can perform message transformation and protocol mediation on the fly. Policy-based configuration enables custom \u201cvirtual\u201d APIs tailored to each device, community of developers or calling application. These format and behavioral changes can be explicit or can be triggered by user identity, app permissions, message content or transaction metadata. Even more complex mediations, such as REST exposure of internal SOAP-based assets, are simple to configure and help to reduce re-implementation costs.\n     Interaction models can also be optimized and tailored to the calling platform. Composition of comprehensive document-based APIs from multiple backend calls can reduce chatty client interactions. Conversely, small messages from memory-constrained devices can be aggregated into larger, less frequent backend calls. Mobile traffic can be optimized using persistent HTTP(S) connections and over-the-wire compression. And content can be cached at any level of granularity, using an in-memory cache like Terracotta, to reduce the number of calls to the application backend.\n As director of one of the world\u2019s most broadly adopted public APIs, Jacobson\u2019s most profound observation is that \u201cpublic APIs are waning in popularity and business opportunity and\u2026 the internal use case is the wave of the future.\u201d API infrastructure needs to support everyone \u2013 open API developers, internal coders, contracted development teams and partner groups \u2013 especially as mobile workforce enablement and BYOD gain popularity. Layer 7 solutions allow enterprises to make that distinction clear through public vs. private APIs, configurable classes of service and role-based access control.\n Jacobson mentions several piecemeal solutions that he and others have attempted to compile into a working platform but notes that those approaches still fall short. Providing an enterprise-grade REST API is no simple feat and it\u2019s great that the truth of the matter is starting to come out. The benefits of a successful API strategy are numerous and well-documented. Layer 7 is the only vendor providing an API management solution that incorporates all the basic necessary functionality and much, much more.","item_date":"May 19 2012 00:25:57","display_item_date":"05-18-2012","url":"http:\/\/www.sys-con.com\/node\/2280763","source":"www.sys-con.com"},{"title":"From A Hackathon Win To A 650K Round And 10,000 Users, Docracy Tells All","details":"The tale of Docracy\u2019s year-long journey is a fun one. When Matt Hall and his partner John Watkinson first went into the Hackathon last year, the only goal was to get a prototype working for an idea they had, a GitHub for legal documents. Sure, a win would\u2019ve been nice, but the main goal was to push out a prototype they could pitch to investors (instead of just an idea) with a firm deadline hanging over their shoulders.\n\n\nBut alas, Docracy took home the top prize despite the fact that they were the first of more than 100 presentations that day. And after last year\u2019s Disrupt NYC (tickets to this year\u2019s Disrupt here), the story only gets better.\n\n\nThe original plan was to push out a fully functional Docracy within a week or so of the Hackathon, but after a chat with a more legal-savvy friend the pair realized that a little extra scrutiny to any liabilities was in order. That week turned into a full seven months, with a hard launch in December of 2011. In November, just a month before, the company received a $650,000 seed round led by First Round Capital. Vaizra Investments, Rick Webb and Quotidian Ventures also participated.\n\n\nMatt and John were able to quit their jobs and focus on Docracy full time. And today the service already has over 10,000 users.\n\n\nThere\u2019s something to be said about the power of a successful Hackathon presentation. It\u2019s not only true with Docracy, but Group.me (which was recently acquired by Skype) won the TechCrunch Hackathon as well and has since gone on to fall under the almighty Microsoft umbrella.\n\n\nDisrupt NYC is set to be one of our biggest shows yet, with returns from Michael Arrington and MG Siegler, along with a variety of big names like Marissa Mayer, Sarah Tavel, Fred Wilson, and David Lee and more. It\u2019s going to be huge.\n\n\nIf you\u2019re interested in checking out Disrupt and\/or the Hackathon yourself, tickets are still on sale here and info on the Hackathon can be found here. Companies who want to join the Battleground can apply for the last remaining spots in Startup Alley. You can find the full agenda here.","item_date":"May 18 2012 22:30:29","display_item_date":"05-18-2012","url":"http:\/\/techcrunch.com\/2012\/05\/18\/from-a-hackathon-win-to-a-650k-round-and-10000-users-docracy-tells-all\/","source":"techcrunch.com"},{"title":"WorldMate API Attracts 50 Developers","details":"More than 50 companies including FlightStats, ExpenseCloud and Shorts Travel Management have licensed an application programming interface from WorldMate, according to the itinerary management provider. WorldMate automates the processing of confirmation emails from more than 1,300 travel intermediaries and suppliers.\u00a0\n The API processes travelers confirmation emails, extracts all of the key information, adds additional content such as an airport geo-code, and then sends the data back to the developer in XML for use in itinerary monitoring, expense reporting, traveler profiling, daily deals or other applications. WorldMate also launched a developer portal.\n With WorldMates API, we will be able to track policy compliance for each booking and consolidate travel data for the travel manager no matter where their travelers book, according to Shorts Travel Management CEO David LeCompte.\n A free license allows users to parse up to 1,000 confirmation emails a week. Developers who require higher volumes or a stringent service level agreement may purchase these separately, according to WorldMate.\n WorldMate rival TripIt for a few years has offered an API, but WorldMate vice president of business development Ian Berman claimed his companys service is different in that end users do not need to create a WorldMate account to benefit--and thus we will not poach by marketing WorldMate services to those users. End users forward their confirmation emails directly to the licensees and not to a WorldMate-branded mailbox.\n WorldMate claims 3 million smartphone users, twice as many as TripIt.\u00a0","item_date":"May 18 2012 16:48:13","display_item_date":"05-18-2012","url":"http:\/\/www.thebeat.travel\/post\/2012\/05\/17\/WorldMate-API-Attracts-50-Developers.aspx","source":"www.thebeat.travel"},{"title":"Is API Growth in a Stall?","details":"Last year when we published the API Economy document, we showed the growth rate of APIs over time. Examining the numbers from the same source \u2014 the ProgrammableWeb \u2014 in 2012 it appears as if the hockey stick growth of over 100% each year is starting to slow down.\n What is really happening?\n The numbers\n Figure 1 shows the original numbers we published in the Open API Economy report. It shows a compound annual growth rate of roughly 100% each year starting in 2005. The source of the numbers is the ProgrammableWeb.\n \n Figure 2 shows an extrapolation of what the numbers should look like using the same growth rate up to the year 2016. The numbers show approximately 30,000 APIs.\n \n Figure 2: Projected Open API Growth Rate. Source: Craig Burton and Phil Windley\n A few days ago, I received an email from a reader who had compiled the numbers from the ProgrammableWeb so far for 2012. Figure 3 shows a graph of the numbers up until the first of May 2012 and it looks like the growth rate is starting to stall.\n \n Figure 3: Open API Growth to May 2012. Source: ProgrammableWeb\n I postulated with the reader that key thing to consider here is that these numbers are based open publicly available APIs and not reflect any private API growth at all. And that in all likelihood any glitches that we see in the Open API growth are expected to happen as the private sector catches up or even surpasses the Open API growth.\n Of course I have absolutely no empirical data to back up my position. \u00a0I am just making it up.\n However, we had a briefing from 3Scale Networks the other day. 3Scale is a major provider of API development and support technology for both small and large companies. 3Scale\u2019s Dr. Steven Willmott gave an excellent presentation at EIC 2012 about the Open API Economy.\n In the briefing \u2014 just last week \u2014 Steve gave us more insight on what is happening with their customers. Steve pointed out that the largest area of growth for their business was in companies that do not publish their APIs and that are being driven by mobile and tablet app growth.\n Their projections show the number of APIs in total \u2014 including both private and public APIs \u2014 to be somewhere around 250,000. He also said that while they watch the numbers published by the ProgrammableWeb, they also do their own data mining and tracking of API growth.\n Now I know that the success of 3Scale\u2019s business depends on a healthy API growth rate. And as such I have to consider discounting somewhat the growth projections they give us. Even so, the growth numbers and logic 3Scale provided in the briefing make a lot of sense and seem to reflect many other indicators and conversations I am having with both customers and vendors.\n I think the slight downturn we see in the Open API growth for the beginning of this year is not a long term trend.\n Every other indicator I look at \u2014 although none of them are exact science \u2014 indicate that if anything the actual growth rate of APIs is still on a steep increase.\n Time will tell.","item_date":"May 18 2012 16:42:02","display_item_date":"05-18-2012","url":"http:\/\/blogs.kuppingercole.com\/burton\/2012\/05\/17\/is-api-growth-in-a-stall\/","source":"blogs.kuppingercole.com"},{"title":"First Sacramento hackathon offers prizes in addition to VC exposure - Sacramento Business Journal","details":"The first Sacramento hackathon set for June has added prizes in addition to VC exposure. A hackathon is an event where small groups are assembled into teams to create, develop and deliver an application after many hours of non-stop effort.","item_date":"May 18 2012 16:38:54","display_item_date":"05-18-2012","url":"http:\/\/www.bizjournals.com\/sacramento\/blog\/mark-anderson\/2012\/05\/hackathon-offers-prizes-vc-exposure.html","source":"www.bizjournals.com"},{"title":"Taming The Social Media Firehose, Part I","details":"This is the first post in our series on what a social media \u201cfirehose\u201d (e.g. streaming api) is and what it takes to turn it into useful information for your organization. \u00a0Here I outline some of the high-level challenges and considerations when consuming the social media firehose; in Parts II and III, I will give more practical examples.\n \n Why consume the social media firehose?\n The idea of consuming large amounts of social data is to get small data\u2013to gain insights and answer questions, to guide strategy and help with decision making. To accomplish these objectives, you are not only going to collect data from the firehose, but you are going to have to parse it, scrub and structure it based on the analysis you will pursue. (If you\u2019re not familiar with the term \u201cparse,\u201d it means machines are working to understand the structure and contents of the social media activity data.) This might mean analyzing text for sentiment, looking at the time-series of the volume of mentions of your brand on Tumblr, following the trail of political reactions on the social network of commenters or any of thousands of other possibilities.\n What do we mean by a social media firehose?\n Gnip offers social media data from Twitter, Tumblr, Disqus and Automattic (WordPress blogs) in the form of \u201cfirehoses.\u201d \u00a0In each case, the firehose is a continuous stream of flexibly structured social media activities arriving in near-real time. Consuming that sounds like it might be a little tricky. While the technology required to consume and analyze social media firehoses is not new, the synthesis of tools and ideas needed to successfully consume the firehose deserves some consideration.\n It may help to start by contrasting firehoses with a more common way of looking at the API world\u2013the plain vanilla HTTP request and response. The explosion of SOAPy (Simple Object Access Protocol) and RESTful APIs has enabled the integration and functional ecosystem of nearly every application on the Web. At the core of web services is a pair of simple ideas: that we can leverage the simple infrastructure of HTTP requests (the biggest advantage may be that we can build on existing web server, load balancers, etc.), and that scaleable applications can be build on simple stateless request\/response pairs exchanging bite-sized chunks of data in standard formats.\n Firehoses are a little different in that, while we may choose to use HTTP for many of the reasons REST and SOAP did, we don\u2019t plan to get responses in mere bite-sized chunks. \u00a0With a firehose, we intend to open a connect to the server once and stream data indefinitely.\n Once you are consuming the firehose, and\u2013even more importantly\u2013with some analysis in mind, you will choose a structure that adequately supports approach. With any luck (more likely smart people and hard work), you will end up not with Big Data, but rather with simple insights\u2013simple to understand and clearly prescriptive for improving products, building stronger customer relationships, preventing the spread of disease, or any other outcome you can imagine.\n The Elements Of a Firehose\n Now that we have a why, let\u2019s zero in on consuming the firehose. Returning to the definition above, here is what we need to address:\n Continuous. For example, the Twitter full firehose delivers over 300M activities per day. That is an average of 3,500 activities\/second or 1 activity every 290 microseconds. The WordPress firehose delivers nearly 400K activities day. While this is a much more leisurely 4.6 activities\/second there still isn\u2019t much time to sleep between the 1 activity every 0.22 s. \u00a0And if your system isn\u2019t continuously pulling data out of the firehose, much can be lost in a short time.\n Streams. As mentioned above, the intention is to make a firehose connection and consume the stream of social media activities indefinitely. Gnip delivers the social media stream over HTTP. The consumer of data needs to build their HTTP client so that it can decompress and process the buffer without waiting for the end of the response. This isn\u2019t your traditional request-response paradigm (that\u2019s why we\u2019re not called Ping\u2013and also, that name was taken).\n Unstructured data. I prefer \u201cflexibly structured\u201d because there is plenty of structure in the JSON or XML formatted activities contained in the firehose. While you can simply and quickly get to the data and metadata for the activity, you will need to parse and filter the activity. You will need to make choices about how to store activity data in the structure that best supports your modeling and analysis. It is not so much what tool is good or popular, but rather what question you want to answer with the data.\n Time-ordered activities done by people. The primary structure of the firehose data is that it represents the individual activities of people rather than summaries or aggregations. The stream of data in the firehose describes activities such as:\n Tweets, micro-blogs\n Blog\/rich-media posts\n Comments\/threaded discussions\n Rich media-sharing (urls, reposts)\n Location data (place, long\/lat)\n Friend\/follower relationships\n Engagement (e.g. Likes, up- and down-votes, reputation)\n Tagging\n \nReal-time. Activities can be delivered soon after they are created by the user (this is referred to as low latency). (Paul Kedrosky points out that a 70s station wagon full of DVDs has about the same bandwidth as the internet, but an inconvenient coast-to-coast latency of about 4 days.) Both bandwidth and latency are measures of speed. Many people know how to worry about bandwidth but latency issues can really mess up real-time communications even if you have plenty of bandwidth. When consuming the Twitter firehose, it is common to realize latency (measured as the time from Tweet creation to the parsing the tweet coming from the firehose) of ~1.6 s \u00a0and as low as 300 milliseconds. WordPress posts and comments arrive 2.5 seconds after they are created on average.\n So there are a lot of activities and they are coming fast. And they never stop, so you never want to close your connection or stop processing activities.\n However, in real life \u201cindefinitely\u201d is more of an ideal than a regular achievement. The stream of data may be interrupted by any number of variations in the network and server capabilities along the line between Justin Bieber tweeting and my analyzing what brand of hair gel teenaged girls are going to be talking their boyfriends into using next week.\n We need to work around practicalities such as high network latency, limited bandwidth, running out of disk space, service provider outages, etc. In the real world, we need connection monitoring, dynamic shaping of the firehose, redundant connections and historical replay to get at missed data.\n In Part II we make this all more concrete. We will collect data from the firehose and analyze it. Along the way, we will address particular challenges of consuming the firehose and discuss some strategies for dealing with them.","item_date":"May 18 2012 16:38:12","display_item_date":"05-18-2012","url":"http:\/\/blog.gnip.com\/streaming-api\/","source":"blog.gnip.com"},{"title":"REST + RDF, finally a practical solution?","details":"This is the blog of William Vambenepe (@vambenepe on Twitter), an architect at Oracle, focused on application management and Cloud.\n \t\t\t\t\n \t\t\t\t \t\t\t\t \t\t\t\tIn API, Cloud Computing, CMDB Federation, CMDBf, DMTF, Everything, Graph query, IBM, Linked Data, Microsoft, Modeling, Protocols, Query, RDF, REST, Semantic tech, SPARQL, Specs, Standards, Utility computing, W3C on May 18, 2012 by @vambenepe  \n  \t\t\t\t \t\t\t\t\t\t\t\t\t\tThe W3C has recently approved the creation of the Linked Data Platform (LDP) Working Group. The charter contains its official marching orders. Its co-chair Erik Wilde shared his thoughts on the endeavor.\n This is good. Back in 2009, I concluded a series of three blog posts on \u201cREST in practice for IT and Cloud management\u201d with:\n I hereby conclude my \u201cREST in practice for IT and Cloud management\u201d series, with the intent to eventually start a \u201cLinked Data in practice for IT and Cloud management\u201d series.\n I never wrote that later part, because my work took me away from that pursuit and there wasn\u2019t much point writing down ideas which I hadn\u2019t\u00a0 put to the test. But if this W3C working group is successful, they will give us just that.\n That\u2019s a big \u201cif\u201d though. Religious debates and paralyzing disconnects between theorists and practitioners are all-too-common in tech, but REST and Semantic Web (of which RDF is the foundation) are especially vulnerable. Bringing these two together and trying to tame both sets of daemons at the same time is a daring proposition.\n On the other hand, there is already a fair amount of relevant real-life experience (e.g. data.gov.uk \u2013 read Jeni Tennison on the choice of Linked Data). Plus, Erik is a great pick to lead this effort (I haven\u2019t met his co-chair, IBM\u2019s Arnaud Le Hors). And maybe REST and RDF have reached the mythical point where even the trolls are tired and practicality can prevail. One can always dream.\n Here are a few different ways to think about this work:\n RESTful API authors too often think they can make the economy of a metamodel. Or that a format (like XML or JSON) can be used as a metamodel. Or they punt the problem towards defining a multitude of MIME types. This will never buy you interoperability. Stu explained it before. Most problems I see addressed via RESTful APIs, in the IT\/Cloud management realm, are modeling problems first and only secondarily protocol\/interaction problems. And their failures are failures of modeling. LDP should bring modeling discipline to REST-land.\n The \u201cRDF was too much, too soon\u201d perspective\n The RDF stack is mired in complexity. By the time people outside of academia had formed a set of modeling requirements that cried for RDF, the Semantic Web community was already deep in the weeds and had overloaded its basic metamodel with enough classification and inference technology to bury its core value as a simple graph-oriented and web-friendly metamodel. What XSD-fever was to making XML seem overly complex, OWL-fever was to RDF. Tenfold.\n Everything that the LDP working group is trying to achieve can be achieved today with existing Semantic Web technologies. Technically speaking, no new work is needed. But only a handful of people understand these technologies enough to know what to use and what to ignore, and as such this application doesn\u2019t have a chance to materialize. Which is why the LDP group is needed. But there\u2019s a reason why its starting point document is called a \u201cprofile\u201d. No new technology is needed. Only clarity and agreement.\n For the record, I like OWL. It may be the technology that most influenced the way I think about modeling. But the predominance of RDFS and OWL (along with ugly serializations) in Semantic Web discussions kept RDF safely out of sight of those in industry who could have used it. Who knows what would have happened if a graph query language (SPARQL) had been prioritized ahead of inference technology (OWL)?\n The Cloud API perspective\n The scope of the LDP group is much larger than Cloud APIs, but my interest in it is mostly grounded in Cloud API use cases. And I see no reason why the requirements of Cloud APIs would not be 100% relevant to this effort.\n What does this mean for the Cloud API debate? Nothing in the short term, but if this group succeeds, the result will probably be the best technical foundation for large parts of the Cloud management landscape. Which doesn\u2019t mean it will be adopted, of course. The LDP timeline calls for completion in 2014. Who knows what the actual end date will be and what the Cloud API situation will be at that point. AWS APIs might be entrenched de-facto standards, or people may be accustomed to using several APIs (via libraries that abstract them away). Or maybe the industry will be clamoring for reunification and LDP will arrive just on time to underpin it. Though the track record is not good for such \u201creunifications\u201d.\n The \u201cghost of WS-*\u201d perspective\n Look at the 16 \u201ctechnical issues\u201d in the LCD working group charter. I can map each one to the relevant WS-* specification. E.g. see this as it relates to #8. As I\u2019ve argued many times on this blog, the problems that WSMF\/WSDM\/WS-Mgmt\/WS-RA and friends addressed didn\u2019t go away with the demise of these specifications. Here is yet another attempt to tackle them.\n The standards politics perspective\n Another \u201cfun\u201d part of WS-*, beyond the joy of wrangling with XSD and dealing with multiple versions of foundational specifications, was the politics. Which mostly articulated around IBM and Microsoft. Well, guess what the primary competition to LDP is? OData, from Microsoft. I don\u2019t know what the dynamics will be this time around, Microsoft and IBM alone don\u2019t command nearly as much influence over the Cloud infrastructure landscape as they did over the XML middleware standardization effort.\n And that\u2019s just the corporate politics. The politics between standards organizations (and those who make their living in them) can be just as hairy; you can expect that DMTF will fight W3C, and any other organization which steps up, for control of the \u201cCloud management\u201d stack. Not to mention the usual coo-petition between de facto and de jure organizations.\n The \u201cI told you so\u201d perspective\n When CMDBf started, I lobbied hard to base it on RDF. I explained that you could use it as just a graph-based metamodel, that you could\u00a0 ignore the ontology and inference part of the stack. Which is pretty much what LDP is doing today. But I failed to convince the group, so we created our own metamodel (at least we explicitly defined one) and our own graph query language and that became CMDBf v1. Of course it was also SOAP-based.\n KISS and markup\n In closing, I\u2019ll just make a plea for practicality to drive this effort. It\u2019s OK to break REST orthodoxy. And not everything needs to be in RDF either. An overarching graph model is needed, but the detailed description of the nodes can very well remain in JSON, XML, or whatever format does the job for that node type.\n All the best to LDP.","item_date":"May 18 2012 16:33:11","display_item_date":"05-18-2012","url":"http:\/\/stage.vambenepe.com\/archives\/1989","source":"stage.vambenepe.com"},{"title":"Learning From Hackathons and How Not to Fail at One","details":"Collaboration and education are two key words intrinsically associated with hackathons (the third being \u2018free pizza\u2019!). Hackathons promise a chance to meet other coders or designers, network with influencers and industry experts, and even find recruitment opportunities. It isn\u2019t surprising that, with the spread of startup culture across geographic boundaries, hackathons too have started mushrooming in the likeliest \u2013 and the unlikeliest \u2013 of places.\nThere are certain rules \u2013 both said and unsaid \u2013 that apply when attending hackathons. While most hackathons remain informal, free-wheeling affairs, you can learn a lot more from them if you follow a few tips:\n1. It\u2019s not about competition\nBarring a few headlining hackathons such as TechCrunch\u2019s TC Disrupt, most hackathons are noncompetitive. As a coder, it can be difficult to refrain from launching into an intellect-measuring competition; but to be successful at a hackathon, this is exactly what you must do. These events are opportunities to learn from others and make new connections, perhaps even find co-founders for a startup. To treat them as competitive code-slinging intellectual wrestling matches would be a terrible waste of the platform.\nEven in competitive hackathons, remain open to overtures from other participants and keep all competitive hostility in check. You never know when the perfect opportunity might drop in your lap, and it could come from anywhere.\n2. Do plan ahead\nMost events have pre-decided themes \u2013 mobile, consumer web, data, etc. Acquaint yourself with the theme beforehand and prepare a flexible road-map for the actual hack day. At the same time, don\u2019t straitjacket your creativity with an obsessively planned-out hack day project. It helps to know what you might build; it helps even more to be receptive to ideas and lessons from other participants at the event.\n3. Attend the right hackathon.\nDifferent hackathons attract different hackers. Some, such as Rewired State, are geared towards open data with a more relaxed atmosphere and plenty of socializing. Others, such as TC Disrupt, are entrepreneurial in spirit and attract startup founders, would-be entrepreneurs looking for co-founders, and even odd VCs looking for the next big thing.\nPick a hackathon that fits your ideology, skills and interests. But at the same time, go beyond your comfort zone by attending a few events diametrically opposite to your domain of knowledge. Open-source, non-profit enthusiasts can learn a lot from for-profit startup founders, and vice versa.\n4. Do socialize\nThe very point of a hackathon is to get coders out of their coding dens into a comfortable space to enable sharing and collaboration. If you are stooped before a solitary computer in a corner of the room on hack day, you are misinterpreting the method and the meaning of hackathons. Move around. Socialize. Hack events are filled with people whose interests and ideologies align. Don\u2019t hesitate to chat up random strangers \u2013 it\u2019s what you both are there for.\nEat, drink, sleep: three words that are often tossed out of the window at hack events. Mountain Dew, Red Bull and beer are often the drinks of choice at hackathons; skip them for a bottle of water and make sure that you stay hydrated throughout the event (which can stretch from a day to a whole week). Likewise for food: eat to fuel your body, but not beyond it. Sleeping is rare at hack events \u2013 part of the fun is late-night coding sessions \u2013 but don\u2019t let that stop you from taking a few hours off to catch your forty winks.\n6. Discuss, analyze, interpret, reconnect\nOnce the hackathon is over and the excitement and nerves have worn off, reconnect with your newfound friends and acquaintances over Twitter, Facebook, and email. Discuss the event, analyze what you learned and interpret your findings. The lessons from one hack event often carry over into real-world projects. Many even find co-founders for the next hot startup in the post-event follow-up (GroupMe was hacked together over a weekend, for instance).\nIt\u2019s easy to get hooked on hackathons, especially if you live in a tech mecca like San Francisco or New York that abounds with them: the chance to meet like-minded individuals, hack for fun, and possibly build something useful is alluring. However, there is a very real danger of getting addicted to the simple highs of hack events, especially if there is a potential of a bigger pay-off in the foreseeable future (an interview with a VC, a spot on TechCrunch, etc.). Too many coders and would-be entrepreneurs end up chasing these improbable rewards in lieu of doing actual work, which rarely works out.\nConclusion\nTreat hackathons as what they are meant to be: informal platforms to build, connect and collaborate with like-minded individuals. You\u2019ll make new friends, learn new things, and perhaps even profit from them. Above all, have fun!\nAbout the Author: Keith Bryant (7 Articles)\n This article has been written by Keith Bryant. He is part of the DWUser.com team that offers software tools for developers and designers, including an easy and free jQuery slider builder and the EasyRotator for WordPress.","item_date":"May 18 2012 16:22:56","display_item_date":"05-18-2012","url":"http:\/\/speckyboy.com\/2012\/05\/17\/learning-from-hackathons-and-how-not-to-fail-at-one\/","source":"speckyboy.com"},{"title":"Cleanweb Hackathon spreads across the country and world","details":"A Cleanweb Hackathon is a gathering of software developers, entrepreneurs, investors and others who stay up for 48 hours or so creating apps that use the Internet, software and social media to address sustainability issues such as energy, waste and waste.\n  The inaugural event was last fall in San Francisco. Theres one today in Boulder, Colo.\n  This year, Burris plans to host 14 Cleanweb Hackathons, including the first one in Texas --Sept. 21 in Houston -- and the first international event -- in June in Paris. \n  Also this year, he plans to organize niche hackathons, such as solar energy and building efficiency. The first one -- a Cleanweb Solar Hackathon -- will take place June 8-10 in Oakland, Calif. Sungevity, a residential solar company, and its founder Danny Kennedy are co-producing it.\n  For each event, Burris provides the website, registration, start kit and guidance and finds local organizers and sponsors to help run and fund each event.\n  Burris brainchild is part of the fast-changing clean technology world. (He also started or co-founded two other Dallas companies -- CoHabitat, a co-working site, and Dynamo Labs Inc., which develops social gaming apps to educate consumers on energy-saving habits.)\n  Clean tech has attracted a lot of attention and venture capital dollars in the past decade, but the last two years have been difficult. VC investments in capital-intensive cleantech startups have fallen because of lackluster exit markets.\n  As a result, people such as Burris are taking more of a crowdsourcing approach to create and fund ideas that might lead to The Next Big Thing.\n  The second Cleanweb Hackathon in January in New York was a much higher-profile event. Spring Ventures partner Sunil Paul was the co-producer. Judges included venture capitalist Fred Wilson and New Yorks chief digital officer Rachel Sterne.\n  More than 100 people participated in the event at New York University. Teams developed 15 apps such as NYC BLDGS, a Web database of the energy consumption of New York buildings presented as a competition, and Econofy, a website that lets consumers compare the energy use by appliances.\n  Heres a more than two hour video from the New York event (it starts 2:38 minutes in):","item_date":"May 18 2012 16:08:04","display_item_date":"05-18-2012","url":"http:\/\/bizbeatblog.dallasnews.com\/archives\/2012\/05\/a-little-project-called-cleanw.html","source":"bizbeatblog.dallasnews.com"},{"title":"A Hackathon to \u201cReinvent Business\u201d","details":"Can social technology enable companies and the people within them to make better decisions? Can it improve corporate behavior? Can it perhaps even help restore the social contract between business and society? These are just some of the questions to be tackled by the \u201cReinvent Business\u201d hackathon \u2013 a collaborative, rapid ideation and programming workshop \u2013 to be held in San Francisco on June 9-10, Hosted by frog and LRN, in partnership with BSR, Carnegie Mellon University, Dachis Group, Net Impact, Silicon Valley Bank, Fast Company, and the World Economic Forum, the two-day event will bring together software developers, designers, gamers, film makers, writers, business leaders, and other creative minds to imagine, design, and build a more human and truly social enterprise. The goal is both simple and bold: to develop concepts and prototypes for innovative products and services that have the capacity to transform business from within.\n The World Economic Forum formed a Global Agenda Council on Values in Decision-Making (of which I am a member) in response to the current economic crisis to realign our collective mindset and develop practical approaches for a more moral economy. It has developed an ambitious agenda to help organizations translate human values into practices and behaviors that strengthen our institutions and positively impact the state of the world. New challenges call for new approaches, and we hope that with this hackathon as the starting point, we can engage the tech and creative communities in an ongoing constructive dialogue on how technology, and especially social technology, can change corporate behavior.\n This need for behavior change, for \u201creinventing business,\u201d is an urgent one, considering the growing number of voices that observe a crisis of capitalism and demand its fundamental transformation.  In an op-ed piece for the Huffington Post, \u201cThe End of Capitalism \u2013 What Comes Next?,\u201d World Economic Forum Founder and Executive Chairman Klaus Schwab draws a clear line between the original, defensible idea of the free market economy and the worrisome decoupling of the financial system and value-creation that makes an overhaul of its technical aspects mandatory. But he also argues that simply correcting excesses is not enough \u2013 we need a more radical reset. Schwab reminds us of the Stakeholder Principle he introduced in 1971, which is now more relevant than ever, propelled by the rise of social technologies:  \u201cIn an age when social networks are enabling greater participation and transparency, companies will only be able to achieve economic success if they can generate long-term benefits not just for their shareholders, but also for the common good.\u201d\n While most companies will subscribe to that point of view rhetorically, the disconnection between business and society (and, as studies show, between management and employees) is indisputable. From Occupy Wall Street to public resignation letters in the New York Times, the widening \u2018trust gap\u2019 between business and society can\u2019t be ignored.   In a recent column, The Economist contends that companies should worry less about their reputations and more about how they do business. And indeed, increasingly, consumers and citizens demand that companies match their words to their actions, far beyond just Mission Statements, Codes of Conduct, and Corporate Social Responsibility (CSR) programs. For today\u2019s consumers and citizens, integrity means that a firm\u2019s decisions and behaviors are fundamentally and consistently aligned with its values and principles as well as those of its stakeholders (which include society at large).\n Yet the crisis of capitalism is not only a crisis of integrity, it is also one of meaning. \u201cIt is the job of companies to produce meaning,\u201d writes management philosopher John Hagel, and with a growing number of consumers and employees expressing a strong desire for purpose beyond profits, and even the Harvard Business Review espousing happiness as a new economic paradigm, this appears to be a ripe time for reinvention. As the world becomes more interconnected and interdependent, the lines between the corporate and the social, the professional and the personal blur, and the younger generation of netizens in particular expects heightened levels of transparency, responsibility, and inspiration \u2013 both as consumers and employees. Being \u201cIn Search of Excellence\u201d therefore no longer suffices; companies and their leaders now need to pursue significance.\n Achieving this vision requires changes in two key dimensions of our economy: what we consider to be \u201cvalue\u201d and the values that guide how we create it. We need a more inclusive value definition that is respectful of all stakeholders\u2019 needs, fully aligned with human values, and transcending the merely transactional; and we need inclusive models of value-creation that are inherently social.\n At the national level, Bhutan\u2019s Gross Happiness Index, as an alternative to the Gross Domestic Product (GDP), has gained many admirers, and both the French and the UK governments have begun to emulate it. Moreover, there are supra-national organizations and NGOs such as the New Economics Foundation in the UK that have long proposed new indicators of economic progress grounded in the belief that we need a more holistic, integrated understanding of the economy instead of one that is purely based on financial terms.\n At the corporate level, social media and networks have disrupted traditional paradigms of productivity and organizational behavior, and enabled participatory models of value creation that fully harness human potential by leveraging social technologies to maximize social capital (e.g. enterprise social networks, crowdsourcing). Consequently, the social enterprise of the 21st century recognizes social capital \u2013 in other words: talent, inside and outside of the organization \u2013 as its key asset. In his article, Klaus Schwab describes this as a significant paradigm shift:\n \u201cCapital is being superseded by creativity and the ability to innovate \u2013 and therefore by human talents \u2013  as the most important factors of production. If talent is becoming the decisive competitive factor, we can be confident in stating that capitalism is being replaced by \u2018talentism.\u2019 Just as capital replaced manual trades during the process of industrialization, capital is now giving way to human talent. I am convinced that this process of transformation will also lead to new approaches within the field of economics. It is indisputable that an ideology founded on personal freedom and social responsibility gives both individuals and the economy the greatest possible scope to develop.\u201d  \n With the rise of \u201ctalentism,\u201d the need for a real understanding of the common human values that connect organizations and individuals is becoming ever more important. Transparency, inclusivity, individual empowerment, and organizational responsiveness are crucial in nurturing businesses\u2019 social fabric and facilitating empathy and collaboration. Yet, the challenge remains: How do we translate these values into day-to-day corporate behavior and into tangible, personal experiences? How can companies become open organizations that harness and build social capital both inside and outside of their institutional boundaries?\n This is where our \u201cReinvent Business\u201d hackathon comes in. Whether it is a mobile app that uses real-time peer feedback from social networks to help address ethical dilemmas, or a tracker that captures and measures alternative types of value, or a data visualization tool that illustrates the unintended consequences and the externalities of decisions, or an \u2018empathy app\u2019 that lets you see the world through someone else\u2019s eyes, we hope for concepts that radically rethink the current business frameworks while at the same time offering practical solutions to business leaders and employees.\n To be clear: We don\u2019t expect this hackathon to produce any silver bullets, nor do we aim to \u201cproductize away\u201d the deep and complex challenges of a moral economy and a more human enterprise. But we are poised to spawn a conversation by tackling concrete problem spaces that might inspire further-reaching, broader solutions.\n                                                     \n                  Tim Leberecht is the CMO of frog and the publisher of design mind.","item_date":"May 18 2012 16:04:58","display_item_date":"05-18-2012","url":"http:\/\/designmind.frogdesign.com\/blog\/a-hackathon-to-reinvent-business.html","source":"designmind.frogdesign.com"},{"title":"Edmunds.com Integrates Vehicle Data with New Online Dealer Pricing Tool","details":"SANTA MONICA, Calif. --         Hundreds of car dealerships will now have an easier way to research, market and accurately price their vehicles, thanks to a new data-sharing agreement between inventory marketing and management leader Nexteppe and Edmunds.com, the premier online resource for automotive information.\nThe agreement makes Edmunds.com\u2019s data available through Nexteppe\u2019s inControl Pro, a system used by dealers to compare their new and pre-owned vehicle prices with other dealers in the market. The tool also allows consumers to appraise their trade-in vehicles by giving them direct access to Edmunds.com\u2019s True Market Value\u00ae (TMV\u00ae) pricing through participating dealership websites.\nBy integrating Edmunds.com data, Nexteppe instantly boosts its product\u2019s value to participating dealers. A recent internal analysis found that \u201cdeeply engaged\u201d car shoppers use Edmunds.com more than any other 3rd party automotive site. The analysis also found that 90 percent of visitors who submit a lead through Edmunds.com purchase a vehicle within 70 days.      \n     \n                      \u201cWe chose to partner with Edmunds.com because of its reputation as the leading authority for both automotive consumers and retailers,\u201d said Nexteppe CEO Ken DellaPorta. \u201cEdmunds.com\u2019s data helps our dealership partners improve their customers\u2019 online experience and will allow them to attract more visitors and to sell more cars quickly.\u201d\nNexteppe also enhances inControl Pro\u2019s Comment Builder tool by importing Edmunds.com\u2019s consumer reviews, which are then made available to shoppers on the dealer site. According to The Nielsen Company\u2019s latest \u201cGlobal Trust in Advertising\u201d report, online consumer reviews are the second most trusted source of brand information and messaging.\nEdmunds.com\u2019s makes its vehicle pricing and review data easily available through the company\u2019s application programming interface (API). Developers can learn more about Edmunds.com\u2019s API at http:\/\/developer.edmunds.com\/docs.\n\u201cOur API gives automotive information and shopping sites easy access to the depth of vehicle research data that only Edmunds.com can provide,\u201d said Edmunds.com Chief Information Officer Philip Potloff. \u201cNexteppe\u2019s integration of Edmunds.com data is exactly what we had in mind when the API was first made available to developers late last year.\u201d\nDealerships have already responded favorably to Nexteppe\u2019s affiliation with Edmunds.com in their evaluations of inControl Pro.      \n     \n                        Sunherald.com encourages an open exchange of affirming and dissenting opinions on our stories, and we consider it an important element of the user experience on sunherald.com. We invite you to comment on our content as part of our interactive community, but please keep the discourse civil and refrain from profanity, obscenity, spam, name-calling or attacking others for their views.\u00a0\u00a0Read more","item_date":"May 17 2012 22:38:33","display_item_date":"05-17-2012","url":"http:\/\/www.sunherald.com\/2012\/05\/17\/3953117\/edmundscom-integrates-vehicle.html","source":"www.sunherald.com"},{"title":"With New Privacy Policy, Twitter Commits to Respecting Do Not Track","details":"Under a new policy announced today, Twitter will be suggesting accounts for Twitter users to follow based on data collected from an individual\u2019s browsing habits on websites that have embedded Twitter buttons. While this is sure to garner scrutiny from the press and public, Twitter is also taking a pioneering step toward respecting users\u2019 privacy choices: it has committed to respecting Do Not Track -- a simple browser setting users can turn on to tell website they don\u2019t want to be tracked. Often framed as a signal from users to behavioral advertisers, Do Not Track isn\u2019t actually about ads we see online; it\u2019s about user control over tracking of our web usage that could be used to build an intimate portrait of our online lives. Twitter is showing an inventive way that websites other than behavioral advertisers can respect Do Not Track. We\u2019re heartened to see this forward-thinking approach and hope other sites with embedded widgets will follow suit.\n If you haven\u2019t done so already, this is a great reminder to turn on Do Not Track; Twitter has a tutorial for doing this on different browsers.\n Here\u2019s how the suggested accounts will work under the new Twitter privacy policy: when you browse around the Internet to pages with embedded Twitter share buttons1, Twitter is able to collect a certain amount of information about you through a unique browser cookie. Then when you sign up for or log into Twitter, the site will be able to suggest that you follow the accounts of individuals who are popular among others who visited the same sites as you. Twitter calls this \u201ctailoring\u201d your Twitter experience based on your web browsing.\n For example, many of those who visit BoingBoing.net likely follow the account of @doctorow, the digital-rights-loving BoingBoing founder Cory Doctorow. If you sign up for Twitter and you\u2019ve got a browser cookie from Twitter showing that you recently visited BoingBoing, you might see @doctorow listed as a suggested user even before you\u2019ve started interacting with Twitter. Twitter will be relying on data collected about your browsing habits within the last 10 days (after 10 days, they start discarding data). When you start a Twitter account, you\u2019ll have the option to turn off the tailored suggestions. Unchecking this box won\u2019t just stop the suggestions from appearing \u2013 it\u2019ll actually remove the unique cookie that Twitter uses to track your browsing habits and show you tailored user suggestions.2\n \n Established Twitter users may find suggested users under the Who To Follow sections of Home and Discover. Just like with new users, established users can uncheck the \u201ctailor\u201d Twitter box in their account settings to stop the data collection about their web browsing.\n Do Not Track makes this a lot simpler \u2013 no messing with account settings or unchecking any boxes to keep your privacy. If you\u2019ve got Do Not Track selected in your browser settings, then Twitter assumes you just don\u2019t want them collecting details of your online browsing habits in an identifiable way. Users who have turned on Do Not Track will find, upon signing up for Twitter, that the \u201ctailor Twitter\u201d button is unchecked by default. Similarly, established users who had Do Not Track already enabled in the days before the new policy took effect will also find the account personalization turned off by default. Users who enable Do Not Track must change their privacy settings if they want the \u201ctailored\u201d Twitter experience.\n \n As with Facebook, Twitter also treats users differently depending on whether or not they are logged out. If you\u2019re a Twitter user wanting to protect your browsing privacy, then remember to log out when you leave the site so that Twitter wont associate your online browsing habits with your Twitter account.\n There\u2019s sure to be a lot of discussion about Twitter\u2019s decision to use data collected through social widgets for increased site personalization. If nothing else, this is a good opportunity for everyone to reconsider the nature of our highly trackable online lives, where corporations we do and don\u2019t have relationships with can vacuum up highly sensitive data about what we do on the web and even a savvy user can\u2019t win the arms race against online tracking. This is exactly the promise of Do Not Track: to make it easy for everyday Internet users to clearly indicate a preference not to be tracked around the web, whether it\u2019s by a social network or an advertiser or another data-hungry corporate entity.\u00a0\n We\u2019ve previously examined Facebook\u2019s practices when it comes to collecting browsing data on users and urged it to respect the Do Not Track flag. So, in the wake of Twitter\u2019s decision to respect Do Not Track, we\u2019re calling on other social networking sites to start respecting user choice as well. The time has come for websites to start listening to users when it comes to privacy, and there\u2019s no easier way for a user to tell companies \u201cDon\u2019t track me\u201d than to turn on Do Not Track.\n Get started now by checking out the tutorial Twitter created for turning on Do Not Track.\n   \n  1. Eff.org has Twitter icons that allow visitors to share content from our site on Twitter but we have chosen not to embed Twitter code on our site, so visiting eff.org will not result in data about your visit going to Twitter.\n 2. Note: this doesn\u2019t mean Twitter will stop collecting all data on you. They\u2019ll still be able to collect aggregate data about your browsing habits for analytics and security, but they won\u2019t set a cookie and they won\u2019t use data to suggest users to you or tailoring your Twitter experience.","item_date":"May 17 2012 21:55:20","display_item_date":"05-17-2012","url":"https:\/\/www.eff.org\/deeplinks\/2012\/05\/new-privacy-policy-twitter-commits-respecting-do-not-track","source":"www.eff.org"},{"title":"Seamless Delivers Food and an API","details":"Seamless is a food ordering and delivery tool that connects buyers with sellers and takes a percentage of the sale. This alone is not a revolutionary idea; using the income that comes from sales commissions to avoid annoying advertisements that turn off users while collecting data on restaurants and eating habits is pretty smart. The goal is to leverage the data they are collecting and become a search resource that can rival companies like Yelp. Seamless has partnered with over 9,500 restaurants in 45+ cities and has released the Seamless API to give developers a chance to explore integration possibilities.\n\n\n\n\n\nI decided to put the Seamless iPhone application to the test, but I wasn\u2019t hungry, so I figured why not see what delivery options would be available for President Obama. I opened the application and entered \u201c1600 Pennsylvania Avenue Northwest, 20500\u201d and Seamless returned a list of restaurant options near The White House. I hope Mr. President is hungry:\n\n\n\n\n\nSeamless also has Android, Blackberry, and iPad applications. The company\u2019s API does not have documentation that is publicly available, but you can request an API key here.\n\n\nThe Seamless API is one of 48 food APIs listed in the ProgrammableWeb directory.","item_date":"May 17 2012 20:18:02","display_item_date":"05-17-2012","url":"http:\/\/blog.programmableweb.com\/2012\/05\/17\/seamless-delivers-food-and-an-api\/","source":"blog.programmableweb.com"},{"title":"Developer Spotlight: Tumblr - Facebook Developers","details":"Tumblr is a blogging platform that lets people post content \u2013 including images, videos, links and audio \u2013 to their tumblelog, a short-form blog.  Tumblr integrated Open Graph into their web and mobile apps to make it easy for people to post Tumblr content to their timelines.\n   \n   Tumblr associates a photo with each post published to timeline and news feed, creating more attractive stories for friends.\n Tumblr uses the message property for personal messages on the Post, Reblog and Reply actions to show more relevant content to friends.     \n   \n Tumblr provides clear messaging about what activity is posted to timeline and simple controls to help users determine what is shared on Facebook.    \n   \nImplementation Profile\n   Actions - Objects","item_date":"May 17 2012 18:46:41","display_item_date":"05-17-2012","url":"http:\/\/developers.facebook.com\/blog\/post\/2012\/05\/17\/developer-spotlight--tumblr\/","source":"developers.facebook.com"},{"title":"Finding Meaning in the Social Firehose","details":"APIs serve up a lot of valuable data and resources, but the most valuable API currency right now is social data, and specifically public data from Twitter. \u00a0APIs are essential in not just accessing this social data, but also delivering meaning extracted from this firehose of social data.\n\nWhile Twitter has a streaming public API available for anyone, only a small number of companies actually have access to the full stream of Twitter\u2019s firehose, and access to historical Tweets, because accessing Twitter at this level is very costly in three ways:\n\nGaining access\n\n Technology and expertise to consume\n\n Technology and expertise to process and abstract meaning from\n\n \n\n\n\n\n\nOne of the companies with access and the resources to consume, process and abstract meaning from the Twitter firehose is PeopleBrowsr. \u00a0PeopleBrowsr enables anyone to find influential people based on their datamine of hundreds of billions of social media conversations from the full Twitter firehose since 2008, public Facebook posts, 40 million blogs and forums, and other sources.\n\n\n\n\n\nThe new PeopleBrowsr Kred API provides a programmatic way to find meaning from real-time social data in four distinct ways:\n\n\n\n\n\nFinding influential people on any subject or within communities connected by shared interests or affinities by considering influence, outreach, RT influence, and followers, while emphasizing interactions and connections over follower count.\n\n Deep Analytics providing author-based analytics which include a summary of anyone\u2019s social presence, historical scores, reach, friends & followers, and location-based data.\n\n Action Analytics that provide aggregated social data metrics for any keyword, hashtag or @name, including mention counts, word clouds and hashtag clouds, with the capability to filter by community, location, keyword and bio keyword for any time range down to a minute-by-minute basis.\n\n Global Kred Score that provides a single unit of measurement for influence in online communities that are connected by interest.\n\n \n\n\n\n\n\nAs a developer, my instinct is to want full access to any dataset for maximum control. \u00a0But in a world of APIs I like the idea of being able to access abstracted layers built on top of big datasets, that derive meaning for me, by really smart people who spend their days deeply understanding this data.\n\n\n\n\n\nAPIs are enabling a really fascinating new layer of resellers who are able to derive meaning from the social firehose, package up this meaning into a new product, and make available to developers via a new API interface, deriving entirely new products from the existing social firehose.\n\n\n\n\n\nPhoto by US Navy","item_date":"May 17 2012 18:43:01","display_item_date":"05-17-2012","url":"http:\/\/blog.programmableweb.com\/2012\/05\/17\/finding-meaning-in-the-social-firehose\/","source":"blog.programmableweb.com"},{"title":"Twitter Implements Do Not Track Privacy Option","details":"It\u2019s no secret that Facebook is worth about $100 billion because it  collected personal data about its users. A lot of data.\nAlthough Twitter tracks its users too \u2014 albeit in a much less aggressive way \u2014 the company has decided to take a different route. It announced Thursday that it is joining Mozilla, the maker of the Firefox Web browser, and giving its users the ability to opt-out of being tracked in any way through Twitter.\nTwitter is doing this by enabling the Do Not Track feature in the Firefox browser that enables people to opt-out of cookies that collect personal information and any third-party cookies, including those used for advertising. The Do Not Track functionality will only work if a Web site agrees to acknowledge it.\nEd Felten, chief technology officer for the Federal Trade Commission, announced Twitter\u2019s involvement in the privacy feature at a New York Internet Week privacy panel.\nThe announcement occurred during a session titled, \u201cOpting in to Do Not Track: A morning mini-conference on privacy, tracking and more.\u201d\nIn a message on Twitter, the company confirmed the FTC\u2019s announcement.\nCarolyn Penner, a spokeswoman for Twitter, said in a statement, \u201cAs the Federal Trade Commission\u2019s CTO, Ed Felten, mentioned this morning, Twitter now supports Do Not Track.\u201d Ms. Penner added: \u201cWe applaud the FTC\u2019s leadership on Do Not Track, and are excited to provide the benefits of Do Not Track.\u201d\nAlthough Facebook has recently started allowing people to see the kind of data the company collects on them, there is no way to opt-out of Facebook\u2019s aggressive information collection, which can even track people who are not logged into Facebook.\nTwitter\u2019s move to put power into the hands of the users with Do Not Track is part of a series of recent announcements by the company that seem to favor its users. Earlier this month Twitter filed a court motion to protect the information about one of its users who made use of Twitter during the Occupy Wall Street protests.\nLast month Twitter was also lauded for announcing the Innovator\u2019s Patent Agreement, a new type of patent agreement that gives legal rights to engineers who are awarded a patent, stopping any potential for a patent to be used for offensive litigation in the future.\nIn a blog post on the Mozilla Web site, the company shared new statistics for the Do Not Track add-on and applauded Twitter for its actions.\n\u201cWe\u2019re excited that Twitter now supports Do Not Track and global user adoption rates continue to increase, which signifies a big step forward for Do Not Track and the Web,\u201d Mozilla said in the post. Mozilla noted that the adoption rates of Do Not Track are now 8.6 percent for desktop users and 19 percent for mobile users.","item_date":"May 17 2012 18:39:04","display_item_date":"05-17-2012","url":"http:\/\/bits.blogs.nytimes.com\/2012\/05\/17\/twitter-implements-do-not-track-privacy-option\/","source":"bits.blogs.nytimes.com"},{"title":"APIs and Copyrights: Monopolizing Ideas or Affording Protection?\u00a0","details":"The video and slides are below. Thanks to our moderator @brianpagano and the entire team for a lively and informative discussion. Wed love to continue the discussion on the\u00a0api-craft forum.\n \n APIs & Copyrights  View more presentations from Apigee\n \n                                                           Here are 10 areas where you need to CYA (cover your apps).                   \n                                        Download Whitepaper","item_date":"May 17 2012 18:38:03","display_item_date":"05-17-2012","url":"http:\/\/blog.apigee.com\/detail\/apis_and_copyrights\/","source":"blog.apigee.com"},{"title":"Bing Bing Search API now available on Windows Azure Marketplace","details":"Following our blog post last month, the Bing Search API is now available on the Windows Azure Marketplace. Starting today, you can sign up in the Marketplace to access web, image, news, and video search results, as well as related searches and spelling suggestions using JSON or XML. \n      \nFor up to 5,000 queries per month, developers can access the API for free on the Windows Azure Marketplace. At this level, the large majority of our existing developers \u2013 including non-profits, educational institutions, and smaller scale applications\u2013 can continue using the service for free.\n  For greater than 5,000 queries per month, developers can purchase a subscription on the Windows Azure Marketplace. Developers interested in using only web results can also sign up for a lower cost offering.\n  \nAs a one stop shop for cloud data, apps, and services, the Windows Azure Marketplace provides opportunities to access hundreds of other datasets and distribute your applications. In transitioning, the Search API is able to serve more relevant and up-to-date results. In addition, we encourage you to review the updated terms of use, which now allow greater flexibility to re-order and blend results so that you have greater control over how Bing data is integrated into services and applications.\n  Right now, we\u2019re offering a free limited time trial for any subscription tier to help in the transition process. If you are a high volume user who needs more queries per month than what is listed on the Marketplace, please submit a request to onboard through a separate process.\n  \n  We encourage existing developers to begin transitioning to the Windows Azure Marketplace before Bing Search API 2.0 AppIDs are decommissioned on August 1, 2012. On and after this date, AppIDs will no longer return results. Developers can continue using the API by signing up for the Windows Azure Marketplace or by submitting a request if greater query volume is needed.\n  To transition your application to the Windows Azure Marketplace:","item_date":"May 17 2012 18:36:23","display_item_date":"05-17-2012","url":"http:\/\/www.bing.com\/community\/site_blogs\/b\/developer\/archive\/2012\/05\/17\/bing-developer-update-2.aspx","source":"www.bing.com"},{"title":"How Yahoo Killed Flickr and Lost the Internet","details":"Web startups are made out of two things: people and code. The people make the code, and the code makes the people rich. Code is like a poem; it has to follow certain structural requirements, and yet out of that structure can come art. But code is art that does something. It is the assembly of something brand new from nothing but an idea.\n This is the story of a wonderful idea. Something that had never been done before, a moment of change that shaped the Internet we know today. This is the story of Flickr. And how Yahoo bought it and murdered it and screwed itself out of relevance along the way.\n Do you remember Flickrs tag line? It reads almost certainly the best online photo management and sharing application in the world. It was an epic humble brag, a momentously tongue in cheek understatement.\n Because until three years ago, of course Flickr was the best photo sharing service in the world. Nothing else could touch it. If you cared about digital photography, or wanted to share photos with friends, you were on Flickr.\n Yet today, that tagline simply sounds like delusional posturing. The photo service that was once poised to take on the the world has now become an afterthought. Want to share photos on the Web? Thats what Facebook is for. Want to look at the pictures your friends are snapping on the go? Fire up Instagram.\n Even the notion of Flickr as an archive\u2014as the place where you store all your photos as a backup\u2014is becoming increasingly quaint as Dropbox, Microsoft, Google, Box.net, Amazon, Apple, and a host of others scramble to serve online gigs to our hungry desktops.\n The site that once had the best social tools, the most vibrant userbase, and toppest-notch storage is rapidly passing into the irrelevance of abandonment. Its once bustling community now feels like an exurban neighborhood rocked by a housing crisis. Yards gone to seed. Rusting bikes in the front yard. Tattered flags. At address, after address, after address, no one is home.\n It is a case study of what can go wrong when a nimble, innovative startup gets gobbled up by a behemoth that doesnt share its values. What happened to Flickr? The same thing that happened to so many other nimble, innovative startups who sold out for dollars and bandwidth: Yahoo.\n Heres how it all went bad.\n Flickr famously began as a feature of another product. Husband-and-wife development team Stewart Butterfield and Caterina Fake had created a photo sharing feature for another product they were working on, Game Neverending. Butterfield and Fake were old-school Web types. The kind with low Metafilter user numbers and WELL accounts.\n And because they knew the Web so fluently, they soon realized that their real product wasnt the game: It was this secondary feature, the ability to share photos online. This was 2003, and photo sharing was still very much a novel problem for people. Flickr was born.\n It was a hit. Bloggers especially loved it, as it solved an age-old photo hosting problem. (This was during the hoary old days of the Web when storage actually cost money.)\n Two years later, in 2005, Butterfield and Fake sold their company to Yahoo, whose deep pockets promised great things for Flickrs users. It upped the monthly storage limit to 100MB for free users, and removed it altogether for pro accounts, for example. Yahoo had bandwidth and engineering to burn. Things were going to be great; things are always going to be great the first time you embrace a new corporate mother.\n When Startups Become Successes\n Very few people manage to build successful startups. But when the one hits, it can change the status quo in an instant. Suddenly, those two elemental ingredients\u2014people and code\u2014become very valuable to the established companies that seem to reside on an untouchable corporate Mount Olympus. It would have to be an overwhelming compliment and sense of validation. How would you handle it? What if you made something beautiful and useful that changed the status quo? Would you sell it? Would you sell yourself?\n Thats the choice successful startup founders are faced with. Build something good, and the buyout offers start rolling in. But while selling out in most other fields of creative endeavor is frowned upon, its a given on the Web.\n Maybe it shouldnt be. For every YouTube, there are horror stories of great people with great products, squandered in the yawning maws of uncaring corporate integration. Dodgeball gets lost in Mountain View. Beloved bookmarking services like Delicious become fields of information left fallow.\n Some upstarts take an independent path. Consider Foursquare. Or Twitter. Or Facebook. Each spurned buyout offers, and none has ever been stronger. All managed to find a business model over time. Or even StumbleUpon, which only found its feet after its founder re-purchased his company from eBay and spun it off again as an indie.\n Its no secret that for many entrepreneurs, the exit is always the goal. Its about the sellout before the first line of code is written. But for a select group, products are meant to be art. They are meant to literally change the world. And for those, selling out can be especially problematic.\n Flickr falls into that camp.\n Integration Is The Enemy of Innovation\n Yahoo was a good fit initially, says Flickr co-founder Caterina Fake, who left the company in 2008. We had offers from various companies, including Google, and I honestly think that Yahoo was a great steward. It was a great steward of the brand. It was allowed to flourish. In the subsequent two years after the acquisition, Flickr blossomed.\n Yet even early on, there were signs that the transplant\u2014which had seemed so successful at first\u2014was going to fail. That the DNA didnt match. This was largely due to how this new appendage was grafted on by Yahoos CorpDev department.\n \n When a new startup comes into an established company, the first wall it typically hits is CorpDev, or corporate development: the group within a business that manages change. CorpDev is usually charged with planning corporate strategy\u2014where a business will grow or shrink, the markets it will enter or exit, and what kind of contracts and deals it may strike with other companies. It often oversees acquisitions. It plans them. Approves them. And then it sets the terms.\n When a big company gobbles up a smaller one, often only a fraction of the money is handed over up front. The rest comes later, based on the acquisition hitting a series of deliverables down the road. Its similar to how incentives are built into the contracts of professional athletes, except with engineering benchmarks instead of home runs.\n Corpdev sets these milestones. They reflect the reason for the acquisition, and how the company\u2014in Flickrs case, Yahoo\u2014can leverage them. Theyre baked into the deal, and an acquisition integration team begins working immediately to make sure they are met. Typically, theyre very engineering-based, designed to integrate the smaller companys product into the enormous corporate machine.\n And because payment schedules are based on achieving those CorpDev terms, it means both companies have a vested (pun intended) interest in putting those milestones ahead of new features. They are a sledgehammer applied with great force to the feet of nimble development. Worse, they often completely ignore what made the smaller target valuable in the first place.\n Take Upcoming, the calendaring site Yahoo bought not long after Flickr. It was a play to get local listings. Local data\u2014especially in smaller cities or for smaller events\u2014can be very hard to come by. Everyone ends up having the same stuff. But Upcomings data was user-generated. It was different. Unique. Valuable.\n The milestones for that acquisition were all based around integrating that local event data into Yahoo. Yahoo didnt care about Upcomings users\u2014the community that created the data. Yahoos approach turned out to be completely backwards. The value of the the company was determined by the index itself, rather than how the index was built\u2014which is to say, by the community.\n It was a stunning failure in vision, and more or less the same thing happened at Flickr. All Yahoo cared about was the database its users had built and tagged. It didnt care about the community that had created it or (more importantly) continuing to grow that community by introducing new features.\n We spent a lot of time in meetings with CorpDev just defending the product and justifying our decisions, said a former Flickr team member.\n And so when Flickr hit the ground at Yahoo it was crushed with engineering and service requirements it had to meet as per demands of the acquisition integration team. Those were a drain on resources, human and financial. Even though many of the resources came from Yahoo, they were debited against Flickr. This created an untenable cycle that actively hampered innovation.\n The money goes to the cash cows, not the cash calf, explains one former Flickr team member. If Flickr couldnt make bucks, it wouldnt get bucks (or talent, or resources).\n Because Flickr wasnt as profitable as some of the other bigger properties, like Yahoo Mail or Yahoo Sports, it wasnt given the resources that were dedicated to other products. That meant it had to spend its resources on integration, rather than innovation. Which made it harder to attract new users, which meant it couldnt make as much money, which meant (full circle) it didnt get more resources. And so it goes.\n As a result of being resource-starved, Flickr quit planting the anchors it needed to climb ever higher. It missed the boat on local, on real time, on mobile, and even ultimately on social\u2014the field it pioneered. And so, it never became the Flickr of video; YouTube snagged that ring. It never became the Flickr of people, which was of course Facebook. It remained the Flickr of photos. At least, until Instagram came along.\n The Flickr team was forced to focus on integration, not innovation. This played out in two key areas.\n Socially Awkward\n Flickrs best feature isnt what you think. Its not photo-sharing at all. Just as photo sharing was a feature hidden within a game, there was another feature hidden within photo-sharing that was even more powerful: social networking. Flickr was, nearly a decade ago, building what would become the Social Web.\n The first point in Flickrs two point mission statement is to help people make their photos available to the people who matter to them. Flickr had\u2014and still has\u2014excellent tools for this. Flickr was an early site that let you identify relationships with fine grained controls\u2014a person could be marked as family but not a friend, for example\u2014instead of a binary friend\/not friend relationship. You can mark your photos private and allow no one else to see them at all, or identify just one or two trusted friends who may view them. Or you can just share with friends, or family. Those granular controls encouraged sharing, and commenting, and interaction. What we are describing here, of course, is social networking.\n Its hard to remember, but back in 2005, Yahoo seemed like it had its game on. After losing out on search dominance to Google, it snapped up a bunch of small-but-cool socially oriented companies like Flickr (social photos), Delicious (social bookmarking), and Upcoming (social calendaring). There was a real sense that Yahoo was doing the right thing. It was, to some extent, out in front of what would come to be widely known as Web 2.0: the participatory Internet.\n But Yahoos social success in those years was almost accidental. It wasnt (and isnt) a company with vision. Its founders Jerry Yang and David Filos great contribution to the Internet? They built a directory of links and then sold ads on those pages.\n It was a gateway, nothing more. This was hardly an innovative idea, or technically complicated to pull off. You dont have to write algorithms to build a portal. Yahoo was little more than an electronic edition of Yellow Pages.\n The founders influence on a companys culture is enormous, and Yang and Filo cared about business, not products or innovation. They didnt foster a culture of computer scientists, like Googles founders did, or cultivate hackers like Facebook. They grew a business culture. For many years that worked quite well\u2014until Google came along. Suddenly nobody needed directories anymore. Why browse a hierarchy when you can jump directly to what youre looking for with a simple query?\n Yahoos CEO Terry Semel had failed to buy Google in 2001, when he had the chance. Now Yahoo was so focused on winning search that it essentially surrendered social. In 2005, Flickr had far and away the best social connection and discovery tools on the Internet. Remember, back then Facebook was still very much a fledgling service, one that didnt even let you upload pictures other than the one in your profile. Yahoo, meanwhile, had existing internal social products, like Address Book and Messenger. Social was clearly the future. What Yahoo wanted, however, wasnt the future. It was to re-fight an old battle from the past. It was to beat Google.\n By the time we were looking at Flickr, Yahoo was getting the shit kicked out of it by Google. The race was on to find other areas of search where we could build a commanding lead, says one high ranking Yahoo executive familiar with the deal.\n Flickr offered a way to do that. Because Flickr photos were tagged and labeled and categorized so efficiently by users, they were highly searchable.\n That is the reason we bought Flickr\u2014not the community. We didnt give a shit about that. The theory behind buying Flickr was not to increase social connections, it was to monetize the image index. It was totally not about social communities or social networking. It was certainly nothing to do with the users.\n And that was the problem. At the time, the Web was rapidly becoming more social, and Flickr was at the forefront of that movement. It was all about groups and comments and identifying people as contacts, friends or family. To Yahoo, it was just a fucking database.\n The first community problems became evident when Yahoo decided all existing Flickr users would need a Yahoo account to log in. That switchover occurred in 2007, and was part of the CorpDev integration process to establish a single sign on. Flickr set it to go live on the Ides of March.\n From Yahoos perspective, there was no choice but to revamp the login. For one, Flickr had grown internationally, and it had to localize to comply with local laws. Yahoo already had tools to solve this, because it had already expanded into other countries. It offered a ready-made solution.\n But moreover, Yahoo needed to leverage this thing that it had just bought. Yahoo wanted to make sure that every one of its registered users could instantly use Flickr without having to register for it separately. It wanted Flickr to work seamlessly with Yahoo Mail. It wanted its services to sing together in harmony, rather than in cacophonous isolation. The first step in that is to create a unified login. Thats great for Yahoo, but it didnt do anything for Flickr, and it certainly didnt do anything for Flickrs (extremely vocal) users.\n Yahoos RegID solution turned out to be a nightmare for the existing community. You could no longer use your existing Flickr login to get to your photos, you had to use a Yahoo one. If you did not already have a Yahoo account, you had to create one. And you did not even log in on Flickrs home page, upon arriving, you were immediately kicked over to a Yahoo login screen.\n Although Flickr grew tremendously with the huge influx of Yahoo users, the existing community of highly influential early adopters was infuriated. It was an inelegant transition, and seemed to ignore what the community wanted (namely, a way to log in without having to sign up for a Yahoo account). This was the opposite of what people had come to expect from Flickr. It was anti-social.\n And it very much delivered a message, to both users and to the team at Flickr: Youre part of Yahoo now.\n That message was also going out to Flickrs team. Flickr prided itself on customer care, which it considered a core part of community building. But Yahoo wanted to manage all that itself with its existing departments. One of Yahoos goals was to move from a system of notice and takedown, to prescreening all the content members posted before it went up online. Flickr saw this as both a costly time-consuming task and one that could very well violate its members privacy, especially when talking about private photos. The Flickr team scheduled a meeting and headed down to corporate headquarters in Sunnyvale for an hour long presentation to make its case. Halfway through the meeting, the vice president who oversaw customer care for Yahoo looked at his watch, announced he had another meeting, and left. It was an open fuck you.\n For Heather Champ, who was Flickrs head of community at the time, the meeting was the beginning of the end. I came out of that meeting knowing I couldnt continue in my role. I didnt want to stay and watch them dismantle everything wed worked so hard to build.\n By mid-2008, a year after the RegID debacle, it was clear to most everyone that Facebook was the big up-and-coming social network. What had been a plaything for college kids and high schoolers was suddenly the network your mom, your dad, your gym coach, and everyone else youd ever met was sending you friend requests from. Microsoft was pumping money into it, and it was fast approaching 100 million users.\n Inside Yahoo, which itself had a massive user base and multiple social products, some were already warning that it was going to be bypassed in social just as it had been bypassed in search.\n I spent years at Yahoo trying to signal the alarm that Facebook was going to take over the adult market unless we stepped in and used our existing social networks to fight back, laments one former Yahoo engineer who worked on products at both the parent company and Flickr. Obviously this never went anywhere for a multitude of reasons.\n Yahoo had already tried to buy Facebook in 2006\u2014for a billion goddamn dollars. And failed. Two years later Facebook was too big to buy. The only way to beat it was to come at it from another direction with a better product. Yahoos best hope for that was Flickr. But by then it was too late.\n Flickr wasnt a startup anymore, explains the engineer, people didnt really want to work that hard to turn the entire product around. Even if they had, Flickr [was] very techie hipster, many didnt use or like Facebook and considered it bland, boring, evil, poorly designed, etc., and were certainly not ready to fast follow it. Emphasis was put more on how things looked, and felt, rather than on metrics and on what worked. The whole experience was very frustrating for me all around, as I slowly watched Flickr and Yahoo fade into irrelevance.\n The Unstoppable Force And His Immobile Object\n Theres a difference between a missed opportunity and a complete fuck-up. When Yahoo failed to capitalize on Flickrs social potential, that was a missed opportunity. But if you want to see where it completely fucked up, where it just butchered Flickr with dull knives and duller wit, turn on your phone and launch the Flickr app. Oh, whats that, you dont have one? Exactly.\n Flickr had a robust mobile Web site way back in 2006\u2014before the iPhone even shipped. You could use it with your piece of crap Symbian phone, or the dinky screen on your Sony Ericsson T68i. But it was basically just a browser. If you wanted to get a photo from your phone to your account, you had to email it.\n And then in 2008, something happened that made the mobile Web a sideshow altogether: apps. The iPhones App Store ushered in a new era that changed the way we interacted. People didnt want mobile web experiences that required them to skip from a camera app, to an editing app, back to the Web and possibly even over to email to upload and share an image. They wanted an app that did all those things. The Flickr team understood that. Unfortunately they couldnt do anything about it.\n Flickr was not empowered to build its own iOS app\u2014or any other mobile app for that matter, laments one former Flickr executive. You had this external team with strong opinions as to what the app should do.\n It was here that the missions of the two companies truly collided, according to insiders. The Flickr app was a top-down decision, driven by Yahoo Mobile and its leader, Marco Boerries. The team at Flickr was iced out.\n \n Boerries had a grandiose vision for something called Connected Life. It was to be a socially seamless mobile experience that brought all your Yahoo services together in the palm of your hand, and connected them with the desktop. It was nothing short of what Apple and Google and Microsoft are all trying to do today with their cloud strategies.\n Boerries was a maniac. Hed built a word processing program called StarWriter as a 16 year-old kid, grew it into the StarOffice suite and sold it to Sun for $74 million in 1999. By 2004, he was running around Silicon Valley giving a demo that was literally making people gasp in wonder.\n He would walk into a room full of investors, pull out his crappy flip phone, and take a picture of the room. Then hed pocket it, open his laptop and refresh the app running on his desktop. Suddenly, the visitors in the room would be confronted with their own skeptical faces. It was automatic. He then explained that he could do the same thing with any other type of data\u2014emails, phone numbers, mp3s, whatever. Anything you did on the phone would be seamlessly reflected on the desktop, and vice versa. Basically, it was iCloud.\n Yahoo bought his company in 2005 for something in the neighborhood of $16 million, largely to buy Boerries. A month later, it would buy Flickr.\n Boerries was a genius, and, by all accounts, a nightmare to work with. One of the most frank depictions of this comes from Kellan Elliot-McCrea, Etsys CTO who, in a past life, was the chief architect of Flickr. On Quora, he writes:\n  Marco Boerries was without a doubt one of the most viciously political, and disliked Yahoo! execs and he reigned for 4 years over the Yahoo Connected Life team which had universal control over all native mobile experiences within Yahoo. Several Flickr internal attempts to build and ship native mobile experiences (going back to 2006) were squashed relentlessly.\n  The Yahoo Mobile team was onerously slow to get an app out the door. Although the iTunes App Store launched in July of 2008, Yahoo Mobile let a year slip away before it released an official Flickr app. When it finally did roll out the long-delayed beast in September of 2009, it was beyond disasterous. The early reviews on the iTunes App Store read like pre-alpha test notes of the worlds worst software.\n  Among other problems, it wouldnt let you upload several photos at once, you had to go in manually submit them one at a time. It was downscaling photos to 450 x 600, murdering image quality. Users had to log in via Safari rather than in the app itself. It was striping EXIF data from photos as they uploaded\u2014precisely the kind of thing Flickrs photo nerds wanted to see.\n People. Fucking. Hated it.\n The app landed like a pile of mud on a wedding gown. As one App Store reviewer put it, For uploading to Flickr, this is really the worst app Ive tried; youre better off just emailing photos direct from the phone in that respect.\n It somehow managed to get Flickrs two key strengths\u2014photo sharing and storage\u2014completely wrong.\n Possibly worst of all\u2014at least from a business perspective\u2014you couldnt sign up for a Flickr account from the app. (In fact, you still cant. It kicks you over to the Web to sign up with Yahoo if you want to register as a new user.) While other apps draw users into their Web services (think Foursquare, Twitter, Facebook, and notably Instagram) the Flickr app that Yahoo Mobile rolled out had no mechanism for that. It was not a recruitment tool. It was just for existing users.\n That was a big oversight, says Fake. Thats an understatement. It was the mother of all fuckups.\n Meanwhile, all manner of new apps were appearing that would not only snap photos for you, but process the images too. Things like Best Camera and Camera Bag were introducing consumers to the idea of applying automatic filters to their mobile photos. A little over a year after the Flickr app hit iTunes, another photography app came along that worked much like a quicker Flickr. It was called Instagram.\n Today, it all seems too late. The iPhone is the most popular camera on Flickr, but the feeling isnt mutual. Flickr isnt even among the top 50 free photography apps in iTunes. Its just below an Instagram clone in 64th place. By way of comparison, an app that adds cats with laser eyes to your photos is 23rd.\n \n If you cant beat laser cat, you probably deserve to die.\n Flickrs mobile and social failures are ultimately both symptoms of the same problem: a big company trying to reinvent itself by gobbling up smaller ones, and then wasting what it has. The story of Flickr is not that dissimilar to the story of Googles buyout of Dodgeball, or Aols purchase of Brizzly. Beloved Internet services with dedicated communities, dashed upon the rocks of unwieldy companies overrun with vice presidents.\n As a result, Flickr today is a very different site than it was five years ago. Its an Internet backwater. Its not socially appealing.\n Recently, Flickr rolled out a Justified view, a way to scan your friends recent photos where they are all placed together like puzzle pieces. Its similar to the way Pinterest lays out images. Its a dramatic, gorgeous way to look at photos\u2014that mostly highlights how rarely many people update now.\n As I scroll down I note that friend after friend has quit posting. At the bottom of the page I am already back in mid 2010. So many of my friends have vanished. It feels like MySpace, circa 2009.\n This is anecdotal, sure, but I follow many of these same people on other networks (Path, Facebook, Instagram) where they tend to be very active. I see photos of the same people, with their same children and their same dogs\u2014all looking a year or two older than on Flickr.\n This justified view also serves to highlight just how many of my friends photos are formatted in perfect squares\u2014the tell-tale sign of an Instagram snap thats been exported. Many of my contacts entire photostreams are made up of Instagram photos. In other words they are mere duplicate streams\u2014with fewer comments and activity\u2014of content that exists in primary form elsewhere. The only reason they are active on Flickr at all is because they automatically export there.\n There are other signals as well. On Stellar.io, a favorites aggregator that tracks what people are linking on Twitter, YouTube, Vimeo and Flickr, the latters links fail to show up even daily in my stream. And of course, there is that damning Quantcast traffic chart.\n Despite years of neglect, Flickrs miniscule yet highly talented team is trying desperately to right the ship.\n Flickr began the year by killing off a slew of features that didnt really make sense\u2014like Photo Sessions, a baffling feature that let you show real time slideshows of your pictures to other people that had Yahoo written all over it. Its also hustling to roll out many more, like that new Justified View and an Uploader that runs on HTML 5. It replaced its photo editor (formerly Google-owned and now defunct Picnik) with and HTML5 tool called Aviary, which lets people make changes to their photos without leaving the page and will play nice with tablets. Its showing pro members photos at 1600 and 2048 pixels now to take advantage of Retina Displays.\n Flickrs product manager Markus Spiering notes that his team gets what it needs from Yahoo now. (Of course, youd also assume he has to say that. But still.)\n We do have a lot of resources which are also within the main company. The people you see on the About page are the core team you see on San Francisco, but a lot of the horizontal development efforts are shared.\n And that hated Yahoo-only login? Gone.\n We dont care so much about what kind of passport you have\u2014a Google ID, Facebook, he says. At the same time, we let you share your images to various places. There are a lot of solid and easy to understand privacy controls, and we see ourselves as the centerpiece.\n Thats where were pushing Flickr towards where its a beautiful, photo centric experience. But whatever you are using, it gets your photos there.\n Mobile is still a disaster. Flickrs iOS app, though improved from the one it rolled out in 2009, is still just awful. It still requires you to log into Yahoo via Safari, for example. And it doesnt offer even the most basic of photo editing or filters that seemingly every other camera app provides.\n I think I can honestly say that especially on iOS we need to provide a better Flickr experience in terms of our own app, but thats something we are working very hard on, says Spiering.\n So lets say Flickr finally gets it together. Lets say it fixes its app, reinvigorates the community, and finally gets back on path. The question is: Is it too late?\n Its under attack not just from Facebook and Instagram and, hell, TwitPic and Imgur (Imgur for fucks sake!) but also the likes of Dropbox, Google Drive, Skydrive, and Box.net. Not to mention Apples iCloud and PhotoStream, Googles Picasa, and yes even Google+, which does automatic photo uploads from Android handsets in glorious full resolution complete with geotags and EXIF data.\n A comeback doesnt seem likely.\n Flickr is still very valuable. It has a massive database of geotagged, Creative Commons- and Getty-licensed, subject-tagged photos. But sadly, Yahoos steady march of incompetence doesnt bode well for making use of these valuable properties. If the Internet really were a series of tubes, Yahoo would be the leaking sewage pipe, covering everything it comes in contact with in watered-down shit.\n Flickrs last best hope is that Yahoo realizes its value and decides to spin it off for a few bucks before both drop down into a final death spiral. But even if that happens, Flickr has a long road ahead of it to relevance. People dont tend to come back to homes theyve already abandoned.\n Flickr is still pretty wonderful. But its lovely in the same way a box of old photos youve stashed under the bed is. Its an archive of nostalgia that you love dearly, on the rare occasion you stumble across it. You pull them out, and hold them up to the light, and remember a time when you were younger, and the Web was a more optimistic place, and it really was almost certainly the best online photo management and sharing application in the world.\n And then you close the box.\n And you click over to Facebook, to see whats new.\n Image: Shutterstock\/Vince Clements","item_date":"May 17 2012 18:04:18","display_item_date":"05-17-2012","url":"http:\/\/gizmodo.com\/5910223\/how-yahoo-killed-flickr-and-lost-the-internet","source":"gizmodo.com"},{"title":"Platform Updates: Operation Developer Love","details":"In the upcoming weeks, we will be updating the Javascript SDK to limit the publicly exposed interface. This is part of an ongoing process to improve the reliability of the SDK. The first step will include removing access to all internal properties and to methods prefixed with _\n  If you are applying patches to the FB object, these will no longer take effect, nor will directly manipulating properties such as FB._https\n. If you are currently relying on accessing internal properties, please refactor this so that you only rely on the publicly available (and officially supported) methods listed at https:\/\/developers.facebook.com\/docs\/reference\/javascript\/.\n  Note that all methods not listed as part of the public API might be subject to change or removal, and you should not use them directly.\n  As announced on the Roadmap and Developer Blog, on June 6, 2012, the following changes will be in effect:\n     Removal of FBML\n   FBML apps will no longer work on Platform. All FBML endpoints will be removed. If your app is still utilizing FBML, please migrate before June 6 to avoid any issues.\n      XMPP Connections must be done over TLS\n   Apps connecting to Facebooks XMPP service will be required to use STARTTLS for all connections. We will start rejecting unencrypted connections.\n   Bugs activity between Tuesday, May 8 and Tuesday, May 15\n  174 bugs were reported\n 27 bugs were reproducible and accepted (after duplicates removed)\n \n Submitting a post request to https:\/\/graph.facebook.com\/<page_id>\/photos returns An unexpected error has occurred. Please retry your request later.\n Open graph action approved but still shows pending\n Facebook Like comment overlay has z-order problems in IE compatibility view\n Recommendation plugin does not display any recommendations\n Application authentication error when uploading picture to ALBUM created using web application\n Used gift notification URLs point to Apps and Games rather than app\n Request Dialogs broken in IE9\n Impossible to publish links on Facebook\n FQL : some type values are not documented on STREAM table\n The user_checkins and friends_checkins is not asked when someone want to install my app.\n Timelines milestones not appearing in graph apis feed or fql results\n Broken link to Post object on stream.publish deprecation note","item_date":"May 17 2012 00:48:16","display_item_date":"05-16-2012","url":"http:\/\/developers.facebook.com\/blog\/post\/2012\/05\/16\/platform-updates--operation-developer-love\/","source":"developers.facebook.com"},{"title":"Facebook schedules one last hackathon before going public (rumor)","details":"Facebook has reportedly scheduled an all-nighter hackathon for Thursday (tomorrow) at its Menlo Park headquarters. It will end with Facebook co-founder and CEO Mark Zuckerberg ringing the Nasdaq bell remotely to signal the company has gone public.\n\n\nThe social networking has set up an internal Facebook Event page for the big day that supposedly already has over 1,000 saying they are going. \u201cWe want to get everyone together and remind ourselves that this company is about building things,\u201d a source told TechCrunch.\n\n\nThis hackathon will probably result in some new product ideas and features, unlike the last one we heard about: Facebook engineers + paint + beer = giant QR code on roof (video). After all, it\u2019s the last one Facebook will have as a private company. In the next few weeks, we\u2019ll probably hear about what Facebook engineers came up with. I admit; I\u2019m excited.\n\n\nEarlier today, Facebook increased its IPO size by 25 percent. The social networking giant is now offering some 421.2 million shares. The news follows yesterday\u2019s IPO price range increase from $28-$35 to $34-$38 per share. This means Facebook could raise anywhere between $14.32 billion and $16.01 billion when it goes public.\n\n\nFacebook is widely expected to start trading on the Nasdaq this week under the \u201cFB\u201d ticker. Many believe this Friday is the big day; shares will be priced on May 17, with trading beginning on May 18. Another rumor says there could be a delay, but as every day passes, that seems less and less likely.","item_date":"May 17 2012 00:47:29","display_item_date":"05-16-2012","url":"http:\/\/www.zdnet.com\/blog\/facebook\/facebook-schedules-one-last-hackathon-before-going-public-rumor\/13229","source":"www.zdnet.com"},{"title":"Iron.io Blog: Iron.io Releases IronCache (Beta)","details":"Iron.io is pleased to announce a soft-launch\/beta release of IronCache, a hosted key\/value data cache. It\u2019s an elastic cloud-based solution built for storing short-term processing results and passing items between asynchronous processes. IronCache is useful for many situations where a database alone isn\u2019t an optimal solution. The service is accessible via a simple HTTP interface and is memcached compatible. It uses OAuth to provide a simple and secure authentication mechanism.\nJoins IronWorker and IronMQ \n IronCache is designed to complement Iron.io\u2019s existing services \u2013 IronWorker and IronMQ. For example, the service provides developers with a simple and secure way to let workers communicate with each other. The genesis for the service was the result of direct requests and a number of use cases from customers. Heavy users of IronWorker were looking for simple ways to maintain state and coordinate asynchronous activities. Essentially, they needed a simple way to hold counters, store transient results, record the state of sequences, and share other types of data and activities between sets of processes.\n Enter IronCache. Instead of using their existing database \u2013 for reasons we\u2019ll outline in another blog post \u2013 or standing up a separate key\/value data store in their cloud stack, users can make use of Iron.io\u2019s hosted cache and eliminate the need to manage infrastructure, deal with redundancy, or address scaling issues. IronCache is designed for use with Iron.io services. It uses the same authentication mechanism, is designed to have low-latency between services, and provides similar monitoring and notification capabilities via the Iron.io dashboard.\nCaches, Data Items, and Atomic Operations \n Developers can create multiple caches, a useful feature to compartmentalize work. Each cache begins with a call to store data. When you tell it to store an item, IronCache behaves intelligently, meaning that if the cache to store data in doesn\u2019t exist, IronCache will create it. If the key doesn\u2019t exist, IronCache will create the item in the cache; if the key exists, IronCache will overwrite the value. The overwriting and creation of items can be configured on a call-by-call basis.\n IronCache supports incrementing as an atomic operation, a critical need for situations where many clients may be modifying items at the same time. For example, developers may wish to count the number of times a piece of code runs. If two clients attempt to first get the value stored in cache, modify it locally, then write it, they may overwrite each other\u2019s modifications. An atomic operation using a data cache ensures this doesn\u2019t happen.\n Data items in the cache are short-lived \u2013 meaning they expire after a specific duration. The default is 7 days and the maximum is 30 days, although each item can be configured with its own expiration or Time-To-Live value.\nGetting Started (with the Beta Release) \n For more information on this soft-launch of\u00a0IronCache, take a look at the product page as well as the section in the Dev Center. Getting started is easy as well, especially if you have an existing Iron.io account. Just use the same auth tokens, create some caches and start storing and retrieving items. Add in some asynchronous processing or communication with an external system and you\u2019re on your way to using the patterns that will be IT mainstays for a long time to come.\nGet Started Now!","item_date":"May 17 2012 00:44:19","display_item_date":"05-16-2012","url":"http:\/\/blog.iron.io\/2012\/05\/ironio-releases-ironcache.html","source":"blog.iron.io"},{"title":"Rethinking Personal Data: Strengthening Trust","details":"Unlocking this potential value requires the establishment of a set of trading rules for this unique asset\u2014guidelines that will balance the interests of all stakeholders.\nThe rules must be complex enough to encompass the extensive and diverse ways in which data can be used and flexible enough to adapt to the new uses that are being invented almost daily.\nThis report outlines the concrete steps that stakeholders can take, including upgrading protection and security, agreeing on rights and responsibilities, and driving accountability and enforcement.\n      \n                                                                                                                Personal data are everywhere. We volunteer data about ourselves on social networks. Payment providers capture our transactions, and governments record our financial details. Mobile phones can track our location at any given moment. Hospitals capture and store our medical tests and clinical diagnoses digitally. And all of this can be used to make inferences about what we might do next.\nWhen used appropriately, personal data can create new economic value by helping to achieve new efficiencies in business; tailor and personalize products; respond quickly to global challenges; and empower individuals to engage in social, commercial, and political activities more effectively. Personal data play a critical role in the development of the Internet economy\u2014which is expected to reach $4.2 trillion in the G-20 countries by 2016\u2014and have been key in the recent valuations of companies that collect and utilize personal data.\nThere is real risk, however, that this value will not be realized. High-profile security-data breaches are commonplace. Individuals are increasingly concerned about intrusions into their privacy and the possibility of data being used for purposes of which they do not approve. Companies are unclear about what they can and cannot do with personal data and are either standing on the sidelines or forging ahead with an unclear understanding of liabilities and the potential for negative impact on their reputations and brands. Governments are proposing various laws and regulations to protect privacy while also aiming to encourage innovation and growth.\nRethinking Personal Data: Strengthening Trust, a new World Economic Forum report, produced in collaboration with The Boston Consulting Group, reveals that unlocking this potential value requires the establishment of rules and safeguards that balance the interests of all stakeholders.\nThe report suggests that personal data are a tradable asset, like water, gold, or oil. And like these assets, they need a set of trading rules to allow for mining, sharing, and utilization. Unlike tangible assets, however, personal data are not consumed when used. Instead, use increases value because new data elements are accumulated, providing greater insights into individuals. This increased insight, coupled with new data mining and \u201cbig data\u201d technologies, often leads to new ways to use and create value.\nAs a consequence, the approach to establishing trading rules\u2014and the rules themselves\u2014have to be different from other asset classes. The rules must be complex enough to encompass the extensive and diverse ways in which data can be used and flexible enough to adapt to the new uses of data that are being invented almost daily. They need to balance the potential value that personal data can unlock with the rights of individuals and societies to determine what are, and are not, legitimate uses of data. And they need adequate safeguards to ensure both compliance with these rules and protection for individuals from the unauthorized access of their data.\nThe report aims to foster a debate around some of the key questions that need to be resolved to unlock this value. Who owns personal data? How can privacy be protected? What is the role of context in setting permissions? How can organizations be held accountable? What is the role of regulators?\nThe report outlines concrete steps that stakeholders can take, focusing on three areas: upgrading protection and security, agreeing on rights and responsibilities for using data based on context, and driving accountability and enforcement. It concludes with a call for leaders to work together to achieve a coordinated yet decentralized approach to this global challenge.\nTo Contact the Authors","item_date":"May 16 2012 21:52:52","display_item_date":"05-16-2012","url":"https:\/\/www.bcgperspectives.com\/content\/articles\/digital_economy_media_entertainment_rethinking_personal_data_strengthening_trust\/","source":"www.bcgperspectives.com"},{"title":"Why Geeks Win | Infochimps Blog","details":"We found this great little chart on Chart Porn today and thought it was an excellent representation of the foundations of our company. \u00a0Yay, geeks!","item_date":"May 16 2012 21:48:38","display_item_date":"05-16-2012","url":"http:\/\/blog.infochimps.com\/2012\/05\/16\/why-geeks-win\/","source":"blog.infochimps.com"},{"title":"The Business of APIs: Elastic Path Interviews API Evangelist Kin Lane       - YouTube","details":"The Business of APIs: Elastic Path Interviews API Evangelist Kin Lane Kin Lane is the co-author of The Business of APIs. An engineer of database driven web a...","item_date":"May 16 2012 21:12:22","display_item_date":"05-16-2012","url":"http:\/\/www.youtube.com\/watch?v=OcIc4_wWJtE","source":"www.youtube.com"},{"title":"Get Even More from Email with our New API Features ","details":"It\u2019s been just over four months since Context.IO 2.0 came out of beta, and since then, we\u2019ve received a ton of great feedback. Over the past 2 to 3 months, we\u2019ve been rolling out enhancements based on that feedback and wanted to give you a quick summary of what\u2019s new. Changes include:\n - For apps that require the most up-to-date mailbox data, we\u2019ve added direct access to IMAP servers for certain calls.\n - The ability to create time-limited direct links to attachments \n -  Creating an account and a source in a single call , saving time and making error handling easier\n - Getting the  raw list of attachments for messages \n -  Pull the bodies, flags and headers for your messages in a single call \n -  Total number of messages and files now exposed when adding a new account.\nHere\u2019s a bit more info about each addition we\u2019ve made:\nNew Folders Resource\nWhile the list of folders is part of the metadata returned for messages, this resource gives you better control of those. The GET method will give you a list of folders a specific message is listed under, POST will let you add or remove folders and PUT allows you to overwrite the list of folders a message is in with a new set.\nThis can be great for productivity apps that are building task lists based on messages in specific folders, and once done, posting the completed task to a new folder. It can also be used to move messages with\u00a0attachments into a specific folder, to sync with a third party storage tool, as in the case of emailtobox.com.\nWhile you can already pull message data with the messages resource, we added a way to pull a list of messages straight from the IMAP server. Before responding, this call checks the IMAP server directly for any new messages and will add them to the response. This way, your list of messages reflects exactly whats on the IMAP server the moment the call is made versus the last time Context.IO synced with it.\nFor any apps that require the most up-to-date list of messages from a specific folder as possible and can tolerate the performance hit caused by connecting to the IMAP server, this call is for you.\nLetting users download individual attachments from your app involved that attachment going through your server before you send it back to the browser. To skip that hop, you can now generate a time-limited link to an attachment and redirect the user to that link.\nUse this call to not only create an account and a source in a single call, but to ease up any error handling: if the source parameters aren\u2019t valid or don\u2019t map to an existing and accessible mailbox, the account isn\u2019t created and the call returns an error code.\nWe automatically filter out files like signature images or winmail.dat files from the files list. If you need the raw list of files and that filtering is causing you more trouble than anything else, you can now turn it off by setting the raw_file_list parameter to 1 when creating sources.\nFind out when your apps\u2019 mailboxes are ready to go! When you add a source, you can now specify a callback URL and we\u2019ll make a POST request to it when the initial sync is complete.\nMessage bodies, flags and headers\u00a0are all available as separate resources that used to require separate API calls each requiring IMAP connections and the related overhead. You can now set any combination of include_body, include_flags and include_headers to 1 as GET parameters to obtain that data in a single call to a message instance.\nUse the optional include_extended_counts paramater to pull the number of unread messages for a specific folder directly from the IMAP server, giving your app the most up to date information.\nWhen listing properties of an account, we now include the total number of messages and files contained within the account, by default. This allows you to do cool things like sort mailboxes by size!\nWe\u2019re always looking for ways to optimize access to rich email data and make it easier for developers to build killer apps on top of it. What do you think about these new calls and API changes? What are some of the ways they\u2019ll improve your app, or do they give you any new ideas for apps you didn\u2019t think were possible before?\nIf you enjoyed this post, please consider leaving a comment or subscribing to the RSS feed to have future articles delivered to your feed reader.","item_date":"May 16 2012 20:51:16","display_item_date":"05-16-2012","url":"http:\/\/blog.context.io\/2012\/05\/new-additions-to-context-io-2-0\/","source":"blog.context.io"},{"title":"Oracle and Google Agree to Copyright Truce (For Now)","details":"Google and Oracle find a way forward on the copyrightability of APIs. Image: o5com\/Flickr\n Oracle and Google have struck a deal in their ongoing Java dispute which could speed the case along, but with one catch: Oracle has now essentially bet the farm on its claim that Google violated its copyrights in cloning its Java application programming interfaces (APIs).\n The two companies have been battling it out in a San Francisco courtroom for weeks now, in a complex case that involves patent and copyright claims along with a novel claim by Oracle that its Java APIs \u2014 essentially the technical guidelines that allow two pieces of software to talk to each other \u2014 are copyrightable. Oracle has other copyright claims where it says Google flat-out copied the Java code.\n Yet the jury threw a monkey wrench into the trial when it returned a partial verdict on Oracle\u2019s API claims. It said Google had violated Oracle\u2019s copyright, but failed to answer the question of \u201cfair use,\u201d which would have let Google off the hook. It did say that Google had copied nine lines of from the Java platform code, but this would likely result in a small amount of damages relative to what Oracle could get for infringement of the 37 APIs. Judge William Alsup later ruled that Google also infringed in used eight decompiled Java files, but this will also mean only a small amount of damages \u2014 at best.\n Regardless, Oracle said it would proceed to a damages phase where it would go after \u201cinfringer\u2019s profits,\u201d call high-ranking Google executives and lay out Android revenues, something Google has kept largely secret. \n \n Oracle\u2019s lead counsel, David Boies, argued that his client was entitled to \u201csome portion of Android revenue\u201d as compensation for the infringement, despite considerable disdain from Judge Alsup. \u201cThere\u2019s no way you could make that argument,\u201d he said to Boies Tuesday afternoon. \u201cYou\u2019re one of the best lawyers in America \u2014 how can you make that argument?\u201d \n Boies eventually suggested that Oracle would concede those damages if Judge Alsup ruled against the APIs\u2019 copyrightability, and on Wednesday morning, both sides signed an agreement reflecting as much. Oracle will now only pursue full damages if Judge Alsup rules that the 37 Java APIs in question were copyrightable at all.\n But this is not be the final chapter in what has been a rollercoaster of a case. If Alsup rules the APIs are in fact copyrightable, Oracle will get a new jury and argue for full damages. If the 37 APIs are ruled not copyrightable, Oracle agreed it will just take \u201cstatutory damages\u201d \u2014 a standard amount \u2014 that doesn\u2019t exceed $150,000 per claim. Oracle has clearly indicated that it will appeal such a decision as well. \n Hanging over either scenario is Google\u2019s motion for a mistrial based on the incomplete verdict, which would mean the entire copyright portion would start from scratch. Google filed for a mistrial, citing the Seventh Amendment and previous case law, since the jury hadn\u2019t answered all questions only moments after the jury returned the verdict, but Judge Alsup has yet to rule.\n Both the copyright and patent proceedings have contained advanced details of Java code and underpinnings. To the surprise of the court, after he admonished Boies, Alsup then disclosed that while he had coding experience in other languages, he had learned some Java programming to be better educated for the trial. \n The jury is now deliberating on Oracle\u2019s accusations of patent infringement. If it finds Google has infringed on any of Oracle\u2019s claims, the trial will move on to a damages phase. However, the sides could agree to simply let Judge Alsup award damages since they are a fraction by comparison to the damages Oracle had sought over copyright infringement.\n Update: This story has been updated to clarify a few key points of the case.\n \nGot a secret? Email caleb_garling [a] wired.com. Caleb covers tech, but loves other stuff like sports, fiction, beer, fun in remote places and music featuring guitars. Encircle on Google+, subscribe on Facebook or\nRead more by Caleb Garling \nFollow @calebgarling on Twitter.","item_date":"May 16 2012 20:49:40","display_item_date":"05-16-2012","url":"http:\/\/www.wired.com\/wiredenterprise\/2012\/05\/oracle-google-truce\/","source":"www.wired.com"},{"title":"App Center Best Practices ","details":"Last week, we announced the App Center, a new channel to grow canvas, mobile, and web apps that integrate Facebook.  We encourage all developers to submit their app detail page this week. Having an app detail page is required for being listed in the App Center, and it will also become the new destination when non-users search for your app.\n  We are already reviewing app detail pages for apps that have high enough quality signals to be listed in the App Center.  We are impressed with the quality and number of submissions to date.  Checkout the impressive app detail pages created by Zuma Blitz, Soundcloud, and Pulse.  \n  We published the App Center guidelines to help you understand what we expect of apps that are listed.  In addition, the below checklist highlights some of the most important things to consider when you\u2019re creating your app detail page:\n  1.) Websites must provide a personalized experience.\n When people click through from the App Center, they are logging into your website.  As a result, do not present these users with \u201cLogin with Facebook\u201d buttons.  \n  All websites and mobile web apps need to immediately present authenticated users with a logged-in experience.  Specifically, we\u2019re looking for personalization, such as name and profile picture, so users know that their Facebook information is being used to create an account.\n  One of the easiest ways to detect user status is to use the JavaScript SDK and the FB.getLoginStatus call on your landing pages.  Learn more.\n  2.) Review your app settings.\n App detail pages are dynamic based on the integrations you\u2019ve specified on the basic settings page of the Developer App.  Be sure to review this and remove any old or non-functional integrations.  If you dont use Facebook Login on your website, please dont list it.\n  For example, if you\u2019re building only an iOS app you shouldn\u2019t have anything in the Site URL, Mobile Web URL or Canvas URL fields.  \n  3.) Provide high-quality images.\n Your images should match the quality of your app.  As specified in the guidelines, images shouldn\u2019t have buttons, excessive text, borders, dropshadows, URLs, promotions, pricing, or third-party logos.  \n  Icons and promotional banners cannot have a white background, rounded edges or borders.  If your logo has rounded corners, please fill the white space in the corners with a color.  Also, all promotional banners must include the name of your app.\n  Screenshots should be screenshots of the actual experience (no additional graphics overlaid, no distorted angles).  Feel free to use concept art in your cover image and icons, but please keep it out of screenshots.  \n  4.) Input accurate names & thoughtful descriptions.\n Don\u2019t add any additional keywords to the name of your app.  For example, if your app is named \u201cBilliards\u201d, don\u2019t submit your name as \u201cBilliards \u2013 Pool\u201d. \n  Proofread your app description for spelling and grammar and remove unnecessary symbols, like stars, hearts and multiple exclamation points.\n  5.) Don\u2019t forget to save and submit.\n We understand that it may take some time to create a great app detail page, so we provide the ability to save your work before submitting.  Once you\u2019re done, don\u2019t forget to click the \u201csubmit\u201d button at the top of the page.  \n  In the coming weeks, we will be providing additional information about how the localization and translation process will work for app detail pages. \n  As a reminder, all app detail pages submitted by May 18th will be given prioritized review for the user launch of the App Center.  If you haven\u2019t already, create your app detail page today.","item_date":"May 16 2012 20:31:46","display_item_date":"05-16-2012","url":"http:\/\/developers.facebook.com\/blog\/post\/2012\/05\/16\/app-center-best-practices\/","source":"developers.facebook.com"},{"title":"TMS and Digitalsmiths Partner For New APIs","details":"Tribune Media Services (TMS), which is a major provider of entertainment metadata, and Digitalsmiths, a provider of video content discovery solutions, have released a new of application programming interfaces (APIs) that the two companies believe will make it easier for developer to create new digital content discovery and viewing experiences.\nThe new APIs are built on Digitalsmiths Seamless Discovery platform that handles more than one billion API transactions per month and TMS massive collection of metadata on TV shows, movies and celebrities.\nThe new APIs will allow developers to access TMS metadata on entertainment content through an intuitive application-programming interface (API) that significantly reduces the engineering and training requirements for developers.\nThe first release of these APIs will focus on such areas as searching for celebrities on TV or discovering new episodes of favorite shows, but the two companies expect to soon begin offering additional product updates.\nThese new solutions provide a seamless combination of rich entertainment metadata and intelligent delivery technologies to drive the accelerated development of enhanced discovery experiences, noted Ben Weinberger, CEO of Digitalsmiths, in a statement.\nUnder the expanded relationship, TMS will also have the ability to market Digitalsmiths Seamless Discovery platform, which offers a set of content-discovery technologies.    \t\t  \t  \t\t  \t\t  \t\t\t\t\t\t\t\t\t  \t\t\t      \t  \t\t\n  \t\t  \t\t\t\t\t\t\t\t\t\n                      Created from the merger between Anystream and Voxant Media, Grab Networks offers a comprehensive video operating system and syndication network for profitably publishing video anywhere on the Internet. The system automatically manages, transcodes and tags video assets- turning cl..more","item_date":"May 16 2012 19:24:26","display_item_date":"05-16-2012","url":"http:\/\/www.broadcastingcable.com\/article\/484609-TMS_and_Digitalsmiths_Partner_For_New_APIs.php","source":"www.broadcastingcable.com"},{"title":"The Business of APIs: Interview with an API Evangelist","details":"Kin is an engineer who has been building database driven web apps for 12 years, and working with data for over 20. Watching API usage really rise in the last few years and seeing it drive innovation in social, cloud computing and more, and so much talk on the technical side of the world, Kin began to explore the business side of APIs. What does it take to deploy, what business elements do you need to support an API? Not just what resources from what department, but what are the building blocks you need to support developers?\nAs an API Evangelist, Kin follows a broad spectrum of API players including Twitter, Facebook, Google, eBay and Amazon to figure out what the best practices are in order to educate business leaders and developers. He currently works on behalf of a local online advertising platform CityGrid to market its API to developers to bring in more, and cultivate the ones they already have.\nInterview recap\nWhat are the big mistakes businesses make when serving developers?\nAPI registration. Developer approval can slow down the process\nPoor documentation. If you can reduce onboarding time from 8 hours to 30 minutes and make it really easy to get going with great documentation, your developer\u2019s chances of success increases\nNo quality code samples. Provide samples in as many programming languages as possible\nNo clear value proposition. A developer shouldn\u2019t have to think too hard about why he or she should get hackin\u2019 on your API. Offer real value\nWhat\u2019s the best way to gather feedback from developers to provide input into your roadmap?\nWithin your API ecosystem, a forum where devs can post comments is key, as well as some sort of email address or ticketing system. But you can\u2019t expect developers to only dialogue on your domain. Have a presence on StackExchange, Twitter, LinkedIn, Stumbleupon, Reddit, HackerNews, etc, where developers already are. Gather feedback within and outside your ecosystem, and evangelize it internally to evolve the roadmap. (That\u2019s a truly \u201copen\u201d API).\nIf you\u2019re a developer, in terms of ecommerce, how important is social?\nAs a business that is using other people\u2019s APIs, what are some of the ways can you protect yourself from that API not being around tomorrow?\nThere\u2019s a lot of complaints around APIs that they can disappear at any moment, they don\u2019t care about developers, or they\u2019re unstable, and that\u2019s true in some ways, but it\u2019s true with any business vendor. The corner sandwich shop should have multiple vendors, if one doesn\u2019t deliver one day, there should be a plan B, C or D.\nBut that doesn\u2019t always apply, especially when it comes to Twitter and Facebook. Where else do you go to get Tweets? It\u2019s difficult to have a plan B, but you should have some form of cache so that when Twitter breaks (which it often does) your app can still function. Have some kind of failover within your code. When applicable, have multiple APIs. For example when Google stopped supporting their Translation API, and then brought it back as a paid product, there are a number of other you could substitute to keep your app working.\nSocial is hot, but where is there opportunity outside social to build great new experiences in the ecommerce world?\nTwitter Firehose and Facebook Timeline are obvious, second to that are some of the pioneers in the space like Flickr. Instagram built a very cool app with social hooks to the other networks, and these hooks made it a viral app. Another one is the video space, we haven\u2019t seen it all yet. Youtube doesn\u2019t embody it all, there\u2019s room for innovation from some of the quiet players out there. E.g. mashups, splicing in products into television, movies, video games, layering in content to sell things to users, then beyond that console or online gaming. APIs allow splicing of products and services to embed commerce.\nKin and his partner have authored The Business of APIs, available on Amazon. Check it out.","item_date":"May 16 2012 19:18:51","display_item_date":"05-16-2012","url":"http:\/\/www.getelastic.com\/the-business-of-apis-interview-with-an-api-evangelist\/","source":"www.getelastic.com"},{"title":"The 1709 Blog: API battle for Google","details":"In 1709 the Statute of Anne created the first purpose-built copyright law.  This blog, founded just 303 short and unextended years later, is dedicated to all things copyright, warts and all. To contact the 1709 Blog, email Jeremy here\n \n  The Electronic Frontiers Foundation have reported on the legal debates surrounding the action brought by Oracle against Google for the use of Oracles APIs, whether copyright can subsist in those APIs and if it did, could Google claim fair use.\n Before we go any further - a brief definition of APIs An application programming interface (API) is a specification intended to be used as an interface by software components to communicate with each other. An API may include specifications for routines, data, structures, object classes. In 2010, Oracle bought Sun Microsystems, which developed Java. When it implemented the Android OS, Google wrote its own version of Java -but in order to allow developers to write their own programs for Android, Google relied on Java\u2019s APIs.\n This is what the EFF have to say:\n Here\u2019s the problem: Treating APIs as copyrightable would have a profound negative impact on interoperability, and, therefore, innovation. APIs are ubiquitous and fundamental to all kinds of program development. It is safe to say that all software developers use APIs to make their software work with other software. For example, the developers of an application like Firefox use APIs to make their application work with various OSes by asking the OS to do things like make network connections, open files, and display windows on the screen. Allowing a party to assert control over APIs means that a party can determine who can make compatible and interoperable software, an idea that is anathema to those who create the software we rely on everyday. Put clearly, the developer of a platform should not be able to control add-on software development for that platform.\u00a0  Take, for example, a free and open source project like Samba, which runs the shared folders and network drives in millions of organizations. If Samba could be held to have infringed the Microsoft\u2019s copyright in its SMB protocol and API, with which it inter-operates, it could find itself on the hook for astronomical damages or facing an injunction requiring that it stop providing its API and related services, leaving users to fend for themselves.\n Another example is the AOL instant messaging program, which used a proprietary API. AOL tried to prevent people from making alternative IM programs that could speak to AOLs users. Despite that, others successfully built their own implementations of the API from the clients side. If copyright had given AOL a weapon to prevent interoperability by its competitors, the outcome for the public would have been unfortunate.\n Setting aside the practical consequences, there\u2019s a perfectly good legal reason not to treat APIs as copyrightable material: they are purely functional. The law is already clear that copyright cannot cover programming languages, which are merely mediums for creation (instead, copyright may potentially cover what one creatively writes in that language). \u00a0Indeed, the European Court of Justice came to just that decision\u00a0last week (Ironically enough, when Sun Microsystems was an independent company, one of its lawyers wrote amicus briefs arguing that interoperability concerns should limit copyright protection for computer programs.)\u00a0  Improvidently granting copyright protection to functional APIs would allow companies to dangerously hold up important interoperability functionality that developers and users rely on everyday.\u00a0 Let\u2019s hope the judge agrees. https:\/\/www.eff.org\/deeplinks\/2012\/05\/oracle-v-google-and-dangerous-implications-treating-apis-copyrightable \nSAS Institute Inc. \u00a0v World Programming Limited\u00a0: CJEU: to accept that the functionality of a computer program can be protected by copyright would amount to making it possible to monopolise ideas, to the detriment of technological progress and industrial development.","item_date":"May 16 2012 19:16:35","display_item_date":"05-16-2012","url":"http:\/\/the1709blog.blogspot.com\/2012\/05\/api-battle-for-google.html","source":"the1709blog.blogspot.com"},{"title":"Innovator Spotlight: Innotas","details":"Innotas is a leading provider of cloud solutions for IT Management. The company helps IT managers align their resources, budgets, and time with strategic business objectives through the Innotas Project Portfolio Management and Application Portfolio Management solutions, both delivered as software-as-a-service (SaaS).\u00a0 Since it was founded in 2006, Innotas has been committed to helping CIOs and IT management gain visibility across initiatives and sustain existing operations.\u00a0 \n Tim Madewell, Innotas senior vice president of services, spoke with Apigee about the Innotas API and how they use Apigee to gain critical visibility and make better business decisions.\n \nWhat is Innotas API strategy?\n We offer an API that is different than many APIs today. Instead of using an API to expose our back-end data or Web services, the Innotas API was built to enable our customers to easily integrate the Innotas IT governance solutions with their back-end CRM, HR and billing systems. Providing an API where customers can \u2018come and get it\u2019 has worked especially well for our large customers, who often have the resources to take the API and run with it to make the integrations happen. We also now offer a turnkey platform in which we build and maintain the integrations for our customers.\n What opportunity were you addressing with this API?\n We had a compelling event \u2013 a customer needed it. In the early days of our company, our average customer size was 25-30 users. But as we started to grow and the SaaS market became more mature, we started working with larger enterprise customers that required a level of integration with their back-end CRM, HR, and billing systems.\u00a0\n So we published our API to meet the needs of a large division of a F1000 customer. In hindsight, offering an API for integration was a great decision early on - it gave our customers a lot more flexibility, including writing the process orchestration around our API to meet the specific requirements of their organization.\u00a0\n How has the Innotas API evolved?\n We offered a SOAP API as the first version, but now we\u2019re evolving. In August of 2011, we launched our integration platform where we will build, deploy and manage the integrations for our customers.\u00a0 Our new platform will be powered by REST APIs.\n How do you work with Apigee?\n Apigee is our API cockpit, where we can view and manage details about our APIs, and use this information to make informed business decisions. We started working with Apigee Enterprise shortly after we published our API to give us visibility and control over our API usage and provide a consistent level of service to thousands of clients on a single SaaS platform.\n Initially, we were primarily using Apigee Gateway as a solution to help us understand who is making API calls, what entity they are hitting, how often, etc. This detailed visibility into our API usage helps us quickly triage and troubleshoot as required. Over time, we extended our use of Apigee to include load balancing and analytics.\n How do you Use Apigee Analytics?\n Apigees real-time analytics is my executive dashboard for operations.\u00a0 I use the analytics all the time see whats happening \u2013 its the first place I go to see integration traffic, even before I look at my real-time monitors. Just yesterday an outage triggered, and with Apigees analytics, I could identify the root cause in about a minute.\n One of our key requirements has always been visibility\u2026you can\u2019t govern what you can\u2019t see. Before we started using Apigees analytics, all of our customers activity was intermingled in the system logs with all the other system-generated events. Figuring out who is doing what was almost impossible and it was difficult to track back the events that were related off of just a user ID \u2013 which is the wrong key. We needed the visibility to be able to relate calls together and construct what happened with meaningful data.\n Once we had this, our second evolution was around maintenance and support. Before, when a customer called and said \u201cit\u2019s not working right\u201d all we could say was \u201cgo look at your code.\u201d But with API analytics we can capture the payload \u2013 trap the data for the customer \u2013 and use our API debugging tools to help them ID the problem message.\n We also use API trend reporting at a macro level over time - common calls, common integration. This gets fed into product management, sales, and performance tuning to make the product better.\n What benefits have you gained from Apigee?\n Apigee is our eyes and ears for our APIs.\u00a0 The visibility we get into customer usage has definitely enabled us to better manage costs and efficiencies. The daily Apigee data help us understand customer stickiness and retention \u2013 which has helped us maintain a consistent 90 percent renewal rate.\u00a0 And understanding customer usage trends and patterns has absolutely helped us make more informed business decisions.","item_date":"May 16 2012 17:15:12","display_item_date":"05-16-2012","url":"http:\/\/blog.apigee.com\/detail\/innovator_spotlight_innotas\/","source":"blog.apigee.com"},{"title":"AllFacebook Stats Releases API","details":"David Cohen on May 16, 2012 11:00 AM\t \t AllFacebook Stats announced the release of an application-programming interface that will offer its users access to data on hundreds of thousands of Facebook pages.\n The data also include analysis and benchmarking of competitors, and AllFacebook Stats said they go as far back as 2010, making them ideal for use in comprehensive analyses, industry reports, and benchmark studies.\n The API will allow users to integrate the data with company and agency dashboards and software solutions, or with other data by merging them into a shared database.\n AllFacebook Stats said it plans to integrate the API into its business editions, and individual solutions are possible, as well.\n The API is targeted toward software vendors, monitoring providers, agencies, and enterprise customers, adding that reasons why it published the API include:\n It was not able to follow up on every individual request, so it chose to release its data pool instead.\n Facilitating content aggregations, as well as individual calculations (economic, scientific).\n To open up cooperation with other analytics and business intelligence tool providers and help them bring additional value to their customers.\n Some of its customers have already asked for an API, including a global player in the telco business.","item_date":"May 16 2012 17:09:39","display_item_date":"05-16-2012","url":"http:\/\/allfacebook.com\/allfacebook-stats-api_b89021","source":"allfacebook.com"},{"title":"Holy moly: Netflix clocks 42B API requests per month","details":"Updated. Netflix clocked close to 42 billion API requests per day in January, according to the company\u2019s director of engineering Daniel Jacobson, who revealed the number as part of a presentation he recently gave to the Paypal engineering team. In his slides, Jacobson pointed out that the number of API requests has grown 70-fold in just two years, from 600 million in January of 2010 to 41.7 billion in January of 2012.\n\n\n\n\n\nProviding an API that is resilient enough to handle these kinds of demands is not a small feat, and Jacobson\u2019s slides reveal some of the principles behind the architecture used for the task. However, there\u2019s an even bigger story to this: The sole reason that Netflix\u2019s API has gotten so popular in the last two years is that the company\u2019s service is seeing huge amounts of traffic from connected devices.\n\n\nNetflix originally built its API with third-party developers in mind, which used it to build websites and apps to manage one\u2019s DVD queue and similar things. However, the Netflix engineering team eventually realized that it could use the very same API to bring Netflix to devices like the Roku, the PS3 and the Xbox. To see how much of an impact that decision had, check out where API requests came from in 2008:\n\n\n\n\n\nCompare that with API requests in 2011:\n\n\n\n\n\nGuess which segment Netflix is focusing on these days? Exactly:\n\n\n\n\n\nCheck out Jacobson\u2019s entire presentation below:","item_date":"May 16 2012 06:07:14","display_item_date":"05-15-2012","url":"http:\/\/gigaom.com\/video\/netflix-42-billion-api-requests\/","source":"gigaom.com"},{"title":"Manager, Technical Operations Project - BBY Open (API Web Services) at Best Buy in Minneapolis, MN","details":"Best Buy provides technology infrastructure enabling partners and developers to access and interact with Best Buys commerce systems through our BBYOpen API. Best Buy has developed a diverse set of API\/Web Services capability and is interested in extending the information and APIs available to developers through BBYOpen (http:\/\/www.bbyopen.com).\nWe are looking for an experienced Technical Operations Associate Project Manager to be part of the BBYOpen platform team. This is a Startup entrepreneurial role within Best Buy - Opportunity to get in on the ground floor. Lead cross-functional projects that deliver operational excellence to our cutting edge BBYOpen API platform. Dig into business requirements, drilling into the architecture and design with development teams. Excited to drive high-visibility, strategic, revenue-generating projects as well as leveraging existing technologies to lead Operational process improvement initiative with enterprise teams.\nThe Technical Operations Associate Project Manager will act as a liaison between the business user group and technical systems group, architecture, design, development and infrastructure, testing and project management. The ideal candidate has experience with API technologies and operating processes, strong Order Management and Retail experience along with application support\/maintenance experience, and application performance (capacity and response time) disciplines. The candidate should also be skilled at building relationships, implementing efficient operational processes, and navigating Enterprise environment s in support of business goals and objectives.\n- Collaborate with Leadership to develop functional requirements, identify and coordinate with cross-functional enterprise teams in defining systems, project\/priorities, scope, approach, timing deliverables and funding.\n- Consult with management to review project proposals, alignment with Organizations business and technical goals, estimate funding requirements, procedures for accomplishing projects, success criteria\n- Project manages initiatives from incubation through implementation and ensuring key execution of requirements and successful execution of projects\n- Accountable delivering projects on time, within scope and quality expectations and within budget.\n- Establish and maintain communication timelines and project plans with Business sponsors and stakeholders (both internal and external)\n- Obtain required sign-offs, execute and coordinate project User-acceptance testing prior to launch and ensure successful completion of projects\n- Act as the primary point of contact and Subject matter expert for BBYOpen business and provide direction to other cross-functional and enterprise teams for project execution\n- Responsible for risk assessment of projects and systems from the business standpoint and provide contingency and mitigation plans as appropriate.\n- Work with release management teams to identify impact to customers and communicate those impacts on a timely basis to the customer base.\n- Analyze process improvement opportunities for the department to enhance and improve on internal and external communications\nBasic Qualifications:\n- Bachelors Degree in Computer Science, Business or Related field\n- 3 or more years experience in business project management and supply chain management\n- 2 or more years experience in retail systems\/e-Commerce\nPreferred Qualifications:\n- 3 or more years experience in technical project management, application support, API Technologies\n- Agile \/ Scrum Project Management methodology\n- Project Management Professional Certification\nJob:  Bestbuy.com\nPrimary Location: United States-MN-Minneapolis","item_date":"May 16 2012 05:58:37","display_item_date":"05-15-2012","url":"http:\/\/www.linkedin.com\/jobs?viewJob=&jobId=3023241","source":"www.linkedin.com"},{"title":"Home - census.ire.org","details":"Can vary in size but averages 4,000 people. Designed to remain relatively stable across decades to allow statistical comparisons. Boundaries defined by local officials using Census Bureau rules.                       \nPlaces\n                       1. What most people call cities or towns. A locality incorporated under state law that acts as a local government.\n 2. An unincorporated area that is well-known locally. Defined by state officals under Census Bureau rules and called a census designated place. CDP is added to the end of name.                       \nCounties (parishes in LA)\n                       The primary subdivisions of states. To cover the full country, this includes Virginias cities and Baltimore, St. Louis and Carson City, Nev., which sit outside counties; the District of Columbia; and the boroughs, census areas and related areas in Alaska.                       \nCounty Subdivisions\n                       There are 2 basic kinds:  1. In 29 states, they have at least some governmental powers and are called minor civil divisions (MCDs). Their names may include variations on township, borough, district, precinct, etc. In 12 of those 29 states, they operate as full-purpose local governments: CT, MA, ME, MI, MN, NH, NJ, NY, PA, RI, VT, WI.  2. In states where there are no MCDs, county subdivisions are primarily statistical entities known as census county divisions. Their names end in CCD.                   \n  Download Bulk Data\n Census Data via Javascript\n     Investigative Reporters and Editors is pleased to announce the next phase in our ongoing Census project, designed to provide journalists with a simpler way to access 2010 Census data so they can spend less time importing and managing the data and more time exploring and reporting the data. The project is the result of work by journalists from the The Chicago Tribune, The New York Times, USA Today, CNN, the Spokesman-Review (Spokane, Wash.) and the University of Nebraska-Lincoln, funded through generous support from the Donald W. Reynolds Journalism Institute at the Missouri School of Journalism.\n      Thanks to the following journalists for their time and expertise: Brian Boyer, Joe Germuska and Chris Groskopf of The Chicago Tribune; Jeremy Ashkenas and Aron Pilhofer of The New York Times; Paul Overberg of USA Today; Curt Merrill of CNN; Matt Waite of the University of Nebraska-Lincoln; and Mike Tigas of the Spokesman-Review.","item_date":"May 16 2012 00:09:58","display_item_date":"05-15-2012","url":"http:\/\/census.ire.org\/","source":"census.ire.org"},{"title":"Timeline Javascript","details":"","item_date":"May 15 2012 23:32:11","display_item_date":"05-15-2012","url":"http:\/\/builtbybalance.com\/github-timeline\/","source":"builtbybalance.com"},{"title":"Why REST Keeps Me Up At Night","details":"This guest post comes from Daniel Jacobson (@daniel_jacobson), director of engineering for the Netflix API. Prior to Netflix, Daniel ran application development for NPR where he created the  NPR API, among other things.  He is also the co-author of APIs: A Strategy Guide and a frequent contributor to ProgrammableWeb and the Netflix Tech Blog.\n\n\nWith respect to Web APIs, the industry has clearly and emphatically landed on REST as the standard way to implement these services.  And for good reason\u2026 REST, which is generally implemented as a one-size-fits-all solution, is an excellent choice for a most companies who wish to expose their content to third parties, mobile app developers, partners, internal teams, etc.  There are many tomes about what REST is and how best to implement it, so I won\u2019t go into detail here.  But if I were to sum up the value proposition to these companies of the traditional REST solution, I would describe it as:\n\n\nIn this model, everyone knows how to behave and it can be incredibly powerful.  The API providers establish a set of rules and the API consumers must adhere to those rules to get what they want from the API.  It is perfect, right?  In many cases, the answer is obviously yes.  But in other cases, as our world scales and the number of ways for people to consume digital content and services continues to expand, this one-size-fits-all model is likely to fall short. \n\n\nThe potential shortcomings surface because this model assumes that a key goal of these APIs is to serve a large number of known and unknown developers.  The more I talk to people about APIs, however, the clearer it is that public APIs are waning in popularity and business opportunity and that the internal use case is the wave of the future.  There are books, articles and case studies cropping up almost daily supporting this view.  And while my company, Netflix, may be an outlier because of the scale in which we operate, I believe that we are an interesting model of how things are evolving.  \n\n\nNetflix is currently available on over 800 different device types, including game consoles, mobile phones, TVs, Blu-ray players, tablets, computers, and almost any other device that can stream video.  Our API alone handles more than two billion incoming requests on peak days, which translates into almost ten billion real-time outgoing requests from the API to internal dependency services.  These numbers are up by about 70x from just two years ago.  Most companies do not have that kind of scale, but it is clear that with the continued growth of the device market more companies are resetting their strategies to be less about the public API and more about internal consumption of their own APIs to support device proliferation. When this transition occurs, the API is no longer targeting \u201ca large number of known and unknown developers.\u201d  Rather, the key audience is a small number of known developers.  \n\n\nThe potential conflict between the internal and public use cases is in the design of the API itself.  Keep in mind that the design implications will not be problematic in many scenarios.  It becomes a potential problem if the breadth of devices becomes so wide that the variability of features across them becomes substantially harder to manage.  It is the breadth of devices that creates a problem for the one-size-fits-all API solutions.  \n\n\nIf your target is a small group of teams with whom you have close relationships, the dynamics around the API change.  For Netflix, we persisted on the one-size-fits-all REST model for quite a while as more and more devices got added on top of the API.  But given our scale, one thing has become increasingly obvious.  Our REST API, while very capable of handling the requests from our devices in a generic way, is optimized for none of them.  This is the case because our REST API focuses on resources that are meant to be granular representations of the data, from the perspective of the data.  The granularity is exactly what allows the API to support a large number of known and unknown developers.  Because it sets the rules for how to interface with the data, it also forces all of the developers to adhere to those rules.  That means that each device potentially has to work a little harder (or sometimes a lot harder) to get the data needed to create great user experiences because devices are different from each other.  \n\n\nThe differences across these devices can be varied and sometimes significant.  Here are some examples of variances across devices that may be challenging for one-size-fits-all models:\n\n\nDifferent devices may have different memory capacity\n Some devices may require a unique or proprietary format or delivery method\n Some devices may perform better with a flatter or more hierarchical document model\n Different devices have different screen real estate sizes which may impact which data elements are needed\n Some devices may perform better having bits streamed across HTTP rather than delivered as a complete document\n Different devices allow for different user interaction models, which could influence the metadata fields, delivery method, interaction model, etc.\n \n\n\nJust think about the differences between an iPhone and your TV and how they beg for different user experiences.  Moreover, the XBox and the Wii, both of which project to the TV, are different in the way users interact with them as well as in the hardware constraints, both of which may require different APIs to support them.  When considering more than 800 different device types, the variance across them becomes overwhelming.  And as more manufacturers continue to innovate on these devices, the variance may only broaden.\n\n\nBecause of the differences in these devices, Netflix UI teams would often have to do a range of things to get around our REST API to better serve the users of the device.  Sometimes, the API team would be required to extend the base service to handle special cases, often resulting in spaghetti code or undocumented features.  And because different teams have different needs, in the REST API world, we would often need to delay feature development for some due to the challenges around prioritization.  In addition to these kinds of issues, significant performance and\/or architectural problems are bound to emerge.  For example, these more granular APIs often result in chattier interactions between device and server or chunkier payloads, as I discussed in a previous post on the Netflix Tech Blog.\n\n\nTo solve this issue, it is becoming increasingly common for companies (including Netflix) to think about the interaction model in a different way.  Rather than having the API create a set of rather rigid rules and forcing the various devices to follow them, companies are now thinking about ways to let the UI have more control in dictating what is needed from a service in support of their needs.  Some are creating custom REST-based APIs to support a specific device or category of devices.  Others are thinking about greater granularity in REST resources with more batching of calls.  Some are creating orchestration layers, such as ql.io, in their API system to customize the interaction.  These are all smart and practical ways around the problem.  But with the growing number of devices, the increasing urge for companies to be on as many of them as possible, and the desire for continued innovation across these devices, these various solutions are still somewhat restricted.  They are still forcing the developers to adhere to server-side rules and non-optimized payloads in an effort to have a one-size-fits-all solution.  These approaches are closer to the flexibility needed in that they are not as rigid as the typical REST-based solution, but when supporting as many devices as Netflix does, we believe they fall short for us. \n\n\nFor Netflix, our goal is to take advantage of the differences of these devices rather than treating them generically.  As a result, we are handing over the creation of the rules to the developers who consume the API rather than forcing them to adhere to a generic set of rules applied by the API team.  In other words, we have created a platform for API development.  In my next post, I will discuss in more detail our implementation of this approach.  In the meantime, if you are interested in helping us solve these and other problems, we are hiring!","item_date":"May 15 2012 19:49:15","display_item_date":"05-15-2012","url":"http:\/\/blog.programmableweb.com\/2012\/05\/15\/why-rest-keeps-me-up-at-night\/","source":"blog.programmableweb.com"},{"title":"SmartBear Launches the First Integrated Solution Bridging the DevOps Divide","details":"BEVERLY, Mass., May 15, 2012 (BUSINESS WIRE) -- SmartBear        Software today introduced API        Complete, a first-of-its-kind solution that enables software        developers, testers and IT operations staff to test and monitor the        quality of APIs (Application Programming Interfaces) and Web Services in        an integrated and streamlined fashion. API Complete helps organizations        improve the quality of the increasing number of APIs and Web Services        used in Web applications and sites, and can replace the fragmented        approach currently used by separate development and operations teams.                                     \n                                          According to Eric Knipp, Managing VP, Gartner Inc., New applications        will increasingly be constructed using agile practices and DevOps --        joint initiatives between development and operations to streamline the        rapid, continuous improvement of applications. Furthermore, an increased        emphasis on analytics will enable more-focused investment in the areas        of applications that really matter to improve user experience,        productivity and, ultimately, profitability. Our recommendation is to        treat a public Web API as a key component of your Web strategy, not as a        bolt-on to an existing project and manage the API with the same care you        would your enterprise Web presence. (predicts 2012:Application        Development)(predicts 2012:December 2)(predicts 2012:2011) [Tweet        this]                                     \n                                          Ian McLeod, Executive Vice President & Chief Product Officer, SmartBear,        continued, The speed and availability of Web Services are a crucial        component of the larger end-user experience delivered by Web        applications but, until today, development and operations teams had to        use disparate toolsets for testing and monitoring. Now, API Complete        provides a unified DevOps        approach to quality for cloud or mobile applications.                                     \n                                          API Complete combines soapUI, the worlds most popular API        testing tool, loadUI for load        testing, and AlertSite, SmartBears leading Web        performance monitoring solution and global monitoring network into        an integrated framework for API lifecycle quality management. Using        common test scripts and validation assets, API Complete helps        development, IT operations and e-commerce teams ensure that APIs are        thoroughly tested pre-deployment and performing well for end-users or        business partners around the world once in production. This        significantly improves efficiency and collaboration, and lowers costs.                                     \n                                          Web APIs are growing exponentially and are required for mobile and        social applications, as well as for innovative forms of e-commerce that        combine Web Services and contextual information to deliver a compelling        user experience. To ensure their quality, it is critical to conduct        meticulous functional and load testing during pre-deployment to identify        and resolve problems early, as well as continuous monitoring and        regression testing post-deployment to ensure ongoing quality of service        and availability.                                     \n                                          Unlike other fragmented API        testing and monitoring solutions, SmartBears integrated API        Complete:                                     \n                                          --         Fosters collaboration among development, IT operations and e-commerce          teams for the testing and monitoring of APIs. When a monitor fails,          the same transaction can be sent back to development or testing teams          to investigate and remediate production issues in a process that          frequently does not have a single business owner, and is difficult to          duplicate in the field.                                     \n                                          --         Is easy to use--leveraging test scripts created for pre-deployment          testing for production monitoring eliminates re-work and custom          scripting. With more than 800,000 users worldwide, there is also a          good chance development and testing teams are already using soapUI for          API testing.                                     \n                                          --         Enables control, visibility and reproducibility into performance of          APIs that live in the black box of cloud-dependent and mobile          applications.                                     \n                                          --         Empowers companies to deliver superior experiences to end users and          measure API performance and availability against contractual          service-level agreements (SLAs) with business partners.                                     \n                                          API        Complete consists of the following products; soapUI, loadUI and        AlertSite. API Complete is available immediately. To purchase, call        1-877-302-5378 or email sales@alertsite.com.                                     \n                                          Webinar                                     \n                                          A SmartBear Webinar featuring Tom Murphy, Gartner, Inc., Your        Guide to the Golden Age of APIs, 5 Ways to Ensure Quality from        Development through Deployment, will discuss best practices for        ensuring API        quality as well as preview API Complete on Wednesday, May 16, 2012,        at 1:00 p.m. EDT (10:00 a.m. PDT)(6:00 p.m. BST). Follow live and share        on Twitter using #APIquality. [Share        this]                                     \n                                          About SmartBear Software                                     \n                                          SmartBear Software provides tools for over one million software        professionals to build, test, and monitor some of the best software        applications and websites anywhere -- on the desktop, mobile and in the        cloud. Our users can be found worldwide, in small businesses, Fortune        100 companies, and government agencies. Learn more about the SmartBear        Quality Anywhere Platform, our award-winning tools, or join        our active user community at    www.smartbear.com    ,        on Facebook,        or follow us on Twitter @smartbear.                                     \n                                          SOURCE: SmartBear Software","item_date":"May 15 2012 19:31:36","display_item_date":"05-15-2012","url":"http:\/\/www.marketwatch.com\/story\/api-testing-and-monitoring-smartbear-launches-the-first-integrated-solution-bridging-the-devops-divide-2012-05-15","source":"www.marketwatch.com"},{"title":"Profile of the Data Journalist: The Data News Editor","details":"Around the globe, the bond between data and journalism is growing stronger. In an age of big data, the growing importance of data journalism lies in the ability of its practitioners to provide context, clarity and, perhaps most important, find truth in the expanding amount of digital content in the world. In that context, data journalism has profound importance for society. (You can learn more about this world and the emerging leaders of this discipline in the newly released Data Journalism Handbook.) \n  To learn more about the people who are doing this work and, in some cases, building the newsroom stack for the 21st century, I conducted in-person and email interviews during the 2012 NICAR Conference and published a series of data journalist profiles here at Radar.\n  John Keefe (@jkeefe) is a senior editor for data news and journalism technology at WNYC public radio, based in New York City, NY. He attracted widespread attention when an online map he built using available data beat the Associated Press with Iowa caucus results earlier this year. Hes posted numerous tutorials and resources for budding data journalists, including how to map data onto county districts, use APIs, create news apps without a backend content management system and make election results maps. As youll read below, Keefe is a great example of a journalist who picked up these skills from the data journalism community and the Hacks\/Hackers group. \n  Our interview follows, lightly edited for content and clarity. (Ive also added a Twitter list of data journalist from the New York Times Jacob Harris.) \n  Where do you work now? What is a day in your life like?\n   I work in the middle of the WNYC newsroom -- quite literally. So throughout the day, I have dozens of impromptu conversations with reporters and editors about their ideas for maps and data projects, or answering questions about how to find or download data. \n  Our team works almost entirely on news time, which means our creations hit the Web in hours and days more often than weeks and months. So Im often at my laptop creating or tweaking maps and charts to go with online stories. That said, Wednesday mornings its breakfast at a Chelsea cafe with collaborators at Balance Media to update each other on longer-range projects and tools we make for the newsroom and then open source, like Tabletop.js and our new vertical timeline.\n  Then there are key meetings, such as the newsrooms daily and weekly editorial discussions, where I look for ways to contribute and help. And because theres a lot of interest and support for data news at the station, Im also invited to larger strategy and planning meetings.\n  How did you get started in data journalism? Did you get any special degrees or certificates?\n   Ive been fascinated with the intersection of information, design and technology since I was a kid. In the last couple of years, Ive marveled at what journalists at the New York Times, ProPublica and the Chicago Tribune were doing online. I thought the public radio audience, which includes a lot of educated, curious people, would appreciate such data projects at WNYC, where I was news director.\n  Then I saw that Aron Pilhofer of the New York Times would be teaching a programming workshop at the 2009 Online News Association annual meeting. I signed up. In preparation, I installed Django on my laptop and started following the beginners tutorial on my subway commute. I made my first Hello World! web app on the A Train.\n  I also started hanging out at Hacks\/Hackers meetups and hackathons, where Id watch people code and ask questions along the way.\n  Some of my experimentation made it onto the WNYCs website -- including our 2010 Census maps and the NYC Hurricane Evacuation map ahead of Hurricane Irene. Shortly thereafter, WNYC management asked me to focus on it full-time.  \n  Did you have any mentors? Who? What were the most important resources they shared with you?\n   I could not have done so much so fast without kindness, encouragement and inspiration from Pilhofer at the Times; Scott Klein, Al Shaw, Jennifer LaFleur and Jeff Larson at ProPublica; , Chris Groskopf, Joe Germuska and Brian Boyer at the Chicago Tribune; and Jenny 8. Lee of, well, everywhere.\n  Each has unstuck me at various key moments and all have demonstrated in their own work what amazing things were possible. And they have put a premium on sharing what they know -- something I try to carry forward. \n  The moment I may remember most was at an afternoon geek talk aimed mainly at programmers programmers. After seeing a demo of a phone app called Twilio, I turned to Al Shaw, sitting next to me, and lamented that I had no idea how to play with such things.\n  You absolutely can do this, he said. \n  He encouraged me to pick up Sinatra, a surprisingly easy way to use the Ruby programming language. And I was off. \n   What does your personal data journalism stack look like? What tools could you not live without?\n  Google Maps - Much of what I can turn around quickly is possible because of Google Maps. Im also experimenting with MapBox and Geocommons for more data-intensive mapping projects, like our NYC diversity map.\n  Google Fusion Tables - Essential for my wrangling, merging and mapping of data sets on the fly.\n  Google Spreadsheets - These have become the backend to many of our data projects, giving reporters and editors direct access to the data driving an application, chart or map. We wire them to our apps using Tabletop.js, an open-source program we helped to develop.\n  TextMate - A programmers text editor for Mac. There are several out there, and some are free. TextMate is my fave.\n  The JavaScript Tools Bundle for Textmate - It checks my JavaScript code ever time I save, flagging me to  near-invisible, infuriating errors such as a stray comma or a missing parenthesis. Im certain this one piece of software has given me more days with my kids.\n  Firebug for Firefox - Lets you see what your code is doing in the browser. Essential for troubleshooting CSS and JavaScript, and great for learning how the heck other people make cool stuff.\n  Amazon S3 - Most of what we build are static pages of html and JavaScript, which we host in the Amazon cloud and embed into article pages on our CMS.\n  census.ire.org - A fabulous, easy-to-navigate presentation of US Census data made by a bunch of journo-programmers for Investigative Reporters and Editors. I send someone there probably once a week. \n  What data journalism project are you the most proud of working on or creating?\n   Id have to say our GOP Iowa Caucuses feature. It has several qualities I like:\n Mashed-up data -- It mixes live, county vote results with Patchwork Nation community types.\n  A new take -- We know other news sites would shade Iowas counties by the winner; we shaded them by community type and showed who won which categories. \n  Complete sharability -- We made it super-easy for anyone to embed the map into their own site, which was possible because the results came license-free from the state GOP via Google. \n  Key code from another journalist -- The map-rollover coolness comes from code built by Albert Sun, then of the Wall Street Journal and now at the New York Times. \n  Rapid learning -- I taught myself a LOT of JavaScript quickly. \n  Reusability -- We used it for  which we did for each state until Santorum bowed out. \n \nBonus: I love that I made most of it sitting at my moms kitchen table over winter break.\n   Where do you turn to keep your skills updated or learn new things?\n  WNYCs editors and reporters. They have the bug, and they keep coming up with new and interesting projects. And I find project-driven learning is the most effective way to discover new things. New York Public Radio -- which runs WNYC along with classical radio station WQXR, New Jersey Public Radio and a street-level performance space -- also has a growing stable of programmers and designers, who help me build things, teach me amazing tricks and spot my frequent mistakes.\n  The IRE\/NICAR annual conference. Its a meetup of the best journo-programmers in the country, and it truly seems each person is committed to helping others learn. Theyre also excellent at celebrating the successes of others.\n  Twitter. I follow a bunch of folks who seem to tweet the best stuff, and try to keep a close eye on em.\n  Why are data journalism and news apps important, in the context of the contemporary digital environment for information?\n  Candidates, companies, municipalities, agencies and non-profit organizations all are using data. And a lot of that data is about you, me and the people we cover.\n  So first off, journalism needs an understanding of the data available and what it can do. Its just part of covering the story now. To skip that part of the world would shortchange our audience, and our democracy. Really.\n  And the better we can both present data to the general public and tell data-driven (or -supported) stories with impact, the better we can do great journalism.","item_date":"May 15 2012 19:23:06","display_item_date":"05-15-2012","url":"http:\/\/radar.oreilly.com\/2012\/05\/profile-of-the-data-journalist-10.html","source":"radar.oreilly.com"},{"title":"Factual Adds Context to Location With New API","details":"Factual has added deeper analytics to its compendium of location data services with the release of its new Geopulse API this morning.\u00a0 The API enables developers to retrieve contextual information \u2014 commercial profiles and density scores as well as demographic indicators like age, gender, and median income \u2014 for a given location across Factual\u2019s 50-country reach.\nThe API will serve as the umbrella for a group of forthcoming products, which appear largely aimed at helping ad tech services optimize their targeting parameters. Bill Michels, VP of business at Factual, says that new \u201cpulses,\u201d which the company plans to release iteratively over the next month, will likely focus on \u201cother great targeting parameters like pulling together social information, social scores, and social pulses around different geo points.\u201d\nFactual was launched in 2008 by Applied Semantics co-founder Gil Elbaz, and has since built a business on selling its data through APIs and direct partnerships with big players like Facebook and Newsweek. The company has pieced together a massive dataset largely on the backs of web crawlers, which use machine learning to identify and structure place information from across the open web.\n\u201cWe have a great business with developers using our API\u2019s for place data, and this is sort of a natural evolution,\u201d Michels told Street Fight. \u201cWe want to add more contextual relevance to the queries they make and open up new use cases with that same place data.\u201d\nThe Geopulse API gives Factual a much-improved presence in the growing advertising technology space and puts the company in direct competition with location intelligence providers like PlaceIQ. \u00a0Though its unclear how the services stack up at this point, Factual will likely go after the mobile exchanges and networks that make up PlaceIQ\u2019s bread-and-butter.\nPart of the \u201cnatural evolution,\u201d which Michels refers to, is in moving beyond thinking of location as a question of \u201cwhere\u201d and beginning to tap into its ability to provide answers around who, what and why. The latter questions not only open up new use cases for local information but are, in a sense, substantially more scalable for larger brands. Though the \u201cping me when I walk next to your store\u201d use case remains attractive on a high-level, tying location to indicators outside of proximity opens a dataset like Factual\u2019s to national brands and their advertising partners, which talk in terms of audience, not location.\nThe company also announced that it has hooked up with hyperlocal advertising startup ThinkNear, a fellow Los Angeles-based company that runs proximity-based mobile ad campaigns, as a launch partner.\nSteve Jacobs is an associate editor at Street Fight.\nThinkNear\u2019s CEO Eli Portnoy and PlaceIQ\u2019s CEO Duncan McCall are both slated to speak at Street Fight Summit West in San Francisco on June 5th. Ticket prices are about to rise \u2014 reserve yours today.\n\u00ab 5 Tools Hyperlocal Publishers Can Use to Improve Online Ad Sales \nJoin top hyperlocal execs for strategy, ideas and networking. Register today! \nGet hyperlocal headlines in your box every morning with Street Fight Daily. For financial analysis and the market potential of hyperlocal businesses, subscribe to The Hyperlocal Investment Report. \nStreet Fights white paper research report is a playbook for anyone looking to succeed in the hyperlocal marketing ecosystem. Combining research from over 50 interviews with expert analysis, the report provides key insight into an exploding market opportunity. Buy your copy today! \nFollow Us\nGet the latest Street Fight news, information and analysis via Twitter and Facebook.\nWhy Check In?\n\u201cI did not want to be mayor of my dentist\u2019s office. Why did I even check in?\u201d wondered Digital First Medias Steve Buttry in a recent tweet. Street Fights David Hirschman takes a look at check-in services from a consumer perspective, and tries to figure out why we feel so compelled to use services like Foursquare.\nSponsored Post: Improving Online Offers\nAlthough couponing has gained mainstream acceptance, coupons have proven an inefficient way to redeem an offer. In a new white paper OfferWise offers guidance on integrating mobile and in-store experiences; creating trackability; and thrilling shoppers.\nFacebook Connect & Local\nWhere Facebook starts to get spooky powerful is in its capability to deliver local ads to a Facebook Connect network that will be personalized and reflect the actions, comments, likes and dislikes of your friends.. This will tap into a recognizable social graph as Facebook seeks to do \u2013 and it could turn into an extremely lucrative, higher CPC line of business.\nThe \u2018Twisted Thinking\u2019 Behind SMB Self-Service\nClosely CEO Perry Evans unpacks the ideas behind attempts to create self-service platforms for SMBs to place online ads: Technology comfort alone will not drive scaled merchant participation.","item_date":"May 15 2012 19:10:27","display_item_date":"05-15-2012","url":"http:\/\/streetfightmag.com\/2012\/05\/15\/factual-adds-context-to-location-with-geopulse-api\/","source":"streetfightmag.com"},{"title":"If You Can Copyright an API, What Else Can You Copyright?","details":"Brian Pagano is a software architect at Apigee, an outfit that does nothing but help companies build and operate APIs, interfaces that let one piece of software talk to another. In describing the APIs his company deals in, he wants to lend some perspective to another question, a question that may soon be answered by the federal judge overseeing the ongoing legal battle between Google and Oracle: Can you copyright an API?\n If Judge William Alsup rules that APIs are subject to copyrights, he would overturn common wisdom in programming circles, potentially exposing many companies and developers who have built software platforms that openly mimic existing APIs. But that\u2019s not all. Such a ruling could shake things up for many other companies across the programming world and beyond. \n \u201cLet\u2019s say you have a communication protocol where you send a packet of data and that packet is organized in a certain way. If the court says that an API is copyrightable, then why isn\u2019t the layout of a data packet copyrightable?\u201d \u2013 Ronald Abramson\n As they evolve into a way of connecting modern web services and mobile applications, APIs are moving towards a common convention called REST, short for \u201crepresentational state transfer,\u201d and the whole idea of REST is to make APIs simple. Sometimes, an API call is similar to a common English noun or verb, and it typically handles the same data formats as other REST APIs. \u201cThe odds of similarity between any two RESTful APIs is quite high,\u201d Pagano says. \u201cI think that\u2019s one of the things that has everyone so concerned.\u201d\n Pagano envisions a scenario where a company makes a land grab of common names and starts enforcing copyrights on APIs en masse. You could lockdown API formats as easily as you lockdown internet addresses or Twitter handles. And if you can do that, what\u2019s to stop you from copyrighting other common computing formats? \n \u201cLet\u2019s say you have a communication protocol where you send a packet of data and that packet is organized in a certain way,\u201d says Ronald Abramson, a lawyer with the international firm Hughes, Hubbard, and Reed. \u201cIf the court says that an API is copyrightable, then why isn\u2019t the layout of a data packet copyrightable?\u201d\n API is short for application programming interface. If a piece of software offers an API, you can build applications that plug into that piece of software. Using the APIs for Apple\u2019s iOS mobile operating system, for instance, you can build an application that runs on the iPhone. And using the Facebook APIs, you can plug that application into Facebook. It\u2019s little more than a way of sending and receiving data.\n In building a new software platform, software companies and other software designers often clone the APIs of an existing platform. It\u2019s a way of making a platform instantly palatable to an established community of application developers. Several cloud-computing platforms mimic the APIs of the runaway market leader, Amazon Web Services, and in building Android, Google mimicked the APIs for the Java platform. This lets developers build Android applications using the Java programming language. \n \n In 2010, Oracle sued Google over this, claiming the search giant infringed on its Java-related copyrights and patents. In San Francisco, a jury is still listening to arguments over Oracle\u2019s patent claims, and though the jury was unable to reach a complete decision on the copyright claims, Judge William Alsup will likely rule on whether the Java APIs can be copyrighted in the first place.\n Abramson believes Alsup will rule than APIs can\u2019t be copyrighted, because this is consistent with existing law. And if Alsup rules that they can, the decision will almost certainly be appealed to a higher court. But the ball will be rolling, and the possibility has many wondering how such a ruling could affect the APIs they\u2019ve built \u2014 or plan to build.\n Uh, Can You Show Me a Java API?\n The Java APIs are a means of talking to software libraries that you include with the Java application you\u2019re building. A typical Java API method looks something like this:\n Public shows who can access the method. Int, short for integer, indicates what type of information will be returned. FetchNumberOfOverdueAccounts is the actual name of the method. And (int groupID) gives the input parameters \u2014 i.e., the information you must supply to the method.\n Because these APIs call software that resides with the application, they\u2019re a bit different from so many APIs in the internet age, which make calls to distant web services. But otherwise \u2014 says Alex Polvi, a developer and founder of a startup called Cloudkick that was acquired by cloud giant Rackspace \u2014 these two types of APIs are similar. Both are calls you make to another piece of software.\n Apigee\u2019s Brian Pagano also points out that the Java APIs are a bit more complex than many of today\u2019s web and mobile APIs, especially those that use the REST convention and the web\u2019s underlying HTTP protocol. \u201cJava \u2014 or other traditional programming libraries \u2014 are more distinctive,\u201d he says. \u201cThey tend to be made up of verbs and be more verbose.\u201d But he agrees that in the end, all these APIs are little more than ways of invoking underlying computer code.\n \u201cThis would be a strong precedent and it would be pretty difficult to distinguish them from other APIs. I would be concerned that the ruling would have broad ramifications for other APIs and beyond APIs.\u201d \u2013 Ronald Abramson\n Given this, says Ronald Abramson, a ruling on the Java APIs could be relevant to almost any other common way of sending data. \u201cThese are a pretty typical APIs. They\u2019re a definition of how you talk to something,\u201d he says. \u201cThis would be a strong precedent and it would be pretty difficult to distinguish them from other APIs\u2026I would be concerned that [the ruling] would have broad ramifications for other APIs and beyond APIs. If you can copyright the structure, sequence, and organization of data, that changes the landscape.\u201d\n Like Pagano, Alex Polvi says that REST APIs for competing products tend to be similar. The data delivered by APIS will be different, but it will be delivered in a similar way. So, even if you\u2019re not consciously mimicking someone else\u2019s interfaces, that odds are your APIs will be similar to those offered by others. \n Pagano argues that generally, APIs do not contain innovative technology that should be protected from competitors \u2014 and he says that Apigee\u2019s customers tend to feel the same. \u201cThe innovation is how you engage with your customers, and how your systems work on the back-end,\u201d says Apigee CEO Chet Kapoor. But they\u2019re also aware that the law may view things differently. \n If a customer does believe that their APIs contain innovative technology, Pagano and Kapoor say, then Apigee advises that customer to do everything it can to patent them. \u201cWe\u2019re not saying that people should just start patenting APIs,\u201d Pagano explains. \u201cWe\u2019re saying that if you believe that\u2019s where the innovation is, then that\u2019s the proper course of action.\u201d\n In other words, the copyright question is unsettled, and this could leave you vulnerable. Of course, the patent system is a mess too. But if you need protection for APIs, that\u2019s the best place to turn.  \n \nCade Metz is the editor of Wired Enterprise. Got a NEWS TIP related to this story -- or to anything else in the world of big tech? Please e-mail him: cade_metz at wired.com.\nRead more by Cade Metz","item_date":"May 15 2012 17:39:05","display_item_date":"05-15-2012","url":"http:\/\/www.wired.com\/wiredenterprise\/2012\/05\/api-copyright\/","source":"www.wired.com"},{"title":"AngelHack Announces Its Second National Hackathon with $50k in Seed Funding to Winning Teams","details":"This will be the first hackathon competition (defined as coding marathons) to unite startup communities nationwide. Bringing together a nations worth of talent means upping the ante on prizes as well. So this will also be the first hackathon to include the top prize of guaranteed seed capital investments to winning teams.\n   \t\t\t\t\t\t \t\t\t   \t\t  \t \t\t  \t\t\t   \t\t\t\t\t\t \t\t\t\t\t\t \t\t\t\t\t\t \t\t\t\t\t\t \t\t\t\t\t\t \t\t\t\t\t\t\t  Whats the big deal?\nUsually, hackathons finish when the last team presents in their home city. With 50K in seed funding at stake, one weekend is just the beginning. After the first round of judging, the top 20 teams from around the country will receive three weeks of mentorship from AngelHack partners, including Lean Startup Machine, to help build those weekend hack projects into launch-ready businesses.\n   \t\t\t\t\t\t \t\t\t   \t\t  \t \t\t  \t\t\t   \t\t\t\t\t\t \t\t\t\t\t\t \t\t\t\t\t\t \t\t\t\t\t\t \t\t\t\t\t\t \t\t\t\t\t\t\t  How It Works:\nInterested developers & designers should register at http:\/\/angelhack.com and come to build the next billion dollar product. Top teams from around the country will be flown to AOL in Palo Alto, CA for a Grand Finals demo on July 12 presenting their ideas to a panel of Angel Investors, technologists, and media, which include TechCrunchs Anthony Ha, Microsofts VP of Developers, Scott Guthrie, and Right Side Capitals David Lambert, who will be investing in the winning team. \nAngelHacks previous events have been supported by some of the most reputable names in the industry, including Google, Microsoft, Intel, and Adobe. Businesses looking to sponsor AngelHack can visit http:\/\/angelhack.com for more information.\n   \t\t\t\t\t\t \t\t\t   \t\t  \t \t\t  \t\t\t   \t\t\t\t\t\t \t\t\t\t\t\t \t\t\t\t\t\t \t\t\t\t\t\t \t\t\t\t\t\t \t\t\t\t\t\t\t  The Mission:\nAccording to founder Gregory Gopman, AngelHacks mission is to create a global launch pad where entrepreneurs can collaborate on big ideas and showcase them to investor communities. \u00a0Were proud to connect these parties and provide winning teams with guaranteed funding for their hack ideas!\n   \t\t\t\t\t\t \t\t\t   \t\t  \t \t\t  \t\t\t   \t\t\t\t\t\t \t\t\t\t\t\t \t\t\t\t\t\t \t\t\t\t\t\t \t\t\t\t\t\t \t\t\t\t\t\t\t  News distributed by PR Newswire iReach: https:\/\/ireach.prnewswire.com \n  \t\t\t\t\t\t \t\t\t   \t\t  \t \t\t  \t\t\t   \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n  Browse our custom packages or build your own to meet your unique communications needs.\n  Start today.\n     \t\t\t \n  Fill out a PR Newswire membership form or contact us at (888) 776-0942.\n     \t\t\t \n  Request more information about PR Newswire products and services or call us at (888) 776-0942.","item_date":"May 15 2012 17:34:49","display_item_date":"05-15-2012","url":"http:\/\/www.prnewswire.com\/news-releases\/angelhack-announces-its-second-national-hackathon-with-50k-in-seed-funding-to-winning-teams-151532675.html","source":"www.prnewswire.com"},{"title":"45 Coupons APIs: Groupon, 8coupons and US Yellow Pages","details":"Our API directory now includes 45 coupons APIs. The newest is the Wishpot Coupon API. The most popular, in terms of mashups, is the Groupon API. We list 10 Groupon mashups. Below you\u2019ll find some more stats from the directory, including the entire list of coupons APIs.\n\n\n\n\n\nIn terms of the technical details, REST and XML lead the way. It should be pointed out that only 30 APIs in this category have a protocol listed. Many providers in this space do not make their documentation openly available instead preferring to partner with developers before providing access. There are 29 coupons REST APIs. Our directory lists 25 coupons XML APIs and 23 coupons JSON APIs.\n\n\n\n\n\nThe most common tags within coupons are 38 shopping coupons APIs, 12 local coupons APIs and 12 deals coupons APIs.\n\n\n\n\n\nOn the mashup side, we list 12 coupons mashups. We named Frugalmate as mashup of the day yesterday.\n\n\nFor reference, here is a list of all 45 coupons APIs.\n\n\n\u00a0\u00a08coupons API: Local deals aggregator\n\n\n\u00a0\u00a0AisleBuyer API: Mobile commerce services\n\n\n\u00a0\u00a0AwardWallet API: Loyalty program management service\n\n\n\u00a0\u00a0Best Buy BBYOpen BBYOffer API: eCommerce shopping services\n\n\n\u00a0\u00a0BiteHunter API: Dining Deals Search Service\n\n\n\u00a0\u00a0BuyDeals.in Local Deal API: Local Group Buy Aggregation Service\n\n\n\u00a0\u00a0Bview Promotion API: Aggregate coupons for UK retailers\n\n\n\u00a0\u00a0CardSpring API: Payment network service\n\n\n\u00a0\u00a0CBS Local Offers API: Local Deals Listings\n\n\n\u00a0\u00a0Cellfire API: Electronic coupon and discount service\n\n\n\u00a0\u00a0CityPockets API: Daily deal and voucher aggregation service\n\n\n\u00a0\u00a0Deal Magic API: List of Daily Deals \n\n\n\u00a0\u00a0Dealinium API: Local deals aggregation service\n\n\n\u00a0\u00a0DealsGoRound API: Online marketplace for daily deals\n\n\n\u00a0\u00a0FindMeSpecials API: Find local specials and events\n\n\n\u00a0\u00a0foursquare Merchant API: foursquare merchant platform\n\n\n\u00a0\u00a0Getsocio API: Daily deal and group buying website platform\n\n\n\u00a0\u00a0Grocery Server API: Grocery search engine\n\n\n\u00a0\u00a0GroopBuy API: Local deals and group buying service\n\n\n\u00a0\u00a0Groupon API: Group shopping service\n\n\n\u00a0\u00a0Hyperpublic API: Geographic data collection service\n\n\n\u00a0\u00a0Lifesta API: Deal buying and selling service\n\n\n\u00a0\u00a0MasterCard Offers API: Discount, deals, and offers service\n\n\n\u00a0\u00a0MerchantCircle.com API: Online marketing service for local businesses\n\n\n\u00a0\u00a0Mphoria API: Deal providing service\n\n\n\u00a0\u00a0MyDealBag API: Local deals aggregation service\n\n\n\u00a0\u00a0OfferGrid API: Deal distribution service\n\n\n\u00a0\u00a0OWS Coupons API: Coupon and deal feed\n\n\n\u00a0\u00a0Paynoy API: Philippines deal service\n\n\n\u00a0\u00a0Peixe Urbano API: Brazilian daily deals service\n\n\n\u00a0\u00a0Pricecut API: Coupons, deals and price comparison service\n\n\n\u00a0\u00a0RapidPoints API: Rewards program services for small businesses\n\n\n\u00a0\u00a0RetailMeNot.com Community Ideas API: Online coupons and discounts\n\n\n\u00a0\u00a0SideBuy API: Local deals and coupons aggregation service\n\n\n\u00a0\u00a0Spotzot API: Deal targeting and loyalty program service\n\n\n\u00a0\u00a0Sqoot API: Local deals aggregation service\n\n\n\u00a0\u00a0StickyStreet API: Loyalty & Customer Management System\n\n\n\u00a0\u00a0The Dealmap API: Local deals\n\n\n\u00a0\u00a0ThinkNear API: Local marketing and advertising service\n\n\n\u00a0\u00a0Tippr API: Daily deals aggregation service\n\n\n\u00a0\u00a0US Yellow Pages API: US yellow pages telephone directory\n\n\n\u00a0\u00a0Vouchers.Im API: Shopping coupons and deals services\n\n\n\u00a0\u00a0Wishpot Coupon API: Coupon aggregation service\n\n\n\u00a0\u00a0Yipit API: Local coupon aggregation service\n\n\n\u00a0\u00a0Zixxo API: Online coupons","item_date":"May 15 2012 17:34:37","display_item_date":"05-15-2012","url":"http:\/\/blog.programmableweb.com\/2012\/05\/15\/45-coupons-apis-groupon-8coupons-and-us-yellow-pages\/","source":"blog.programmableweb.com"},{"title":"Superfeedr : Protocols over APIs","details":"There is not a week where I don\u2019t see developers discovering that a given API has been deprecated. Today\u2019s disappointment (at least in my twitter stream) comes from the Google Chart Tool API.\n Don\u2019t get me wrong. I feel for these developers and I would be upset should the same problem happen to me. Now, being upset is great. Learning how to prevent that is better, and I believe there are many ways to avoid that. The most obvious one is to re-invent the wheel as suggested by Adactio.\n Next time someone asks why I\u2019m reinventing the wheel, I\u2019m telling them it\u2019s because the wheel API is being deprecated. developers.google.com\/chart\/terms \n\u2014 Jeremy Keith (@adactio) May 15, 2012 \nThat is obviously not always possible\u2026 and is probably not the right solution either because re-inventing the wheel can be a real rabbit-hole which may take you to building a power grid!\n Another solution is to favor protocols over APIs. To over-simplify things, an API is just a combination of uni-laterally decided upon protocol and an endpoint. The Twitter API is the twitter protocol used against twitter.com. Got it? This is what Status.net understood when they decided to write a compatible API\u2026 for which you would just have to change the endpoint from twitter.com to status.net. That worked until Twitter changed its API.\n Now, if developers relied more on protocols, that means that when a providers disappears or deprecates a service, developers could just change the endpoint, without changing anything in their code to make sure everything keeps working. Re-using code is the most satisfying feeling for many developers.\n Want an example? Last week, I got tired of myopenid.net inability to stay up. I used them as a provider for my personal openid endpoint. Changing the provider was as simple as changing the openid2.provider\n href in the HTML and I was done.\n When you decide to work with a given\u2019s service API, please tell them you want their API to become a protocol, something for which you are a stake holder, that you can help improve. Also, if there is any open protocol or any schema that does what this API aims at doing, please convince them that it\u2019s way to go, that this will make the web better and that will make your code more useful to more people :)\n At Superfeedr, we\u2019re obviously big promoters of protocols (XMPP, PubSubHubbub, ATOM, ActivityStreams) over APIs, and we obviously kept the same philosophy for Msgboy.\n Addendum: what makes me even sadder about this story is that Brad Fitz, who wrote the Social Graph API is the one who told me first about protocols being better than API..; and guess what? The Social Graph API has been deprecated as well. :(","item_date":"May 15 2012 17:32:49","display_item_date":"05-15-2012","url":"http:\/\/blog.superfeedr.com\/protocols-over-api\/","source":"blog.superfeedr.com"},{"title":"API Strategy Lessons from Factual\u2019s Upgrade of its Mobile\/Local APIs","details":"This guest post comes from Dan Woods, CTO and Editor of CITOResearch.com and co-author of APIs: A Strategy Guide. He writes about API Strategy and related topics.\n\n\n Factual Inc, a company founded by ex-Googler Gil Elbaz that is creating a collaborative data platform, announced extensions to its Factual APIs today that are aimed at improving the ability to target advertising and provide other geo-based capabilities in mobile applications.  The three new APIs, Geopulse, Reverse Geocoder, and World Geographies, fill gaps and extend the scope of Factual\u2019s API portfolio. But the way that Factual thinks about its APIs also holds lessons for anyone who is mapping out an API strategy of their own.\n\n\nFactual offers access to data sets for Global Places, U.S. Restaurants, U.S. Healthcare providers, and World Geographies. Factual\u2019s first batch of APIs provided access to the foundational location data for 50 countries (the Core API), enabled entity resolution for a place (Resolve), and provided mappings to a place from dozens of sources such as Foursquare, Yelp, Eventful and so on (Crosswalk API). The new APIs help mobile app developers, advertising companies and demand side platforms target ads better in the mobile environment, and companies attempting to understand how to target ads:\nGeopulse is an API that accepts a latitude and longitude and returns four different types of information, called pulses, related to that location: \n\n\nFactual Commercial Density: the relative density of businesses nearby\n Factual Commercial Profile: the types of businesses nearby\n Nearest: the closest Place in the Factual database.\n Demographics: Age, gender, race, and median income based on US census data (US only).\n Reverse Geocoder is an API that converts a longitude and latitude into an address (US only) or region (49 other countries). \n World Geographies is an API that provides the names and interrelationships between the world\u2019s natural and administrative geographies \u2014 countries, cities, states, continents, regions, and time zones \u2014 enhancing the global 60 million businesses and landmarks Factual currently offers. The API provides approximately 6 million geographies and over 8 million place names in numerous languages. \n All of these APIs are being released in beta. The Reverse Geocoder and World Geographies APIs fill gaps that developers of mobile and web asked have asked for. \n The Geopulse API fills an emerging need to assemble as much information as possible related to a specific location. Instead of just knowing that you are at a specific location with a certain type of phone at a specifice time of day, the Geopulse API provides many more signals that can be used to improve the targeting of an ad.\n \u201cThe signals were are releasing in GeoPulse are just the beginning,\u201d Factual\u2019s Eva Ho said. \u201cIn the future we will layer on top social signals and many other data sets that will further improve the amount of information that can be used by application developers or ad networks. Both groups are hungry for as much information as they can get.\u201d \n The way that Factual is gradually extending its APIs reveals a pattern that should be useful to product managers and designers of APIs or portfolios of mobile apps. The operative principle: Follow the value chain.\n Often, when someone creates an API or a mobile app, the effort is a shot in the dark. Will anyone be interested? How will it make a difference? If the API or application takes hold, the next question is: What comes next?  \n Following the value chain means looking at what the API or mobile app is being used for. Ask not only what more can be done but what are the adjacent activities that are related? How can these be supported? Is there an emerging API economy of the sort my co-authors and I described in \u201cAPIs: A Strategy Guide\u201d? If so how must the existing APIs grow and what new APIs are required? \n The growth of Factual APIs follows the value chain created by mobile apps. First, the mobile and web apps needed information about places. This is where Factual APIs first took hold. The success of these apps lead to the need to offer better targeting of ads and other services. This is the mission of the APIs Factual announced today. \n \u201cOur early customer wins were all around web and mobile developers,\u201d Ho said. \u201cTheir next big worry was how were they going to monetize their applications, and we realized that we needed to provide more data to enable that. Going into local targeting makes complete sense given that we started with this rich dataset of locations.\u201d\n While the first users of the APIs will likely companies who sell targeted ads, it is likely that companies who buy targeted ads will also use the APIs to figure out how to improve their rules for targeting.\n Ho said that many more data sets are on the way. As these data sets create new applications, Factual will undoubtedly follow the value chain with new sets of APIs.","item_date":"May 15 2012 17:32:36","display_item_date":"05-15-2012","url":"http:\/\/blog.programmableweb.com\/2012\/05\/15\/api-strategy-lessons-from-factuals-upgrade-of-its-mobilelocal-apis\/","source":"blog.programmableweb.com"},{"title":"Foursquare Delivers a Decisive Blow to Stalkers","details":"This week foursquare announced API changes to eliminate privacy concerns and creepy stalker apps like Girls Around Me. So, what are they changing? Foursquare is removing the ability for users to see people (even those not on their friends list) who are checked in to a venue without being checked in to the same venue themselves. Akshay Patil (foursquare API evangelist) explains it further: \u201cmuch like how the users could see each other by looking around in real life.\u201d Users will still be able to see friend check-ins regardless of whether they\u2019re checked in to the same venue or not.\n\nCreepy apps like Girls Around Me (which got a lot of buzz in March) leverage foursquare data to display a list of people (strangers) who are checked in nearby filtered by gender. The API change will essentially render these apps worthless because users will only be able to see their friends.\n\nOn the flipside, eliminating this data also affects the less-creepy apps like Sonar or Banjo that connect strangers based on location, particular interests or mutual friends. Sonar leverages data from multiple networks including foursquare, Twitter, LinkedIn and Facebook to suggest people nearby you should connect with, based on interests and\/or mutual friends.\n\nBanjo uses data from Twitter, foursquare, Instagram and others to display where people are and what they\u2019re saying or doing based on check-ins or tweets, not interests.\n\nThe foursquare API change will be implemented in June to allow time for the less-creepy app developers to make some changes. The negative impact will be low for these developers, considering many of them use multiple platforms and are not 100% reliant on foursquare for data.\n\nI am a big fan of foursquare and consider it a safe service when used properly. This change will make it easier for the cautious non-users to convert and give the application a try.\n\nWhat do you think? Does this make you feel more secure about sharing your location? Does this make you want to give foursquare a try if you haven\u2019t already?","item_date":"May 15 2012 01:09:31","display_item_date":"05-14-2012","url":"http:\/\/smartdatacollective.com\/maggiefox-social-media-group\/50762\/foursquare-delivers-decisive-blow-stalkers","source":"smartdatacollective.com"},{"title":"Oracle Wins (Tiny) Battle in War With Google","details":"Oracle scored a victory on Friday in its ongoing battle with Google over the use of Java on the Android mobile operating system. But it was a very small victory.\n\n Judge William Alsup ruled that evidence presented during the trial had shown that Google infringed on Oracle\u2019s copyrights by decompiling eight Java files and copying them in their entirety for use with Android.\n\n Previously, the jury in the case decided that Google had not infringed with its use of these eight files.\n\n Last week, Oracle filed an extensive brief (.pdf) arguing that Google\u2019s use of the eight decompiled files \u2014 source code in seven \u201cImpl.java\u201d files and the one \u201cACL\u201d file \u2014 could not be considered \u201cde minimis,\u201d or insignificant. Oracle also noted that Google did not dispute that the files were copied.\n\n \u201cWhen compared on a file-to-file basis (per the Court\u2019s jury instructions), it is clear that Google\u2019s copying of the eight files was not de minimis,\u201d read the brief by Oracle lead counsel Mike Jacobs. \u201cGoogle copied each entire file, so by definition, the copying is both quantitatively and qualitatively significant.\u201d\n\n With his ruling, Judge Alsup agreed. \u201cNo reasonable jury could find that this copying was de minimis,\u201d he wrote.\n\n In 2010, after acquiring Sun Microsystems, the maker of the Java programming language, Oracle sued Google, claiming that the search giant violated Sun copyrights and patents in building a new version of the Java platform for its Android mobile operating system. The trial \u2014 which began on April 16 \u2014 was divided into three phases: one that would address the copyright claims, one for the patent claims, and one for damages.\n\n During the copyright phase, the jury was unable to decide whether Google went beyond fair use in mimicking 37 Java APIs, or application programming interfaces, and this is the most important question in the case. But Oracle now has two small victories. In addition to Alsup\u2019s ruling on the eight copied files, the jury decided that Google infringed in lifting nine other lines of Java code involving the TimSort.java and ComparableTimSort.Jav files.\n\n Judge Alsup has yet to rule on the API question, and this still looms large over the trial. He has also yet to rule on Google\u2019s motion for a mistrial. The search giant argues that the jury\u2019s partial verdict cannot stand, citing the Seventh Amendment and settled Supreme Court law.\n\n The ongoing patent phase of the trial has been whittled down to just a pair of patents. And on Friday, the two sides argued over whether the damages phase should proceed given that the jury was unable to decide on the API fair use issue. But apparently, Judge Alsup will let this go ahead.","item_date":"May 15 2012 01:07:27","display_item_date":"05-14-2012","url":"http:\/\/www.wired.com\/wiredenterprise\/2012\/05\/google-oracle-decompile\/","source":"www.wired.com"},{"title":"Enterprise APIs and OAuth: Have it All","details":"Enterprises often frustrate developers. Why do Enterprises always seem so behind when it comes to the very latest technology? In particular, a trend we are seeing is the continued struggle to marry Enterprise authentication with the burgeoning world of REST APIs. Developers want to use REST, but Enterprises need enterprise grade API security.\n We think this problem will only worsen as Enterprises continue their rapid adoption of APIs. It seems clear that SOAP, while capable of Enterprise grade authentication through X.509 and SAML, will be left behind as the \u201cSkinny jeans Facebook generation\u201d puts the final nail in SOAP\u2019s coffin.\n The Dilemma\n Among our own customers and the stories we\u2019ve heard, Enterprises are left with a dilemma with four horns concerning the protection of REST APIs:\n (a) Use mutual authentication with client-side SSL\n (b) Use HTTP authentication (password or digest) with server-side SSL\n (c) Use OAuth, either 3-legged or 2-legged\n (d) Use a de-facto standard or \u201croll your own\u201d\n Each option has challenges and benefits. In particular, the challenges balance time to market and time to value (which I term developer friction), security, and manageability. The success of an API is directly related to it\u2019s perceived ease of access and the amount of friction involved in using it. \u00a0An Enterprise will get more value out of an API that is actually used versus one that lies dormant. At the same time, however, as APIs become a new tunnel into the Enterprise, security and manageability cannot be sacrificed for the sake of adoption.\n Current Options and Challenges\n Let\u2019s consider the options and \u201cscore\u201d each of them.\u00a0 In option (a), most developers would rather shoot themselves than deal with client-side X.509 certificates, especially after experiencing the apparent ease of use of OAuth as evidenced by SaaS providers such as Salesforce.com and social platforms such as Facebook and Twitter. To compound this, the issue of key rotation and certificate management is a weighty burden to deal with from an Enterprise perspective. Echoes are heard from the grave, \u201cBut this year, will be the year of PKI!\u201d Now, the \u201cYear of PKI\u201d conjures up not a picture of a secure Enterprise, but an enterprise fraught with wasteland scenes of Armageddon.\n SSL with Mutual Auth Score:\n Security: High\n Friction: Extremely High\n Management Burden: Extremely High\n Developer Attitude: I hate my life\n Result: A secure API that nobody uses has lowered value to the Enterprise\n \nIn option (b), HTTP Authentication with server-side SSL, we\u2019ve overloaded an interactive web-authentication mechanism developed in 1996 and tried to marry it to the API economy of 2012.\n Well, you say\u2026. At least it is a standard \u2013 and we console ourselves by the fact that it is a protected channel. The issue here of course is the proliferation and management of passwords.\n Security is reduced because we\u2019ve coded in a username and password and we\u2019re probably using the same username and password for multiple applications and not telling management about it. Security is low when the password is in the clear and somewhere around medium-low when digests are used, depending on your perspective on rainbow tables. Friction is high because eventually I\u2019ll need to (or should) rotate that password if I have even a rudimentary password policy\n HTTP Basic Auth with SSL Score:\n Security: Medium-Low\n Friction: High\n Management Burden: High\n Developer Attitude: Really? We\u2019re doing this? \n Result: An API with a \u201chackish\u201d authentication method and suspect security. \n \nIn option (c), OAuth, developers will cheer but problems remain. First and foremost, traditional 3-legged OAuth is an authorization protocol that puts permission control in the hands of a user, who ostensibly owns an asset they are expressing authorization for.\n In the traditional Enterprise context, however, control should be given to administrators, not users. Most users don\u2019t own Enterprise assets, the Enterprise owns their own assets. With X.509 certificate authentication models, administrators can revoke permissions easily by revoking a certification while OAuth delegates permissions to users. This is why OAuth works well for social websites \u2013 content is owned by users.\n One solution to this is to use 2-legged OAuth or some organic variation such as xAuth, which is notionally similar to SAML in that it allows the exchange of a username\/password for an access token. \u00a0Note: If xAuth is still OAuth why is it called something different? Answer: It is different (?)\n 2-legged OAuth has advantages over the previous option in that it stops the proliferation of passwords, which is a good thing, but the failure of the OAuth 2.0 specification to carefully define what 2-legged OAuth entails is a bit worrisome for both security and interoperability.\n  Third, depending on the specific OAuth data flow, such as the authorization code flow, implicit grant flow, or client credential flow, these all have varying levels of security and friction associated with them. The most secure OAuth data flows involving a confidential client demand strong client authentication\u2026.. \u2026with a public\/private key pair (ssshhh!!).\n Finally, as history has taught us, never place all your bets in the security of a protocol. Many of us were shocked at the late 2009 news of a critical man-in-the-middle attack on SSL. If we count the Netscape days to the time of the vulnerability, the protocol was battle tested in industry for 14 years before the vulnerability was found. Some would argue that OAuth just hasn\u2019t been around the block long enough.\n \nIn option (d), roll your own, we\u2019ve seen a number of solutions that involve API keys or shared secrets along with authenticators like HMAC-SHA1. Amazon web services (AWS) is a famous example of this approach, and many of our own customers have copied this approach as a best practice.\n These solutions have the advantage over HTTP Basic Authentication in that they don\u2019t shoe-horn themselves into an outdated standard designed for another purpose. The flip-side is that these aren\u2019t official standards. The idea is to use an HTTP header to store the credentials in a bespoke way defined by the Enterprise. The security model here is essentially the same as username and password, except for the fact that \u00a0we\u2019re calling it an API key to make it sound like it\u2019s not a password. Management burden is increased here because interoperability outside the organization is reduced.\n \u201cRoll Your Own\u201d Score:\n Security: Medium-Low\n Friction: Medium\n Management Burden: Medium-High\n Developer Attitude: Eh, It\u2019s not OAuth, but at least I don\u2019t have to deal with X.509 certificates\n Result: A reasonable, albeit non-standard solution to API authentication. Friction is reduced but password or shared secret proliferation remains a problem\n \n One approach to solving the API access mechanism dilemma is to reject the dilemma and have it all. This can be done with a front-end or security intermediary that, much like a diligent mediator can make both sides happy. In this model, a front-end proxy is used on top of existing APIs, essentially retrofitting them for OAuth without having to get into the \u2018weeds\u2019 of actually modifying enterprise APIs. Everything is done on the wire at network layer 4 and above.\n To take one example, if you are an Enterprise with REST APIs that use mutual SSL authentication, you don\u2019 t have to enforce this fact on your developers. Instead, have a gateway handle it for you. You can give your developers a choice \u2013 they can fall on the sword of X.509 if they\u2019d like, or use OAuth.\n \n In the previous diagram the gateway handles the OAuth dance for incoming clients and maps the identity to a username from Active Directory. Then, the gateway initiates a mutual SSL connection to the Enterprise REST API and sends the username of the original requestor over this secure channel in an HTTP header. Simple.\n Developers are happy because the Enterprise is now \u201cliving in the 21st century\u201d and the\u00a0 enterprise application developers are happy because they don\u2019t need to\u00a0 retrofit something that already works, and the security architects are happy because they can manage it all on a security gateway with a few clicks and drop downs. Party down!\n A thumbnail screen shot is shown from the \u201cAAA\u201d Policy from Intel\u00ae Expressway Service Gateway:\n \n While there are a few more settings required, particularly to enable the full OAuth dance, as well as exception handling, the screen shot shows that the main credential mapping can be done with selectors and not code. This helps bridge the gap between Enterprise Applications and new emerging standards. At Intel we think this gateway or proxy model is a superb answer for solving some of these real challenges. After all, Enterprises can\u2019t wait for the standards to catch up with the new generation of developers.","item_date":"May 15 2012 01:04:54","display_item_date":"05-14-2012","url":"http:\/\/www.sys-con.com\/node\/2275857","source":"www.sys-con.com"},{"title":"Lab Soft News: Practice Fusion Announces a Laboratory API","details":"The business model for Practice Fusion, a web-based physician office EMR that can be obtained free, has been a source of great interest for me (see: Practice Fusion CEO Calls His Company the Largest EMR Provider;\u00a0Practice Fusion Supported by Advertising and Owns Anonymized Data). A recent press release from the company caught my eye\u00a0(see: Practice Fusion Launches API to Democratize Lab Integrations). Heres an excerpt from it:\n  Practice Fusion, the free Electronic Medical Record (EMR) company,...announced the launch of its new lab API, which allows any laboratory in the country to connect directly to the EMR. The lab API - first of its kind in the health sector - enables rapid deployment of new laboratory connections that would ordinarily take weeks or months to establish, giving over 150,000 medical professionals easy access to their local commercial, hospital and private laboratories....\n  Practice Fusions Lab API allows laboratories to connect to the EMR platform instantly with almost no turnaround time\n  Labs can use the API connection to instantly send lab results to Practice Fusion users with standard HL7 data files\n  Any commercial laboratory - including clinical, pathology, reference and hospital systems - can send lab results to users directly within the EMR\n  Minimal development is required from the lab and Practice Fusion offers free integration support\n  Over 150,000 medical providers using the EMR can connect to their national and regional labs of choice at no cost\n  \nAs many readers of this blog will already know,\u00a0an application programming interface (API) is a specification for an interface that can be used for communication between two computer systems. The large national reference labs like Quest and Lab Corp have large IT staffs and can quickly churn out interfaces to the most popular office EMRs. Quest, for example, also sells a web-enabled office EMR called Care360. By developing and providing an API, Practice Fusion is facilitating interface development for the smaller regional and esoteric labs, which can then transmit test results to the Practice Fusion\u00a0EMR for access by physician office clients. If this is, indeed, the first lab API in the health sector, its a valuable step and a long time in coming. Its also evidence of the high value that clinicians place on access to test results in their EMRs.\n  I was also fascinated by the choice of words in the press release. The API is said to be a democratizing step. This caught me by surprise -- the use of a political vocabulary with reference to IT. Its possible to chalk this up to an overly enthusiastic ad agency. However, there may also be deeper meaning to it. We live in a era of Big Medicine (see: Physician Private Practice Declines; the Last Barrier to Emergence of Big Medicine;\u00a0Hospitals Use Their Medical Schools, Residencies for Later Physician Recruitment;\u00a0FTC Approves Merger of Express Scripts and Medco). Enabling office-based physicians to rapidly access lab test results could serve as one small antidote to this phenomenon.","item_date":"May 15 2012 01:00:10","display_item_date":"05-14-2012","url":"http:\/\/labsoftnews.typepad.com\/lab_soft_news\/2012\/05\/practice-fusion-.html","source":"labsoftnews.typepad.com"},{"title":"On The Issue Of Standardization Around AWS APIs","details":"I am an vocal opponent of the idea of standardization around AWS API *at this point of time*. I think that it is too early to standardize and too risky especially when Amazon has not released the APIs under one of the open licenses like Creative Commons. Stephen O\u2019Grady from Redmonk highlights the second part very well in his blog post and I thought it is worth sharing in this space.\n That said, it\u2019s worth noting that many large entities are already behaving as if APIs are in fact copyrightable. The most obvious indication of this is Amazon. Most large vendors we have spoken with consider Amazon\u2019s APIs a non-starter, given the legal uncertainties regarding the intellectual property involved. Vendors may in certain cases be willing to outsource that risk to a smaller third party \u2013 particularly one that\u2019s explicitly licensed like a Eucalyptus [coverage]. But in general the low risk strategy for them has been to assume that Amazon would or could leverage their intellectual property rights \u2013 copyright or otherwise \u2013 around the APIs in question, and to avoid them as a result. Amazon, while having declined to assert itself directly on this basis, has also done nothing to discourage the perception that it has strict control of usage of its APIs. In doing so, it has effectively turned licensed access to the APIs into a negotiable asset, presumably an outcome that advocates of copyrightable APIs would like to see made common.\n I think it is time to not repeat the mistakes of the past and standardize around a proprietary technology. As I told before AWS-Eucalyptus deal happened because AWS is weak on the enterprise market and anything that can bolster their credentials there will be ok for Amazon now. However, AWS may not be so \u201cfriendly\u201d on the service provider side because they don\u2019t want federation (a.k.a competition). Any arguments in favor of standardization around AWS APIs at this point of time is short sighted at best. #endsaturdayrant","item_date":"May 15 2012 00:07:05","display_item_date":"05-14-2012","url":"http:\/\/www.cloudave.com\/19602\/on-the-issue-of-standardization-around-aws-apis\/","source":"www.cloudave.com"},{"title":"The Boston Cleanweb Hackathon Rocked","details":"I was absolutely blown away by how creative and productive the hackers were at this weekends cleanweb hackathon here in Boston.\n  \tAs a recap: Around 15 teams of programmers and other hackers came together at Greentown Labs to spend 30 or so hours creating new cleantech-related web and mobile apps from scratch. Most came in with ideas in mind, of course, but they had to write fresh code to compete. Many worked through the night Saturday, and then, bleary-eyed and over-caffeinated, they presented their work on Sunday afternoon to a panel of entrepreneur and investor judges. They were all scored on the categories of Impact, Originality, User Experience, and Completeness.\n  \tI frankly wasnt expecting much, given such a short amount of time available to the teams. And yes, some were necessarily just a mock-up and some teams got a bit tongue-tied in their presentations. But overall, it was an amazing showing by all these teams -- one of those events where you can only have three winners and you wish prizes could be awarded to a lot more teams than that.\n  \tThe competitors included:\n  \t\tA mapping program tied to available wind turbine sound data and wind level data to help smaller developers estimate the sound levels over ambient from a wind turbine at a particular site\n \t \t\tA mobile app to allow green-minded travelers to easily find green-rated hotels and restaurants\n \t \t\tA site where registered viewers can review political ads (and this year, energy will play a big role in that) and vote on whether a claim is a lie or the truth\n \t \t\tA cool presentation of available data on fracking activity matched with water data\n \t \t\tA remote diagnostic and energy optimization tool for greenhouses\n \t \t\tA couple of tools for helping homeowners identify energy-saving appliances\n \t \t\tA tool for helping energy auditors more easily look up the energy consumption of devices they inventory in a building\n \n \tThere was also a host of other entries. As you can see, there was a wide range of topics, speaking to the breadth of clean IT opportunities.\n  \tFor me, what was striking was how far these teams were able to push their ideas in only a short amount of time. Yes, this is totally an apples-to-oranges comparison, but some of these teams were able to push their ideas to more customer-ready results in 30 hours than many more science-based cleantech startups are able to tangibly demonstrate in 30 months. Thats not to diminish the more hardcore scientific efforts out there at all, but it does speak to how powerful the combination of cleantech and IT can be in some ways.\n  \tIn any case, the winners came up with some fun stuff:\n  \tThird place went to a team from Divya Energy that is developing an online comparison shopping calculator for residential solar customers.\n  \tSecond place went to a car-sharing app (Ride With Me) that creatively included a customer loyalty program to entice repeat users.\n  \tFirst place went to a team from WegoWise for their very fun, very clever way of bringing better visualization and competitiveness to get homeowners to focus on energy usage. Using Green Button data, your houses stats are randomly matched up against another users house, and then you duke it out -- with awesome 1990s-esque video game graphics. Michael Tysons Punch House was also voted the crowd favorite. You can see the happy team, plus a couple of the volunteers who made the whole event happen, in the obligatory giant check picture below.\n  \t\n  \tSpeaking of which, many thanks to the great team of volunteers for making this terrific event happen, mostly from scratch. I cant name them all (but will list some of their Twitter handles below; please follow them), but want to particularly call out Matt Liebhold\u00a0(currently independent) and Jason Hanna\u00a0(Coincident). Based on the scrappy execution they demonstrated in pulling this off and bringing the community together like this, you could do worse than to get to know them and work with them in some way. Yes, consider this a recommendation.\n  \tOthers to thank \/ follow: @blakestar, @greentownlabs, @tallmatt, @matthewnordan, @markvasu, @fyietc, @dkarmano, @emilylreichert, and @CleanSuchi\u00a0(if I missed any of you, ping me and Ill gladly add you in).\n  \tThese cleanweb hackathons are getting rolled out all over, apparently, so find one near you and check it out. Not only was it a lot of fun to watch, it inspired a lot of ideas for directions Id like to take with my Lightzy.com experiment, if I can enlist some more expert help. Heres to the next one in Boston!\n  \t\t\t  \t \t\n \t \t\tRob Day is a Partner with Black Coral Capital, based in Boston.\u00a0 He has been a cleantech private equity investor since 2004, and acts or has served as a Director, Observer and advisory board member to multiple companies in the energy tech and related sectors.\u00a0 Rob was a co-founder of the Renewable Energy Business Network (www.rebn.org), a non-profit organization which was acquired in 2009 by the Clean Economy Network.\u00a0 The views expressed on this blog are those of Rob, not necessarily the views of any of his colleagues and affiliated organizations.\u00a0Contact Rob at .(JavaScript must be enabled to view this email address) .","item_date":"May 14 2012 23:59:54","display_item_date":"05-14-2012","url":"http:\/\/www.greentechmedia.com\/cleantech-investing\/post\/the-boston-cleanweb-hackathon-rocked\/","source":"www.greentechmedia.com"},{"title":"Facebook's IPO: The social business implications","details":"It\u2019s at the intersection of companies that wish to engage with Facebook\u2019s audience, and the Facebook platform itself, where the vast majority of the business opportunity lies. It\u2019s likely \u2014 though not yet a certainty \u2014 that the world\u2019s largest and most well-known social network will finally go public in the coming week. With over 900 million users, Facebook is by any measure a tremendous online success story. A recent summary of the IPO data points from Reuters says it all: The social media juggernaut has some of the highest levels of user engagement in Internet history (483 million daily active users), a decent revenue stream of $3.7 billion in 2011 (85% of which is advertising dependent), and its net revenue skyrocketed 65% last year.\n\n\nThe move to open ownership of the company to the world at large is almost certainly going to be a watershed moment in the nascent social media industry.  This is an industry that long last is finally coming into its own after years of rapid growth and finding its way in terms of business models, user adoption strategies, and competitive positioning. It\u2019s no accident that Zynga represents about 12% of Facebook\u2019s revenue: Social games have become a substantial trend and it was smart for Facebook to ensure it had a stake in this hot new area of social media.\n\n\nRelated: Enterprise gamification: Will it drive better business performance?\n\n\nThere\u2019s little question that the timing itself is good: With the stock market coming back, the acceptable performance of other social media IPOs (LinkedIn and Jive Software have done well, or at least not lost value since their debut), the undoubtedly intense pressure by investors and vested workers to get their payday, and the need to capitalize growth and new products to maximize their potential and fend off competitors, all points to one thing: Going public is really the logical next step.\n\n\n\n\n\nBut going public brings with it all new pressures. For venture backed companies such as Facebook, it means a difficult transition from the multi-year expectation on delivering returns, to the quarterly results horizon. Even in the extremely fast moving world of Internet companies, this can come as a shock to the system. Facebook\u2019s business simply must perform and perform well every quarter for the foreseeable future, especially with a high opening stock price. And this is where its business model will face a major test to live up to what most financial experts are saying is a lofty estimate of its valuation, nearly $100 billion on the high end by many sources.\n\n\nBut ultimately, as has been pointed out, if a service is entirely free to an audience, then that audience is the product. Not only is Facebook free to its users, there isn\u2019t even a \u201cpro\u201d version, like LinkedIn offers. When the size and health of an ecosystem (i.e. network effect) is so central to digital success of any kind these days, any barrier to growth and reach is considered anathema. In other words, Facebook feels it must go elsewhere, and not its users, to make money.  This in turn, will have significant implications to the second industry growing up next to social media: That of the world of social business, which aims to situate social media \u2014 adapted specifically to enterprise needs \u2014 to improve the way businesses operate inside and outside their walls.\n\n\nAs Derek Harris wrote on Gigaom this morning, Facebook must make the most of the data it has about its users while walking an incredibly fine line in terms of privacy and trust.  The more Facebook exploits its invaluable datasets on user behavior, wants, and desires, the less users will be inclined to divulge such information, eroding the inherent value of its social network.  The fear is, because its intrinsic value is really under the control of its audience (namely us), it\u2019s only a few serious missteps away from a participative collapse or end-user backlash, which would spell significant trouble for the company valuation. Today\u2019s business world is rife with examples of social media fueled tempests that have had long term impact to their brands. Facebook\u2019s very business model is based on an approach that has high probably in creating such a backlash in my opinion, creating significant long-term challenges to real growth.\n\n\n\n\n\nBut there\u2019s also little doubt that Facebook\u2019s management team is acutely aware of all of this and has prepared the company for all of these issues and the transition to becoming a public company. Or at least, for their investors\u2019 sake, I hope that\u2019s the case. Instead, what\u2019s more interesting are the implications for organization that are trying to use the social networking giant to better engage with their customers, workers, and suppliers. Facebook pages and Facebook apps are now extremely popular and cost-effective of ways of going to where people are today and engaging with them for marketing, sales, informational, user support, and other community-powered functions.  In fact, it\u2019s at the intersection of companies that wish to engage with Facebook\u2019s audience, and the Facebook platform itself, where the vast majority of the business opportunity lies.  With all but 15% of its revenue coming from advertising and with all eyes on its revenue growth, Facebook must continue to rapidly innovate in helping businesses better engage with their various constituents inside of the company\u2019s industry-leading social network.\n\n\nRelated: Facebook IPO demand is strong and weak\n\n\nThis pressure to perform, combined with untold as-yet-untapped opportunities to make the most out of nearly a billion socially connected users \u2014 is where the most interesting social business implications of Facebook\u2019s transition to a public company exist.","item_date":"May 14 2012 23:55:52","display_item_date":"05-14-2012","url":"http:\/\/www.zdnet.com\/blog\/hinchcliffe\/facebooks-ipo-the-social-business-implications\/2066","source":"www.zdnet.com"},{"title":"Judge Decides Oracle and Google Will Battle Over Damages","details":"In the ongoing legal battle between Oracle and Google over the Android mobile operating system, the presiding judge has ruled that the trial will proceed to a damages phase \u2014 a third and final phase where a jury will decide whether Google must pay Oracle for infringing on its copyrights and patents. \n The jury was unable to reach a verdict on whether Google\u2019s use of 37 Java application program interfaces (API) went beyond \u201cfair use\u201d of Oracle\u2019s copyrights. But it did decide that Google infringed on the company\u2019s copyrights in lifting nine lines of code involving a programming method called rangeCheck. And on Friday, Judge William Alsup ruled that Google also infringed in lifting code from eight decompiled Oracle files.\n The jury has yet to decide whether Google infringed on two Oracle patents in building a new version of the Java platform for Android. But Alsup indicates that even if the jury finds in favor of Oracle here, the software company is unlikely to receive large damages from Google.\n Though Alsup said that Oracle was entitled to argue for damages in front of a jury, he frowned on Oracle\u2019s effort to win damages above and beyond the standard \u201cstatutory damages,\u201d saying the copyright decisions against Google did not warrant this. \u201cThere is no way that the law should allow a disgorgement of hundreds of millions of dollars over range check and decompile,\u201d he said, calling the proposition \u201chyper extreme.\u201d\n The trial kicked off on April 16 with the copyright phase of the trial, which addressed Oracle\u2019s copyright claims, and then it continued into the patent phase, which is ongoing. Last week, Judge Alsup suggested that the two sides forgo a damages phase and let him \u2014 rather than the jury \u2014 decide on damages so that the trial could be streamlined. Google agreed, but Alsup said he needed agreement from both sides to do so, and Oracle declined.\n Oracle sued Google in 2010 after it bought Sun Microsystems, accusing Google of infringing patents and copyrights related to Java, which Sun had developed. Because the jury was unable to reach a decision on the \u201cfair use\u201d question, Google has asked for a mistrial, citing previous Supreme Court law and the Seventh Amendment to the U.S. Constitution, but the judge has yet to rule on the motion.\n Judge Alsup is also set to rule on whether APIs can be copyrighted in the first place, the big question that has hung over the trial.\n \nGot a secret? Email caleb_garling [a] wired.com. Caleb covers tech, but loves other stuff like sports, fiction, beer, fun in remote places and music featuring guitars. Encircle on Google+, subscribe on Facebook or\nRead more by Caleb Garling \nFollow @calebgarling on Twitter.","item_date":"May 14 2012 23:39:27","display_item_date":"05-14-2012","url":"http:\/\/www.wired.com\/wiredenterprise\/2012\/05\/damages-oracle-google-2\/","source":"www.wired.com"},{"title":"API Guru and Technical Author","details":"Are you an API guru and technical author with outstanding abilities, who is interested in working for a small and dynamic team? If so, we are interested in hearing from you.\n Flexiant develops Cloud Computing Orchestration software for hosting companies, data centre operators, and telecoms providers.\n The company is based in Livingston, Scotland, and provides a competitive salary with private health care, stock options, and benefits.\u00a0The company is based in Livingston, Scotland, and provides a competitive salary with private health care, stock options, and benefits. Salary will depend on experience: we\u2019re open as to whether we hire someone with 2 years\u2019 or 20 years\u2019 experience. As a guideline, we\u2019d expect to be paying between \u00a333,000 and \u00a337,000.\n We\u2019d prefer someone who could work from our offices in Livingston (an easy commute from either Edinburgh or Glasgow), but would consider remote workers who can demonstrate a history of successful remote working on collaborative programming projects, need minimal supervision, and are prepared to visit our offices every couple of weeks so we can remember what you look like.\n This role is unusual: our product is modular and highly configurable, and the modules communicate with each other through APIs. Programmers do not always produce the finest quality documentation, and what they do produce is often not suitable for third parties. What we are looking for is someone who can combine technical authorship capabilities with understanding the code behind it. You will be responsible for:\n Producing our development manuals documenting our APIs;\n Helping to produce the technical aspects of our administration manual covering configuration and installation of the product;\n Understanding how our APIs work and ensuring that they serve customer needs;\n Ensuring that the product manuals (particularly on installation and configuration) remain up to date and relevant whilst the underlying product changes;\n Providing a resource to our support team when they have queries on our APIs; and\n Producing unit tests to ensure our documentation is correct, and provide examples in the manuals.\n \n A successful candidate will be a highly capable technical author, with an excellent command of the English language, and a proven ability to communicate clearly and concisely. The candidate will be very familiar with SOAP and Web Service APIs, and XML, and be happy to read, and occasionally modify Java code.\n The ideal candidate would also have:\n Experience with XML, and maintaining SOAP and Web service APIs;\n A fully working knowledge of Java;\n Experience of working using version control systems on projects where more than one programmer is participating; and\n Familiarity with system administration of Linux at the command line.\n \nThe main thing we need is the ability to take whatever is thrown at you (we\u2019re a small company after all), to work independently, and to produce fantastic documentation from sometimes illegible code, preferably some time before we\u2019ve even thought to suggest you start writing it.\n Interested?\n If so, please download our application pack, which will tell you how to apply.\n Application pack (.pdf 300 KB)\n We are not currently interested in hearing from recruitment agencies, so please do not email us.","item_date":"May 14 2012 23:18:47","display_item_date":"05-14-2012","url":"http:\/\/www.flexiant.com\/2012\/04\/04\/api-guru-and-technical-author\/","source":"www.flexiant.com"},{"title":"CivicPlus Putting Hackathon Ideas to Work to Launch New City Website in Time for Joplin 'Day of Unity'","details":"More than two dozen programmers, content specialists, designers and city officials from CivicPlus and the City of Joplin have worked tirelessly since the late April Hackathon to rebuild the City\u2019s website. Driven by the goal to launch the site by May 18th, four days before the \u201cDay of Unity\u201d which will commemorate the one-year anniversary of the EF-5 tornado that ravaged the city, the project team has already logged nearly 1,000 hours. \n  \u201cTheir cohesive efforts are compressing a project that normally would take four -to -six months into less than three weeks,\u201d said Michael Ashford, CivicPlus community engagement evangelist. \u201cIt\u2019s another example of the \u2019Miracle of the Human Spirit\u2019 that 135,000 volunteers have shown as we each do our part to rebuild Joplin.\u201d\u00a0\u00a0\u00a0\u00a0 \n  Hackathon Judge and GovLoop.com Founder Steve Ressler blogged about the lessons of \u201chacking for good\u201d that came out of the Joplin Hackathon. \u201cThe impact of hackathons can be huge,\u201d Ressler wrote. \u201cBuilding technology infrastructure that helps the city is just as an important volunteer task as handing out food or rebuilding homes. A modern communication infrastructure isn\u2019t just nice to have, but essential.\u201d \n  The new site will include features to increase community engagement and enhance emergency management. The City of Joplin will utilize CivicPlus Emergency Alerts and Alert Center Modules, which provide an alert bar at the top of each website page to inform residents of active emergencies, as well as a permanent section on the homepage to notify citizens of information pertaining to an active alert.\n  Joplin residents also will be able to sign up to receive a wide variety of non-emergency alerts and updates from the City via text or email including project bid opportunities, job postings, calendar and news items.\n  Ashford said another feature, Citizen Request Tracker, will facilitate two-way communication between the City and its residents. \u201cCitizen Request Tracker gives residents a convenient way to alert the City of any issues or concerns,\u201d he said. \u201cWhether reporting streetlight outages, potholes or neighborhood issues, once an account is created, this interactive feature will offer residents easy access to the city official assigned to handle that particular topic.\u201d\n  In addition, a \u201cJoplin\u2019s Voice\u201d button will give residents an open forum for dialogue on topics that matter to them. Ideas and opinions can be voted up or down, and the City has a chance to enact ideas based on feedback.\n  So much is being done in so little time, said City of Joplin Information Systems Director Mark Morris. All eyes will be on Joplin on May 22nd for the anniversary of the EF5 tornado.  We will go live with the new website a few days before on May 18th.  Weve been maintaining seven separate websites housing all this content... and now, well be able to house it all within one website and empower various departments to manage their respective areas even better than before.  It gives me hope.\n  About CivicPlus  \nBased in Manhattan, KS, CivicPlus has designed more than 1,100 local government websites serving 43 million citizens throughout North America. A recipient of the Center for Digital Government\u2019s Best Fit Integrator Award for delivering extraordinary digital solutions to public IT projects, CivicPlus transforms municipal websites into powerful two-way communication platforms that let citizens talk to and participate in local government. The digital community engagement scale is a free online evaluation tool developed by CivicPlus to help local government measure the effectiveness of their website. In 2012, CivicPlus will invest more than $2 million in products, research and development. The innovative CivicPlus Government Content Management System\u2122 now offers more than 60 applications including Community Voice, Citizen Request Tracker\u2122, MuniMobile\u2122, Facebook and economic development tools. Founded in 2001, CivicPlus was selected by Inc. Magazine as \u201cOne of the Fastest-Growing Privately Held Companies in the U.S.\u201d in 2011. For more information visit civicplus.com.","item_date":"May 14 2012 23:18:26","display_item_date":"05-14-2012","url":"http:\/\/www.prweb.com\/releases\/2012\/5\/prweb9505717.htm","source":"www.prweb.com"},{"title":"A Series: Practical API Development","details":"When you first set out to build your API, the number of factors to consider may be overwhelming.\u00a0 Which functions do you need to expose to your clients?\u00a0 Which should remain hidden?\u00a0 What if you change your mind in the future?\u00a0 Can your servers handle all of that additional load?\u00a0 The list of questions goes on and on, and some of the trickiest issues may not apparent until you are stuck in the middle of them.\u00a0 Fortunately, there are several existing patterns you can follow to address some of the most common concerns.\n In this series, we outline the practical considerations when building an API.\n APIs are now an essential part of building business. \u00a0Through a network of APIs, companies can more rapidly develop new applications and products and bring them to market more quickly. \n APIs or Web Services are to web applications what interfaces are to classes in object oriented programming. \u00a0Versioning is an important component in the API lifecycle.\n Versioning Overview\n As web services continue to proliferate as more and more companies begin to offer APIs to their services, the importance of proper versioning and continued compatibility for their early adopters becomes more and more important. A successful and easily maintainable versioning strategy can mean the difference between happy customers and productive developers or, if executed poorly, a technical disaster.\n An API is a Contract\n Once it is live, you cant change it. More specifically, once you have customers, any changes you make to your API will increase the development costs for your customers. Renaming that endpoint thats been bugging your developers internally might mean hours of changes for each of your customers and possible downtime if you dont have a system for notifying them, staging changes, and coordinating releases. If you have hundreds of clients, coordinating a release can be daunting if not impossible.\n Though you cannot change the API while it is live, you can count on the fact that APIs do change. Plan accordingly. As soon as you release your API, a new use case that you did not account for will pop up and your customers will want you to add support for it. Or, perhaps a competing service offers some must have new feature that you simply cant ignore and you will be forced to add support for it. A critical bug will be discovered that puts the security of your backend and risk and youll be forced to release a patch to your API. No matter how well you plan, you will have to change your API at some point. If you plan for changes and have a process to implement them, you will have happier customers and ultimately provide a better product.\n Backwards Compatibility\n \ufffcOne of the most critical features for any API is backwards compatibility. Achieving this is a lot harder than it sounds. Good products are constantly improved and refactored to ensure they keep pace with the market. As developers we strive for perfection and cant resist rewriting a bit of ugly code or fixing that embarrassing spelling mistake that somehow slipped through QA. Oftentimes, the pressure to bring a product or feature to market quickly will force us to release unfinished or rough code that isnt as intuitive as it could be or just doesnt work as expected.\n The best way to ensure your changes arent going to break existing applications, is to implement a test client or sample app that you can use as a regression testing tool for your API developers.\n This should fit right in with your TDD and agile methodologies. As an added benefit you can use the sample app for demonstration purposes for potential customers to learn your API.\n Once youve released an API, the functionally is essentially locked. You can make changes, but only changes that maintain backwards compatibility. Examples of acceptable changes are things like adding a new endpoint, adding a new property to a response document, or adding new parameters to an API call to filter or constrain results differently. There are times when developers have written clients that do not handle new document formats effectively, so only endpoint additions are truly safe in all cases.\n When Not To Version\n Here are the types of non-destructive changes you can make without versioning your API:\n   1) Renaming endpoints - This change can be non-destructive if you proxy the previous resource location to the new one. This will add overhead to your operations department when it comes time to deploy or scale your API, so be careful with this one as it is likely to break at some point.\n   2) Adding endpoints. You can add new endpoints without breaking anything provided that these endpoints are not required for prior API clients (for example, if you started requiring a login endpoint to be called before a data retrieval endpoint that already existed, this would break the functionality for existing clients). Be careful using this technique to add new versions of existing endpoints as this practice can become unwieldy and confusing to your customers.\n   3) Renaming or Adding parameters. You can safely add new parameters to your API as long as you arent changing the existing behavior of your API. New parameters could provide new ways of filtering or sorting your data (sorting and filtering as convenience methods or provided when it would be impractical to handle on the client, i.e.: too much data, filters on proprietary information or dynamic data, etc). Renaming parameters is non-destructive as long as you continue to support the old parameter names. It should be simple to alias parameters in your code.\n Additionally, it is safe to make document format changes if you are positive there are no existing clients that will break when you change the format of your document. Lets say your Customer object provides first name and last name as distinct properties, but youve started collecting nicknames and want to add this to your API. Adding new fields or properties to your response documents is generally a safe way to introduce new data into our API. Existing customers will simply ignore the new fields (assuming theyve coded defensively) and can make use of them at their leisure. One caveat I would mention here is that your API should never contain formatting. Leave content formatting to the client application and only provide raw data through your API.\n \ufffcIf you want to be extra safe when changing your document, it is a good idea to add metadata to the document itself indicating the version of the document. This can (and should) be different from the version of the API; document version indicates the version of the document schema, whereas API version indicates the version of the request methods (changes to endpoints, functionality, etc.). This allows your clients to pass in a request parameter indicating which version of the document they want if they wish to continue using your current API version but arent able to make use of the new document format.\n In the next installment we will dive into the specific strategies you can use to add versioning information to your API, as well as an example that shows how you can put these strategies into practice.\n Shane Holland is a Managing Partner at Above Labs where he and his team help startups and enterprise to develop great products and platforms.  Shane has been developing and working with APIs for 15 years.","item_date":"May 14 2012 23:18:21","display_item_date":"05-14-2012","url":"http:\/\/blog.mashery.com\/content\/series-practical-api-development","source":"blog.mashery.com"},{"title":"Expand Fair Use: So what is an API, anyway? And why is Google so evil for copying one?","details":"The only decent solution to the current copyright mess on the internet is the dramatic expansion of fair use rights to include all non-commercial use.\n\nSo what is an API, anyway?  And why is Google so evil for copying one? \n\n(Woops, I missed a day. \u00a0I was travelling over the weekend to see Mom and I guess I lost track.\n\nIll die before it happens again.)\n\n In the news lately, Oracle has been suing Google for copying its Java APIs in Android. \u00a0For 99.9% of people, this raises the question, what the hell is an API? \u00a0Thats a good question, and Im going to show you, because you cant really think about something if you dont know what it is.\n\n API stands for Application Programming Interface. \u00a0Here is the first paragraph from that Wikipedia page, which wont mean anything to you:\n\nAn application programming interface (API) is a specification intended to be used as an interface by software components to communicate with each other. An API may include specifications for routines, data structures, object classes, and variables. An API specification can take many forms, including an International Standard such as POSIX or vendor documentation such as the Microsoft Windows API, or the libraries of a programming language, e.g. Standard Template Library in C++ or Java API.\n\n Now, Im going to show you what that really means, and hopefully that will give you some vague idea of what the above is all about.\n\n Have you heard of RSS or Atom feeds? \u00a0It stands for Really Simple Syndication, and it is a kind of API. \u00a0This blog (along with all Blogger blogs) has an RSS feed, which is simply another version of this website delivered via...an API. \n\n If you want to look at it for this blog, just go to:\n\n You may note the rss as the end of the URL. \u00a0This is Googles way of saying give me the RSS version of this blog (Google owns Blogger, the service I use to host this blog).\n\n If you go to that page, it will be rendered in various different ways, depending on your browser. \u00a0Unless you are using Chrome, it will look like a greatly simplified version of this blog. \n\n View the source of the page. \u00a0If you did it yesterday, you will see something that looks like this:\n\n<?xml version=1.0 encoding=UTF-8?><rss xmlns:atom=http:\/\/www.w3.org\/2005\/Atom xmlns:openSearch=http:\/\/a9.com\/-\/spec\/opensearchrss\/1.0\/ xmlns:georss=http:\/\/www.georss.org\/georss xmlns:gd=http:\/\/schemas.google.com\/g\/2005 xmlns:thr=http:\/\/purl.org\/syndication\/thread\/1.0 version=2.0><channel><atom:id>tag:blogger.com,1999:blog-4061227211666562229<\/atom:id><lastBuildDate>Fri, 11 May 2012 03:13:37 +0000<\/lastBuildDate><title>Expand Fair Use<\/title><description>The only decent solution to the current copyright mess on the internet is the dramatic expansion of fair use rights to include all non-commercial use.<\/description>\n\n etc. etc. etc. \u00a0Here, let me format this in a friendly way for you, and get rid of some of the cruft:\n\n<title>Expand Fair Use<\/title>\n\n<description>The only decent solution to the current copyright mess on the internet is the dramatic expansion of fair use rights to include all non-commercial use.<\/description>\n\n<description>Torrentfreak has an interesting analysis of the effect of movie piracy on the new Avengers movie. Their conclusion, unsurprisingly, is...\n\n Can you dig it? \u00a0Trust me, if you view the source on this page, youll find some dramatically more complicated stuff. \u00a0This is a simplified version done to an existing standard called RSS. \u00a0What you see there is a kind of API. \u00a0It is a very simple, well defined way of delivering information, so that programmers can easily read or parse the information in meaningful way.\n\n Everything is carefully tagged--for example the <managingEditor> tag is followed by some information (my name and email (kinda..thats not a real address, obviously)) followed by the closing tag with the \/ in it: <\/managingEditor>. \n\n These are XML tags, which are a superset of HTML tags, if you are familiar with those.\n\n So, for example, if I want to write a program that reads blogs and prints them out, I could have code like:","item_date":"May 14 2012 17:07:30","display_item_date":"05-14-2012","url":"http:\/\/expandfairuse.blogspot.com\/2012\/05\/so-what-is-api-anyway-and-why-is-google.html","source":"expandfairuse.blogspot.com"},{"title":"Infield Healths builds mobile health solutions with Twilio SMS","details":"Can a text message a day keep patients on-track with health programs? Three years ago Doug Naegele, president of Infield Health, decided to find out.\n\nInfield Health uses mobile technologies like SMS messaging and secure mobile web to help people adhere to programs with a certain health goal in mind. By delivering information straight to mobile phones, the company helps people better manage their health between visits to the doctor. The results, according to Infield, are better outcomes for patients and decreased costs for providers.\n\nThe company\u2019s emphasis on mobile solutions meant finding the right telecom partner was crucial for its success. Rather than hire a developer with expertise in esoteric protocols and complex technologies, Infield chose to leverage the power of cloud communications.\n\nCloud communications platforms use the Internet to provide voice telephony services and text messaging capabilities to web and mobile applications. Not only does cloud communications simplify the development of telecommunications apps, it also ensures those apps can instantly scale to tens of thousands of users, or more.\n\nNaegele recalled the first time he heard about Twilio: \u201cI was in the office of the chief technology officer for the DC city government. There were two young programmers working on a text messaging project for parking tickets or something like that, and they were using Twilio.\u201d\n\nBack at home, Infield\u2019s IT pros set up a Twilio account and started playing with the Twilio SMS API. They were immediately struck by its power and ease of use. \u201cWe can do in a day what we thought would take a month!\u201d remarked one Infield developer.\n\nBesides a highly reliable, scalable and secure platform, Twilio offered one big advantage to a growing company like Infield Health: It made it easy to prototype SMS applications on standard ten-digit phone numbers and move them to short codes after they were approved by a client\u2014without any changes to the app.\n\nCarriers require organizations that send large volumes of text messages to use short codes\u2014five or six digit numbers that can send SMS messages at a rate of 30 texts per second. A random short code costs $3,000 for a three-month lease, and a vanity short code costs $4,500.\n\nNaegele said it took about four months to build and test Infield Health\u2019s first product: InfieldCMS. All Infield clients now use that content management system, built specifically for SMS and mobile web content. \u201cBecause we didn\u2019t have to spend $50,000 integrating to a custom telecom SMS back-end with an arduous annual contract, we were able to build InfieldCMS without a first customer, then show a completed product to potential customers. As a start-up, speed-to-product was absolutely crucial,\u201d said Naegele.\nOne beneficiary of the Infield\/Twilio linkage is the American College of Cardiology. Infield helped ACC develop CardioSmartTXT, an app that promotes heart-healthy living. \u201cMost of ACC\u2019s heart-healthy outreach went from idea to testing to production-ready in under two weeks. Twilio\u2019s flexibility makes that happen,\u201d Naegele added.\nNaegele said Twilio makes it possible for his team to \u201chave an idea on Monday and start testing it by Wednesday.\u201d This applies not only to features, but to entirely new apps. \u201cWhen we schedule a meeting with a new prospective client, we are able to build a fully-functioning demo app and show it to them on the first day. Leading with software that works enables us to compete with firms much larger than ours,\u201d he said.\n\nOne project that Infield Health built for WellCall, an employee wellness firm, integrates with Salesforce.com. If an employee needs a health coach, he or she simply sends a text with the word \u201ccoach\u201d and an approved coach gets a message in Salesforce to call the employee back within one business day. \u201cBy matching a live health coach to a client who needs advice immediately, and to put it all in motion with a simple SMS, we keep our users on-track with their health goals without missing a step,\u201d said Kerry Bradley Sylvester, director of coaching services for WellCall.\n\nOther apps launched by Infield Health include a text message add-on to FirstBorn, which supports new mothers before and after delivery. \u201cWith Infield, FirstBorn counselors deliver healthy challenges (with incentives), patient follow-up content, and appointment reminders right to mom\u2019s mobile phone,\u201d said Dr. Miguel Tirado, who works with the Center for Telehealth and Cybermedicine Research at the University of New Mexico in Albuquerque, NM and the Ben Lujan Institute at New Mexico Highlands University.\nOver the past year, data from Infield\u2019s current campaigns suggest a 50 percent improvement in self-reported outcomes versus users who didn\u2019t receive health messages. This summer, Virginia Commonwealth University will follow two Infield projects and strictly measure results.\n\n\u201cI love the idea of delivering health advice straight to someone\u2019s mobile phone. Regardless of income, it\u2019s the one device everyone has and everyone uses,\u201d Naegele said.","item_date":"May 11 2012 17:05:50","display_item_date":"05-11-2012","url":"http:\/\/www.twilio.com\/blog\/2012\/05\/mobile-health-solutions-twilio-sms-api.html","source":"www.twilio.com"},{"title":"Twitter + GitHub = TwUI ","details":"GitHub for Mac is powered by TwUI, Twitters open source, Core Animation-based UI framework for Mac. Its the same framework that drives Twitter for Mac. TwUI lets us create fast, animatable UIs using modern APIs. Its fantastic.\n\nWeve made a lot of fixes and additions in our TwUI fork:\n\nWeve been working with the fine folks at Twitter to figure out the best way for us all to make TwUI more amazing.\n\nThey invited us to join forces.\n\nWhat does it mean?\nTwUI has a bright and glorious future.\n\nAll of our changes are now in the original TwUI repository. That includes a lot of bug fixes, HiDPI support, and our sweet new popover:\n\nWere excited to work with Twitter to make TwUI even more awesome.","item_date":"May 11 2012 07:00:00","display_item_date":"05-11-2012","url":"https:\/\/github.com\/blog\/1133-twitter-github-twui","source":"github.com"},{"title":"US Government Has More \"Big Data\" Than It Knows What to Do With","details":"Its a valid question: Why has all the data the government has been collecting turned out to be too big to handle? The results of a U.S. and state government IT survey released this week by the public sector IT community MeriTalk sheds a bright, halogen spotlight on the answer: Its because its being collected in an unfiltered format and is waiting for someone - anyone - to claim it and write viable applications for it.\n \t\t  \t \n             \t\t\t\t\t\t\tIf youve followed along with RWWs expanding coverage of technology trends in the public sector, youll recall  we take an occasional look at the MeriTalk survey, which is an ongoing dialogue with IT professionals in the U.S. federal government. In a report released Monday summarizing the views of 151 respondents, only 60% say they capture big data to any degree.\u00a0All respondents in that segment said they then analyze the data, but when asked to qualify what that meant, fewer than half (49%, or about 44 respondents) said they extrapolate any meaningful trends from it. Some 28% of respondents collecting big data say they dont do any collaboration with other agencies whatsoever to discover what it all means.\n  But that may not be the whole picture, as 31% of the IT professionals polled whose agencies are affiliated with the Department of Defense or with intelligence (about 23 in all) say they cannot even discuss the subject of big data. Among all agencies polled, approximately 52% say they are in the learning stage about what they can actually do with all this data. But when asked how long this phase should be expected to last, the average response was three years!\n  What will happen to government big data during that time? MeriTalks calculations conclude that the agencies it polled currently store some 1.61 petabytes (quadrillion bytes) of total data, both structured and unstructured. (MeriTalk estimates that 31% of the data collected by agencies polled is actually unstructured.) In an astounding conclusion based on its respondents estimates, MeriTalk predicts that the rate of expansion of public-sector data stores is about 0.51 petabytes per year. So by the time these agencies have completed their assessment of what to do with all the data they have collected throughout their history, the size of those data stores will have nearly doubled.\n   \t\t  \t\t\t\t\t  \t\t     \t\t\t                       \t\t\t\t\t\t\n  Based on the rough conversion table provided by WhatsAByte.com, we calculated that if each individual agency polled was to store all the data its currently collecting on paper, the (very) old-fashioned way, it would need to requisition at least 20 four-drawer filing cabinets every minute of every day simply to contain it all. When asked to select the top three challenges to being able to manage all this data from a long list, 40% of respondents checked storage capacity, 36% being able to distribute and share it, 35% being able to query the data in any form and 34% said having enough time to actually process the results.\n  Of those who could accurately estimate their current big data storage capacities, some 57% said it was already too late: The infrastructure is not in place for them to be able to work with what they have, and that includes cloud capacity.\n  What appears to have happened is this: During the past few years, once it became feasible for government agencies to begin amassing data from Internet-based sources rather than through direct collection or communication, they deployed the first generation of big-data tools immediately, with the idea that theyd plan for how to use it later. The result was like the plan for how to pay for the first wave of tax credits in 2001: Well do it now, and figure out how to pay for it when the time comes. The trick, of course, is to know when that time comes, but now more than half of federal IT workers speaking with MeriTalk say that time has already passed.\n  Lead image courtesy of Shutterstock.","item_date":"May 10 2012 23:06:41","display_item_date":"05-10-2012","url":"http:\/\/www.readwriteweb.com\/hack\/2012\/05\/us-government-has-more-big-data-than-it-knows-what-to-do-with.php","source":"www.readwriteweb.com"},{"title":"Interactive Documentation \u00ab The Scripted Company Blog","details":"Your new-fangled, mobile-first social network will never get any traction if its UI is sloppy. Pinterest, Instagram and Airbnb have taught us that design is paramount to adoption. But APIs with awful documentation are a dime a dozen.\u00a0An API\u2019s documentation is its human interface, so why make it look like it was written for robots?\u00a0Why can\u2019t API documentation eat its own dogfood?\n The Scripted API Documentation\u00a0is built on the Scripted API. It is\u00a0dynamic and interactive. You can use it to build queries and test them without ever writing a line of code. If you are particularly masochistic, you could even use the API Docs instead of our user interface and walk away with 90% of the site\u2019s functionality.\n Just to prove that concept, let\u2019s have a blog post written using only the Scripted API Docs.\n  \t\t\t\tCreating a Job using the Scripted API Docs0. Sign in or Join.\n 1. Select the industries that your writer should understand.\n 2. Turn off sandboxing.\n 3. Run the query.\n Voila! In 5 days, you\u2019ll have a Blog Post titled 10 Reasons to Post to the Scripted API.\n My hope is that this is the kind of documentation that even non-technical folks can get excited about, but what do you think? Would you share it with your Uncle Karl? If so, tell him to follow me on twitter (@jakekring).\n So without further ado, here are the aforementioned\u00a010 Reasons to Post to the Scripted API\u00a0by Frank Caron (a Scripted Writer):\n 10. Elegant Recommendations For Professional Profiles\n Everyone loves to get recommendations on their online professional profiles, but very few people \u2014 especially those whose recommendations would really count for something \u2014 love writing them. Scripted to the rescue.\n By including a \u201cWrite This Recommendation For Me\u201d button, sites like LinkedIn and Branchout could allow busy managers to jot a few bullet points and have a well-written recommendation penned for them in no time.\n 9. Full-service Cover Letters For Job Portals\n Some of us need those recommendations now more than ever, too. The job market is still rocky, and standing out among the thousands of applicants on popular online job portals can be tough. A cover letter is really the only way to make an impression online, and writing good ones is no small feat.\n Fear not, job seekers. Scripted\u2019s got you covered. By including a \u201cWrite Me A Cover Letter\u201d button*, online job portals can give members a way to get a professionally-written cover letter written just by providing some basic details about their goals, hopes, dreams, and passions.\n 8. Compelling Small Business Copy For Local Listing & Review Sites\n Sites like Yext and Yelp are giving small businesses a chance to play with the big boys these days, but big businesses still have a definitive edge thanks to their massive marketing budgets and heaps of catch phrases, slogans, and jingles.\n What if compelling copy was just a click away? Using the Scripted API, these sites could offer business owners a way to get location or service descriptions that are as SEO-sensitive and effective as they are charming and memorable.\n 7. One-Click Content For Inbound Marketing Suites\n Speaking of marketing, there are plenty of software suites out there that offer businesses of all sizes the ability to manage marketing and lead generation. These suites specialize in automating and streamlining the marketing process as much as possible.\n Why stop at just saving money through process refinements? By integrating the Scripted API, sites like\u00a0Marketo\u00a0and\u00a0HubSpot could offer marketeers the chance to get copy written for any number of different marketing initiatives effortlessly: tweets, blog posts, tag lines, product copy\u2014anything and everything a marketing team needs could be had in no time thanks to our inexhaustible pool of talented writers.\n 6. Ghost-written Posts For Blogging Platforms\n The same idea could also work for blogging platforms, particularly enterprise ones. SEO is a tricky art, but Scripted writers are world-class experts at blending careful SEO with compelling writing. Our writers don\u2019t just wantonly inject keywords into fluff articles at the last second: they write content that readers love reading, setting your brand apart in ways that go beyond just the search result rankings.\n Given that Scripted has a deluge of experience in producing rock-solid ghost-writing for major corporate clients, direct integration with blogging platforms and content management systems is a no-brainer.\n 5. Emails For Email Marketing Platforms\n While we\u2019re solving business problems, why not solve the biggest of them all? Writing emails. Writing emails is a chore and writing marketing emails doubly so: grabbing people\u2019s interest and offering them something of value while avoiding the dreaded wall of text is as much a challenge as avoiding spam filters.\n Charming and carefully-written emails can do wonders, though. By integrating with Scripted, MailChimp\u00a0and Constant Contact could give marketeers an effortless way to create engaging emails that are sensitive not just to junk mail filters but also to readers\u2019 mental filters.\n 4. Effortless Descriptions For Event Planning Sites\n Of course, let\u2019s not forget about the event personnel on marketing teams around the world. Often they have to come up with compelling copy without the luxury of dedicated copywriting staff.\n Event planners would rather focus on planning the event, and we agree. That\u2019s why the Scripted API makes perfect sense for sites like Eventbrite: planners simply pump in some basic event details and a link to their company\u2019s website, and our writers will pump out a Press Release that has the crowd lining up for an event (in an orderly fashion, on time, and following instructions to the letter, mind you).\n 3. Awesome Ads For Classifieds & Auction Sites\n Business clients are great, but we at Scripted never want to forget about the little guy. Online personal classifieds and auction sites can benefit from the same type of marketing-centric love.\n How can classified and auction sites leverage the Scripted API to help the everyperson sell used yoga mats and old Mad Men DVDs? You guessed it: a simple \u201cWrite My Ad\u201d button integrated into the ad listing process could give sellers the chance to get amazing ads written for them.\n 2. Website Copy And Content For Web Page Building Platforms\n Ads aren\u2019t the only thing the little guy may need hand written, mind you. Plenty of little guys and gals are making websites using Weebly and Yola. Given that Scripted already offers content-writing services for websites, a one-click \u201cWrite Me Some Content\u201d button would be a natural fit for these platforms.\n 1. Personal Notes For Flower & Gift Delivery Services\n While long-form and copy writing seems like the natural fit for Scripted\u2019s services, putting feelings to a page with just a few words can be infinitely more difficult. That\u2019s why major greeting card manufacturers can charge top dollar for clever, catchy, and contagious cards.\n Poetry is hard. That\u2019s why Scripted writers diligently read Dr. Seuss and Shakespeare alike. Integration into the checkout process for these sites via the Scripted API could give everyone the chance to have a personal poet who will move your loved one to tears, smiles, laughs, or all of the above.\n * Okay, I admit this one is a bit over the top.\n And lastly, a big thanks to the good folks at Bootstrap for making design easier.","item_date":"May 10 2012 20:56:15","display_item_date":"05-10-2012","url":"http:\/\/blog.scripted.com\/dev\/interactive-documentation\/","source":"blog.scripted.com"},{"title":"Sensis Developer Centre - Yellow Pages API and White Pages API : Zombie App Ideas","details":"We hoped that by invoking a zombie apocalypse, we might inspire a couple of developers to get creative and take part in our first ever bounty challenge. Turns out, we didnt have to worry. With 18 teams signed up so far, its clear you all care very much about protecting the human race from the zombie hordes! Its reassuring to know youve got our backs.\n\nA lot of you seem to be working on a map\/guide application for survivors, so give some thought to how you might distinguish your app from others. It might be a killer UI, it might be some flashy design elements, or it might be that youve mashed SAPI up with some other, unexpected API services. Consider your user needs... is your app meant to help users get ready before the zombies attack? Is it meant to help users during an attack? Or is it meant to help users pick up the pieces when its all over? Each one of those might require a different type of application.\n\nMost importantly, since this is a competition, what can you do to ensure your app stands out?\n\nOur favourite twist so far? Probably the team working on an app FOR Zombies... because - as they said in their submission - Im pretty sure zombies can use apps, so we really shouldnt discriminate. Makes sense to us.\n\nHere are a few more examples of what you guys are working on:\n\nName: Plan9\nDescription: Allow the living to coexist in blissful harmony with the living dead in spite of chants of Brains Yum Yum.\nName: Zombie Survivor (Survivor Zombie?)\nDescription: A tracker for finding nearby supplies for survivors, based on their location and businesses nearby, and based on what they already have. Or... a tracker for finding nearby Brains Yum Yum for those survivors recently-turned-zombie, based on their location and nearby businesses. Im pretty sure zombies can use apps, so we really shouldnt discriminate.\nName: Untitled Zombie App\nDescription: The app will prepare users for the inevitable zombie apocalypse by generating a to-do list based on their location. The list will be populated with nearby businesses that can help with supplies. So while the zombies are all like Brains! yum yum! youll be all like You dont scare me suckers, Ive prepared for this day!\nName: iSurvive\nDescription: This application will help the average joe survive in an undead world. When the dead rise, and they will rise, how will someone know how to survive in this deadly new environment? Through the Sensis API, civilians will be able to stop zombies saying Brains Yum Yum as they chew on their corpses by locating nearby businesses which may be of use to them in their survival. For example, one could locate a chemist for medical supplies, or hardware store for weapons.\nName: Where Iz\nDescription: Cant keep up with killing those zombies? You need Where Iz!!! Find the nearest places to stock up on your essential survival needs. Explore the best hiding spots to get away from the daily grind. Connect with others who need Your Help! What do you think of that Combie the Zombie?..... Brains Yum Yum!\nPlease remember that this bounty closes on 9:00 am Thursday May 24, and your application must be freely and easily accessible to judges (ie. available in an app store, online at a publicly accessible URL, etc.) at that time to qualify. So if you have chosen to develop an iOS app, you really want to be submitting that to the app store soon in case Apple does that Apple thing they do and sits on it for a while.\n\nLet us know how you are going, and give us some insight into what you are building on Twitter using #sapihack. Whatever back-story you choose to share... be it the development process you are undertaking, or your personal philosophies about the undead... your back-story will be taken into consideration when judging. We want to hear from you.","item_date":"May 10 2012 07:00:00","display_item_date":"05-10-2012","url":"http:\/\/developers.sensis.com.au\/blog\/read\/Zombie_App_Ideas","source":"developers.sensis.com.au"},{"title":"Judge Moves to Kill Third Round of Google v. Oracle","details":"As the epic legal battle between Google and Oracle comes to the end of its fourth week, Judge William Alsup has indicated he may eliminate the damages phase of the trial \u2014 the third and final phase designed to decide if Google must pay Oracle for infringing on its copyrights and patents.\n\n\u201cYou ought to find a way to streamline the trial,\u201d Judge Alsup told lawyers for Google and Oracle on Thursday, saying that \u2014 at least where this trial is concerned \u2014 the potential size of the damages Google must pay to Oracle has dwindled to the point where a third phase may not be necessary.\n\nGoogle agreed with Alsup, but it has not officially filed a motion asking him to truncate the trial, and this means the judge is not yet in a position to rule on the matter.\n\nIn 2010, after acquiring Sun Microsystems, the maker of the Java programming language, Oracle sued Google, claiming that the search giant violated Sun copyrights and patents in building a new version of the Java platform for its Android mobile operating system. The trial \u2014 which began on April 16 \u2014 was divided into three phases: one that would address the copyright claims, one for the patent claims, and one for damages.\n\n\nThe copyright phase ended with a partial verdict and the patent phase \u2014 which is ongoing \u2014 has been whittled down to just a pair of patents. On Thursday, Google and Judge Alsup seemed to agree that potential damages in the copyright portion are now so slim that it might make more sense for the judge to simply rule on damages, rather than asking the jury to hear additional arguments from both sides.\n\nOracle lead counsel Mike Jacobs said his team needed to review all the pieces of the trial before deciding if the case should proceed with the damages phase. \u201cWe don\u2019t want to mix pears and apples,\u201d he said.\n\nMeanwhile, the patent phase of the trial rolls on.\n\nThe case has been closely watched because it could affect the way the law treats APIs, or application programming interfaces, used across the software industry. On Monday, the jury ruled that Google had infringed on the overall structure, sequence, and organization of copyrighted material involving 37 APIs used by the Java platform. But it was unable to decide whether Google\u2019s infringement should be considered fair use under he law.\n\nIn building Android, Google created a new version of the Java platform known as the Dalvik virtual machine, and this mimicked the Java APIs, which are essentially a way for Java applications to talk to the platform.\n\nGoogle has argued that according to precedent, the trial can\u2019t proceed without answering the fair use question, and it has asked for a mistrial. But the outcome of the trial \u2014 and the potential for additional trials and appeals \u2014 is still very much in the air.\n\nOracle had filed a motion asking Judge Alsup to rule that Google\u2019s use of Java APIs in the Dalvik virtual machine went beyond \u201cfair use\u201d of Oracle\u2019s copyrights. But he denied the request, saying Google had made a good enough case for fair use during the trial and that Oracle had originally asked a jury to make the decision and it needed to live with that.\n\nOn Thursday, Judge Alsup said because of this, Oracle could claim few damages in the third phase of the trial. The jury did find that Google infringed on Oracle copyrights in lifting nine lines of code from the Java platform, but because there was no ruling on fair use, Oracle can\u2019t claim damages on additional infringement. The case could be retried, however, and it could be appealed.","item_date":"May 10 2012 07:00:00","display_item_date":"05-10-2012","url":"http:\/\/www.wired.com\/wiredenterprise\/2012\/05\/damages-oracle-google\/","source":"www.wired.com"},{"title":"Building a More Resilient Financial System: Are We There Yet?","details":"Almost five years after the onset of the global financial crisis, much has been done to reform the global financial system, but much is still left to accomplish. Comprehensive reform, once agreed to and implemented in full, will have far-reaching implications for the global financial system and the world economy. In a new book, Building a More Resilient Financial Sector, edited by Aditya Narain, Ceyla Pazarbasioglu, and myself, we summarize our views on various reform proposals discussed since 2008, ranging from various regulatory reforms to supervision, too-important-to-fail (TITF) proposals, restricting banks\u2019 size and scope, resolution, and to living wills.\n\nThe International Monetary Fund (IMF), alongside the Bank for International Settlements and Financial Stability Board, has been at the forefront of discussions on shaping the new financial system to reduce the possibility of future crises and limit the consequences if they occur. Current reforms are moving in the right direction towards building a more resilient financial system capable of supporting sustainable economic growth, but many policy choices\u2014both urgent and challenging\u2014still lie ahead. Progress has been made in some areas, including in reforming capital (including for systemically important financial institutions\u2014SIFIs), recognizing the importance of a wider regulatory perimeter to oversee shadow banks, improving supervision, disclosure, and resolution regimes, and addressing incentives for risk-taking. Policymakers put forward some novel ideas, such as living wills and contingent capital (CoCos). But they lagged in implementation in many areas, while disagreeing over others.\n\nThe task is surely not easy. The ongoing crisis led to an unprecedented dislocation in financial markets. It damaged stability and confidence in many advanced financial systems, with major consequences for growth and employment. Policymakers face many setbacks as they ride through uncharted territories created by cracks in the financial architecture. The tough policies needed to reform the system have been further complicated by negative feedback loops that have set in between sovereign and banking risks as growth prospects deteriorate. The threat of disorderly deleveraging further undermines successful implementation of the reforms.\n\nAt the same time, financial stability risks continue to rise as economic and financial uncertainties surrounding the euro area linger. As Jos\u00e9 Vi\u00f1als, the Financial Counsellor of the IMF, noted recently, \u201clasting stability is not yet ensured.\u201d Complementing the key policy steps he highlights to ensure durable stability is to finalize the financial reforms proposed to address the market failures that planted the seeds of the crisis. This is necessary not only to limit the regulatory uncertainty weighing on the financial system and the real economy, but also to provide a yardstick for emerging markets that need to limit a buildup of financial imbalances.\n","item_date":"May 10 2012 07:00:00","display_item_date":"05-10-2012","url":"http:\/\/blogs.worldbank.org\/allaboutfinance\/node\/731","source":"blogs.worldbank.org"},{"title":"AWS re: Invent - Global Customer and Partner Conference","details":"Block off November 27-29 on your calendar, book a flight to Las Vegas, and plan to spend three days at AWS re: Invent learning more about AWS from AWS engineers, architects, partners, and customers in over 100 sessions.\n  Attend \nAWS re: Invent is our inaugural conference for customers and partners. Well take over the Venetian Hotel and have a great time, wrapping things up with a Global Block Party featuring superstar DJs from around the world.\n  This event contains content targeted at Developers, Start-ups, and Enterprise IT:\n  Developers can attend language-specific getting started sessions along with advanced sessions that will cover architecture, monitoring, configuration management, and performance optimization. Well be running an all-day Coding Challenge in the Developer Lounge with cool prizes and a chance to show us (and your peers) what you can do.\n  Start-ups can learn about best practices for architecture, scalability, and cost savings from many of the worlds fastest growing internet start-ups. Stealth start-ups can launch launch in front of press, analysts, VCs, and the entire AWS community in our Launch Theater, where well also host the finale of the global AWS Start-Up Challenge.\n  Enterprise IT can attend getting started and advanced sessions, with a focus on migrations, operations, and security.\n  Youll also get to hear customers, partners, and AWS talk about the solutions and workloads that are of particular interest to you, whether you are interested in gaming, mobile, web, internal IT, disaster recovery, big data, HPC, or analytics.\nIf you have something youd like to share with the AWS community -- perhaps a success story, some lessons learned, or some best practices -- [submit your abstract] and well do our best to find a spot for you.\n  I am also planning to tape a couple of episodes of The AWS Report. If you would like to appear, let me know (send some email to awsreport@amazon.com).\n  Get Notified \n Registration will open up in June. In the meantime, mark your calendar, click here, enter your email address, and well let you know when you can register.\nI want to emphasize that this is not going to be one of those stuffy vendor conferences. I don\u2019t want to give away any secrets but you can rest assured that this is something you don\u2019t want to miss.\u00a0\u00a0 \nWe are planning to keep you well informed, well fed, entertained, and fully engaged for the duration of the conference.\u00a0 There will be a wealth of opportunities for you to learn more about AWS, forge some connections with the service teams, meet others in the community and see what they are up to.\u00a0 You might need a vacation after this \u2013 it is Vegas after all and Vegas (and the Cloud) never sleeps. \nStay tuned for more details, including the agenda, and see you in Vegas\u2026","item_date":"May 09 2012 19:48:21","display_item_date":"05-09-2012","url":"http:\/\/aws.typepad.com\/aws\/2012\/05\/save-the-date-aws-reinvent-global-customer-and-partner-conference.html","source":"aws.typepad.com"},{"title":"42 Environment APIs: AMEE, Brighter Planet and Carma","details":"Our API directory now includes 42 environment APIs. The newest is the EnergyIQ. The most popular, in terms of mashups, is the AMEE API. We list 4 AMEE mashups. Below you\u2019ll find some more stats from the directory, including the entire list of environment APIs.\n\n\n\n\n\nIn terms of the technical details, REST and XML lead the way. There are 26 environment REST APIs and 16 environment SOAP APIs. Our directory lists 33 environment XML APIs and 16 environment JSON APIs.\n\n\n\n\n\nThe most common tags within environment are 18 green environment APIs, 12 government environment APIs and 11 science environment APIs.\n\n\n\n\n\nOn the mashup side, we list 57 environment mashups. We named Car Emissions Calculator as mashup of the day in December.\n\n\nFor reference, here is a list of all 42 environment APIs.\n\n\n3TIER API: Global wind and solar resource data\n\n\nAMEE API: Carbon and energy calculation service\n\n\nBrighter Planet Climate Data API: Reference data (e.g. zip codes, airlines, automobiles)\n\n\nBrighter Planet Emission Estimates API: Real-time carbon footprint estimates\n\n\nCarbon Calculated API: Enterprise Carbon Management\n\n\nCarma API: Carbon emissions data\n\n\nCEERN API: Conservation and Environment Resource Database\n\n\nCO2 Benchmark API: CO2\/greenhouse gas estimation service\n\n\nColorado HBGuest API: Colorado water resources data service\n\n\nColoradoWaterSMS  API: Colorado surface water resource data service\n\n\nDataFed WCS API: Air quality data visualization service\n\n\nDataONE API: Biological research data repository\n\n\nDeepwater Oil Reporter API: Crowdsourcing Oil Reporting App\n\n\necofreek API: Search engine for free or swap items\n\n\nEcolabel Index  API: Ecolabel Database Tools\n\n\nEEA Discomap API: Environmental Mapping Service\n\n\nEnergyIQ API: Building energy efficiency benchmarking service\n\n\nEnergyStar ABS API: Energy management and conservation service\n\n\nEnergyStar Third-Party Certification API: Energy efficiency certification processing service\n\n\nEPA Envirofacts API: Environmental database\n\n\nEPA Project Catalog API: EPA project information service\n\n\nEPA Station API: Find EPA monitoring stations by geographic area\n\n\nEPA Station Catalog API: EPA monitoring station information service\n\n\nEPA Watershed Summary API: EPA watershed information service\n\n\nGoodGuide API: Green healthy product ratings\n\n\nGreen Thing API: Green lifestyle suggestion service\n\n\nGreencheck API: Website sustainability checker\n\n\nGreengoose API: Wireless motion sensor game service\n\n\nMokugift API: Tree planting service\n\n\nNational Renewable Energy Lab API: Renewable energy information service\n\n\nNoiseTube API: Noise pollution monitoring service\n\n\nNWQMC Water Quality Portal API: Water quality data service\n\n\nOnset HOBOlink API: Data logger products\n\n\nOpen Charge Map API: Electric vehicle charging point database.\n\n\nOpenEco  API: Greenhouse gas production tracking service\n\n\nPowershop API: Electrical power purchasing service\n\n\nUSGS National Water Information System API: Water quality data service\n\n\nUSGS Waterservices API: Water quality information service\n\n\nWattzOn API: Online Power Usage Tracking\n\n\nWebservice-Energy API: Renewable energy information service\n\n\nWEHUB API: Open source water and environment data platform\n\n\nWiserEarth API: Environmentalists online community","item_date":"May 09 2012 18:33:10","display_item_date":"05-09-2012","url":"http:\/\/blog.programmableweb.com\/2012\/05\/09\/42-environment-apis-amee-brighter-planet-and-carma\/","source":"blog.programmableweb.com"},{"title":"Open API Economy","details":"","item_date":"May 09 2012 17:33:30","display_item_date":"05-09-2012","url":"http:\/\/www.google.com\/url?sa=X&q=http%3A%2F%2Fwww.craigburton.com%2F%253Fp%253D3412%2526utm_source%253Drss%2526utm_medium%253Drss%2526utm_campaign%253Dopen-api-economy&ct=ga&cad=CAcQARgAIAEoATAAOABA4YKk_QRIAlgAYgVlbi1VUw&cd=xfvQ5pfNd_M&usg=AFQjCNEiZJC5j5PO1OypafPyvJFNXQlzzA","source":"www.google.com"},{"title":"Finextra: MasterCard unveils digital wallet and open API","details":"MasterCard has unveiled a digital wallet enabling users to make purchases in-store, online and through their mobile phones.          \n      \n                \tThe card giants PayPass Wallet Services will let customers make contactless payments through cards and mobile phones in stores with NFC terminals. In addition, online shoppers will be able to store their details in the wallet and then make purchases with a single click.\nThe wallet is open, with banks, merchants and other partners able to white label their own efforts while consumers will be allowed to pay with American Express, Discover, Visa cards as well as MasterCard ones.\nMeanwhile, an API lets partners connect their own digital wallets into the PayPass Acceptance Network, tapping MasterCards check-out, fraud detection and authentication services and enabling their customers to make purchases wherever PayPass is accepted - online and in store.\nEd McLaughlin, chief emerging payments officer, MasterCard, says: Consumers are looking to pay for goods when, how and where they choose. Merchants want flexibility to easily accept digital payments so they can convert more browsers to buyers both online and in store. We realize that when it comes to payments, no single wallet will rule them all. PayPass Wallet Services simplifies the shopping experience while providing flexibility and choice to merchants, banks and consumers.\nThe wallet will be made available to partners in the third quarter, initially in the US, Canada, UK and Australia, with more countries to follow. Bank of Montreal, Commonwealth Bank, Citibank, Intesa Sanpaolo and Metro Bank are among the financial institutions committed to the services, while merchants on-board include American Airlines and Barnes & Noble.\nVisa is in the process of rolling out its V.me digital wallet, offering similar services, while technology firms such as Google and PayPal, as well as telcos, are also developing systems.\nTo coincide with the wallet launch, MasterCrad has unveiled new research in the form of a Mobile Payments Readiness Index (MPRI) that analyses and ranks 34 markets worldwide in terms of how ready (or not) they are to implement mobile payments. The initial results suggest its early days for mobile payments, with the top five markets identified as Singapore, Canada, the United States, Kenya, and South Korea.\n  \n                                            \t                Facebook           Twitter       \n            LinkedIn           Digg         Send story","item_date":"May 09 2012 17:33:27","display_item_date":"05-09-2012","url":"http:\/\/www.google.com\/url?sa=X&q=http:\/\/www.finextra.com\/News\/Fullstory.aspx%3Fnewsitemid%3D23688&ct=ga&cad=CAcQARgAIAAoATAAOABA1Yyk_QRIAlAAWABiBWVuLVVT&cd=8Bo-H28GzDM&usg=AFQjCNF--qnDwZBhxl4PY7pAQKCcT2wONA","source":"www.google.com"},{"title":"Demystifying REST Constraints","details":"Who do you think the actor is in the sentence \u201cBrenda was happy to interview the actor\u201d, a Hollywood celebrity or a member of the local community theater? The REST constraints described in Roy Fielding\u2019s dissertation enjoy celebrity status. I cannot mention constraints without people immediately assuming that I either talk about them, ignore them, or argue against them. Few realize that I talk about additional application constraints or perhaps about strengthening one of Fielding\u2019s constraints.\u00a0 I wish I could use another word, but the well-known REST constraints and application-specific constraints aren\u2019t much different and combine to define a concrete RESTful protocol.\n I want to be free to talk about constraints because by definition a protocol is a set of rules, many of them constraints. The Wikipedia definition of a communication protocol, \u201c\u2026a system of digital message formats and rules for exchanging those messages in or between computing systems and in telecommunications\u201d mentions rules, but examples from the  Merriam-Webster dictionary illustrate even better that the word protocol is synonymous with a set of rules:\n The soldier\u2019s actions constitute a breach of military protocol [rules or regulations].\n They did not follow the proper diplomatic protocols [rules or etiquette].\n What is the proper protocol [rules or conventions] for declining a job offer?\n \nThe Geneva Protocol and the Kyoto Protocol set forth international regulation agreed upon by the participating countries. Similarly, the rules of a communication protocol are agreed upon by participants in the communication, not imposed by an external authority. Standards and specifications like RFC 2616 merely formalize the results of the agreement.\n A constraint is a rule. What kind of rule is surprisingly hard to define exactly. Some would say a constraint is a rule about what not to do. This is close, but not close enough. Not close enough because any statement about what to do can be rephrased, however awkwardly, to say not to do the opposite. You can say \u201cthe protocol must be stateless\u201d or \u201cthe protocol must not force the server to keep track of the client\u2019s state\u201d. Either way, it is the same constraint.\n Constraints are like good parenting: instead of telling your kinds what to do, you set safe, age-appropriate limits, but otherwise let your kinds live their life and make their own decisions. Protocol constraints are used the same way to ensure safe, reliable, performant communication without restricting what the communication should be about. Constraints are also used to remove unessential variations which would complicate protocol implementation without any clear benefits. For example you can make you protocol simpler if you support JSON only. This is a valid constraint as long as XML support is not essential.\n Constraints should never restrict an essential freedom. This is so important that we have an expression for such mistake: to over-constrain. I find it quite unfortunate that the noun constraint derives from the verb to constrain, which has meanings like \u201crestrict by force\u201d or \u201cto secure by or as if by bonds\u201d. These meanings suggest firmness and strength. Some of us subconsciously think of constraints as exceptionally strong and important rules. If somebody ever reminded you in a stern voice of a REST constraint, you know what I mean. To discuss constraints rationally we have to let go of our emotions. We don\u2019t have to enforce constraints more vigorously than other protocol rules.\n Let me give you some examples of relatively strict constraints. To show you that my use of constraints is consistent with Fielding\u2019s and that some of his REST constraints are implicit in mine, I\u2019ll gradually relax mine until they become equivalent with two of his.\n Imagine a simple HTTP interface for a key-value data store. Clients send GET, PUT, and DELETE requests using URIs as keys and message bodies as values. The following constraint expresses a one-to-one mapping between URIs and message bodies: \u201cAs a response to a GET request the server must either return the status code 404 Not Found, or 200 OK and a message body which is a copy of the message body received in the last successful PUT request to the same URI\u201d. Just to show I can, I expressed this as a rule about what the server must do. The remaining constraints are indeed easier to express in terms of what the server must not do, such as:\n The server must not return the 404 status code if there was a previous successful PUT request to the same URI\n The server must not return other HTTP status codes, for example 302\n The server must not allow the byte sequence representing the value assigned to the URI to change by any other means except as a response to an explicit PUT request\n The server must not return a message body for any URI for which no value was yet set\n \n\u2026 and a few more. These constraints give my protocol some nice, useful properties. I can fully test my server via the protocol, for example.\n \u201cThis is not REST\u201d, you may object, \u201cwhere is the hypermedia?\u201d Good point. The hypermedia constraint does not say you must use hypermedia everywhere.\u00a0 That would be a bit too strict and would render most of the media types from the IANA registry unfit for REST. The constraint says to use hypermedia to indicate what the valid client state transitions are and to help clients discover URIs. In my protocol all client state transitions are valid and the client does not need to discover URIs because it controls the URI space. Hence there is no hypermedia. You may say that the hypermedia constraint is not applicable. I prefer to say that it is satisfied.\n To extend the applicability of my protocol beyond key-value data stores I need to relax some of my constraints. To turn my data store into a simple web server I need to eliminate constraint 4 and allow URIs pre-loaded with static content. Unfortunately the loss of constraint 4 also means that my clients no longer know what URIs exist and what they mean. Now I do indeed need to satisfy the hypermedia constraint. There are still many different ways to do it, for example I can provide a site map. Clients can still rely on their knowledge that values associated with URIs never change, use caches that never expire, for example.\n Now imagine that my URIs identify Wikipedia articles. I don\u2019t expect articles to change much over time, but I want to allow minor edits or formatting changes by eliminating constraint 3. Without constraint 3, however, message bodies for two successive GET requests may no longer be identical and I lose my ability to programmatically verify URIs still point to valid pages. Indeed, Wikipedia needs volunteers to make sure pages were not vandalized.\n I hope I\u2019ve emphasized enough that if I relax my constraints my protocol\u2019s applicability increases but I lose some useful protocol properties. Just how far can I go with relaxing my constraints? Let\u2019s see how far the editors of Wikipedia go. They accept in the page identified by the URI http:\/\/en.wikipedia.org\/wiki\/Tutankhamun any information about the Egyptian pharaoh that meets the quality standards of an encyclopedia. They would definitely not accept information about someone\u2019s pet just because it happens to be named Tutankhamun and they would be equally perplexed to find information about Joe DiMaggio at this URI. Sounds like common sense? I\u2019ve told you that I\u2019m demystifying the REST constraints.\n Let me generalize a bit. We expect both URIs and message bodies to mean something. When they are used together in HTTP, we expect their meanings to be related and this relationship between their meanings to remain the same. This is the same as to say that we don\u2019t want to receive (or send) two HTTP message bodies with entirely different meanings from the same URI. This relaxed constraint was implicit in the first version of my protocol, which asked for strict one-to-one mapping between URI and message body. The relaxed version permits many-to-many relationship between the two as long as the relationship between the meaning remains one-to-one.\n How useful is this relaxed constraint? The full answer doesn\u2019t fit into this post, but let\u2019s see if we can build a SOAP-like RPC protocol without having HTTP message bodies with different meanings sent to or returned from the same URI. If our service has a single URI, the message bodies mean \u201cinvoke method A\u201d, \u201cinvoke method B\u201d, and so on. Even if each method has its own URI, the message body sent means \u201cinvoke the method\u201d while the message body returned means \u201cthis is the result\u201d. Can we say they mean the same thing? I\u2019m not saying the constraint was explicitly formulated just to prevent us from doing RPC over HTTP, only that it has this result as well.\n Roy Fielding states the same constraint when he says that message bodies should be representations of resources identified by URIs even though the explanation some people give goes like this (I\u2019m kidding you not): \u201cTwo of the REST constraints are the identification of resources and the manipulation of resources by representations. An identifier identifies a resource. A resource is anything that can be identified. A representation can be anything you can send over the network. Got it?\u201d This is not what Fielding says although he says all these things. Each sentence is a quote from his dissertation, but taken out of context, combined into a meaningless blurb, and passed on as REST design wisdom. I won\u2019t attempt to untangle this mess. The dissertation is available online, read at least section 5.2.\n Fielding\u2019s REST constraints are neither laws of nature nor religion. Following them does not guarantee success and violating them does not bring inevitable doom. They play the same role as \u201cStay on trails\u201d signs in our national parks. If you stray from the beaten path there is a slight chance you will discover something wonderful nobody saw before, but you also risk falling into a ravine, starting an avalanche, or being killed by a grizzly bear.\n  \n This work is licensed under a Creative Commons Attribution-ShareAlike 2.5 Canada License.","item_date":"May 09 2012 17:33:23","display_item_date":"05-09-2012","url":"http:\/\/theamiableapi.com\/2012\/05\/06\/demystifying-rest-constraints\/","source":"theamiableapi.com"},{"title":"StrikeIron's IronCloud powers the DataFlux Marketplace","details":"StrikeIron, the cloud leader in data quality and data communications, today announced a licensing agreement with DataFlux, a leading provider of data management solutions. Through the agreement, DataFlux has licensed StrikeIron\u2019s cloud data delivery platform, IronCloud, to deliver and manage high-quality customer data validation solutions as DataFlux Marketplace, a software-as-a-service (SaaS) set of products that helps businesses validate mailing and email addresses within existing on-premises and cloud-based applications.\n  IronCloud, recently named the 2012 cloud award infrastructure winner, is the most mature and robust data-as-a-service (DaaS) platform available in the industry. DataFlux chose StrikeIron based on IronCloud\u2019s reliability, scalability, ease of deployment, and track-record of success delivering data on-demand to over 1500 customers. As a platform that has matured and been innovated upon for over eight years, IronCloud is a powerful, dependable cloud-based data delivery and API architecture that processes millions of customer records every day through its multi-tenant, high performance infrastructure.\n  Tony Fisher, president and CEO of DataFlux, said, \u201cThe ability to deliver data quality capabilities via the cloud keeps DataFlux ahead of the innovation curve. We are pleased to join forces with StrikeIron to provide cloud-based data quality solutions.\u201d\n  DataFlux Marketplace, available now, leverages all of the capabilities of the award-winning high-performance IronCloud platform and is also fully integrated into DataFlux\u2019s Data Management platform, enabling DataFlux to deliver a proven, comprehensive DaaS solution to its customers. It provides the ability for DataFlux customers to have easy access to data validation and enrichment functions for mailing addresses, phone numbers, and email addresses, using the real-time solution to increase the value of their organizational data assets.\n  Sean O\u2019Leary, president and CEO of StrikeIron, said, \u201cStrikeIron is excited to partner with DataFlux, an industry leader in data management. The collaboration will help businesses validate and enhance customer contact information in the cloud. The benefits of high-quality data, especially customer data, are numerous. We are delighted to provide technology that can further add value to DataFlux customers.\u201d\n  About StrikeIron: \n  StrikeIron is the leader in Data-as-a-Service (DaaS), delivering data quality and communications solutions via our cloud platform IronCloud. We provide address verification, email verification, phone validation, phone append, SMS text messaging, and sales tax solutions to customers in a variety of markets. Visit us on the web at http:\/\/www.strikeiron.com and Twitter @StrikeIron.\n  About DataFlux:\n  DataFlux enables business agility and IT efficiency by providing innovative data management technology and services that transform data into a strategic asset. A wholly owned subsidiary of SAS (http:\/\/www.sas.com), DataFlux helps organizations manage critical aspects of data through unified technologies and expertise that provide the benefits of data quality, data integration and master data management (MDM). To learn more visit http:\/\/www.dataflux.com.","item_date":"May 09 2012 17:30:19","display_item_date":"05-09-2012","url":"http:\/\/www.prweb.com\/releases\/2012\/5\/prweb9457590.htm","source":"www.prweb.com"},{"title":"Copyrights, APIs, and Oracle vs Google","details":"We still don\u2019t know what will happen with Oracle\u2019s accusations that Google violated its patents. Given that Oracle itself doesn\u2019t value the two remaining patents as being worth much, that decision won\u2019t matter much. No, the real question is what will U.S. District Court Judge William Alsup will do with notion  that Java\u2019s application programming interfaces (API)s, and hence all APIs, could  be copyrighted. That\u2019s the real $64-million (billion?) question\n\n\nAlsup instructed the jury to treat APIs as if they could be copyrighted, and they agreed with him on that. What they couldn\u2019t do is decide though whether Google had violated fair use in what it did with Java\u2019s APIs in creating Android.\n\n\nAs Pamela Jones, intellectual property law reporter, paralegal, and founder of Groklaw explained to me, \u201cThe jury didn\u2019t decide API are copyrightable. They can\u2019t. That\u2019s a question of law, and the judge is the one that has to decide that issue.\u201d\n\n\nJones stated that Alsup \u201cdecided that he\u2019d let the jury decide the fair use issue first, and then if they found fair use, he wouldn\u2019t have to reach that decision. But if they found infringement and no fair use, then he would decide if APIs are copyrightable and more specifically if their arrangement is protectible.\u201d\n\n\nIs it? Oracle would have it that APIs are like music. Yes, APIs are just made up of descriptions of inputs and outputs, but then music is just made up of notes.\n\n\nTo this argument, Thomas Carey, a partner at Sunstein, a major intellectual property (IP) law firm and chair of its Business Department said, \u201cOracle\u2019s lawyers compared the creation of APIs to writing a piece of music, to which I say \u2018Balderdash.\u2019\u201d\n\n\nThe First Circuit opinion in Lotus v. Borland found the command structure of Lotus 1-2-3 to be unprotectible under copyright because it was functional, not expressive. According to that opinion, the IP protection for functionality is to be found, if at all, under the patent laws, not under copyright.\n\n\n\u201cIs there anything more functional and less expressive than an API?\u201d continued Carey. I don\u2019t think so, and I suspect that you don\u2019t either. Thus, the infringement of APIs should not be possible unless they are patented. The First Circuit [which ruled in Borlands favor in this important case over a programs menu interface] got the principle right (even if I disagree with them about the command structure of 1-2-3).\u201d\n\n\nSo why did the jury find \u201cBalderdash?\u201d Jones thinks it because the final jury instructions led the jury to find \u201cinfringement because they thought they were supposed to.\u201d In the end, the jury came up with a conclusion that leaves the question of whether APIs can be copyrighted in the judge\u2019s hands.\n\n\nWhat happens next? Matthew Levy, a partner with the small IP firm, Cloudigy Law, said: Judge Alsup has not made any decision yet as to whether APIs are copyrightable. And it\u2019s very likely that whatever decision he makes will be appealed to the 9th Circuit, so we won\u2019t know the answer for some time.\u201d\n\n\nLevy continued, \u201cEven if Judge Alsup holds that Java\u2019s APIs are copyrightable, that doesn\u2019t necessarily mean that all APIs are copyrightable. Other languages use header files for APIs; a header file contains little more than method\/function signatures, type definitions, exposed variable names, and constants. Those are completely dictated by the function of the code. Arguably, the way those things are arranged in a header file requires some creativity, but I think that\u2019s stretching the law too far.\u201d\n\n\nIf the judge finds that APIs can be copyrighted, \u201cI see a couple of big problems with allowing all APIs to be copyrightable. First, developers will have to be very careful in choosing a programming language. The reality is that things won\u2019t change a lot (although Java will take a hit), because most programming languages already come with fairly broad licenses. Still, I expect that developers will start to read those licenses a lot more carefully. But the bigger problem is for consumers. If all APIs are copyrightable, we can end up with a situation where a company builds a specialized API to control a device or other platform and then locks down the market for after-market components using copyright of the API combined with the Digital Millennium Copyright Act,\u201d Levy concluded.\n\n\nThe Electronic Frontier Foundation (EFF) has a far darker view of what the world would be like if APIs could be copyrighted. \u201cTreating APIs as copyrightable would have a profound negative impact on interoperability, and, therefore, innovation. APIs are ubiquitous and fundamental to all kinds of program development. It is safe to say that all software developers use APIs to make their software work with other software. For example, the developers of an application like Firefox use APIs to make their application work with various OSes by asking the OS to do things like make network connections, open files, and display windows on the screen. Allowing a party to assert control over APIs means that a party can determine who can make compatible and interoperable software, an idea that is anathema to those who create the software we rely on everyday. Put clearly, the developer of a platform should not be able to control add-on software development for that platform.\u201d\n\n\nThe EFF then gives two concrete examples: \u201cTake, for example, a free and open source project like Samba, which runs the shared folders and network drives in millions of organizations. If Samba could be held to have infringed the Microsoft\u2019s copyright in its SMB protocol and API, with which it inter-operates, it could find itself on the hook for astronomical damages or facing an injunction requiring that it stop providing its API and related services, leaving users to fend for themselves.\u201d\n\n\n\u201cAnother example is the AOL instant messaging program, which used a proprietary API. AOL tried to prevent people from making alternative IM programs that could speak to AOL\u2019s users. Despite that, others successfully built their own implementations of the API from the client\u2019s side. If copyright had given AOL a weapon to prevent interoperability by its competitors, the outcome for the public would have been unfortunate.\u201d\n\n\nLet\u2019s hope, oh how we should hope, it doesn\u2019t come to that.\n\n\nRelated Stories:\n\n\nAndroid chief called back in Oracle-Google trial to discuss patents\n\n\nGoogle: We developed Android not knowing Sun\u2019s patents\n\n\nThe muddled mess of the Oracle vs. Google trial\n\n\nAfter mixed copyright win over Google, Oracle looks towards patents","item_date":"May 09 2012 17:19:45","display_item_date":"05-09-2012","url":"http:\/\/www.zdnet.com\/blog\/open-source\/copyrights-apis-and-oracle-vs-google\/10943","source":"www.zdnet.com"},{"title":"Open Legislation Hackathon - June 2nd, 2012","details":"What is it?  The Queens Printers Association of Canada (QPAC) and the Open Data Society of BC are sponsoring a day of fun, code and advocacy at the 2012 Open Legislation Hackathon. Its a chance to build mobile or online applications using legislative data from across the provinces of Canada.  What is this event for?  The goal is to share the experience, findings, and difficulties of creating online apps based on legislative data published by the different Queens Printers of Canada.  Present your apps and experience to representatives of the annual QPAC conference from across Canada! Your app and presentation may change the way other provinces look at legislative data, online licensing and open data policies!  What are we building?  Anything you like!  It would be great if it uses legislative data. We want to see what you can come up with! Focus on multiple jurisdictions, or just one.  When: Saturday, Jun 2nd, 2012 between 8am and 6pm Where: Queens Printer BC - 563 Superior Street, 3rd floor, Victoria, BC  Join us for a day of coding, discussion, food, and fun. Following the hackathon, appies and drink tickets will be provided to participants to continue the discussion at a local pub.  For more info: please contact clecio.varjao@qp.gov.bc.ca \n  \n Available for presentation during the afternoon of June 5th (Tuesday)?  We would like you to present your application and share your experience during the QPAC Conference.  Yes\n  No","item_date":"May 09 2012 16:55:07","display_item_date":"05-09-2012","url":"https:\/\/docs.google.com\/spreadsheet\/viewform?pli=1&formkey=dDlLVFo0ZENUZHYtRTdfWUM2VVBkUHc6MQ#gid=0","source":"docs.google.com"},{"title":"Creating a Fast RESTful API Using CakePHP and New Relic","details":"This post is written by Martin Samson of The Hotel Communication Network. We love when customers post about how they use New Relic in their environment. Martin posted it on his own blog, and we asked him if we could share it with our extended audience. He said yes. Thanks Martin!\n In early 2012, the development team at The Hotel Communication Network set out to build a RESTful API to simplify communication between the various components of our platform. Early in the process, we began to evaluate the different options available to us: Ruby on Rails, CakePHP, Django, Lithium and ExpressJS.\nThe application started as a simple CRUD interface to be completed with a two week deadline. All frameworks were up to the task, albeit CakePHP was well known by all of us.\nReasons we chose CakePHP:\n* We already use 1.x extensively.\n * Our database was already compatible.\n * The IRC channel is vibrant.\n * It\u2019s easy to understand.\n We started development in late March 2012. None of us had any experience with the new version of the framework; as it was just released. Once all the entities were defined, we built the API from the outside in by defining how resources should be accessed, modified and created. A lot of time was saved because our database already followed CakePHP\u2019s naming conventions; the bake tool was used to generate the models and their test skeletons; the validation rules, routes and controllers quickly followed.\nCakePHP has matured quite nicely, associations are now lazy-loaded. Object loading greatly improved, you can easily apply \u201cObjects on Rails\u201c\u00a0principles. Most of the logic was moved from models to distinct objects resulting in slim controllers and not-so-fat models, with clean business objects. It allowed our models to handle only data concerns (validation and persistence) thus giving us faster unit tests. CakePHP\u2019s move to PHPUnit made for better tests and reports.\nOur app, code named Butterfly, quickly grew from a simple CRUD application to the core of our platform. Most business logic was consolidated, eliminating code duplication.\nFinally, all the REST routes were manually specified to give us more flexibility and control.\nTips when using CakePHP: \n* Use model validation for everything. Really. Use it to verify a related entity\u2019s existence.\n * Use components to wrap repetitive controller logic such as GET parameter parsing.\n * Separate concerns between models and business objects. CakePHP 2.x allows loading any classes with App::uses(). See Objects on Rails\u00a0by Avdi Grimm\n * Use the whitelist when saving data. (See RoR \/ GitHub mass assignment security problem.)\n * Write unit tests! We have over 300 tests and thousands of assertions.\n * Use simple views combined with the RequestHandler. Our API is only accepting and serving JSON, but we could easily add XML.\nRunning Butterfly on Apache 2.2 + mod_php seemed fast on our development machines although it quickly crumbled in our staging environment. At that point, we really didn\u2019t know what was wrong and why responses were getting slower and slower. We were blind.\nEnter New Relic \nNew Relic is a real-time monitoring service built to help you squeeze performance out of your applications. We heard lots of good things about them from a newly employed engineer and Shopify\u2019s Tweets.\nThe whole process of signing up, installing the PHP module and getting a t-shirt took less than 10 minutes on FreeBSD! We were blown away once we viewed the dashboard: it automatically detected the framework and produced useful, near real-time graphs. From slowest controller to detailed database queries, we could drill down to a specific section of the system to determine what needed be optimized. A great feature is the ability to backtrace from any SQL query to its callers.\nTo our surprise, CakePHP\u2019s built-in authentication with ACL was the slowest part of the application, due to joining in the aro\/aco\/users tables. We also discovered one of our applications was not caching properly, requesting object schemas on each call.\nNGINX + PHP-FPM is being marketed as a really fast alternative forapache + mod_php. The metrics allowed us to compare NGINX to Apache, within our environment. Switching to it resulted in a 100% speed increase!\nNew Relic enabled us to optimize early in the development process, gave us visibility and the metrics to back changes. We are now entering the production phase and New Relic will help us keep track of improvements, enabling us to deliver a better end-user experience.","item_date":"May 09 2012 16:52:20","display_item_date":"05-09-2012","url":"http:\/\/blog.newrelic.com\/2012\/05\/08\/creating-a-fast-restful-api-using-cakephp-and-new-relic\/","source":"blog.newrelic.com"},{"title":"Oracle's Expert Says Android Ripped Off Java Patents","details":"Google\u2019s Android operating system infringes on two patents owned by Oracle, according to Stanford professor John Mitchell, an expert witness hired by Oracle in the ongoing court battle over Android and its use of the Java programming language.\n With its case, Oracle claims that Android steps on two Java-related patents \u2014 U.S. Patents 6,061,520 and RE38,104 \u2014 and, yes, Mitchell agrees. The Stanford academic took the stand on Wednesday during the patent phase of the trial that pits Oracle against Google, and he was paid by Oracle. \n During the trial \u2014 which is now into its fourth week \u2014 Judge William Alsup has chided both Google and Oracle for calling paid witnesses who seem predisposed to completely agree with their arguments. \n Oracle sued Google in August of 2010, claiming that the search giant violated both its copyrights and its  patents in building a new version of the Java platform for Android. Rather than license the Java platform from Sun, Google created its own virtual machine \u2014 known as Dalvik \u2014 for running applications written with the Java programming language. \n \n On Monday, the jury decided that Google infringed on Oracle copyrights covering the overall structure, sequence, and organization of 37 of Java\u2019s application program interfaces (APIs) \u2014 software that lets Java programs talk to the Java platform on PCs, smartphones, and other devices. But it was unable to agree on whether Google\u2019s use of the copyrighted material constituted fair use under the law. Google immediately moved for a retrial, arguing that you can\u2019t decide on infringement without deciding on fair use, but Judge William Alsup has yet to address this. \n In the meantime, the trial has proceeded into its second phase, which addresses Oracle claims that Google also violated its patents. \n U.S. Patent RE38,104 \u2014 aka \u2019104 \u2014 describes a \u201cmethod and apparatus for resolving data references in generated code.\u201d Basically, it covers a way of improving the software compilation \u2014 i.e., the process of translating programming code into an executable application. It uses \u201csymbolic references\u201d to identify data during compilation rather than numeric memory locations. Google argues that Dalvik does not use symbolic references, whereas Oracle says otherwise.\n The second patent \u2013\u2019520 patent \u2014 describes a \u201cmethod and system for performing static initialization,\u201d a way of consolidating classes of files so that virtual machines execute less code than they otherwise would. Oracle claims that Google uses \u201csimulated execution\u201d with Dalvik, whereas Google says it merely parses files. \n Taking the stand on Wednesday, John Mitchell spent the better part of the afternoon taking questions from Oracle\u2019s lead counsel Mike Jacobs and walking the jury through extensive diagrams and software code in an effort to show that Google has indeed infringed on these two patents. Mitchell also discussed a number of tests he ran on the Android code prior to the trial, saying that these prove infringement.\n Although Mitchell is paid by Oracle, the jury has been instructed to view his testimony as fact. Google will cross-examine Mitchell on Thursday, and after Oracle rests its case, it will have the chance to call its own paid expert witness.\n To prove infringement, Oracle must show that Google was \u201cwillfully blind\u201d of Sun\u2019s patents when it developed the Dalvik virtual machine. Earlier on Wednesday, Andy Rubin, who oversees the Android project, took the stand, and Jacobs asked if he was aware of Sun\u2019s Java patent portfolio. \u201cAs an engineer, you shouldn\u2019t study someone else\u2019s invention when you\u2019re trying to come up with your own,\u201d Rubin said.\n Jonathan Schwartz, Sun\u2019s former CEO, testified during the copyright phase of the trial, but Oracle has asked that the judge prevent him from taking the stand during the patent phase. With his previous testimony, he said that although Sun didn\u2019t like that Google had built Android, he \u2014 as CEO \u2014 did not believe Sun should take legal action against the search giant. \n On Wednesday, in an attempt to challenge Schwartz\u2019s testimony, Oracle called Jonathan Sutphin, a former Sun executive who reported to Schwartz. Oracle lawyer Mike Jacobs asked him whether Sun ever made a definitive decision not to sue Google. \u201cNot that I\u2019m aware of,\u201d Sutphin answered.\n Google\u2019s Robert Van Nest countered by showing that Schwartz was the ultimate decision maker at Sun, not Sutphin. \u201cOther than the board, he was the highest-ranking official at the company?\u201d  \n \u201cYes,\u201d Sutphin answered.\n While at Sun, Schwartz published a blog post espousing his support for the new platform. Oracle has pointed out that this is just a blog post, not a legal or official document. But in questioning Sutphin, Google\u2019s Van Nest pointed out that typically, public companies must notify the SEC of public statements from their CEOs and that Sun had done so with Schwartz\u2019s post. The implication was that the blog post was official.\n \nGot a secret? Email caleb_garling [a] wired.com. Caleb covers tech, but loves other stuff like sports, fiction, beer, fun in remote places and music featuring guitars. Encircle on Google+, subscribe on Facebook or\nRead more by Caleb Garling \nFollow @calebgarling on Twitter.","item_date":"May 09 2012 07:00:00","display_item_date":"05-09-2012","url":"http:\/\/www.wired.com\/wiredenterprise\/2012\/05\/oracle-google-patent\/","source":"www.wired.com"},{"title":"3 Problems AWS Needs to Address","details":"A few days ago, in a fit of pre-launch, late-night, frustration, I issued the following 140 missive.\n   To my surprise, this actually got a response. Someone monitoring the @awscloud account opened a trouble ticket to my email address asking for clarification. The exchange was friendly and hopefull, and I think its worth sharing here.\n     It\u2019s a pretty compelling situation: cloud service offerings and web browser technology have advanced to the point where S3 and CloudFront should be all one needs to deliver an incredibly performant and cost-effective user experience, letting small startups compete in the time-to-first-render game on an even playing field with the likes of Google and Yahoo. Instead, developers are forced to settle for ugly workarounds and outright hacks due to a few crucial shortcomings.\n  My team at Boundless is has been working on a cutting edge single-page HTML5 app. We are hosting it on S3 and CloudFront, and its underlying API lives on EC2. Without getting into too much detail, the architecture a lot like #newtwitter.\n  1. S3 Restricts Response Headers\n  Despite initial appearences, and without much justification from Amazon, the S3 API severly restricts which headers can be attached to an object.\n  Cache-Control\n \n. CSS3 brings the ability to embed arbitrary fonts on the web. Fonts are the words which clothes wear, and CSS3 is why the web is looking so sharp lately. The difficulty is that W3 puts fonts under a same-origin restriction. Thus, embedding these fonts requires these additional headers:\n \n  Serve the entire domain through CloudFront. This is fine unless there\u2019s anything on your domain which shouldn\u2019t be cached, and I\u2019m sure things get even more complicated if you throw SSL into the mix.\n Skip S3 and serve everything from EC2. S3 has eleven nines of durability. Go ahead, reproduce that with a couple of NginX boxes.\n Insert some proxy servers to add the headers. I think what you mean is, add yet another hop in your network while increasing your attack footprint and your EC2 bill.\n Mix fonts into stylesheets using data:\n URIs. Now every time you adjust a <div>\n tag, your visitors have to download your fonts again. You could break your CSS into multiple files, but this is in direct opposition to one of the tennents of website optimization: minimize the number of HTTP requests. Also, 7-bit encoding means your fonts are now 37% fatter on the wire.\n \nHere is a forum post from 2009 bringing this to Amazon\u2019s highly dismissive attention. What really irks me about this is that Amazon chose to bless a few headers instead of letting the end-user decide what is best for our customers.\n  2. S3 and CloudFront Won\u2019t Compress Anything\n  RFC2616 allows that a client may suggest to a server that it would like to have the response encoded as something other than raw bytes before transmission. One common encoding is gzip\n, and lots of HTTP traffic includes a header like Accept-Encoding: gzip\n. Most web servers will comply with this suggestion, reducing plain text like HTML, CSS, and JavaScript by well over 50%.\n  Two notable exceptions to \u201cmost web servers\u201d are S3 and CloudFront. A possible workaround involves Content-Encoding\n being among the allowed HTTP headers for S3 objects. The image below has been compressed with gzip -9\n before uploading, and has Content-Encoding: gzip\n  \n in the response even if they didn\u2019t request it. Users of IE7 and previous versions will see a broken image icon. wget\n will also result in corruption unless those tools are explicitly configured to always use compression. This is really a quasi-violation of RFC2616 Section 14.3, but it does sort of work.\n  If you want to be compliant, you most choose between S3 and compression. CloudFront, at least, will cache both compressed and raw versions of each object depending on the clients which have requested it.\n  3. CloudFront\u2019s TCP Stack Lacks Tuning\n  I have harped on this issue before, but Amazon CloudFront exhibits one of the smallest initial TCP congestion windows in the CDN marketplace. They\u2019re at 2. Consensus is growing that it should be closer to 10. Rather than making an argument for it here, I\u2019ll let some Googlers do it for me.\n  An Argument for Increasing TCP\u2019s Initial Congestion Window\u201d, Nandita Dukkipati, Tiziana Refice, Yuchung Cheng, Jerry Chu, Tom Herbert, Amit Agarwal, Arvind Jain, Natalia Sutin, ACM SIGCOMM Computer Communications Review, vol. 40 (2010), pp. 27-33.\n [ccr.sigcomm.org] [pdf] [search]\n   The @awscloud guys are apparently considering my request. If I get any answers back, I will be sure to post them here.","item_date":"May 09 2012 06:47:55","display_item_date":"05-08-2012","url":"http:\/\/blog.jacobelder.com\/2012\/05\/3-problems-aws-needs-to-address\/","source":"blog.jacobelder.com"},{"title":"Google and Oracle Enter Round Two of Heavyweight Legal Fight","details":"Zuckerbergs Hoodie a \u2018Mark of Immaturity,\u2019 Analyst Says\u00a0 \u2014\u00a0 Mark Zuckerberg is known for having arrived late for a meeting with prospective investor Sequoia Capital and giving a less-than-serious pitch while wearing pajamas.\u00a0 \u2014\u00a0 Now, as Facebook preps its initial public offering \u2026 \n   More: CNET, Om Malik, The Next Web and The Technology Chronicles   Tweets:\u00a0@k and @getwired\n \n Facebook changes IPO pitch in Boston, scraps video\u00a0 \u2014\u00a0 (Reuters) - Facebook Incs initial public offering pitch played to some bad reviews in New York, so for its Boston audience on Tuesday, the 30-minute video was scrapped and the company took more questions from analysts and potential investors.\n   More: Mercury News, ZDNet, NBC Bay Area, VentureBeat and The Next Web\n  \n BREAKING NEWS: Twitter Stands Up For One Of Its Users\u00a0 \u2014\u00a0 Twitter has filed a motion in state court in New York seeking to quash a court order requiring it to turn over information about one of its users and his communications on Twitter.\u00a0 This particular case involves a Twitter user \u2026 \n  \n Twitter: Were still the free-speech wing of the free-speech party\u00a0 \u2014\u00a0 As various levels of government both in the U.S. and around the world have stepped up their attempts to track down dissidents through social networks, the pressure has intensified on companies like Twitter and Facebook \u2026 \n   More: NEWS.com.au and AllThingsD\n  \n Sources Say AOL Seeking Buyers for Engadget and TechCrunch.\u00a0 Arrington \u201cNot In The Least Bit Interested\u201d\u00a0 \u2014\u00a0 We werent sure about this one at first, but now we have two independent sources confirming that AOL is exploring the sale of its cornerstone technology sites Engadget and TechCrunch.\n   More: Betabeat and Current Editorials.\u00a0 Tweets:\u00a0@edbott, @karaswisher, @obrien, @cbm, @davepell, @alexia and @benparr\n  \n HP unveils Envy Spectre XT Ultrabook, other thin-and-lights in various sizes\u00a0 \u2014\u00a0 Whats that?\u00a0 Youre hungry for more HP laptops?\u00a0 Good!\u00a0 Because were not nearly done yet!\u00a0 The company just expanded its line of Ultrabooks from two to five, and thats not even counting this guy.\n   More: LAPTOP Magazine, PC World, blogs.ft.com, Gizmodo, VentureBeat, The Verge, CNET, CNET, The Verge, The Verge and The Verge\n  \n Thousands of Twitter passwords exposed\u00a0 \u2014\u00a0 Twitter is investigating the release of what appear to be thousands of user account passwords and e-mail addresses.\u00a0 \u2014\u00a0 \u201cWe are currently looking into the situation.\u00a0 In the meantime, we have pushed out password resets to accounts that may have been affected \u2026 \n   More: airdemon.net, Bits, Neowin, ZDNet, Mashable!, Geek News Central, Gizmodo, The Verge, Business Insider, CNNMoney.com, The Next Web and AllThingsD   Tweets:\u00a0@twittercomms, @twittercomms and @qwghlm\n \nWhat Is and Is Not A Technology Company\u00a0 \u2014\u00a0 When I was a kid, I had a morning routine with my family.\u00a0 Over breakfast, wed divvy up the newspaper.\u00a0 Id go straight for the Business section, and from there to the back pages with yesterdays stock prices.\u00a0 Scanning down the lines of tiny text, Id look for SUN, ORCL, MSFT, AAPL, SGI.\n   More: GigaOM\n  \n StumbleUpon CEO Garrett Camp Steps Down, 10 Years Later\u00a0 \u2014\u00a0 Garrett Camp, the co-founder and long-time CEO of Web discovery service StumbleUpon, is moving on.\u00a0 Or up, you could say \u2014 Camp will now be chairman of the board, while the company undertakes a public search for a new CEO.\n   More: TechCrunch, StumbleUpon.com, Business Insider, The Next Web and VentureBeat\n  \n AT&T Mobility CEO: Family data plan coming soon (scoop)\u00a0 \u2014\u00a0 AT&Ts Ralph de la Vega says he feels good about the plan, a reversal of the uncertainty he expressed just a few months ago.\u00a0 NEW ORLEANS\u2014AT&Ts version of the family data plan may be coming sooner than you think.\n   More: IntoMobile, USA Today, msnbc.com, WebProNews, Mashable!, VentureBeat, iPhone, Associated Press, Android Phone Fans, iDownloadBlog.com, eWeek, Ars Technica, App Advice, Gotta Be Mobile, BGR, PadGadget, Ubergizmo, blog.chron.com and TUAW\n  \n Nano-SIM update: Apple design modified to fix concerns, standard will be decided this month\u00a0 \u2014\u00a0 We just spoke with SIM card maker (and pioneer) Giesecke & Devrient here at CTIA about progress on the creation of the 4FF standard \u2014 the so-called nano-SIM \u2014 over which Apple and Nokia have been warring in recent months.\n   More: MacRumors, Engadget and Gadgetsteria\n  \n AT&T plans small cell tests around year end\u00a0 \u2014\u00a0 (Reuters) - AT&T Inc plans to start testing new wireless network devices known as \u201csmall cells\u201d later this year hoping to boost its network coverage, a top executive for the telephone company said on Tuesday.\u00a0 \u2014\u00a0 John Donovan, executive vice president \u2026 \n   More: CNET, Bits, Gadgetsteria and Engadget\n  \n Fred Wilson: what crowdfunding means for the VC business\u00a0 \u2014\u00a0 For the past couple of decades, venture capitalists have had the upper hand.\u00a0 Theyve had the funding and, traditionally, theyve held most of the power in the startup ecosystem.\u00a0 But, Fred Wilson, managing partner of Union Square Ventures \u2026 \n   More: Forbes and TechCrunch\n  \n This is the type of photo youll be able to take with Googles Project Glass\u00a0 \u2014\u00a0 Google is teasing us about the future of its \u201cProject Glass\u201d, the glasses that will help you navigate the world, or just take really awesome pictures like this:\u00a0 \u2014\u00a0 The team is asking you to suggest the type \u2026 \n   More: Sergey Brin, Ubergizmo, TPM IdeaLab, 9to5Google and 1001 Noisy Cameras\n  \n Smoked by Windows Phone by the numbers (and new ads!)\u00a0 \u2014\u00a0 When I kicked off Smoked by Windows Phone at CES in January, I did it with one thing in mind - show the world that Windows Phone is simply faster at the everyday stuff that people do on their smartphones.\n   More: WMPoweruser, Gotta Be Mobile, The Verge, The Next Web, GeekWire, Seattle Times, WinBeta, Engadget and PhoneArena\n  \n Y Combinator-Backed Swiftype Builds Site Search That Doesnt Suck\u00a0 \u2014\u00a0 In my four-plus years covering tech, I dont think Ive ever met another blogger who was happy with the search feature on their website.\u00a0 The options range from terrible to functional, but its never good \u2026 \n  \n Apple accidentally reveals iCloud.com banner notifications\u00a0 \u2014\u00a0 A test of a new banner notification feature at Apples official iCloud website was mistakenly made public on Tuesday, revealing a forthcoming feature for the service.\u00a0 \u2014\u00a0 Users who visited the iCloud.com website could find \u2026 \n   More: TUAW, MacRumors, CNET, Engadget, WebProNews, 9to5Mac and MacNN\n  \n Foursquare CEO Reveals Plans for Personalized Coupons\u00a0 \u2014\u00a0 Foursquare doesnt want to be another popular\u2014but unprofitable\u2014social network.\u00a0 Its new plan to make money?\u00a0 Personalized coupons.\u00a0 \u2014\u00a0 The company, which lets users alert their friends to their location by \u201cchecking in\u201d \u2026 \n  \n  Atlassian Blogs:  Between a rock and a hard place - our decision to abandon the Mac App Store\u00a0 \u2014\u00a0 On June 1st, Apple will change the rules of the Mac App Store to require all applications to run inside of a \u2018sandbox\u2019.\n \n  The Box Blog:  The Box Platform, OneCloud and You\u00a0 \u2014\u00a0 Here at Box, we care a lot about building the products that allow us to work with as many cool companies and technologies as possible.\u00a0 That way, if you work at a Fortune 500 \u2026 \n \n  Sponsor Techmeme \n \n  Whos Hiring In Tech?\n  Google:  The end to your job search.\n  Ask.com:  Want a new gig? You just have to ask.\n  Tumblr: Like websites? This is a website.\n  Shape Security: Who let the bots out?\n  DoubleDutch: Mobile like a fox.\n  Postmates: Get ship done.\n  Yammer: Making work social.\n  @ClimateCorp: Big data tackles climate change.\n  Amazon: Work hard. Have fun. Make history.\n  Facebook: Best place to build & make an impact.\n  Zynga: Its fun over here.  Lets play.\n  Twitter: Less characters; more fulfilling.\n  foursquare: Making the real world easier to use.\n  Square: Come simplify the complex.\n  Add your company here \n \n The most current version of the site as always is available at our home page. To view an earlier snapshot click here and then modify the date indicated. \n  From Mediagazer","item_date":"May 09 2012 06:47:54","display_item_date":"05-08-2012","url":"http:\/\/www.techmeme.com\/120508\/p73#a120508p73","source":"www.techmeme.com"},{"title":"Time to Host a 'Hackathon' of Your Own","details":"You might be forgiven if you thought a \u201chackathon\u201d was an event where a bunch of hackers get together to plot the overthrow of the digital universe. In reality, it\u2019s a more innocuous event where developers and business people come together to focus on creating new types of applications.\nThe folks at Rally Software, a provider of application lifecycle management (ALM) software, hosted one of these this week at its annual RallyON conference, but you don\u2019t really need a vendor to create your own \u201chackathon.\u201d According to Todd Olson, vice president of products at Rally Software, Rally Software hosts these events as part of an effort to get people to focus on developing innovative applications.\nPart of the problem that most companies have these days is that they are working so hard on managing the business that there is no time to think about how to improve the business. There\u2019s a wealth of application development technology out there, but the people who have the skills don\u2019t really know much about the business. They need to spend time with business people in order to understand how software could be applied to create some truly innovative pieces of software.\nIn an ideal world, companies would host their own annual retreats where developers and business people come together to just hack some code. Chances are that not only would some critical institutional knowledge be shared, some pretty compelling business applications might be created.\nGreat software just doesn\u2019t magically happen. People have to take the time to help create it as part of a collaborative exercise. But that can\u2019t happen if everybody is actually too busy running the business to actually think about how to make it better.","item_date":"May 08 2012 21:47:57","display_item_date":"05-08-2012","url":"http:\/\/www.google.com\/url?sa=X&q=http:\/\/www.itbusinessedge.com\/cm\/blogs\/vizard\/time-to-host-a-hackathon-of-your-own\/%3Fcs%3D50344&ct=ga&cad=CAcQARgAIAEoATAAOABAte6l_QRIAlgAYgVlbi1VUw&cd=AcILAIHUSuU&usg=AFQjCNH0ZDlxtTTpfPv7xqsm0caP3i2UBw","source":"www.google.com"},{"title":"REST and SOAP","details":"essay on SOAP + REST in which he proposes four web service world-views: everything is a resource; everything is a get; everything is a message; everything is a procedure. I think he\u2019s missing the point. My understanding of the key difference between the REST and SOAP positions is that resources should be addressable. In REST this means GETting a specific URI to retrieve the representation of, say, the weather forecast for Rome. In SOAP the URI is used as a command target, the command being to retrieve the weather for Rome. The same URI can be resused for retrieving the weather for Moscow and, perhaps, last week\u2019s temperature chart for Athens.\n Sam also makes the smart, analogy of SOAP services being the dark matter of the Internet since the lack of a GET interface makes the services inaccessible the majority of Internet clients. His key recommendation here is to maximise the \u2018surface area\u2019 of the service by supporting other methods such as GET\n Worth reading as another viewpoint on the whole REST vs SOAP debate.","item_date":"May 08 2012 21:47:55","display_item_date":"05-08-2012","url":"http:\/\/blog.iandavis.com\/2002\/07\/22\/rest-and-soap\/","source":"blog.iandavis.com"},{"title":"Google+ Hangouts On Air and SMBs","details":"Yesterday Google unwrapped the Google+ Hangouts on Air to general public with relatively little fanfare. This is part of Google\u2019s social strategy but the product has the potential to disrupt many startups in the space, especially the ones targeting consumers and SMBs. Google could flex their Youtube muscle to literally shove them away. Google+ hangouts are available as a part of Google+ from the beginning. Google+ Hangouts on Air takes it much further even offering an option to live broadcast on Youtube. Though this is significant from the point of view of their Youtube business in the long run, I am going to see it from the other side and talk about how SMBs can take advantage of this nifty tool.\n First a brief background on some of the new features that comes with Google+ Hangouts on Air:\n Broadcast publicly. By checking \u201cEnable Hangouts On Air,\u201d you can broadcast your live hangout\u00e2\u0080\u0094from the Google+ stream, your YouTube channel or your website\u00e2\u0080\u0094to the entire world.\n See how many viewers you ha\u0080\u0099ve got. During your broadcast, you can look inside the hangout to see how many people are watching live.\n Record and re-share. Once you\u2019re off the air, we\u00e2\u0080\u0099ll upload a public recording to your YouTube channel, and to your original Google+ post. This way it\u2019s easy to share and discuss your broadcast after it\u2019s over.\n \nThese are some of the features that could help musicians, comedians, etc. but these features can be tapped by a larger section of SMBs to their advantage. In fact, I got an early access to Google+ Hangouts on Air and I am using it as a part of my research firm to run online panels. I already see many other uses for my organization. Since I see it as a great tool for SMBs at an affordable price (free), I thought I will list out some of the use cases here and open it up to the CloudAve community to suggest other use cases. This way all of us can benefit with this really nifty tool.\n Some of the use cases that come to my mind are:\n In today\u2019s world, we have a distributed workforce and a free tool like this can significantly help SMBs in their internal collaboration. Since Google+ is integrated with Google Apps, organization using the service can easily use Hangouts for internal collaboration. It\u2019s integration with Gmail can extend this collaboration externally to customers, partners, etc.. I was expecting Salesforce to use DimDim acquisition to offer similar feature inside Chatter. They do have some real time chat and screen sharing capabilities but not a video collaboration tool like Hangouts. But I expect them to put DimDim technology to use soon.\n Hangouts on Air can be used for running Office Hours for customers. This gives the customers a personal face to face feeling and improves on the customer satisfaction bottom-line. This can also be used in the same way organizations use GetSatisfaction but with video.\n Hangouts on Air can be used for running webinars to educate customers and potential customers on the products and services offered by the businesses.\n Hangouts on Air can be used to run online panels with experts on various topics. A smart organization can even take the tool and run their own user\/customer conferences.\n Hangouts on Air can be a good educational tool for reaching out to students. I know of many companies running coaching classes using Skype. Hangouts on Air is a much better tool to run this.\n \nI could think of many other scenarios where this tool can be put to good use. I would love to hear from CloudAve readers on how they are using or planning to use this tool for their organization. There are many other tools available which offers similar functionality but this one excites me because it is free and it is integrated with Google Apps (which I use for all my professional use). I also see the dangers of putting all information with Google but it is a different discussion and in this post I want to focus on the use cases.","item_date":"May 08 2012 18:03:41","display_item_date":"05-08-2012","url":"http:\/\/www.cloudave.com\/19493\/google-hangouts-on-air-and-smbs\/","source":"www.cloudave.com"},{"title":"Oracle gets a chance to rewrite software law","details":"The eyeless, mouthless Java mascot named Duke cartwheels across a T-shirt from a JavaOne conference.\n (Credit: Stephen Shankland\/CNET) \n  Every now and again, a court case comes along that stands to rewrite the legal rules of the computing industry -- and we might just be at such a juncture right now. \n Oracles suit against Google over Java and Android could be one such case. Its putting to the test the notion that application programming interfaces -- APIs -- can be copyrighted. \n In a partial verdict today, a jury gave Oracle a hard-fought yes when U.S. District Judge William Alsup asked it, As to the compilable code for the 37 Java API packages in question, taken as a group: Has Oracle proven that Google has infringed the overall structure, sequence, and organization of copyrighted works? \n Oracles lawyers cant crack the champagne yet, though: the jury didnt answer a key follow-up question about whether Google was actually permitted to copy the technology through a doctrine called fair use, leading Googles lawyer to call for a mistrial. The jury also took Googles side on a second question concerning whether it violated Oracles copyright on Java documentation. \n Last, and perhaps most significant, although Alsup had told the jury to assume the APIs were copyrightable for purposes of their deliberation, he also said hed rule on the matter if the jury found Google to have infringed. Finally, regardless of how the judge sorts out the muddle, an appeal seems likely. \n   Related stories\n Oracle v. Google jury returns partial verdict, favoring Oracle \n No partial verdict in Oracle-Google copyright case after all\n Oracle-Google: Prospect of a partial verdict or mistrial looms\n More jury questions, hair-splitting in Google v. Oracle \n Programming languages do not enjoy copyright protection, EU court says\n \n\n \n  Thus, the door remains open for Oracle to make a case that could reorder the software business. \n APIs are a defined mechanism by which one program can talk to another to get something done -- everything from telling Windows to open up a new window to telling Google Maps to show where the Canary Islands are. In the Oracle case, Java programs use those APIs to call upon the services of pre-written modules called class libraries. Oracle argues that its descriptions of these APIs are copyrighted and that Googles use of them to create Android therefore is infringement; Google of course disagrees. \n The case isnt likely to affect most consumers directly one way or the other. But its a different story for programmers, including those in the particularly fast-moving new area of cloud computing. Copyrighted APIs raise the possibility of new barriers to entry in a market, new ways businesses can stave off competition, new involvement of lawyers in product development. \n Intellectual-property lawyers may be fine with that future, but plenty of others shudder at the prospect, especially given how software patents, a reality since the 1990s, have resulted in infringement suits from patent trolls who dont have any business beyond suing deep-pocketed companies for alleged infringement. \n What will the impact be should APIs prove copyrightable? It is likely to be extensive, cascading and a lesson in unintended consequences, said Redmonk analyst Stephen OGrady. A decision in favor of copyrightable APIs is likely to be at least as damaging as the patent system is today. \n  Happier times: Sun and Google were Java allies in 2005, when Suns then-president Jonathan Schwartz, left, and CEO Scott McNealy, center, joined Google CEO Eric Schmidt to tout a partnership that ultimately fizzled.\n (Credit: Stephen Shankland\/CNET) \n  Ripple effects\n Some APIs are very simple, but collections of them can form a mechanism that lets programmers tap into a foundation for general-purpose computing. Java is one example, but there are more -- operating systems such as Windows or cloud-computing technology such as Amazon Web Services (AWS). \n Cloud computing is a particularly hot market right now. It lets companies use their own servers more efficiently, and it lets start-ups quickly ramp up operations without having to spend so much on hardware and software. AWS is the powerhouse, but there are competitors including Google App Engine, VMware vCloud, Eucalyptus, Open Nebula, Nebula, OpenStack, Nimbula, Zimory, CloudScaling, and more. \n Eucalyptus is an interesting case because it aims to let customers build internal systems that mirror what Amazon offers on the Internet. In March, Eucalyptus announced a partnership with Amazon, but Chief Executive Marten Mickos said he wasnt worried about infringing on Amazon intellectual property. \n We are ourselves very comfortable in our decision in 2007 to implement AWS API semantics in our open-source product. We did the implementation entirely on our own and without trespassing on anyone elses intellectual property, said Mickos, who by the way worked for Java creator Sun Microsystems for a time after it acquired the MySQL database company he previously ran. The reason for us to enter into the deal with AWS was to speed up our API compatibility work and to address customers jointly with AWS. \n Not everyone is so sanguine. \n APIs are something that takes an input and gives an output, said Ossi Niiranen, a lawyer with Linclaw in Finland. If thats protected, then the problem arises when you try to develop an API that takes same kind of input and gives similar type of output. Then youre potentially infringing. \n Amazon didnt respond to a request for comment. \n If APIs should become copyrightable, a new tool could arrive to protect a business through legal action. Application developers neednt worry, but those building platforms might have to. \n Its not an issue if youre using this API providers own platform to develop your apps. It turns into a problem when youre trying to compete with a platform, Niiranen said. This is really relevant for those bigger companies fighting for whose platform will be No. 1. \n Another interesting case is Yahoos Flickr, which offers a very widely used API that lets programmers build mobile applications that tap into the photo-sharing service, display its photos on external Web sites, and more. \n Yahoo didnt comment on whether it considers the Flickr API copyrighted, but it raises the possibility in the Flickr API terms of service: The Flickr APIs may be protected by copyrights, trademarks, service marks, international treaties, and\/or other proprietary rights and laws of the U.S. and other countries. \n And API design isnt necessarily a simple matter. Take the opinion of Murat Yener, who has constructed APIs for the Eclipse Libra project. \n The API should be easy to understand and fun to code on to attract developers. The API should be consistent on naming, usage, and behaviour so the developers will feel like its a platform, Yener said. The API should be well designed to be able to [accommodate] future requests and changes in a friendly way -- which you usually realize after you start developing version 2. \n At the heart of the API copyright matter is how much API design is an act of creativity -- whether an API is a mechanical byproduct of underlying technology on the one hand or a creative work on the other. \n What is copyrightable is creative expression, said Julie Samuels, an attorney with the Electronic Freedom Foundation What is not [copyrightable] is functional information. The programming language is not. You cant copyright a language. Its what you make of that language. \n Added Bruce Wieder of the firm Dow Lohnes, Originality is important. If theres one way to do something, then you have a real problem whether its copyrightable. \n Intellectual property and software\n You dont get too many chances to change how the courts see software. Lawsuits arent uncommon, but every now and again, one of them changes the relationship between software and intellectual property concepts such as patents and copyrights. \n One was a 1983 decision in a case in which Apple, then in its first glory years selling its Apple II computers, sued a company called Franklin whose ACE 100 computers used operating system software copied from Apple. That court concluded that software -- both the underlying source code programmers write and the resulting binaries the computer understands -- is protected by copyright. \n Then in 1986, in a case between Whelan Associates and Jaslow Dental Labs, the court extended copyright protection beyond the code to to the programs structure, sequence, and organization, or SSO. The courts analogy was that a program was like a books words, but the SSO was like its plot, which is also protected by copyright. (Remember that term, SSO? Its what the first jury question concerned in the Oracle v. Google case.) \n The Whelan decision gained clout when it was affirmed in a 1990 case pitting Lotus against Paperback Software; Paperback went out of business before it could appeal. That affirmation perhaps encouraged Lotus to sue Borland for spreadsheet software; Borlands product could run automated command sequences called macros that had originally been designed for Lotuss product. The case was once again about SSO. \n Borland lost the first round but won on appeal in 1995, and Lotus appeal of that decision failed to convince the Supreme Court. Thus was SSO hobbled. \n Another big case involved Apple yet again, this time against Microsoft, which Apple accused of violating copyright by adopting in Windows elements of the Mac operating systems look and feel. In 1992, the court rejected Apples contention. \n Even as courts defined some limits for software copyrights, though, another litigation option arrived in 1998 in the State Street Bank v. Signature Financial Group case: software patents. Where copyrights protect creative works, patents protect inventions that are new and useful. The lower court found that the software in question couldnt be patented because it was either a mathematical algorithm or a business method, but appeals court took a broader view of the federal laws title 35, section 101, which defines what is patentable as any new and useful process, machine, manufacture, or composition of matter, or any new and useful improvement thereof. \n The appeals court rejected the lower-court ruling: It is improper to read limitations into section 101 on the subject matter that may be patented where the legislative history indicates that Congress clearly did not intend such limitations, the court found. \n Unsurprisingly, given the size and competitiveness of the software industry, many software patent lawsuits arrived afterward -- many of them brought by so-called patent trolls that own patents but that dont have a business beyond selling rights to them. Software patents also figure in the second phase of the Oracle-Google case that has just begun. \n The most recent big case involving software patents came with Bilski v. Kappos, which went all the way to the Supreme Court. That court sidestepped an opportunity to reject software patents in general, with the majority opinion stating that the court need not define further what constitutes a patentable process. \n Four judges, though, added a concurring opinion that indicates that software patents dont enjoy broad support in the courts. Patents on business methods are patents on business itself. Therefore, unlike virtually every other category of patents, they are by their very nature likely to depress the dynamism of the marketplace, the concurring opinion stated. \n Copyrighted APIs\n For a detailed look at the technology underlying this court case, check our FAQ about Java and Android in the Oracle-Google case. A brief version goes like this, though: Java, absorbed into Oracle with its Sun acquisition in 2010, lets a given program run on a multitude of devices. \n To achieve that flexibility, Java comes with a virtual machine that adapts the Java program for the particular hardware its running on, and it uses class libraries that provide a wide range of pre-built abilities so that programmers dont have to write everything from scratch. To use these libraries, Java programs rely on each librarys API. A group of organizations called the Java Community Process defined these libraries and their APIs, but Sun kept copyright to the documentation that describes what the APIs do. \n For Android, Google wanted a running start so programmers could write Android apps as soon as possible, so it had long discussions about licensing Java from Sun. The companies couldnt agree to terms, though, and Google decided on a programming approach that was closely akin to Java. Google didnt use Suns Java source code (except for a nine-line tidbit that Google admitted shouldnt have made its way in), the Java brand, or the test kit that ensures Java foundations are compatible so that Java programs will run. \n Google built its own virtual machine, called Dalvik, and for the necessary class libraries, it relied on clean-room work to reproduce the software and on an open-source project called Harmony attempting to reproduce the Java environment. In all, it used 37 of Javas APIs. \n Sun objected to Googles Java approach, but it didnt sue. But Oracle did, in August 2010. \n Oracle argues that the APIs, not just its versions of the software itself, are copyrighted. \n The APIs are a detailed, intricate blueprint that is the product of over a decade of development work, Oracle argued in one brief. The APIs at issue are far more creative than the pieces of source code Google hired contractors to write over a period of months, when Google re-implemented the Java APIs according to the design it copied. \n Oracle also argued that in order to read the API documentation, Google needed to agree to particular conditions -- for example, that it maintain compatibility with Java (Android does not) and that it use the Java Technology Compatibility Kit (TCK) to prove it (Google did not). \n Google disagrees, as it wrote in one brief: \n\nThe APIs are merely the medium through which Java language developers express themselves. Here, with all due respect to Marshall McLuhan, the medium is not the message. It is a system that can be used to express. And a system, by definition, is outside the realm of copyright protection. Google also argued that its use of the Java APIs was transformative, meaning that it created something new and not merely derivative out of Java. Transformation is one way that a work can be protected from copyright infringement claim under the fair use provisions. Androids Java-like foundation includes APIs that Java does not, and Java includes APIs that Android does not. In addition, Android includes lower-level software such as a Linux kernel to interface with smartphone hardware and higher-level software such as utilities to keep track of contacts and synchronize calendars.  The APIs come up in two ways in the trial. First are the libraries themselves to which Oracle holds copyright, which the judge calls compilable code. Second is the documentation describing the APIs, including comments in Java library source code and specification details. \n  Oracles slideshow alleging how Google copied Java (images) \n 1-2 of 14\n  Scroll Left Scroll Right \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \n  The trial puts the API copyright question to the test through a two-step process. Alsup effectively told jurors to assume Oracles view of the situation is correct. The finding that Google infringed copyright with regard to the APIs means that the judge -- assuming he doesnt grant Googles motion for a mistrial and that he chooses to go ahead even though the jurors didnt decide whether Googles copying was permitted -- will himself decide whether the APIs are copyrightable. A jury decision that there wasnt infringement would have meant that Alsup could have left the question open. \n Copyright never protects any procedure, process, system, method of operation, concept, principle, or discovery, Alsop explained to jurors. For purposes of your deliberations, however, I instruct you that the copyrights in question do cover the structure, sequence, and organization [SSO] of the compilable code. \n Its a smart way to do it on the judges part, said Ed Walsh, an intellectual property attorney with Wolf Greenfield. If the jury says there was no copying, he gets to punt on the whole thing. If they say there was, he gets to decide on that. \n Leaving the matter to the judge is preferable, Niiranen said: Its very much a legal issue and and not really an issue of fact. It will most likely it will be appealed. \n The irony\n The case poses some ironies for students of the computing industry. Sun, often playing the role of feisty underdog to Microsoft, espoused software with open interfaces that let customers swap out one companys products for anothers -- a virtue co-founder Scott McNealy and former CEO extolled as a low barrier to exit. \n Although Sun advocated standards that could ease interoperability, though, it didnt exactly apply them to Java. In the 1990s, it backed out of a plan to standardize Java through a standards group called ECMA, a route that Microsoft eventually did take with its Java rival .Net. \n McNealy came to Oracles defense, but his successor, Jonathan Schwartz, who was in charge during the Google discussions, ultimately chose not to fight Google. That doesnt mean he was happy with Googles choice. After Java father James Gosling opined that Google totally slimed Sun, Schwartz tweeted, Goog slimed Sun w\/Harmony. Like Orcl slimed Sun w\/ #Linux, then slimed #RedHat w\/OEL [Oracle Enterprise Linux]. Capitalisms so inconvenient. \n Changing his position now would hardly reflect well on his decisions while Sun was negotiating with Google, though. At the same time that Google and Oracle were wrestling over Java licensing, Suns business was struggling after the financial crisis blasted its prime customer base, financial services firms. Sun didnt have much time and money to fight Google, but yielding meant Google didnt have to send any checks to Sun. \n A further irony: Android was founded to counter Microsoft, but if Google were to take the extreme step of scrapping Java -- something it threatened in discussions with Sun -- .Net and accompanying C# language could be a viable replacement. Thats because Apple actually emerged as the dominant mobile technology company. Theres even a conveniently available open-source implementation of .Net called Mono, with more permissive licensing than Java. \n Embracing .Net would require a top-to-bottom overhaul of Android and its developer community, though, so dont expect it. \n At the request of Google co-founders Larry Page and Sergey Brin, Googles Tim Lindholm looked for technical alternatives to Java, but in a 2010 memo, he concluded, Weve been over a bunch of these, and think they all suck. \n Now, with Microsoft a mobile underdog and Oracle a potentially very expensive thorn in Googles side, maybe .Net doesnt look so unpleasant. \n Heres the jurys verdict in Oracle v. Google (copyright phase):","item_date":"May 08 2012 17:33:17","display_item_date":"05-08-2012","url":"http:\/\/news.cnet.com\/8301-1001_3-57429147-92\/oracle-gets-a-chance-to-rewrite-software-law\/","source":"news.cnet.com"},{"title":"Will Data Monopolies Paralyze the Internet?","details":"Google has built an advanced database of street geometries by sending cars like these around the world. The data they collect would be a crucial component in driverless cars, and the database itself may be forbiddingly expensive to replicate from scratch. But the streets that Google has mapped remain available for some innovative entrepreneur to map in a different way. (Image credit: Getty Images via @daylife)\n \n Tim O\u2019Reilly spoke with me last week about Internet companies acquiring massive proprietary data sets. \u201cWe\u2019re kind of heading toward data as a source of monopoly power in some cases,\u201d he told me, comparing data to \u201cIntel Inside\u201d as a barrier to market entry. He qualified a moment later, \u201cThere\u2019s so much innovation still ahead that I\u2019m pretty confident there\u2019s room for more.\u201d Tim\u2019s venture capital partner, Bryce Roberts, followed up with a thoughtful post that foresees the end of Web 2.0 once all of that free-living user-generated data that we celebrated a decade ago\u2013blogs, shared photos, message boards\u2013moves behind password protection on social networks.\n Data monopolies are a real possibility, but I think their rise will be tempered by ways of collecting data that barely existed just a couple of years ago. The world\u2019s data isn\u2019t something that can be held captive by a single operator, in the way that the world\u2019s scandium mines could conceivably be bought by a nefarious baron who would laugh menacingly while twirling his moustache (\u201cFools!\u201d he would say, his craggy face lit from below by a flickering fireplace).\n Take as an example Google Maps: Google has built an extremely accurate roadway database by dispatching a fleet of telemetry cars around the world to collect data from high-definition cameras, laser rangefinders and GPS receivers. That data will be central to the company\u2019s efforts to develop commercial autonomous cars. If those cars become widespread, Google will enjoy an enormous commercial advantage on the quality of its roadway data, which by then would be extremely expensive for anyone else to reproduce from scratch by the same method.\n \n But regardless of how Google shares\u2013or doesn\u2019t share\u2013its roadway data, the roads it has mapped will still exist outside of Google, ready to be mapped again by some inventive entrepreneur. In fact, Google\u2019s method for collecting its roadway data\u2013buying and outfitting cars and hiring humans to drive them up and down every street in the world\u2013already looks a little old-fashioned. You might imagine that the next great roadway database could be compiled by stitching together geotagged images found on photo-sharing sites to create 3D models, or by aggregating location data from mobile phones or in-car telematics to find road lane centerlines and infer speed limits.\n Social networks are somewhat different, of course: privacy-minded users have handed over vast troves of valuable data to Facebook and then locked it behind passwords, where it\u2019s less accessible to neutral crawlers. Once you\u2019ve handed over your biographical details and consumer preferences to Facebook, then Facebook can make money off of them, sure, but those biographical details and consumer preferences still exist outside of Facebook. They appear in all sorts of other things that you do and are waiting to be captured by another company that maybe doesn\u2019t exist yet. Unless Facebook finds a way to copyright your birth date, the enormous value of its database will also serve as an enormous incentive for new companies to look for the same data elsewhere.\n And, for what it\u2019s worth, Facebook has found it has to be reasonably free with its users\u2019 data in order to become the foundational platform for the entire social Internet. The site\u2019s API allows outside applications to operate in much the same way that users operate\u2013posting status updates, seeing \u201clikes\u201d\u2013once users permit them to do so.\n You don\u2019t even need user approval to get lots of valuable information via screen scraping. I\u2019ve written scripts that impersonate, by submitting the right cookies, a Facebook member I created named Testingoutsome Features\u2013a twenty-year-old retired Penn Central Railroad fireman in Altoona who is a fan of the birther movement. Facebook didn\u2019t react to the easily-noticeable incongruities of a man whose career ended 15 years before he was born\u2013or to the fact that Testingoutsome was visiting as many as ten member profiles per second over periods of several days. I was able to extract location attributes for a majority of the 175,000 or so people who have written on Sarah Palin\u2019s Facebook wall\u2013data that\u2019s not available through the API without user authorization. It\u2019s true that Facebook could abruptly shut off access to become an impenetrable fortress of mineable chirps, but it seems that, in practice, something in the market has given Facebook reason to let some of its data leak out.\n Some very promising data hasn\u2019t been collected on a large scale yet and might be less susceptible to monopolization than things like status updates. Lots of people I spoke with at the Where conference last week were excited about new ways to approach ambient data. Companies like Alohar Mobile, which Tim mentioned in my interview, promise to collect the little specks of data that we\u2019re constantly releasing\u2013our movements, via smart phone sensors; our thoughts, via Twitter feeds\u2013and turn them into substantial data sets from which useful conclusions can be inferred. The result can be more valuable than what you might call deliberate data because ambient data can be collected consistently and without relying on humans to supply data on a regular basis by, say, checking in at favorite restaurants. It also offers great context\u2013another crucial theme from my conversations in California\u2013because constant measurements make it easier to understand changes in behavior.\n Bigger companies have obvious advantages in getting into ambient data (Facebook\u2019s installed app base would give it an immense advantage if it decided to start collecting more data from phone sensors). But that sort of data is more difficult, in some ways, to confine within a monopoly.\n We should watch carefully for the emergence of data monopolies, but I\u2019m optimistic. Lots of very innovative people are working on new ways to harvest data, and any kind of monopoly will only make their work more lucrative.","item_date":"May 08 2012 17:20:08","display_item_date":"05-08-2012","url":"http:\/\/www.forbes.com\/sites\/jonbruner\/2012\/04\/12\/will-data-monopolies-paralyze-the-internet\/","source":"www.forbes.com"},{"title":"With 3scale New Infrastructure Open, Distribute and Manage APIs in Just Three Easy Steps","details":"3scale (www.3scale.net), one of the leading players in the API infrastructure market, today announced their new free self-service API management infrastructure platform. The platform removes many of today\u2019s barriers to launch and operate web APIs \u2014 enabling any company to provide access to their APIs whilst staying in full control of data volumes, audience and usage.\n\n Web APIs are fast becoming key to many web businesses as companies seek to open their data and services to others for integration, to power mobile applications and enable powerful ecosystems around their products. \u201c3scale mission is to make it fast and easy for everybody on the web to distribute and operate APIs \u2014 something we see as the underpinnings of where the Web is going,\u201d says Steven Willmott \u2014 co-founder and CEO of the company. \u201cTo date, API Management solutions have been expensive and inaccessible for general use \u2014 but as APIs become more critical we wanted to make sure best of breed solution was available to everybody.\u201d\n\n The new platform updates much of 3scale\u2019s existing infrastructure and as well as adding key new features such as:\n\n Your developer portal at developer.yourdomain.com driven with a fully-fledged CMS which provides full control over branding, content and developer signup workflows.\n\n Powerful Interactive \u201cActive Docs\u201d API documentation based on the Swagger Framework which allows developers to explore APIs live from documentation web pages.\n\n An easy to install open-source and on-premise API Traffic manager based on the Varnish proxy that enforces API traffic rules and integrates seamlessly with 3scale\u2019s infrastructure.\n\n \n\nThe new 3scale Free platform is available up to 50,000 API transactions per day and can scale as your API grows up to 100\u2032s of Million of calls per day with the Enterprise offering including enterprise grade SLAs, support and additional features and tools. \n\n 3scale powers 100+ APIs and has over 60,000 developers writing applications using these APIs. Current 3scale customers) and new APIs launching about to launch are already taking advantage the new 3scale API Management platform.\n\n \u201cThe new platform brings together the best solutions in a single API Management platform that enables almost anybody to distribute an API \u2014 with as few barriers to entry as possible and scalability all the way up to enterprise grade high volume APIs,\u201d says Guillaume Balas, CMO at 3scale. \u201cProviding these tools for free will means more companies and individuals can open APIs and in turn help power the API economy as it grows.\u201d\n\n Access to the new platform is via instant signup at www.3scale.net\n\n 3scale is one of the world\u2019s leading cloud-based API infrastructure providers and enables developers, and enterprises to securely open, control, manage and monetize their APIs. 3scale unique suite of infrastructure services brings API providers unprecedented control, visibility on API activity and peace of mind.","item_date":"May 08 2012 17:06:29","display_item_date":"05-08-2012","url":"http:\/\/www.3scale.net\/2012\/05\/3scale-democratizes-apis-with-new-free-self-service-api-management-infrastructure\/","source":"www.3scale.net"},{"title":"Hackers target Twitter spammers in massive account data breach","details":"Twitter is investigating after 55,000 account details \u2014 including username and password combinations \u2014 were published online.\n\nAccount details seemingly belonging to spammers were uploaded to Pastebin, a code-sharing site often used by hackers to post the results of their hacking escapades.\n\n\n\nThe accounts were published over five Pastebin pages \u2014\u00a0one, two, three, four, five. Legitimate users who are on the list are advised to change their passwords immediately.\n\n\n\n\n\n\n\n\n\n\n\nA Twitter spokesperson said the company was\u00a0looking into the situation.\u00a0\u201dWe have pushed out password resets to accounts that may have been affected,\u201d they added.\n\n\n\n\n\n\n\n\n\n\n\n\u201cWe\u2019ve discovered that the list of alleged accounts and passwords found on Pastebin consists of more than 20,000 duplicates, many spam accounts that have already been suspended and many login credentials that do not appear to be linked \u2014 that is, the password and username are not actually associated with each other.\u201d\n\n\n\n\n\n\n\n\n\n\n\nMany of the accounts however appear to be associated with \u2018bot\u2019 users, such as those representing machines which tweet based on keyword recognition or otherwise.\n\n\n\n\n\n\n\n\n\n\n\nOne user on Y Combinator\u2019s Hacker News noted that many of the accounts when logged in requested an email confirmation, suggesting the accounts may not in fact belong to human users.\n\n\n\n\n\n\n\n\n\n\n\nMany were suspended or only had a small number of followers, the user said.\n\n\n\n\n\n\n\n\n\n\n\nSpeculation has already erupted as to the source of the breach.\n\n\n\n\n\n\n\n\n\n\n\nAt this stage, it\u2019s unlikely to point the finger at Twitter itself. Based on the number of \u2019spam\u2019 accounts listed in the breach, it would not come as a massive surprise to learn that a third-party breach may have led to the disclosure.\n\n\n\n\n\n\n\n\n\n\n\nTwitter has become a short-message haven to all but every kind of person from all walks of life, from politicians to journalists, news wires and celebrities.\n\n\n\n\n\n\n\n\n\n\n\nBut it has also become a haven for spammers and bots that retweet and generate malicious links to tempt ordinary users into downloading malware. Twitter regularly shuts down spambots and fake accounts regularly, but many ordinary users notice spam on a daily basis.\n\n\n\n\n\n\n\n\n\n\n\nThe site recently said it would take spammers to court, claiming \u201cbad actors who build tools designed to distribute spam on Twitter\u201d make it easier for others to \u201c engage in this annoying and potentially malicious activity.\u201d\n\n\n\n\n\n\n\n\n\n\n\nSome suggest that these bot accounts are used to boost the popularity and follower share of other users, leading to suggestions there could be a \u2018black market\u2019 type situation outside the site\u2019s control.\n\n\n\n\n\n\n\n\n\n\n\nOne user explained: \u201cAutomatically generated accounts, profiles, and tweets. These accounts are used for services that provide paid followers and retweets. It\u2019s actually pretty interesting stuff if you look at the automatically generated \u2018Twitter Ipsum\u2019 that is their profile descriptions and how they randomly pick quotes from famous people to tweet.\u201d\n\n\n\n\n\n\n\n\n\n\n\nHow Twitter will respond to this will be interesting.\n\n\n\n\n\n\n\n\n\n\n\nIt can denounce the leak \u2014 despite the high chances of the data breach not coming from Twitter itself \u2014 or it can actively do something about the persistent spam issue.\n\n\n\n\n\n\n\n\n\n\n\nEither way, Twitter has to acknowledge that while the vast majority of its\u00a0140 million users are legitimate, the site still has a large proportion of fake accounts and those that tweet vast amounts of spam to its users.","item_date":"May 08 2012 07:00:00","display_item_date":"05-08-2012","url":"http:\/\/www.zdnet.com\/blog\/btl\/hackers-target-twitter-spammers-in-massive-account-data-breach\/76482","source":"www.zdnet.com"},{"title":"Big Broad Data: The role of Data APIs","details":"In previous posts on this topic of Big Broad Data, we looked at some of the reasons for and implications of enterprises shifting their focus from the \u201cbigness\u201d and technology hype of \u201cBig Data\u201d to breadth and diversity, signal extraction, analytics and deep insights.\n\nThe future is around the easy consumption, the flow and interaction of data, which drives a revolution in the world of Data APIs. The structure of the Data APIs becomes increasingly important.\n\nBuilding an Information Halo around APIs\nLet\u2019s consider enterprises as one of two types from a data perspective: those for whom data is the core business and those who give data away to attract increased transactions to the core of their business. \n\nIn the latter case, the data (information) itself is not necessarily monetizable, but it attracts people to the business.\nFor a great discussion about the notion of information halos around your core business, see Sam Ramji\u2019s talk: Amundsen\u2019s Dogs, Information Halos and APIs: The epic story of your API Strategy.\n\nIn both scenarios, data is a fundamental and critical part of an API strategy\nEnterprises who are monetizing around data are beginning to plant flags in different domains. Weather, finance, real estate, Internet traffic and dozens or hundreds of other domains are forming.\n\nThe people building the data in any domain are doing so by collecting from disparate data sources. To build out any one domain, they\u2019ve probably stitched together data from a large number of data sources, cleansed and standardized it before finally exposing it as an API.\n\nWhat are companies using to collect and stitch the data?\nA natural and familiar stitching technique is the linked data model (linkeddata.org). While linked data techniques are excellent at accessing individual data elements, I argue that this is not the model that these data providers need. Instead they need to crawl, bulk load, and access data in large quantities, before cleansing, standardizing, and delivering it.\nIf linked data is not the most effective method to stitch data together to create the domains (at the bottom of the stack), can linked data become the de-facto standard to express data out of the information halo (at the top of the stack and as the Data API for domains)?\nThe answer is probably yes \u2013 eventually. I think it is an unlikely scenario just yet. Today, the challenge is how to cleanse, standardize, unify and use the data in individual domains. Linked data techniques have the right characteristics to bring together data that have already been cleansed, standardized, and stitched but is not a great model to do the initial stitching. It will most likely become useful and common in the future when the inter-linking of domains becomes more important than it is today.\nIf linked data is not the approach to expose data as APIs, what is?\nThe school of thought to which I subscribe is one of schema-based access to data APIs patterned after relational models. Here are a few examples of data APIs, which highlight three common kinds of data access patterns.\n\n* Primary key lookup - to get to a specific data element.\n* Imposed hierarchy-based lookup - in which you have classes with hierarchy and in effect traverse the hierarchy to get to the data elements.\n* Rectangular lookups - defined by typical relational lookups of rows and columns.\n\nAll of these techniques are being built around single data sources as opposed to massively linked data sources.\n\nThe structure and \u201cRESTification\u201d of  Data APIs\nThere are several approaches to Data APIs. In addition to the perspective that is Pragmatic REST for SQL Developers, there\u2019s Microsoft\u2019s OData approach. As I asserted in a recent talk at an OData Meetup event at Microsoft, OData is a step in the right direction but there are certain things OData needs to do to become the de-facto standard.\nThe \u201cRESTification\u201d of the Data APIs is a fundamental imperative and both the Pragmatic REST for SQL and the OData approaches are good starting points.\nWhatever the solution is, it cannot be vendor specific. Data is too important, and the data revolution too fundamental for it to be associated with any one vendor.\nOData technologies need to be available in all ecosystems, not just in the Windows Foundation Classes (WFC) library and the .NET Framework. Similarly pragmatic REST and other techniques cannot be available in Apigee or any other single vendor offering only.\nLet the call to action be to come together as a community; get the best of the linked data and OData ideas and techniques together and transform the world with Data APIs.\nThe conversation has already started and we\u2019d love to hear more of your perspectives and arguments over on api-craft.","item_date":"May 08 2012 07:00:00","display_item_date":"05-08-2012","url":"http:\/\/blog.apigee.com\/detail\/big_broad_data_role_data_apis\/","source":"blog.apigee.com"},{"title":"EU's top court: APIs can't be copyrighted, would \"monopolise ideas\"","details":"The European Court of Justice ruled on Wednesday that application programming interfaces (APIs) and other functional characteristics of computer software are not eligible for copyright protection. Users have the right to examine computer software in order to clone its functionality\u2014and vendors cannot override these user rights with a license agreement, the court said.\n  The case focuses on the popular statistical package SAS. A firm called World Programming created a clone designed to run SAS scripts without modification. In order to do this, they bought a copy of SAS and studied its manual and the operation of the software itself. They reportedly did not have access to the source code, nor did they de-compile the softwares object code.\n  SAS sued, arguing that its copyright covered the design of the SAS scripting language, and that World Programming had violated the SAS licensing agreement in the process of cloning the software.\n  The EUs highest court rejected these arguments. Computer code itself can be copyrighted, but functional characteristics\u2014such as data formats and function names\u2014cannot be. To accept that the functionality of a computer program can be protected by copyright would amount to making it possible to monopolise ideas, to the detriment of technological progress and industrial development, the court stated.\n  The purchaser of a software licence has the right to observe, study, or test the functioning of that software in order to determine the ideas and principles which underlie any element of the program. Any contractual provisions contrary to that right are null and void, the court ruled.\n  American courts have generally agreed with the European courts position that functional characteristics of computer programs are not eligible for copyright protection. (But the idea is currently under debate in the high-profile legal battle between Google and Oracle. Oracle says Google violated its copyrights by cloning Java APIs for use in Android.)\n  But American courts have been reluctant to overrule license agreements, which often remove rights that a user would otherwise possess. For example, in 2010 the US Court of Appeals for the Ninth Circuit upheld EULA terms that prohibited reverse-engineering World of Warcraft.\n  After Wednesdays ruling, European software users enjoy broader rights to clone software than do users on this side of the pond.\n                                                                  Photograph by C\u00e9dric Puisney                     \n                                                                Further reading\n           EU Court of Justice: No Copyright on Computer Functionality or Computer Languages (groklaw.net)","item_date":"May 08 2012 05:40:41","display_item_date":"05-07-2012","url":"http:\/\/arstechnica.com\/tech-policy\/news\/2012\/05\/eus-top-court-apis-cant-be-copyrighted-would-monopolise-ideas.ars","source":"arstechnica.com"},{"title":"Google Calls for Mistrial After Jury Says Android Stole From Java","details":"A jury has ruled that Google infringed on Oracle\u2019s copyrights in building a new version of the Java platform for its Android mobile operating system, but it was unable to reach a decision on whether this infringement was acceptable under the law.\n With this paradoxical partial decision, the jury has left the case very much in the air, and Google has already moved for a mistrial.\n On Monday, as the Google-Oracle case entered its fourth week, a jury ruled that Oracle has proven that Google infringed the overall structure, sequence, and organization of copyrighted works of 37 APIs used by the Java platform. In building Android, Google created a new version of the Java platform known as the Dalvik virtual machine, and this mimicked the Java APIs, or application programming interfaces, which are essentially a way for a Java application to talk to the platform.\n But the jury was unable to reach a decision on whether Google\u2019s Java clone constituted \u201cfair use.\u201d A fair use decision would let Google off the hook.\n After receiving the verdict, Judge William Alsup told the jury it would not have to answer the fair-use question, and Google immediately asked that the judge declare a mistrail, arguing that precedent says you can\u2019t decide on infringement without deciding on fair use. Google will file a brief and expand on this argument tomorrow.\n \u201cWe appreciate the jury\u2019s efforts, and know that fair use and infringement are two sides of the same coin,\u201d reads a canned statement from Google sent to Wired. \u201cThe core issue is whether the APIs here are copyrightable, and that\u2019s for the court to decide. We expect to prevail on this issue and Oracle\u2019s other claims.\u201d\n But in its statement to the press, Oracle treated the decision like a victory. \u201cOracle, the nine million Java developers, and the entire Java community thank the jury for their verdict in this phase of the case,\u201d the statement read. \u201cThe overwhelming evidence demonstrated that Google knew it needed a license and that its unauthorized fork of Java in Android shattered Java\u2019s central write once run anywhere principle.  Every major commercial enterprise \u2014 except Google \u2014 has a license for Java and maintains compatibility to run across all computing platforms.\u201d\n The jury also said that Google infringed a particular part of Oracle\u2019s Java code: the \u201crangeCheck\u201d method in the files \u201cTimSort.java\u201d and \u201cComparableTimSort.Java.\u201d But it decided that Google did not infringe on Oracle\u2019s Java documentation, two other portions of Oracle\u2019s code (the source code in seven \u201cImpl.java\u201d files and the one \u201cACL\u201d file), or the English-language comments in a third portion of code (\u201cCodeSourceTest.java\u201d and \u201cCollectionCertStoreParameters Test.java\u201d).\n Oracle sued Google in August of 2010 after purchasing Java maker Sun Microsystems, claiming that Google infringed on its Java-related patents and copyrights in building Android. Oracle accused Google of infringing maliciously and without regard for Sun\u2019s licensing agreements and the intellectual property that Oracle now owns. \n The case has been closely watched because of Oracle\u2019s claim that Google violated copyright on 37 of its application\u2019s programming interfaces. APIs define standard ways for two pieces of software to talk to each other, and open-source developers generally consider it fair game to write programs that can connect via other people\u2019s APIs. Linux, for example, contains code written to the POSIX (Portable Operating System Interface) APIs.\n  Photo: Ariel Zambelich\/Wired  \n Because Android has been a runaway hit for Google, the case could send waves through the tech world. Google allows anyone to download and work with Android\u2019s code for free, and it\u2019s been used in everything from smartphones to microwaves to cars. At this point, it\u2019s not possible to know how hardware markers and coders will be affected by the verdict until damages are assessed. Before the jury reached its verdict, Alsup noted: \u201cThat\u2019s a problem I haven\u2019t had in past cases.\u201d Right now, Oracle is seeking roughly $1.1 billion, but may argue for licensing rights as well.\n The Android operating system is most widely known for its use on mobile devices. Amazon\u2019s Kindle Fire, Samsung\u2019s Galaxy and HTC\u2019s Flyer use Android, among many others, to power their tablets. During closing arguments, Oracle attorney Mike Jacobs said there were 750,000 Android activations on mobile devices each day. In February, comScore, an analytics outfit, reported that Android had just crested 50 percent of the smartphone market, a 17 percent jump since February of 2011.\n Last week, just before the jury began deliberations, Judge William Alsup defined the boundaries of the case, saying that copyright protects the \u201cexpression of ideas\u201d but not a procedures, processes, systems, methods of operation, concepts, principles, or discoveries. For the purposes of this case, he said, Oracle\u2019s copyrights cover the \u201cstructure, sequence, and organization\u201d of the actual software code that underpins the Java platform.\n But it seems that Alsup may change his stance on this.\n Throughout the trial, Google had tried to poke holes in Oracle\u2019s accusations, saying that the codes were very different from one another \u2014 Android has about 15.3 million lines; Oracle\u2019s recent Java version has about 4.7 million \u2014 and that they were only similar in their \u201cmethod signatures,\u201d code that defines the inputs and outputs for part of a computer program. Google also presented e-mail evidence that when it built Dalvik, it went to great lengths to find engineers that wouldn\u2019t be influenced by previous Java programming work. Out of those 15.3 million lines of Android code, Oracle could only accuse Google of copying nine. \n \u201cWe at Google have always believed that the best kind of software is generally open,\u201d said former CEO Eric Schmidt during his testimony.\n Taking the stand for Google, Sun CEO Jonathan Schwartz said that as Android was being built, he didn\u2019t like it, but that he wasn\u2019t going to stop it. Sun was taking the longer view that more developers using Java-based languages would benefit his company and help them break up the grip Microsoft had on the office software market, according to Schwartz.\n \u201cWe wanted to build the biggest tent and invite as many people as possible,\u201d Schwartz said. \u201cYou have open APIs and compete on implementations.\u201d\n Today\u2019s partial verdict only covers the \u201ccopyright\u201d portion of the case. Now comes the \u201cpatent phase,\u201d where the tech giants will argue over two patents Google allegedly violated when building Android. Regardless of whether Google loses in this second section, a \u201cdamages\u201d phase will follow where the two sides will argue over what exactly Google owes Oracle.\n Oracle Google jury form 05-07-2012","item_date":"May 07 2012 19:43:06","display_item_date":"05-07-2012","url":"http:\/\/www.wired.com\/wiredenterprise\/2012\/05\/oracle-google-verdict\/","source":"www.wired.com"},{"title":"Could an Oracle Win Against Google Blow Up the Cloud?","details":"A San Francisco court has spent the past few weeks considering a copyright question that could weigh heavy on the future of cloud computing.\n It\u2019s part of a high-profile lawsuit between Oracle and Google. Oracle says that Google violated its copyrights and patents when it wrote its own version of Java for the Android mobile operating system. Part of what the court is trying to figure out this week is whether Google wronged Oracle by writing software that mimicked the Java Application Programming Interfaces (APIs are coding standards that let programs communicate with one another).\n The conventional wisdom in the coder community has been that it\u2019s fine to reproduce the interface of someone else\u2019s APIs, so long as you don\u2019t actually copy their software. So if the court finds that APIs are copyrightable, it could have major implications for any software that uses APIs without explicit permission \u2014 Linux for example. But it could affect things in the cloud, where there are several efforts to clone Amazon\u2019s Web Services APIs.\n \u201cIf APIs can be copy-protected, that would be incredibly destructive to the internet as a whole for so many different reasons,\u201d says George Reese, Chief Technology Officer with enStratus Networks, a seller of cloud management services. \u201cBut with respect to cloud, in particular, it would put any company that has implemented the Amazon APIs at risk unless they have some kind of agreement with Amazon on those APIs.\u201d\n An open source effort called OpenStack is the most prominent example of a project that mimics Amazon\u2019s APIs, and the case could give Amazon legal grounds to seek licensing deals from OpenStack users such as Hewlett-Packard and Rackspace.\n But other projects reproduce Amazon\u2019s APIs, including Citrix\u2019s CloudStack project and middleware such as Jclouds and Fog.\n \u201cThe problems that would face cloud computing are many of the same problems we\u2019d see, frankly, all over the internet if APIs were copyrightable,\u201d says Julie Samuels, an attorney with the Electronic Frontier Foundation who has been following the trial.\n Depending on how U.S. District Court Judge William Alsup rules, the U.S. could have a different take on this question from the rest of the world. This week, a European court ruled that APIs are not copyrightable, and Alsup has asked Google and Oracle to submit briefs on how that ruling should be viewed by the court. Both parties have until May 14 to comment on this, so it doesn\u2019t look like Alsup plans to rule on the copyright question until after then. Just to make matters more complicated, a jury is simultaneously deliberating Oracle\u2019s case, but they won\u2019t be answering the API copyright question; that\u2019s up to Alsup himself.\n One thing that makes the issue particularly troubling for open source projects is the extremely long shelf life of copyrights, Samuels says. Patents expire after less than 20 years, but copyright would protect the Amazon APIs for 95 years from the date they were first published, she says. \u201cCopyright lasts a hell of a lot longer than patent protection.\u201d\n On the bright side, at least for open source hackers, is the possibility that a ruling in favor of copyright-protecting APIs could push cloud providers to come up with new, open, standard APIs. But it\u2019s not much of a sliver lining, according to enStratus\u2019s Reese. \u201cWhile that\u2019s potentially useful for cloud [computing],\u201d he says. \u201cI am much more concerned about the implication for the internet as a whole. Or, more realistically, America\u2019s role in building internet companies. No other country is going to honor the idea of copyrighted APIs.\u201d\n Adding to the uncertainty, Amazon has never said whether it thinks companies that implement its APIs violate its copyright. Amazon, for example, has a partnership with another cloud company that implements its APIs, called Eucalyptus, but neither company could immediately provide a comment saying whether their agreement covered API copyright or not.\n \u201cNo one actually knows outside of Amazon what their attitude is toward the stewardship of their APIs and what people can do with them,\u201d says Jason Hoffman, the chief technology officer with Amazon competitor Joyent. Joyent uses its own APIs, not Amazon\u2019s.\n Hoffman says that Joyent is fine with other companies cloning its APIs, because its core intellectual property lies elsewhere. \u201cThe APIs are not the thing that makes or breaks our business,\u201d he says. \u201cOur margins depend on extremely good software that\u2019s meant to manage a bunch of data centers.\u201d","item_date":"May 07 2012 18:48:50","display_item_date":"05-07-2012","url":"http:\/\/www.wired.com\/wiredenterprise\/2012\/05\/oracle_clou\/","source":"www.wired.com"},{"title":"Cloud API Security Panel at Infosec","details":"This guest post comes from Andreas Krohn, API Specialist and consultant at Dopter. He blogs about APIs at Mashup.se (in Swedish) and develops tools to make APIs successful at APIHQ.\n\n\nCloud API Security was the topic for a panel discussion at the Infosec conference in London April 26th. After a brief introduction of what  APIs are, how companies are becoming platforms and what security  implications this has the discussion mostly focused on how to secure  mobile apps and how to keep security tokens protected.\n\n\n\n\n\nThere is a significant risk with baking in a security token in a  mobile app. It is quite easy to listen to the traffic between the app  and the server and catch the token, even if the traffic is encrypted.  Once the token is out the only way to handle the situation is to use a  new token and release a new version of the app. This in turn requires  users of the app to update before the old token can be invalidated. To  get all users to update within a small time frame is almost impossible,  so the security breach will remain for a long time.\n\n\nInstead of baking in the security token in the app it is better to  put the security on the user level instead of in the app level. Let the  user authenticate the data access, for example via OAuth 2.0. This means  that there is no risk that a token giving access to all the data can  get into the wrong hands, because such a token does not exist. Also, if  there is a security breach and a token has fallen into the wrong hands  it is easy for a user to invalidate the token and create a new one  without involving any other party.\n\n\nThe recent security problems with the Facebook Android SDK shows that  there is an inherit risk of using 3rd party modules. In that case a  security token was by mistake left in a log file that another 3rd party  application could access. The SDK lowered the bar of creating a Facebook  Android app, but it also propagated the error. Even if Facebook acted  quickly and patched the security hole it did not really help since it  requires all app developers using the SDK to update, and all the  end-users using those apps to update. Since this is not going to happen  in 100% of the cases it means that insecure SDK implementations will be  out there for a very long time. One way to get around such a problem is  to build web apps instead of native apps. Then any security problem can  be quickly fixed in one central location and there is no need to wait  for users to update their local versions of an application.\n\n\nLetting somebody else get access to your security token is the same  as letting them get access to your credit card. They are free to  impersonate you to get at your data and run up your tab while doing so.  This can be very expensive for APIs that charge a premium for access.  Developers need to be aware of this connection and treat security tokens  just as they do their credit card.\n\n\nOne way of making security tokens more secure is to limit the access  of a given as much as possible. Do not use one token or key for an  entire corporation, instead tie the token to a specific user. This  limits the potential damage as well as creates clear accountability in  case something happens.\n\n\nIt is clear that APIs is an area that can not be ignored anymore and\u00e6  that companies soon will have to provide APIs to survive. Having APIs  exposed to 3rd parties brings with it a new set of security problems of  which we all need to be aware. Since technologies change so rapidly it  is important to use a pluggable and extendable security solution that  allows for quick adaptations rather than to be stuck in one scenario.  This security solution need to be able to handle core security services  such as federation, single sign on and user authentication.\n\n\nParticipants in the panel were Travis Spencer from Ping Identity, Steven Willmott from 3Scale, Per H\u00e4ger\u00f6 from Technology Nexus and Andreas Krohn from Dopter. Moderator was David Terrar from Eurocloud UK.","item_date":"May 07 2012 18:48:45","display_item_date":"05-07-2012","url":"http:\/\/blog.programmableweb.com\/2012\/05\/07\/cloud-api-security-panel-at-infosec\/","source":"blog.programmableweb.com"},{"title":"Twitter API Has Spawned Other APIs And Lots Of Opportunity","details":"SoulPancake is looking for a Channel Optimization and Audience Development Manager.  \t\t\t\t\t \t\t\tBabble Media, Inc. is looking for a Blog & Social Media Editor.  \t\t\t\t\t \t\t\tImagination is looking for a Account Manager (Social Media). see all \t\t\t\t\t\t\n  \tTwitter API Has Spawned Other APIs And Lots Of Opportunity\n \tRaj Dash on April 7, 2010 3:04 PM\t \tNot only have the Twitter platform and API enabled hundreds of 3rd-party applications, they have helped spawn 43 other APIs \u2014 or Application Programming Interfaces \u2014 as well as a decent number of application opportunities.\nAPIs, if you\u2019re not familiar with them, are the coding means by which web services companies allow developers outside their employ to integrate their services into other applications. Companies like Twitter offer up a sort of \u201cblack box\u201d of functionality that other developers can access, to build their own apps. As Programmable Web writes, the Twitter API itself has birthed no fewer than 43 other APIs at present. Some of them are relative unknowns, but some of the APIs are for fairly well known services, including TweetLater, TwitPic, and others.\n This quantity of other APIs simply shows that Twitter has a thriving developer ecosystem. It also means that there are many opportunities for developing applications built over the Twitter platform to satisfy various needs. In that regard, VC Fred Wilson has a fascinating look at the Twitter platform\u2019s inflection point, comparing the situation to how certain killer apps helped save Apple Computers \u2014 way back when the desktop publishing era first started \u2014 as well as to how popular apps are adding to the value of Facebook. He mentions that Comscore data indicates that Twitter\u2019s global platform is the 33rd-largest in the world, as of Feb 2010. (Twitter\u2019s first official developer\u2019s conference, Chirp, takes place in San Francisco on Apr 14-15.)\n Where does Fred Wilson see the opportunity for killer apps on the Twitter platform? In several areas, including social gaming, various verticals, enterprise, discovery and analytics, all of which he explains in more detail on his post. Personally, I think there\u2019s a great deal of opportunity in the analytics area. Consider the recent report from two HP Labs researchers that they\u2019ve devised a model that uses Twitter to predict box office sales for new movies. There\u2019s also room for applications that manage the Twitter social graph, especially if these apps are targeted towards social marketers, entrepreneurs and others. As for social gaming, Twitter\u2019s Evan Weaver, Manager of Infrastructure, pointed out at the recent GDC 2010 (Game Developers Conference) that the company is looking to hire engineers who could help build out the platform to enable 3rd-party developers to create social games. Add mobile devices such as smartphones and the iPad into the mix, and there are even more opportunities.\n So the opportunities for applications are there, along with a wide array of Twitter-related APIs for development. What remains is for developers to determine how to monetize these applications.","item_date":"May 06 2012 22:21:35","display_item_date":"05-06-2012","url":"http:\/\/socialtimes.com\/twitter-api-has-spawned-other-apis-and-lots-of-opportunity_b9628","source":"socialtimes.com"},{"title":"59 New APIs: Intuit, StumbleUpon and FireText","details":"This week we had 59 new APIs added to our API directory including a StumbleUpon URL utility, semantic text analysis service, SMS marketing service, financial analysis, planning, and reporting service, Intuit hosted payments service and a data analysis and management service. In addition we looked at how the Screenr API can help support your systems. Below are more details on each of these new APIs.\n\n\n AdaptivePlanning API: The service provides a suite of financial analytics and planning functions, including annual budgeting, forecasting, variance analysis, and consideration of what-if scenarios. It integrates with with many familiar general ledger (GL) systems, as well as customer relationship management (CRM), and human resource (HR) planning systems. It can meet analytical needs of many enterprise resource planning (ERP) functions.\n\n\nAPI methods support importing data from integrated systems to perform analysis, then processing by spreadsheet-like row and column structures. Methods export results of data analysis back to integrated systems in similar formats.\n\n\n Appointment-Plus API: Appointment-Plus is an online reservation software provider. Business can embed Appointment-Plus links onto their websites or use Appointment-Plus as a standalone booking service as their website. It also provides POS functions for businesses to accept payment through the appointment-making interface. Their RESTful API provides functionality for integrating scheduling making software, and replies in JSON and XML formats. Developers must request access to the API and its documentation. \n\n\n Ask AMEE API: AMEE is a platform for greenhouse gas calculation, with access to emissions factors, calculation algorithms, and storage for data history. Ask AMEE is an experimental search platform that attempts to provide natural language query capabilities to AMEE data. The Ask AMEE API gives developers access to the functionality of this service. The API uses RESTful calls and responses are formatted in JSON.\n\n\n ATutor AChecker API: ATutor is a Web-based Learning Content Management System. The software gives users access to a variety of features including student profiles, collaborative groups, file storage, messaging, content tracker, social networking and more.\n\n\nAChecker is a Web accessibility evaluation tool. It lets users review web page accessibility using a number of international accessibility guidelines. The API allows users to review accessibility validation and to save or reverse decisions made on accessibility checks that a human must make. The API uses RESTful calls and responses are formatted in XML.\n\n\n ATutor AContent API: ATutor is a Web-based Learning Content Management System. The software gives users access to a variety of features including student profiles, collaborative groups, file storage, messaging, content tracker, social networking and more.\n\n\nAContent is a learning content authoring system and respository used to create Web-based learning content. It can be used along with learning management systems to develop, share, and archive learning materials. The API allows developers to access AContent from third party applications. The API uses RESTful calls and responses are formatted in XML.\n\n\n Avatax Sales Tax API: Avalara offers cloud based sales tax management services. With services such as AvaTax, users can manage their sales taxes, exemption certificates, and filing compliance.\n\n\nThe AvaTax API exposes the functionality of the AvaTax service. With it users can get applicable tax details for the given geographic coordinates and sale amount, calculate the tax for one or more invoiced items, validate the supplied address and more. The API uses RESTful calls and responses are formatted XML and JSON.\n\n\n BeachGuard API: BeachGuard is an online system originally used in Michigan that gathers beach sampling data from local health agencies. BeachGuard has since been implemented by three other states; Illinois, Indiana, and Ohio. The service contains two main functionalities, retreiving beaches via query and returning information on beach closures and advisories. The API uses RESTful calls and responses are formatted in XML.\n\n\n BellaDati API: BellaDati is a data analysis and management service. Users can connect their databases and cloud services, such as MySQL and Twitter, to BellaDati\u2019s business dashboards to produce analytical reports, graphs, and charts. They can then integrate the results and visualizations onto other platforms. The API is a RESTful protocol with JSON responses that gives developers access to their mobile product, BellaDati Piccolo.\n\n\n Carrier411 API: The service allows customers for trucking and transportation services to review the records of competing providers and evaluate their qualifications to meet specific needs. It allows applications to monitor changes in trucking companies insurance, safety records, and compliance with operating requirements. Users can qualify motor carriers by their CSA 2010 BASIC scores and create Due Diligence Certificates or Eco-Cert Certificates.\n\n\nAPI methods support retrieval of company listings for motor carriers, the dates and contents of safety report filings, and BASIC scores. Methods also support starting and ending monitoring of a carrier, which triggers notification of any  status changes on that companys listing.\n\n\n Comindware API: Comindware is a provider of web based work management software. Users get two pieces of software, Comindware Tracker and Comindware Task Management to help streamline their project management. The software can track tasks and issues, support collaboration, and show lists of viewers, collaborators, members and owners. The JSON API allows for integration with third party applications. Full documentation is not publicly available.\n\n\n DataDog API: DataDog is a multifunctional data management tool. It can be used for data visualization, contextualization, and other analytics. These functions are embedded into a collaborative work stream or dashboard to share data insights with colleagues. DataDog\u2019s programmatic RESTful API runs on a low-level HTTP interface and responds with JSON. It can be used for access to events and metrics, or for integration with other tools. \n\n\n Deezer API: Deezer is a web-based music streaming service operating out of France. Users can access their music via web browsers, mobile apps and offline. Currently the service offers a library of over 15 million songs. The service is not currently available in the United States. The API gives users access to the sites data making it available for use in third party web sites and applications. This data includes albums, artists, comments, editorials, folders, playlists, radio, searches, tracks and users. The API uses HTTP calls and responses are formatted in XML, JSON and PHP.\n\n\n Duo REST API: Duo Security enables protected login and transactional functions for smartphones users. It does this by sending users authorization verification from web-based platform to their phones, in addition to their login. Their REST API provides integration to their two-factor authentication process. It is a RESTful API, and returns JSON (defaut), BSON, and XML responses. \n\n\n Duo Verify API: Duo Security enables protected login and transactional functions for smartphones users. It does this by sending users authorization verification from web-based platform logins to their phones. Their Verify API provides access to the verification function with a one-time PIN number, as well as lookup methods for specific phone numbers and IP addresses. The API is RESTful and responds with JSON, BSON, and XML. \n\n\n Emoome API: Emoome is an emotional profiling tool. Users input words, feelings, and events at the app\u2019s prompting a few times a day. The app will chart, color-code, and create a timeline of the variety of emotions it interprets. The RESTful API provides developers and users access to all of their personal data stored in the system. The API uses a variety of formats, including, JSON, XML, and PHP.\n\n\n Experian QAS Address Validation API: The service integrates the providers Pro World Version (WV) functionality to return a complete, valid mailing address based on submission of variably formatted and even incomplete input supplied by a user or other source. The validation check occurs in the background, invisible to the user, and corrected address information allows enforcement of policies and prevents later troubleshooting of misdirected shipments due to inaccurate address input.\n\n\nAPI methods support submission of address data as entered by the user, including partial address information. Returned data provide full address information in standard format that reflects local formatting, to be transparently populated to available address fields. \n\n\n FireText API: FireText is an online SMS marketing platform that lets users send and receive instant SMS text messages with their customers. In addition to sending texts, users can schedule and receive messages, manage their contacts, report on deliveries and more. The FireText API gives users access to the functionality of the platform while allowing them to integrate it into their own applications including those from Magento and Zendesk. The API uses RESTful calls. Developers can contact the provider for full documentation. \n\n\n Fundraise.com API: Fundraise.com is platform for social fundraising. Users can create and manage fundraising campaigns from scratch or as newly integrated components of long-term projects or other social media platforms. Their API provides access to an accounts database functionality, allowing developers to retrieve information about donors, events, organizations, and more in JSON formats. \n\n\n FutureSimple Base API: Base is FutureSimple\u2019s CRM and sales tracking service for small business. It is designed for both web and mobile experience. It functions include methods for tracking sales and managing contacts. The API allows users access to the data in their Base user account. It is a RESTful API and provides responses in JSON and XML. \n\n\n GiftRocket Gift Card API: GiftRocket is gift card money transferring service. First, users buy their friends a gift card online and then GiftRocket sends the recipients money to redeem the present, by check, PayPal, or bank transfer. The API provides users access to gift card purchasing functionality. It is a RESTful API that responds in JSON. Access and an API key are granted by request.\n\n\n Global Matrix API: The service provides outsourced support for travel resellers, including research on rates and fares, selection of reservation options, and booking of reservations.  It aggregates offering information for hotel rooms, car rentals, and more from major GDS providers and presents options detailing possible choices. Applications can format for user selection, then return that data to the service, which processes the reservation.\n\n\nAPI methods support submission of requirements for travel-related services then returns matching offers from providers. Methods also accept user selections and complete registrations, returning appropriate confirmations. The API supports changes to previously booked reservations, as well.\n\n\n Homes.com Neighborhood Envoy API: The service provides a neighborhood profile for real-estate buyers based on 300 factors related to schools, population demographics, employment, climate, and local features. It allows integration with Multiple Listing System (MLS) property search to allow buyer review of prospective properties.\n\n\nAPI methods support submission of a property location and return data about the surrounding neighborhood. Methods allow integration with property displays, both on the web and via mobile device, along with reporting by location.\n\n\n Hotelston API: The service provides search and reservation booking for travel accommodations. It offers access to hotel room reservations, but also condos, villas, and apartments for short-term rental, depending on the destination. Services include identifying properties by date and location, booking reservations, and canceling previous bookings.\n\n\nAPI methods support submission of a location with check-in and check-out dates. Returned data include available properties, specifications, and rates with additional functionality to reserve accommodations. Methods also support reservation look-up, review, and modification or cancellation.\n\n\n iCasework UsefulFeedback API: The service performs customer service functions of accepting and logging inquiries and contacts, then tracking responses and resolutions. It allows applications to create cases with documentation of those involved, their concerns or comments, and responses intended to resolve those concerns.\n\n\nUsefulFeedback! provides a range of REST style web-services that allow both update and query operations such as case creation, adding case notes or retrieving case details to be invoked remotely. Tracking functions capture communications modes (phone, email, etc.) in addition to name, contact information, and relevant dates.\n\n\nAPI methods support case creation, including contact type (complaint, comment, suggestion, etc.), person initiating the case, along with response address, contact method, time, and language. Methods also document the topic of the contact, the agent responding, action requested, and response undertaken.\n\n\n Impact Radius API: Impact Radius provides performance-advertising services for web marketing expansion. Their API is available to advertising clients and media partners. It provides advertisers with functionality for uploading transactional histories from back-office systems, crediting media partners, and linking ecommerce systems to submit returns. For media partners it provides functionality for keeping approved campaigns up to date, build new promotional links, and receive live performance and earnings data. The API is RESTful and access requires an account with Impact Radius.\n\n\n Insured Rating API: The service enables users to develop a well-documented rate plan reflecting risk levels posed by prospective customers. Quote requests can then be made via the Insured Rating API which return coverage and premium information. The data generated helps to manage insurance product offerings and provide accurate rate quotes matched to determinants of risk.\n\n\nAPI methods support input of prospective customer data and return risk ratings compliant with the industry ACORD standard. Methods can be adapted to an applications existing XML schema if needed. Users can utilize Insured Ratings SOAP or REST endpoints.\n\n\n Intelius Search API: Intelius is a predictive information commerce company. Clients may order background checks, criminal records, and more to assess the risks of potential relationships. Their Search API provides functionality for programmatic queries to their database. Developers may use it for apps and sites that provide services similar to Intelius. The API uses SOAP and WSDL.\n\n\n Intellexere API: The service is based on natural language programming underlying the providers Mnemoo intelligent search engine. It is intended to analyze source content and return highly relevant results based on semantic cues inherent in the input. The service is part of a start-up effort by Complexity Intelligence LLC focused on the fields of SOA architectures, NLP, and robotics.\n\n\nAPI methods support submission of a complex text query and language parameter. The service processes the input and returns results with strong semantic relationships to the elements in the request. Other services include named entity recognition, complex spell-checking, and markup clean-up.\n\n\n Intuit QuickBooks Online API: Intuit Hosted PayPage is a payment form that can be integrated into a software application. PayPage is optimized for merchant-facing or customer-facing applications. The PayPage is often used for eCommerce applications where the user wants to accept direct payments from their customers.\n\n\nThe API consists of getting a transaction ticket and operation id, redirecting the merchant to the PayPage terminal, querying for transaction status, and optionally processing transaction results sent from PayPage to the users website. It uses RESTful calls and responses are name value pairs.\n\n\n Kudos Flockworks API: Kudos Knowledge is a company that develops knowledge management products. Their products are backed by Semantic Social Intelligence, a form of AI that has the ability to discover meaningful relationships inside any content. \n\n\nFlockworks is an artificial intelligence service that analyzes written language to extract relationships and meaning. Once a text has been analyzed, the words are placed in a Semantic Space which acts as a way of quickly discerning which topics are close in meaning. The Flockworks API allows users to access their own Semantic Space, store objects within it, query it and overlay tags and ratings. Developers can use this to create software that clusters, auto-categorizes, auto-rates, auto-tags and performs semantic searches. The API uses RESTful calls and responses are formatted in JSON.\n\n\n NAIC SERFF API: The service from the National Association of Insurance Commissioners (NAIC) encourages voluntary compliance with its SERFF program (System for Electronic Rate and Form Filing). It presents a single, consistent way for insurers to comply with reporting requirements of NAIC-member state governments. It facilitates automated filing of insurance provider rate and form documentation, triggering regulatory review and approval.\n\n\nAPI methods support periodic filing of required insurance rate documentation and other required forms. Methods also support submission, response, and tracking of correspondence between providers and regulatory agencies along with updating previous submissions and managing file attachments as part of submissions.\n\n\n NetProspex API: NetProspex provides its users a crowd-sourced database of verified business contact information, cleaned by their proprietary CleneStep algorithms. Their Data Cloud API integrates business contact information with B2B sales and workflows. This allows users to access whole demographic and firmographic records at critical points in an automated workflow. The API is RESTful and returns JSON and XML responses. \n\n\n Project Insight API: Project Insight is a provider of mid-market project management software. Features include portfolio, project and resource management and collaboration. The API allows developers to integrate Project Insight with their own applications. Data exposed by the API includes company, custom fields, invoices, projects, reference, time, billing and users. The API uses SOAP protocol and responses are formatted in XML. Full documentation is not publicly available.\n\n\n Qirina API: Qirina is a database that represents a websites\u2019 relations on a mapped out matrix of keywords. This allows users who submit their websites to their database with an analysis of their SEO strengths and weaknesses. Their RESTful API provides users access to query the Qirina database for an analysis of a particular website. Neither the API nor its documentation is available without permission. \n\n\n RentJuice API: RentJuice provides rental relationship management software for brokers and landlords. Their software provides functionality for data organization and sharing. Brokers and landlords can digitize paperwork, share market and inventory information, and communicate with colleagues and prospective renters. Their RESTful API uses JSON, PHP, and XML. It provides integrative access to users\u2019 raw listings data through outside applications. \n\n\n SalsaDev API: Salsadev is a software company offering enterprise level search solutions that make sense of unstructured content. The technology can read, process, analyze and understand unstructured text in any form and perform automated actions based on the results. Services offered by the Salsadev include intelligent search, auto-categorization and related-content retrieval.\n\n\nThe salsaAPI uses semantic technology to index and analyze unstructured data and can be used for services such as search, categorization, and keyword extraction. Developers can use the API for various tasks such as indexing unstructured data, performing full-text, keyword, or constrained search, extracting keywords from content, and Categorizing content and documents. The API uses RESTful calls and responses are formatted in XML.\n\n\n Sandglaz API: Sandglaz is a task management application that allows users to create, track, and manage their to-do lists. The application allows for collaboration making it suitable for project management tasks. With the Sandglaz API, users can write applications that integrate with Sandglaz. The Sandglaz API has a RESTful interface, providing programmatic access to most of its data. This data includes grids, tasks, labels, users and more. It accepts and returns JSON content\n\n\n Sapo Alerts API: SAPO is a Portuguese Internet service provider whose main web site acts as a web portal featuring a directory of Portuguese sites. SAPO Services provides a large suite of web services and applications to developers worldwide. \n\n\nSapo Alerts service allows users to subscribe to events and receive notifications as they happen in various formats including email, SMS and IM\/XMPP. Functionality includes adding and activating subscriptions, retrieving destinations lists and event details, removing users and more. The API uses SOAP protocol and responses are formatted in XML.\n\n\n Sapo Chart API: SAPO is a Portuguese Internet service provider whose main web site acts as a web portal featuring a directory of Portuguese sites. SAPO Services provides a large suite of web services and applications to developers worldwide.\n\n\nSapo Chart allows users to render an image of a chart based upon specified parameters. Currently these parameters allow for a chart with 20 points, a bar graph and a pie chart. It uses RESTful calls and responses are formatted as PNG images.\n\n\n Sapo Games API: SAPO is a Portuguese Internet service provider whose main web site acts as a web portal featuring a directory of Portuguese sites. SAPO Services provides a large suite of web services and applications to developers worldwide.\n\n\nThis API returns RSS feeds from the Sapp GameOver service. The feeds include all articles, articles by platform and content related with a game sheet. The API uses RESTful calls and responses are formatted as RSS.\n\n\n Sapo Holiday API: SAPO is a Portuguese Internet service provider whose main web site acts as a web portal featuring a directory of Portuguese sites. SAPO Services provides a large suite of web services and applications to developers worldwide.\n\n\nThe Holiday API returns a list of Portuguese national, regional and municipal holidays for a specified year. Users can search for specific hoidays including Easter, Carnival and Good Friday. Holidays can be returned for all years between 1582 and 2299. The API uses SOAP protocol and responses are formatted in XML.\n\n\n Sapo PunyURL API: SAPO is a Portuguese Internet service provider whose main web site acts as a web portal featuring a directory of Portuguese sites. SAPO Services provides a large suite of web services and applications to developers worldwide.\n\n\nThe PunyURL service is a URL shortening tool. Users can submit a URL and it returns two URLs, one in unicode based on RFC3492 of PunyCode and the other with alphanumeric characters in lowercase. Users can also retrieve the original expanded URL for a given short URL. The API uses RESTful calls and responses are formatted in JSON.\n\n\n Sapo Semantic Lists API: SAPO is a Portuguese Internet service provider whose main web site acts as a web portal featuring a directory of Portuguese sites. SAPO Services provides a large suite of web services and applications to developers worldwide.\n\n\nSemanticLists is a service that contains lists of words organized into semantic categories. Example categories include nationalities, organizations and jobs. The word lists are only in Portuguese at this time. The API uses RESTful calls and responses are formatted in XML and JSON.\n\n\n Sapo Tags API: SAPO is a Portuguese Internet service provider whose main web site acts as a web portal featuring a directory of Portuguese sites. SAPO Services provides a large suite of web services and applications to developers worldwide. \n\n\nThe Tags API allows users to query the Sapo platform for items that are tagged. Users can search bookmarks, tags associated with a user, and collections. The API uses RESTful calls and returns RSS feeds.\n\n\n Sapo Verbetes API: SAPO is a Portuguese Internet service provider whose main web site acts as a web portal featuring a directory of Portuguese sites. SAPO Services provides a large suite of web services and applications to developers worldwide. \n\n\nVerbetes is a service that allows users to perform WhoIs queries for public personalities. The information used by Verbetes is collected from various news sources and is updated hourly. The service can also answer WhoIs queries for jobs of public personalities if they are known. The API uses RESTful calls and responses are formatted in XML and JSON.\n\n\n Screenr Business API: Screenr Business is a web-based screen recording service designed for businesses\u2019 client and colleague support systems. It allows clients to capture errors and share the video privately with support staff for diagnostic purposes. The API provides integration functionality between the embeddable recorder and URLs. It is a RESTful API with JSON formatting. \n\n\n ShopIgniter API: ShopIgniter is social ecommerce package. Their products include ecommerce storefronts and promotions, a scalable commerce platform that integrates with social media, and analytical tools for ROI optimization. Their API is RESTful and returns JSON and XML formats. It gives developers and users access to the functionality of the systems major commercial resources, such as, product, shopping carts, brands, and more. \n\n\n SIGKAT Credentials API: SIGKAT is a credentials ecosystem where users can manage and exchange credentials in a secure manner at little or no cost. Credential issuers, holders and verifiers can receive, submit, create, evaluate and exhance credentials. SIGKAT provides an API for creating, submitting, and verifying credentials. The API includes methods for working with client accounts, certificates, credentials and more. It uses RESTful calls and responses are formatted in JSON.\n\n\n SISSVoc API: The service is part of the Spatial Information Services Stack (SISS) devoted to promoting interoperability among mapping information systems, including implementations by Australian government agencies. This function establishes an interface for integrating descriptive vocabularies to standardize terminology use, primarily via machine-to-machine sharing of HTTP links representing terms. A human interface is also provided, but that function is secondary to API-based exchanges of data.\n\n\nAPI methods support submission of queries for geospatial terms to the service, which returns standard versions and variants matching the request. Methods indicate semantic equivalence or other relationships between terms managed within the vocabulary.\n\n\n SnapAppointments API: SnapAppointments is booking service for appointment-based businesses. Small and complex businesses can use it to share their calendar availability with potential clients who can book appointments over the Internet. SnapAppointment automates schedules for employees and can send reminders to clients via text, email, or phone. Their RESTful API provides access to both client-side booking-query functionality as well as business-side appointment management. \n\n\n Sohu Microblogging API: Sohu.com is a multiservice Chinese online media network. It provides news, entertainment, and more social\/web 2.0 services to a huge user base. This API is gives developers access to their microblogging service. It is RESTful and provides JSON and XML responses to queries about user profiles, social networks, and other data. Functionality for messaging, tweeting, and posting statuses is also provided. The API documentation is translated into English, but most pages are in Chinese.\n\n\n Su.pr API: StumbleUpon is website discovery app and Su.pr is its link shortening tool. The Su.pr API provides functionality for URL shortening, expansion, and posting to authorized services, such as Facebook. The API also includes a parameter to switch versions of the API without interrupting production code. It is a RESTful API and returns JSON and XML responses. \n\n\n Terrapin Hurricane Tracking API: The service provides access to data used in the providers hurricane-information site, based on weather satellite input from the U.S. National Weather Service. It reports on status and location of all current, active hurricanes, tropical storms, and tropical depressions, over the Atlantic and Pacific Oceans. It is intended to supply time-critical information about storms that threaten damage to the U.S. mainland, beginning when the NWS starts issuing warnings.\n\n\nAPI methods support listing current storms, either hurricanes or tropical storms, and retrieving the location and status of any individual storm. Methods also support retrieval of forecasted changes in location and status.\n\n\n Thumbsnap API: Thumbsnap is a photo and image sharing service. It allows users to upload pictures from their computers to the Internet without changing their format. This gives users a URL to share their images with others. The Thumbsnap RESTful API provides developers access to their uploading functionality and responds with XML documentation of the uploaded image\u2019s URL. For access to the API, developers should email Thumbsnap. \n\n\n TransHotel API: The service provides hotel search and look-up function with ability to book accommodations. It is designed for integration with travel services to allow selection of accommodations, with reservation booking, cancellation, and transfer capabilities. It includes flexible payment options, including direct payment to the properties booked.\n\n\nAPI methods support submission of location and check-in\/check-out dates with filters for hotel rating, specific amenities (e.g., swimming pool, gym, restaurant, etc.), and local features. Returned data provide matching properties and availability, with ability to book the specified reservation. \n\n\n Unofficial Google Currency Converter API: This is Googles unofficial currency converter. It can convert a huge variety of currencies from around the globe. The API provides functionality for returning conversion rates between currencies, and calculating conversions from a sum in one currency into another. The API uses a RESTful protocol and the results are JSON responses. \n\n\n UptimeFu API: UptimeFu is a website and server monitoring service. Users input their domain (or multiple domains) when they register and UptimeFu monitors their status for free. The monitoring system will send an email if a site or server\u2019s status changes. Their RESTful API returns results the status updates in JSON. It informs users of the status and latency of their domain, and a timestamp of the last time it was checked. \n\n\n VIP Auto Shipping API: VIP Auto Shipping provides car service transport across the entire US. The API provides users with both a high and low value on the estimated car shipping costs based on the distance provided. This data can be integrated into third party sites or applications. The API uses SOAP calls and responses are formatted in XML.\n\n\n ZingChart API: The service allows creation of charts and graphs in Flash or HTML5 formatting using programmatic controls. Applications supply data to be charted and specifications for the graphical appearance to be generated. The service renders imagery reflecting the input data according to the specified output formatting.\n\n\nAPI methods support uploading of datasets along with output formats: image format and quality level, width and height, encoding standards, file location, etc. The service renders the chart specified by the output variables in the request. Tutorials demonstrate creation of static images, downloadable PDF, and overall use of the API methods.","item_date":"May 06 2012 18:03:03","display_item_date":"05-06-2012","url":"http:\/\/blog.programmableweb.com\/2012\/05\/06\/59-new-apis-intuit-stumbleupon-and-firetext\/","source":"blog.programmableweb.com"},{"title":"Better Late Than Never: Deezer Releases APIs To Court Developers In Latest Bid To Catch Up To Spotify","details":"Music streaming provider Deezer is one of the bigger (and older \u2014 it\u2019s been around since 2006) services of its kind, with some 20 million registered users in 48 territories, and 15 million tracks in its catalog \u2014 but it has failed to capture mindshare in quite the same way as rival Spotify; and some might argue that this will make it harder in the long run for Deezer to keep up and grow.\n\n\nToday sees the launch of the company\u2019s latest attempt to turn that situation around: \u201cOpen Deezer,\u201d a new set of APIs for iOS, Android and Java-based web apps, which it hopes will help it bring on a new raft of developers to create music services around its platform.\n\n\nThe move underscores Deezer\u2019s need to ramp up its offering to make it more dynamic, specifically in the face of competition from Spotify, which has been offering APIs since last year and now has a few dozen apps available on its platform.\n\n\n\u201cWe know we don\u2019t have a monopoly on great ideas,\u201d Daniel Marhely, the founder and CTO of Deezer, said in a statement. \u201cThere is still so much we can create to enhance people\u2019s music experience.\u201d\n\n\nThe APIs give developers not only access to the Deezer catalog of 15 million tracks and related metadata, but also Deezer\u2019s HTML5-based music streaming technology, which can be used without downloading a desktop client (unlike Spotify, until recently).\n\n\nDevelopers also get \u201cuser data\u201d access but it\u2019s unclear whether that\u2019s simply a case of user profiles or a wider look at analytics around what that user has consumed.\n\n\nWhile today\u2019s announcement opens the APIs up to independents, Deezer says that \u201cearly partners\u201d have already been working on apps with Open Deezer. These partners include\u00a0social networks Facebook, Twitter and last.fm; carriers Orange, Belgacom and T-Mobile; hi-fi makers Sonos and Logitech; and car companies Nissan and Parrot.\n\n\nAnd \u2014 just as later entrants to the mobile \u201cplatform wars\u201d have used financial incentives to attract app developers to their operating systems \u2014 so, too, will Deezer. They will get \u00a39.99 ($16) \u2014 or one month of a premium subscription \u2014 for every new user that they bring to Deezer through their app. It seems like that reward comes regardless of whether the subscriber is taking Deezer\u2019s free or premium service.\n\n\nDeezer\u2019s also running two Hackathons in Berlin and Paris to drum up some buzz.\n\n\nWill this be enough to lure in the magic app makers?\n\n\n\u201cI think it\u2019s exiting that there are more tools in place for developers to drive innovation, but we have not decided yet whether we\u2019ll develop for Deezer,\u201d says Inge Andre Sandvik, one of the founders of Soundrop, one of the more popular apps for Spotify, in which a Turntable-style app lets users enter \u201crooms\u201d where they can listen to the same music together and create\/add to others\u2019 playlists off the Spotify catalog.\n\n\n\u201cFor now our small team very busy making the best possible way to discover music in Spotify,\u201d he says. \u201cI would like to be very opportunistic about taking Soundrop to more platforms, but we have a very good relationship with Spotify that we invested a lot in.\u201d\n\n\nHe also points out that for developers that are targeting web apps the new Deezer development platform \u201clooks very interesting\u201d and at least may have an advantage over another player, if not Spotify itself:\n\n\n\u201cI would assume Deezer will get a lot of interest. Both Deezer and Rdio seem to be well positioned now for web apps, but Deezer\u2019s strength is that they have a head start on the global reach.\u201d\n\n\nUpdate: As it was pointed out below, APIs are not only the preserve of Spotify and Deezer; others like Grooveshark also offer them as a way for developers to tap into their music platforms.\n\n\nAlso: just to answer another couple of questions below: Deezer tells me that currently it has \u201cno plans\u201d to offer its service in the U.S. \u2014 which, given that this is Spotify\u2019s fastest growing market, may be seen as a missed opportunity for more users for Deezer, too.","item_date":"May 05 2012 19:03:32","display_item_date":"05-05-2012","url":"http:\/\/techcrunch.com\/2012\/05\/03\/better-late-than-never-deezer-releases-apis-to-court-developers-in-latest-bid-to-catch-up-to-spotify\/","source":"techcrunch.com"},{"title":"TradeKing Enhances Open API","details":"TradeKing\u2019s SVP of Product\u2014Kevin Delo\u2014recently commented: \u201cTradeKing has always been about providing tools that help clients trade in the ways that best suit their personal style.\u201d With a series of recent enhancements to TradeKing\u2019s open API, TradeKing furthers its mission to enable clients.\n\n\nThe enhanced API adds financial data that was previously unavailable through the open API (i.e. option quote and option chain calls, top gainers\/top losers\/highest volume, streaming quotes, historical data). However, clients also have access to non-financial data (e.g. news displays based on keywords or symbol). Developers can now aggregate financial and non-financial data in an application from a single API. This simplifies the data gathering process for developers who wish to build a trading platform on multi-factor premises.\n\n\n\n\n\nThe TradeKing API utilizes a REST protocol and uses OAuth for Authentication. Recent enhancements include FIXML protocol which makes real time data available.\n\n\nTraders employ endless combinations of factors in developing a trading strategy (financial and non-financial). With the increasing publications of open APIs across data sources (e.g. media, finance, government\/politics, etc.), the ability to create trading platforms that focus on a given trader\u2019s personal set of factors is unprecedented. TradeKing\u2019s API grants access to many types of factors from a single interface.","item_date":"May 05 2012 18:55:05","display_item_date":"05-05-2012","url":"http:\/\/blog.programmableweb.com\/2012\/05\/04\/tradeking-enhances-open-api\/","source":"blog.programmableweb.com"},{"title":"Google Plugin for Eclipse now provides richer tooling for Cloud SQL and Google APIs - Google Developers Blog","details":"We are pleased to announce the latest release of Google Plugin for Eclipse (GPE 2.6) with improved tooling for Cloud SQL and Google APIs. GPE 2.6 introduces the following features:\nTooling for using Java Persistence API (JPA) to access Cloud SQL\n Importing the latest Google APIs into your GPE project\n \nObject-Relational Mapping (ORM) frameworks are very popular in the Java community for accessing  relational databases. The Eclipse Web Tools Platform offers a robust set of tools to configure and use JPA with an implementation of your choice. With the new Google Plugin for Eclipse (GPE) 2.6, you can now take advantage of these tools with Cloud SQL and Google App Engine.  In any GPE project, JPA can now be enabled and configured as a project facet. The screenshot below shows the JPA facet configuration for a GPE project.\n \n With GPE 2.6, you now have access to all the latest Google APIs at the click of a button within Eclipse. You can now download the latest Google APIs Java client library with the required dependencies to access Google APIs right within your App Engine project using GPE. Update notifications for API version changes will appear in your App Engine project, so you can easily keep your client libraries updated all the time. The screenshot below shows the GPE UI for adding a Google API to a GPE project.\n \n The next time we update the App Engine Engine SDK, you will be happy to see an update notification within Eclipse prompting you to update to the latest SDK.\n Please go ahead and install GPE 2.6 by following the instructions here. You can start using the ORM tooling for Cloud SQL and the latest Google APIs for your App Engine projects. We always love to hear your feedback and the GPE group is a great place to share your thoughts.\nSriram Saroop is the Product Manager for the Google Plugin for Eclipse and the Google Admin APIs. He has been a software engineer in his previous life and he is now working toward creating an awesome developer experience for Google products.","item_date":"May 05 2012 18:46:25","display_item_date":"05-05-2012","url":"http:\/\/googledevelopers.blogspot.com\/2012\/05\/google-plugin-for-eclipse-now-provides.html","source":"googledevelopers.blogspot.com"},{"title":"Developer Week in Review: Are APIs intellectual property?","details":"Returning after a brief hiatus due to my annual spring head cold, welcome back to your weekly dose of all things programming.  Last week, I was attending the Genomes, Environments and Traits conference (Im a participant in the Personal Genome Project), when I got notified that WWDC registration had opened up. I ended up having to type in my credit card information on my iPhone while listening to the project organizers discuss what they were doing with the saliva I had sent them.  The conference itself was very interesting (although I was coming down with the aforementioned cold, so I wasnt at the top of my game).  The cost to sequence a genome is plummeting \u2014 its approaching $1,000 a pop \u2014 and it has the potential to totally revolutionize how we think about health care.\n  Its also an interesting example of big data, but not how we normally think about it.  An individual genome isnt all that big in the scheme of things (its about 3GB uncompressed per genome), but there are huge computational challenges involved in relating individual variations in the genome to phenotype variations (in other words, figuring out what variations are responsible for traits or diseases).\n  While all the West Coast developers who slept through the WWDC registration period lick their wounds, heres the rest of the news.\n  APIs are copyrightable, unless they arent?\n  These days, I feel like you need to consider a minor in law to go with your computer science degree.  In the latest news from the front, we have conflicting opinions regarding the status of APIs.  On the one hand, the judge in the Oracle versus Google lawsuit has instructed the jury they should assume that APIs are copyrightable.  As the linked article discusses, this could have ominous implications for any third-party re-implementation of a programming language or other software that is not open source.\n  Over in Europe, however, a new ruling has stated that programming languages and computer functionality are not copyrightable.  So, depending on which side of the ocean you live on, APIs are either open season, or off limits.  No word yet as to the legal status of APIs on the Falkland Islands ...\n   Fluent Conference: JavaScript & Beyond \u2014 Explore the changing worlds of JavaScript & HTML5 at the OReilly Fluent Conference (May 29 - 31 in San Francisco, Calif.).\nSave 20% on registration with the code RADAR20\n  Code to make your head hurt.\n  For those of you who like to celebrate the perversities of life, its hard to beat the International Obfuscated C Competition, which just released its 2011 winners.  For your viewing pleasure, we have programs that compute pi, chart histograms, and even judging programs for obfuscation, all written in a manner that will have code reviewers running to the toilet with terminal bouts of nausea.\n  And speaking of C ...\n  We tend to focus a lot of attention on emerging languages, partially because many of them have novel features, and partially because the grass is always greener in a different language.  Its instructive to step back sometimes and take a look at what people are actually using.  The latest TIOBE Programming Community Index, which measures how much code there is out there in each of the various languages, has a new top dog, and its our old friend C.  In fact, when you factor in C#, C++ and Objective-C, C-related languages pretty much own the category.  Java has now fallen to the second position, and you have to go all the way down to sixth place to find a scripting language, PHP.\n  Importantly, all the hot new languages, like Erlang and Scala, dont even make the top 20, and you only need half-a-percentage point to get in that list.  As much as we like the new darlings on the block, the old veterans still are where most of the action (and money) is.\n  Got news?\n  Please send tips and leads here.\n  Related:\n  The next wave of programming languages\n  More Developer Week in Review coverage","item_date":"May 05 2012 18:24:18","display_item_date":"05-05-2012","url":"http:\/\/radar.oreilly.com\/2012\/05\/api-oracle-google-copyright-c.html","source":"radar.oreilly.com"},{"title":"Oracle Wants Judge to Bar Ex-Sun CEO Schwartz's Testimony in Google Suit | PCWorld Business Center","details":"Oracle has asked a judge to bar Google from using some testimony given by former Sun Microsystems CEO Jonathan Schwartz in the companies  intellectual-property suit over the Android mobile OS, saying it has no legal and factual predicate.\n  \t\t  \t\t\t      \t \t \t \t\t \t\t\tSimilar Articles:\n \t\t\t \t\t\t \t\t\t\t\t\n \t \t \t      \t\t  \t  \t\tSchwartz has provided some of the strongest testimony for Google in the case so far. Appearing on the stand last week, Schwartz was asked by a Google attorney whether, as CEO of Sun, he had made a decision not to sue Google over its use of Java in Android.\n  \t\t  \t  \t\tYes, Schwartz replied. We didnt feel we had any grounds.\n  \t\t  \t  \t\tOracle acquired Sun early in 2010, gaining control of the Java programming language. It sued Google later that year, claiming Android violated patents and copyrights it holds on Java. Google has denied wrongdoing, saying Android is a clean room Java implementation that doesnt violate Oracles rights. The trials first phase, which covers copyright liability, began in April and went to the jury this week.\n  \t\t  \t  \t\tOracle asked the court to bar Google from referencing Schwartzs testimony during the trials second and third phases, which will cover Oracles patent claims and damages, respectively.\n  \t\t  \t  \t\tGoogles question called for a yes or no answer, but Mr. Schwartz in response volunteered an opinion as to what we felt about the grounds for pursuing litigation against Google over Android, its motion states.\n  \t\t  \t  \t\tThe question appeared to ask only whether Mr. Schwartz had made a decision not to pursue litigation, Oracle added. But Mr. Schwartzs answer -- and Googles subsequent use of that answer -- implicates Suns (now Oracles) privileged discussions by suggesting that there was an unidentified group of people (we), who had made some final decision as to whether to pursue litigation and the strength of those claims.\n  \t\t  \t  \t\tWhat legal grounds Suns management felt they had or what decisions they were considering is clearly privileged, Oracle said. Moreover, Mr. Schwartz had no right nor any basis to make such a statement which subjects Oracle to privilege waivers that Mr. Schwartz has no authority to invoke.\n  \t\t  \t  \t\tAlso, the suggestion that Oracle had decided not [to] sue is clearly against the weight of the evidence presented in this case, Oracle said.\n  \t\t  \t  \t\tEvidence presented in the trial showed that Sun and Google had discussions following the announcement of Android in 2007, that those talks went on after Oracle bought Sun, and that Google officials considered buying all the rights to Java from Sun in order to ward off lawsuits, according to the filing.\n  \t\t  \t  \t\tIf Oracle had decided to rebut Schwartzs testimony at the time he made it, that would have placed the company in the quandary of having to decide whether to waive privilege on the spot, Oracle added. If Google is allowed to rely on this testimony, and the jury is allowed to believe that it matters, the trial will divert into an irrelevant sideshow over Mr. Schwartzs subjective state of mind, instead of Suns affirmative acts.\n  \t\t  \t  \t\tLawyers for the two sides discussed the matter in court Friday morning. Googles attorney, Robert Van Nest, indicated that Sun wanted to bring Schwartz back to the stand to testify in the patents phase of the trial. In his deposition testimony, Schwartz made a similar comment that Sun did not see a reason to sue Google on patent grounds, Van Nest said.\n  \t\t  \t  \t\tGoogle declined to comment today.\n  \t\t  \t  \t\tMeanwhile, Oracle has said it plans to bring back to the stand Tim Lindholm, a Google engineer who wrote an email that was seen as a key piece of evidence in the copyright infringement part of the trial. Oracle says Lindholm can give testimony relevant to the patents part of the case.\n  \t\t  \t  \t\tVan Nest told the court that if Oracle can bring Lindholm back and parade him around before the court, then Google should be able to bring Schwartz back.\n  \t\t  \t  \t\tThe judge didnt seem keen to have either witness back. Both seem peripheral to the dispute over patents, he said Monday, and he asked the lawyers to try make a deal that neither witness be brought in to testify again.\n  \t\t  \t  \t\tThe matter is likely to be discussed again at a future hearing.\n  \t\t  \t  \t\tA verdict in the copyright phase of the case could come as soon as Friday, but a question from a juror to the judge on Thursday raised the possibility that the jury could be deadlocked on the copyright claims.\n  \t\t  \t  \t\tChris Kanaracus covers enterprise software and general technology breaking news for The IDG News Service.  Chriss e-mail address is Chris_Kanaracus@idg.com","item_date":"May 05 2012 17:54:27","display_item_date":"05-05-2012","url":"http:\/\/www.pcworld.com\/businesscenter\/article\/255067\/oracle_wants_judge_to_bar_exsun_ceo_schwartzs_testimony_in_google_suit.html","source":"www.pcworld.com"},{"title":"Reinventing Government with Open Data Is No Joke","details":"This (long) post is in response to the blog post by Tom Slee, titled  \u201cWhy the Open Data Movement is a Joke.\u201d Every once in a while, an article that dismisses Open Data as irrelevant, a joke, or a pipe dream catches our imagination and compels us to chime in, even though thoughtful people we respect like David Eaves, Alex Howard and Tom Lee have already weighed in on the issue (here, here and here, respectively). Some people might dismiss our views as self-serving and profit-motivated. We are after all, a for-profit technology start-up that has chosen to focus on enabling Open Data from a technology standpoint, by providing a turnkey, purpose-built, cloud-based platform that government organizations around the world use to share their data.\n Or, one might consider that we are a team of people who care that our technology enables positive change. Reinventing government with Open Data is not an empty slogan we use for marketing. It is a promise we make to our customers. Numerous government organizations share in this vision and have trusted Socrata to help them achieve it. We felt it would inform this debate if we shared the all the different ways that our customers are using Open Data as a platform for innovation. So yes, we are biased, but we do have a a unique perspective and a viewpoint that was shaped by working in the trenches, as part of the Open Data community, since 2008.\n First, Let\u2019s not conflate Open Data with Open Government\n The first point has to do with the definition of Open Data. Since it is a broad term that describes a nascent and rapidly-evolving movement (sic) it lacks the structure and definition of more mature concepts, like e-Government. This makes Open Data easy to love and easy to criticize at the same time. In this regard, Tom\u2019s post is helpful because it challenges the community of advocates, activists, volunteers, developers, entrepreneurs and government leaders who participate in shaping Open Data\u2019s future to be crisper, more purposeful and grounded in our approach.\n The first problematic assertion that Tom Slee makes: that is conflating Open Government with Open Data. The two are related, but not the same (See the opening graphic in Alex Howard\u2019s article to visualize the distinction.) In our view. An open Government strategy needs to include Open Data as a component of enabling transparency and engaging citizens. However, Open Government is also about a commitment to open public meetings; releasing public information in all its forms, if not proactively at least in a timely fashion; engaging the public in decision making; and it is also a general mindset, backed up by clear policy, that citizens need to be empowered with information and a voice so they can hold their government accountable.\n At the same time, a good Open Data strategy should support Open Government goals, by making structured data that relates to accountability and ethics like spending data, contracts, staff salaries, elections, political contributions, program effectiveness\u2026etc. available in machine- and human-readable formats.\n This brings us to the second incorrect statement that Tom Slee makes. He states that:\n  The Open Data movement is doing nothing for transparency and accountability in government\n  This is simply not true. Let\u2019s take Ethics.gov, for example. It is a great government-led example of enabling the public to easily search 7 different sources of information on lobbying reports, ethics records, and campaign finance filings in the U.S. federal government. At a more local level, Chicago\u2019s public performance management program provides every performance indicator about city services in easy-to-understand, interactive charts, in addition to raw data and programmatic access via APIs (more on APIs later). Here is one such example:\n  Performance Metrics \u2013 Streets & Sanitation \u2013 Garbage Cart Requests\n \n Another striking example of Open Data for transparency is the City of Chicago\u2019s publicly available and detailed lobbyist data, which Derek Eder, a local developer, used to build this fantastic app.\n The State of Oregon, like many of our customers provides checkbook-level expenditure data from all its agencies. It\u2019s important to note that Oregon\u2019s success is in part due to their embrace of Open Data as a Platform, instead of merely providing a download of their expenditure data. The distinction is important. The former transforms data into an active ingredient of a service delivery pipeline. The latter is entirely divorced of any ability to deliver service.\n Beyond transparency, Open Data directly impacts our quality of life\n Opening up weather data, or GPS data or TIGER data had nothing to do with transparency, nor was it labeled Open Data at the time. It had everything to do with releasing a vital layer in the information infrastructure that we all depend on for the quality of life that we now enjoy. The same is true for new types of data that are being released under the Open Data banner: realtime crime data in Seattle neighborhoods, restaurant inspection data in NYC, new permits and licenses, zoning changes, property taxes in Baltimore, foreclosures and abandoned buildings in Chicago, San Francisco\u2019s 311 data, national or regional product recalls, radiation levels as a result of the Japan earthquake, the effects of the Oil spill in the Gulf, and the list goes on.\n Making this information available in ways that ordinary people can easily access, interact with, make sense of and contextualize is at the heart of the Open Data movement. There is real innovation happening here in terms of building the new digital infrastructure that supports the flow of information between governments, citizens and businesses.\n Open Data helps governments become more efficient\n If the previous argument was about expanding citizen access to vital information, this part of the argument looks at the supply side of the information flow and the costs of delivering online services to constituents. Let\u2019s take the State of Oregon as an example. In this video case study, the team from Oregon gives two concrete examples of how they are doing more with less, by leveraging Open Data as a platform to deliver online services more cost-effectively. The Secretary of State office, one of the state agencies, is embracing news ways to manage and deliver online databases for trademark applications and business registrations in the state. By turning the experience into a self-service online model, they expanded access for their customers and improved their own productivity at the same time. A win-win scenario.\n The Substance Abuse and Mental Health Services Administration in the federal government, is using Open Data as platform where the same nationwide database of treatment facilities is accessible via multiple self-service channels, including ubiquitous SMS. This illustrates two important aspects of innovative approaches to Open Data: reusing the same information across channels and multiple citizen interfaces (economies of scope) and leveraging the power of cloud-based APIs to connect the different interaction channels with the data.\n Open Data APIs, the plumbing for Digital Government\n The digital world we live in and the online services we consume and use very day are connected together via Application Programming Interfaces, or APIs for short. Facebook, Twitter, Google Maps are just three examples of powerful API-enabled services that are people use on other sites, often without realizing they\u2019re interacting with the service via APIs. In a recent article in Programmable Web, Adam Duvander predicts that every company will have an API. This prediction is supported by the explosive growth of APIs in recent years, shown in the graph below:\n Source: 5,000 APIs: Facebook, Google and Twitter Are Changing the Web, by Adam Duvander. Programmable Web, 2012.\n Why then, would we accept that the public sector, which represents approximately 40% of our economy, would remain isolated from this wave of API-enabled innovation? It should not be.\n  The most popular API category from the last 1,000 APIs is government. In total, we list 231 government APIs and nearly half of them have been added in the last four months. Many looked to 2009 and 2010 as the years of government\u2019s embracing transparency. That was true in terms of opening up many gigabytes of data. However, it appears 2011 and 2012 are the years where that data becomes easily consumable in bite-size chunks, something that APIs make much easier than Excel files.\n  APIs are a fundamental part of the Open Data fabric, which is why every dataset that Socrata customers put online, is automatically API-enabled. We can argue that some don\u2019t need to be, but the point here is that Open Data APIs are enabling the next wave of innovation which is all about connecting government data with apps and online services, in real-time and at scale, which only APIs can support. This is the Next Big Thing in the evolution towards a Digital Government.\n The final point that we\u2019d like to raise is the assertion by Tom Slee that Open Data is not a movement. We are not experts on what qualifies as a movement, but if you follow #opendata on Twitter, or go to any hackathon, app contest and CityCamp that happen every week around the world, you\u2019ll quickly realize that Open Data has captured the imagination of thousands of people. Most of them donate their time and skill to advocate for more openness, scrape data and curate it so they can put it online, build apps that deliver social value, do the research and write thoughtful papers about Open Data and its impact. Many of them are government employees who travel on their own dime, and spend a weekend with civic hackers building something worthwhile.\n Is that a movement? Is this all a joke? One way to find out is to go out there, get involved and contribute something.\n Thanks for reading this long post.\n Kevin Merritt is the Founder and CEO of Socrata.","item_date":"May 03 2012 20:43:09","display_item_date":"05-03-2012","url":"http:\/\/www.socrata.com\/opendata\/reinventing-government-with-open-data-is-no-joke\/","source":"www.socrata.com"},{"title":"Code less, make more with Cumula","details":"A simple framework for building web applications on top of great web services.","item_date":"May 03 2012 20:09:41","display_item_date":"05-03-2012","url":"http:\/\/cumula.org","source":"cumula.org"},{"title":"Oracle v. Google: Are APIs Covered By Copyright Law? - Forbes","details":"The high stakes lawsuit between Oracle and Google is based on claims that Google\u2019s Android infringes Oracle\u2019s copyrights and patents related to Java. This article will attempt to explain the first phase of the trial relating to the copyright claims. The patent claims will be dealt with in a subsequent and separate phase of the case.\u00a0 And the final phase of the trial will address Oracle\u2019s damages, if any.\n Java is a programming platform originally developed by Sun in the 1990s and acquired by Oracle as part of its purchase of Sun in 2010. The Java programming language itself is distributed by Oracle under an open-source license which essentially means it is free to use. However, Oracle has reserved all rights to, and charges for the use of, certain items such as its programming and development tools, the Java trademark, and the right to certify software as Java compliant. Google\u2019s Android is a mobile operating system that is currently the most widely used platform for smartphones. It runs on over 300 million phones, tablet PCs and other devices \u2013 more than Apple\u2019s competing iOS.\n In an attempt to avoid any obligation to secure a license and pay royalties to Oracle, Google was careful to refrain from using the foregoing reserved items. In particular, instead of using the class libraries (i.e., the quintessential programming building blocks) of Oracle\u2019s Java programming and development tools, Google used a substitute open-source set of independently developed class libraries (Apache Harmony) that mimic Java\u2019s application programming interfaces (APIs).\n APIs are the specification \u201chooks\u201d that developers insert into programs to allow other programs to communicate with the developers\u2019 programs and take advantage of their functions. In other words, the Java programming language is just an outline, the class libraries contain the building blocks that form the programming language\u2019s foundation, and the APIs are the hooks that enable other programs to access and leverage the class libraries\u2019 functions.\n The heart of the copyright phase of the trial is Oracle\u2019s claim that Google is infringing its copyrights in and to 37 Java APIs used by Android by copying their \u201cstructure, sequence and organization\u201d. Google claims such APIs are not subject to copyright protection.\n Google also copied a small number of lines of Java code into Android. But Google has admitted it was a mistake and removed all of such lines of copied Java code, so I do not believe that the limited Java code copying will have a material impact on the resolution of this dispute.\n The judge has signaled that he may be leaning towards Google\u2019s point of view, citing the fair use provisions of the U.S. Copyright Law and stating that he does not view the 37 APIs as the sacred cow of copyright law. To review, copyright law protects original works of authorship, including computer code, fixed in a tangible medium of expression. Even if the specifications that make up an API are deemed to be copyrightable expressions (which is a major point of debate in this trial), copyright law has limitations including certain \u201cfair use\u201d factors to consider such as the amount and substantiality of the portion used in relation to the copyrighted work as a whole. In this case, Android has over 15,000 lines of source code in addition to the source code compiled into the open source Apache Harmony class libraries incorporated into Android.\n On the other hand, the judge instructed the jury to assume Java\u2019s APIs are copyrightable. Of course, those instructions favor Oracle. Nonetheless, since judges decide matters of law and juries decide issues of fact, whether APIs are copyrightable will ultimately be decided by the judge, albeit after the jury considers the issues and renders its verdict.\n A win for Google here would mean not having to pay Oracle up to $1 billion in damages for prior uses of Android, not being exposed to the possibility of an injunction, not having to negotiate a license from Oracle to continue using Android, and not having to defend third party Android-based hardware manufacturers and application developers who could all be subject to similar infringement claims from Oracle.\n But the consequences of this landmark case are potentially much broader for the software industry, as a win for Oracle could fundamentally change the legal standing of programming languages from their current status as generally free instruments to create software applications, to an altered status as products themselves that must be licensed by platform developers, hardware manufacturers, app programmers, and all the other participants in the food chain.","item_date":"May 03 2012 19:05:55","display_item_date":"05-03-2012","url":"http:\/\/www.forbes.com\/sites\/oliverherzfeld\/2012\/05\/01\/oracle-v-google-are-apis-covered-by-copyright-law\/","source":"www.forbes.com"},{"title":"Programming languages not copyrightable rules top EU court","details":"May 02, 2012, 9:13 AM \u2014  Europes top court ruled on Wednesday that the functionality of a computer program and the programming language it is written in cannot be protected by copyright.\n The European Court of Justice made the decision in relation to a case brought by SAS Institute against World Programming Limited (WPL).\n SAS makes data processing and statistical analysis programs. The core component of the SAS system allows users to write and run application programs written in SAS programming language. Through reference to the Learning Edition of the SAS System, which WPL acquired under a lawful license, WPL created a product that emulates much of the functionality of the SAS components, so that customers application programs can run in the same way on WPL as on the SAS components. \n \nThe court found that although WPL used and studied SAS programs in order to understand their functioning, there was nothing to suggest that WPL had access to or copied the source code of the SAS components. It ruled that The purchaser of a license for a program is entitled, as a rule, to observe, study or test its functioning so as to determine the ideas and principles which underlie that program.\n If it were accepted that a functionality of a computer program can be protected as such, that would amount to making it possible to monopolize ideas, to the detriment of technological progress and industrial development, decided the court, echoing the opinion given last November by the courts Advocate General, Yves Bot.\n The result is that the court finds that ideas and principles which underlie any element of a computer program are not protected by copyright under that directive, only the expression of those ideas and principles. \n This in effect leaves the door open for other software companies to reverse engineer programs in many cases without fear of infringing copyright.\n \n Follow Jennifer on Twitter at @BrusselsGeek or email tips and comments to jennifer_baker@idg.com.","item_date":"May 03 2012 19:00:30","display_item_date":"05-03-2012","url":"http:\/\/www.itworld.com\/software\/273828\/programming-languages-not-copyrightable-rules-top-eu-court","source":"www.itworld.com"},{"title":"BetaKit \u00bb Startups in the Business of APIs","details":"Instead of starting from bare bones and building everything involved in an app, including news feeds, notification systems and more, application programming interfaces (APIs) allow companies to focus on what makes their product unique. APIs offer a scalable and reliable way to simplify the development process, avoiding rework by using existing tools built by others. They can give developers a massive advantage by shortening the development cycle and helping them be first to market. Increasingly startups are making APIs their business, allowing developers to use their tools or pull from their base of APIs to add functionality to their apps.\n Twitter clients don\u2019t log in to Twitter.com, retrieve tweets and then display them. They log in through an API, which allows them to handle more data in a secure way. Building a service that scrapes Twitter.com is inefficient and insecure, and it means that every time Twitter changes their site, developers have to change their tool. Twitter created an API almost at the launch of\u00a0the product, and developers were free to play around and create things which Twitter didn\u2019t have the resources or time to build themselves. APIs are shortcuts, as Context.IO founder Bruno Morency explained in an interview.\n \u201cHumans interact with a product\u2019s data through a graphical user interface,\u201d he said. \u201cAn API is an alternate way to interact with that same data, but it\u2019s not meant to be used by humans, it\u2019s meant to be used by computer code.\u201d Morency\u2019s company Context.IO provides what it calls \u201cthe missing API\u201d for developers to build email apps by calling up the server directly to retrieve a user\u2019s email data.\u00a0One usage of Context.IO\u00a0is\u00a0EmailToBox, a tool which syncs a user\u2019s new email attachments to their Box account.\n Companies are making one-stop API shopping their primary business focus.\u00a0Urban Airship\u00a0offers a range of mobile APIs, providing in-app purchases, push notifications, analytics and other solutions that let app developers focus on what makes their app unique. Verizon, Dictionary.com, Tapulous, and\u00a0Warner Bros. are a few of the companies utilizing Urban Airship\u2019s technology. They now\u00a0manage API calls from hundreds of millions of devices on iOS, Android and BlackBerry.\n A similar company is\u00a0spire.io, and\u00a0founder\u00a0Daniel Yoder points out that while API use is on the rise, there\u2019s still plenty of room for it to grow. \u201cEach generation of APIs commoditizes an entire class of application features, raising the bar for everyone building new applications.\u201d\u00a0He believes that more and more developers are utilizing APIs, but it still isn\u2019t at the point where using one is the default choice. \u201cWe\u2019ve at least reached the point where developers prefer frameworks like Rails or libraries like Socket.io to rolling their own. But they still aren\u2019t typically following that line of reasoning to its logical conclusion, which is to use a hosted API like ours,\u201d Yoder said.\n Yoder points out that APIs for real-time notifications specifically are reaching the level of the default choice. spire.io\u2019s latest product is a\u00a0real-time notifications API called\u00a0Messages, and they plan to release several more APIs over the year to help developers save months of engineering time.\n The question many developers have is whether an API will always be around for them to use. One common example pointed to is Twitter, which has tightened the chain on developer access to their API. The realization that a company could lose access to an API for almost no reason, and then lose their business overnight is a fear for any entrepreneur. However, companies like Urban Airship and Spire.io\u2019s core business is selling APIs, where Google\u2019s or Twitter\u2019s isn\u2019t. Spire.io\u2019s Yodel feels that the benefits of rushing to market by avoiding to build certain features can outweigh the fears. \u201cEven if you have to in-house those services later, you\u2019re still ahead of the game because you got to market more quickly or with a better consumer experience.\u201d\n Disclaimer: BetaKit contributor Sarah Jane Morris is a current employee of Context.io, a company mentioned in this article.\n             Like this article? Share it with others\n                                  Tweet","item_date":"Apr 29 2012 17:19:04","display_item_date":"04-29-2012","url":"http:\/\/betakit.com\/2012\/04\/28\/startups-in-the-business-of-apis","source":"betakit.com"},{"title":"Oracle President testifies, jury told to assume APIs are copyrightable","details":"The presentations of evidence in the Oracle v. Google copyright case wrapped up on Friday, and only a few hours of closing arguments now stand between the jury and their deliberations.\n  The beginning of the week was dominated by Googles defense case, with more star witnesses, including two former CEOs of Sun Microsystems: Eric Schmidt, who later became CEO of Google, and Jonathan Schwartz. Schwartz testified that he never considered suing Google over Android, because we didnt feel we had any grounds.\n     On Friday, Oracles president and CFO, Safra Catz, testified today about how she and her colleagues at Oracle approached and tried to get the company to pay for a license to Java. But Google responded forcefully, insisting that it had only used open-source material properly, and didnt violate any Oracle IP rights.\n  Google v. Oracle on Ars\n Case preview: Oracles IP war against Google finally going to trial: Whats at stake\n Day one in court: Oracle tells jury you cant just step on somebodys intellectual property\n Day two in court: Ellison, Page both take the stand as Google argues Java language is free and open\n Google Chairman testifies: Sun wanted up to $50 million from Google for Java license, Schmidt says\n Googles brief, Oracles response: Google tries to destroy Oracles case, asks for judgment on Java copyrights\n \n Near the end of those negotiations, Google engineer Alan Eustace wrote to Catz, rejecting her companys demands that Google pay up for Android. That e-mail, which was shown to the jury, reads as follows:\n  Safra,\n  I have discussed your proposals with Google engineers, lawywers, founders, and executives, and they are not acceptable. We will not pay for code that we are not using, or license IP that we strongly believe we are not violating, and that you refuse to enumerate. Google engineers spent considerable time and effort building from scratch open source alternatives to closed systems. This effort is one that we believe is important to Google and our partners.\n  Google and Oracle have a great opportunity to work together to make Java the preferred choice for mobile, client, and server computing. Your customers are begging for mobile solutions, and your enterprise business will depend on delivering them. We want to be your partner, but this discussion seems to be going in the wrong direction.\n  Aside from a nine-line function thats been deleted from Android, Oracle acknowledges that Google didnt copy any code from Java. Oracle says that by copying the structure, sequence and organization of 37 Java APIs, Google has violated copyright law. That phrase\u2014structure, sequence and organization\u2014came up so frequently when the lawyers were arguing over jury instructions today that both sides started to refer to the concept by an acronym, SSO.\n  The overall case also involves accusations of patent infringement, but those will be dealt with in the second phase of the case. For now, the jury is just getting ready to decide the copyright claims over Java APIs. The trial, now two weeks old, may last as long as eight weeks.\n  Friday was a long day in court. After the final testimony from Catz and experts in the morning, lawyers argued all afternoon about what the jurys final instructions will look like.\n  There is one thing the jury wont know: the issue of whether APIs can be copyrighted at all is actually up in the air. Its a legal gray area that will be decided by Judge William Alsup\u2014but only after the jury gives its verdict in this case. (When that happens, even a jury verdict in Oracles favor could be a hollow one.) During debates over the jury instructions today, Alsup deleted a notation about how hell decide the copyrightability issue. Instead, the instructions will simply tell the jury to assume that Java APIs\u2014which are like sets of instructions for how a programming language can be used\u2014can indeed be copyrighted. Since that order emphasizes the importance of the APIs, it probably weighs in Oracles favor.\n                                                                  Photograph by Javier Pedreira","item_date":"Apr 29 2012 17:13:42","display_item_date":"04-29-2012","url":"http:\/\/arstechnica.com\/tech-policy\/news\/2012\/04\/oracle-president-testifies-jury-told-to-assume-apis-are-copyrightable.ars","source":"arstechnica.com"},{"title":"John Sheehan : The API Developer Experience Baseline","details":"You\u2019ve invested in building an API and now you want developers to use it. Very few companies can get away with creating a successful API ecosystem on technical merit alone. You\u2019ll need documentation, SDKs, sample apps, debugging tools and everything else that goes into a great \u201cdeveloper experience\u201d.\n  Based on my experience of working with an array of different APIs, here is what I believe to be the baseline level of support structure anyone who is serious about their API program should implement. This is geared toward web service APIs, but the principles can be applied toward any set of developer tools.\n  This is A LOT of work and not every company will have the resources to do this. Strive to support as much of it as you can. I\u2019m a proponent of owning the entire experience, but if that\u2019s not feasible for your situation one of the many API infrastructure companies out there can help you achieve much of this.\n  If you\u2019re in the midst of making plans to open up a new API, make sure to consider the cost of building related tooling so that it\u2019s not neglected until just before release, or worse, after the API has been made public. Developer\u2019s first impressions are so important.\n  Within each section the items are ordered by importance.\n \n  A method-by-method, endpoint-by-endpoint, property-by-property breakdown of every possible request and response to and from the API. For requests, all parameters should be listed and described with a clear designation for required parameters. For responses, a sample representation should be shown in each of the supported data formats along with a list of properties and their descriptions.\n  In addition to the detail for each endpoint, an overview should be provided that describes how requests are to be made, the response formats supported, how authentication works, and anything else required to understand the conversation happening between the developer\u2019s app and your API.\n  If you use a complicated authentication scheme like OAuth, a through description of the user authorization process with descriptions of each request and response should be included.\n  Lastly, a list of all possible errors should be provided. The first thing a developer will do when they get an error is Google it so make sure they get a hit.\n  A higher-level guide to using the API. This should be organized by task. If a task requires multiple API calls, an step-by-step walk-through for each task should be described. If a SDK is the primary interaction point with the API the examples in the User Guide should be demonstrated using each of the supported languages\/frameworks, though it is helpful to show raw HTTP requests\/responses or cURL examples as well.\n  If every API endpoint is a logical single unit-of-work, the User Guide could be combined with the API Reference.\n  Getting Started Tutorial\n  A simple example that is short enough to be easily copy and pasted into the dev\u2019s editor and run with only minor modifications (like setting a variable to their API key). This is to prove that the API works in the shortest amount of time possible. Keep it simple, simple, simple. Provide a version for each of the supported SDKs.\n  SDK-specific Documentation\n  Include instructions for obtaining the SDK and any installation or configuration steps required to use it. Provide examples for making a sample request and demonstrate the best way to handle errant requests for that language or framework. Any other information only relevant a specific SDK (for instance, a class reference or auto-generated documentation) should also be included.\n  Sample Applications\n  Sample apps should be standalone applications that demonstrate common use cases that help deepen understanding of what the API provides or gives devs a starting point from which to build their app. They should be functional with only minimal configuration and easily run or deployed. If your API requires OAuth, provide a sample app demonstrating how to authorize users. If another sample app requires authentication to work, include the auth example in the same package. Provide sample apps in all of your SDK flavors, ideally with all the samples being available in all languages.\n  Sample apps should be written using the idioms of each language and framework they are written for. They should instantly feel familiar for a developer experienced with that language. This will prevent the focus from shifting to the quality of the code contained in the example and away from the task at hand.\n  If an error is generated in a request from a properly authenticated user log and make it available to view later. Indefinite history is not required, but the more the better. This is particularly useful if an app is making requests of other users and the log includes errors for federated requests as well (minus any sensitive customer data). The more visibility you give into errors the less support you\u2019ll need to provide for mystery circumstances and the more data your support team will have when they do need to help a developer consuming the API.\n  Usage Logs and Limits\n  If your API charges for or restricts the amount of usage, provide a complete history of all requests that incur a \u2018cost\u2019. All information that was made available to the API client in the API response should be available in the logs.\n  For usage that is metered, provide an as-close-to-realtime-as-possible indication of the current amount of usage and the available amount remaining in the current time period.\n  OAuth Utilities\n  For APIs that use OAuth (and I haven\u2019t met one yet that wouldn\u2019t benefit from it), make it easy for the developer to get started more quickly by pre-populating access tokens that authorize their user account to access their own app. For a great example of this check out Twitter\u2019s developer center.\n  Provide a tool to make requests to the API and see responses. These should be real API requests, just from a web-based interface. Apigee provides a free console you can embed.\n  For APIs with requests that have side effects, provide a mechanism to simulate a real request\/response interaction with semantics as close to the real thing as possible. Be careful with code-based \u2018test mode\u2019 flags, those tend to get left on in production. Make it easy to distinguish between real and test requests without requiring too much ceremony to make test requests.\n  Provide the same level of tooling for all relevant tools in this list (logs, etc).\n  Billing History\n  For recurring or usage-based billing, provide a complete history of the payments made to the account. For APIs that don\u2019t involve a lot of requests, make it easy to see which requests were associated with which charge. For APIs with a lot of requests, provide a usage breakdown by type for each payment. Give developers as much information as possible so they can adjust their usage to meet budget restrictions.\n  Someone will post their API key on GitHub, I guarantee it (I have many times). Don\u2019t make the developer wait for support to have a new key generated. Make available a self-serve API key reset function. Make it scary, provide a lot of warning and confirmation. Just let them fix it themselves.\n  Multiple Credentials Management\n  Allowing API consumers to create and revoke multiple sets of credentials on demand is useful for a lot of cases, but one in particular worth mentioning: demos. When demoing how an app was built (which happens a lot with APIs) make it easy for your developers to create temporary credentials they can disable when the video is over without the pain of having to create and configure a separate account.\n  SDKs and Libraries\n  Common Language\/Framework Support\n  Provide officially-supported SDKs for the most popular server-side languages and frameworks:\n  Java\n JavaScript\/Node.js\n .NET\n \nIf it makes sense to request your API from a mobile device directly, provide iOS and Android SDKs. Mobile SDKs should also include authentication helpers\/controls and common UI elements.\n  The list of frameworks and languages you should support is not static or globally applicable. Don\u2019t ignore a big potential audience because you\u2019re not familiar with it yourself.\n  SDKs should be in sync with the latest version of the API with updates being released in tandem with new API releases.\n  Proper Distribution\n  Each SDK should be distributed in the proper package manager for each langauge (npm, Maven, NuGet, pip, Pear, RubyGems).\n  The code for SDKs should be open source under a OSI-approved and popular license. Make it available on a public code hosting site, preferably GitHub. Accept community contributions and fix bugs quickly.\n  Co-opting and paraphrasing the dictionary defintion: using, containing, or denoting expressions that are natural to a native developer. Appropriate to the style associated with a particular group.\n  Community Code\n  Provide a list of community-provided libraries, extensions and components for all languages and frameworks, even the officially supported ones.\n  Communication Channels\n  This section is more human than machine and usually involves a set of professions dedicated to it, so I\u2019ll just hit on the key points.\n  API Status Dashboard\n  Set up a site outside of your API infrastructure to provide availability updates. If there is an outage, report it there first. Have it push to a RSS or Twitter feed. Provide email updates for individual incidents. Keep it updated regularly throughout an issue. If it\u2019s not a consistently reliable source of information for up-to-date status you can expect repeated \u201care you down?\u201d support emails.\n  Blog\n  Provide a blog for announcing new releases, demonstrating how to use features and explaining commonly-occuring issues and questions. A blog is also a good place to do a test run for new documentation or samples to gauge feedback.\n  Communicate new blog posts, releases and availability issues via Twitter account (or multiple accounts if it makes sense). Be very attentive and quickly respond to developer questions. Treat it like a conversation, not a broadcast mechanism.\n  Changelog\n  Provide a list of recent changes to your API, SDKs or documentation. Heroku has a good example.\n  Provide a way for your users to talk to and help each other. This can cut down on your support load and also keep developers engaged. If you outsource this to a site like Stack Overflow, watch it VERY closely.\n  Email Support\n  If your API is free, this may be low priority for you. If it is paid, move this to the top of this section. Some problems can only be solved by looking at private code.\n  Future Must Haves\n  Here are a couple things that I think will be standard issue very soon:\n  Real-time (meaning no page refresh) logs and dashboard updates\n In-place example code editing and running from within documentation\n One-click sample app deployments to platform-as-a-service providers (like the Facebook\/Heroku integration).\n \nI\u2019m sure I\u2019ve missed something. What is a must have for you when working with an API?\n  Looking to hire more staff to build all of this for your API? Post a listing on API Jobs.","item_date":"Apr 27 2012 03:49:20","display_item_date":"04-26-2012","url":"http:\/\/john-sheehan.com\/post\/21850777760\/the-api-developer-experience-baseline","source":"john-sheehan.com"},{"title":"Introducing SwaggerSocket: A REST over WebSocket Protocol","details":"REST is a style of software architecture which is almost always delivered over HTTP. \u00a0It powers most of the World Wide Web and has enabled the rapid construction of APIs\u2013it has allowed developers to easily integrate many complex services with their applications. \u00a0It has been truly transformational over the last 10 years.\n The problem is, HTTP is a chatty, synchronous communication protocol based entirely on \u201crequest\/response\u201d. \u00a0That is, there is no easy way to open continuous communication between a client and a server. \u00a0You have to \u201cask a question\u201d and wait for the response. \u00a0There have been a number of techniques developed to make HTTP less \u201cchatty\u201d\u2013these include long-polling, Comet, HTTP streaming and recently, web sockets. \u00a0While REST over HTTP has transformed \u201cover-the-internet\u201d communication, it is usually a poor choice for high-throughput or asynchronous communication. \u00a0For example, most software programs do not communicate to their databases over HTTP. \u00a0It\u2019s typically too slow because of HTTP overhead.\n Websockets, however, have provided a new type of communication fabric\u00a0for the Internet. \u00a0By being full duplex\u2013meaning, a program can both ask a question and listen to a response simultaneously\u2013and truly asynchronous, they can not only speed up software by \u201cdoing more things at once\u201d, they can provide a more efficient pipe to send information through.\n The challenge, though, is that WebSockets are simply a protocol\u00a0for communication\u2013they do not define the structure itself. \u00a0All the goodness that came with REST (structure, human readability, self-description) is abandoned for the sake of efficiency. \u00a0This is why Wordnik created SwaggerSocket!\n How Does SwaggerSocket Work?\n First, let\u2019s see how SwaggerSocket improves performance over a typical REST implementation:\n \n Note this is a simple, sample of REST over WebSocket performance.  A follow-up post will go into the gory details of testing methodology and scenarios, as well as to the theoretical why performance will be better.  For the record, the above graph was produced on an Amazon M1.Large EC2 server.\n The REST resource used for testing looks like this simple Swagger annotated resource:\n object Counter { \u00a0\u00a0@volatile var count: Double = 0 \u00a0\u00a0def increment: Double = { count += 1 count } }   trait TestResource { @GET @Path(\/simpleFetch) @ApiOperation(value = Simple fetch method, notes = , \u00a0\u00a0responseClass = com.wordnik.demo.resources.ApiResponse) @ApiErrors(Array( new ApiError(code = 400, reason = Bad request))) def getById(): Response = { \u00a0\u00a0Counter.increment Response.ok.entity(new ApiResponse(200, success)).build } }   @Path(\/test.json) @Api(value = \/test, description = Test resource) @Produces(Array(application\/json)) class TestResourceJSON extends Help \u00a0\u00a0with ProfileEndpointTrait \u00a0\u00a0with TestResource \n On the client we incrementally hit the resource by either using the Jersey REST Client (for REST) or using SwaggerSocket Scala Client for WebSocket (based on AHC). As you can see, SwaggerSocket can easily outperform normal REST requests.  To keep the comparison fair, the SwaggerSocket client used is blocking waiting for the response.  This is the same way that a typical REST client would behave\u2013using an asynchronous programming style will not only be faster for the client, but it allows the server to degrade more gracefully under load as well as perform batch operations transparently.\n Introducing the SwaggerSocket Protocol\n The SwaggerSocket Protocol is:\n Pipelined: unlike the HTTP protocol (unless HTTP Pipelining is used), when you open a connection and send a request, you don\u2019t have to wait for the response to be arrive before being allowed to send another request. SwaggerSocket requests can be sent without waiting for the corresponding responses. Unlike HTTP Pipelining (which is not supported by all browsers), SwaggerSocket works with any browser supporting WebSocket.\n Transparent: The SwaggerSocket Protocol implementation from Wordnik is transparent and doesn\u2019t require modification of existing applications. For example, any existing JAX RS\/Jersey applications can work with no changes.\u00a0 For the client side, an application can either use the SwaggerSocket\u2019s Javascript library,\u00a0 SwaggerSocket\u2019s Scala library or open WebSocket and manipulate the protocol object.\n Asynchronous: The\u00a0 SwaggerSocket Protocol is fully asynchronous. A client can send several requests without waiting or blocking for the response. The server will asynchronously process the requests and send responses asynchronously.\n Simple: The\u00a0 SwaggerSocket Protocol uses JSON for encoding the requests and responses. This makes the protocol easy to understand and to implement.\n \n The SwaggerSocket Protocol uses JSON to pass information between clients and servers via a WebSocket connection. A client first connect to the server and wait for some authorization token. On success, the client can start sending requests and gets responses asynchronously.\n \n SwaggerSocket as a protocol can be implemented in nearly all programming languages. \u00a0The Wordnik implementation currently supports Java, Scala, Groovy and JRuby for the server components, and ships with a Scala and Javascript client library. \u00a0The swagger-codegen will be updated to support SwaggerSocket for other clients\n How can I try SwaggerSocket?\n The easiest way to try SwaggerSocket is to go to our Github site and download one of the sample and read our Quick Start. Details of the\u00a0 SwaggerSocket Protocol can be read here.\n Getting involved\n To get involved with SwaggerSocket, subscribe to our mailing list or follow us on Twitter or fork us on Github!\n \n \t\t\t\t\t  Wow, this is a perfect fit for a project of my own called Jongo. I\u2019ll do some stress testing to see the performance.","item_date":"Apr 27 2012 03:35:56","display_item_date":"04-26-2012","url":"http:\/\/blog.wordnik.com\/introducing-swaggersocket-a-rest-over-websocket-protocol","source":"blog.wordnik.com"},{"title":"Official Google Enterprise Blog: Introducing Google Drive, the newest member of Google Apps","details":"Today, we\u2019re introducing  Google Drive  \u2014a place where you can create, share, collaborate, and keep all of your work. Whether you\u2019re drawing up floor plans with a client, creating a presentation with classmates or planning next year\u2019s budget with colleagues, Drive makes it easy to work together. You can upload and access all of your files, including videos, photos, Google Docs, PDFs and beyond.  \n  \nCreate and collaborate.  Google Docs is built right into Google Drive, so you can work with others in real time on documents, spreadsheets and presentations. Once you choose to share content with others, you can add and reply to comments on  anything  (PDF, image, video   file, etc.) and receive notifications when other people comment on shared items. As a business user, you  can share files or folders with specific people, your entire team, or even customers and partners outside your company, controlling who can view or make edits. \n \nStore everything safely and access it anywhere (especially while on the go).  All your stuff is just...  there . You can access your stuff from anywhere\u2014on the web, in your home, at the office, while running errands and from all of your devices. You can install Drive on your Mac or PC and can download the  Drive app   to your Android phone or tablet. We\u2019re also working hard on a Drive app for your iOS devices. And regardless of platform, blind users can access Drive with a screen reader. \n \nSearch everything.  Search by keyword and filter by file type, owner, activity and many more. Drive can even recognize text in scanned documents using  Optical Character Recognition   (OCR) technology. Let\u2019s say you upload a scanned image of an old newspaper clipping. You can search for a word from the text of the actual news article. We also use image recognition so that if you drag and drop photos of the Grand Canyon into Drive for an upcoming ad campaign, you can later search for [grand canyon] and photos of its gorges should pop up. This  technology   is still in its early stages, and we expect it to get better over time. Open more than 30 file types right in your browser\u2014including HD video, Adobe Illustrator and Adobe Photoshop\u2014even if you don\u2019t have the corresponding program installed on your computer. \n \nWe know you rely on your files to get work done every day. Drive uses the same infrastructure as other Google Apps services, meaning it also has the same admin tools,  security and reliability  , including:  \n Centralized management:  New tools  available in the Apps control panel for administrators to   add  or  remove  storage for individuals or teams of users.\n Security:  Encryption   on data transfer between your browser and our servers, and optional  2-step verification   that prevents unauthorized account access by having users sign in with a secure code from their mobile phone.\n Data Replication:   Simultaneous data replication   in multiple data centers, so that in the unlikely event that one data center is unavailable, your files will still be safe and accessible.\n Uptime:  99.9% uptime guarantee   so you can be confident that your files will be available whenever you need them.\n Support:   24\/7 support   for assistance when you need it.\n  \nEach Apps user gets 5GB of storage included and administrators can centrally purchase and manage more. When a user reaches their limit, administrators on Google Apps for Business accounts can buy storage as it\u2019s needed. Start with an additional 20GB for $4 per month and add as much as 16TB. (Just as before, Google Docs don\u2019t count against your storage quota.) \nStarting today, Google Apps administrators will see new controls for Drive in the control panel. \u00a0Users at organizations on the  Rapid Release   track will be able to opt-in to Drive at  drive.google.com\/start  .   \n  \nDrive is built to work seamlessly with your  overall Google experience  . Drive is also an open platform, so we\u2019re working with many third-party developers so you can do things sign documents with  DocuSign   and  HelloFax  , design flowcharts with  Lucidchart   and manage projects and tasks with  Smartsheet   directly from Drive. To install these apps, visit the  Chrome Web Store  \u2014and look out for even more useful apps in the future. \nThis is just the beginning for Google Drive; there\u2019s a lot more to come.  Contact our sales   team or a Google Apps reseller if you are interested in signing up for Google Apps and Drive.   \n \nNote:\u00a0At launch it is not possible for organizations using Google Apps for Education or Google Apps for Government to centrally purchase and manage additional storage.","item_date":"Apr 24 2012 16:52:04","display_item_date":"04-24-2012","url":"http:\/\/googleenterprise.blogspot.com\/2012\/04\/introducing-google-drive-newest-member.html","source":"googleenterprise.blogspot.com"},{"title":"The API Economy \u2013 Tapping into Identity and the Inside-Out Enterprise","details":"APIs are increasingly being recognized as one of the most important infrastructure trends for enterprises in the next few years as more companies seek to open up their internal systems to customers, partners and other third parties. The API Economy Session at EIC 2012 last week was a great snapshot of both he potential and the challenges involved in such a transformation. \n Craig Burton (KuppingerCole) provided an an excellent overview of the opportunities of the API Economy and there were presentations and inputs from Fulup Ar Foll (KuppingerCole), Kim Cameron (Microsoft) and Martin Kuppinger (KuppingerCole). The day before Ping identity\u2019s Andre Durand also highlighted Cloud APIs as one of the most important new frontiers for Identity in his keynote.\n \n Reflecting on the event, many of the challenges organizations will face in taking advantage of these new opportunities revolve around the fact that they are authenticating and authorizing new types of users to their systems and this is happening in two dimensions simultaneously: \n From Internal to External:  Many of today\u2019s identity management solutions focus on the internal systems of the enterprise \u2013 which employees, divisions and systems are permitted to access which systems. This is an extremely challenging problem. However, it becomes even more challenging when access must provided to external parties as is generally the case with Open API \u2013 often the external parties are trusted to an extent but not completely and different levels of access are needed to determine who gets what, furthermore user directories are no longer fully under the control of the owner of the data or service in question.\n From Human to Machine: Today\u2019s systems are primarily concerned with granting \u201cHuman access\u201d to systems with interfaces accessible in the Browser or through some other UI. This means certain types of authentication are appropriate and the expected capacity of users is reasonably well understood. For API\u2019s however, this changes radically since access is generally granted to software systems developed by external third parties \u2013 software systems which might be orders of magnitude faster or more data hungry than human users. \n \n Given these trends and the economic imperative for open APIs, the next few years are likely to see a powerful trend towards managing identity across organizational boundaries and \u2013 viewing enterprises as a collection of services they expose and manage access to, rather than as a set of end products they produce.","item_date":"Apr 23 2012 15:56:15","display_item_date":"04-23-2012","url":"http:\/\/www.3scale.net\/2012\/04\/23\/the-api-economy-tapping-into-identity-and-the-inside-out-enterprise\/","source":"www.3scale.net"},{"title":"Oracle v Google could clear way for copyright on languages, APIs","details":"Analysis Computer languages and software interfaces may fall under copyright protection if Oracle succeeds in its Java lawsuit against Google. Amazingly, copyfighters appear to have paid little or no notice to this rare extension of copyright into new realms. But the consequences and costs for the software industry could be enormous.\n  First, let\u2019s look at what\u2019s at stake. Author Rob Levine has a useful, if not perfect, metaphor of thinking about copyright in \u201cthree dimensions\u201d. One is the extent: the areas of life which may be protected by copyright. Another is the length: the duration of the protection. The third is the depth: how effective is the enforcement.\n    \n  The third part is crucial, since a law that cannot be enforced is nothing more than symbolic, and without protection for authors the economic incentive to create disappears. (In Levine\u2019s view, copyright needs to be shorter and leaner, but more effective.)\n \n  While source code is globally recognised as copyrightable (in the United States since 1964), this is not the case for languages and interfaces. The two relevant international treaties are TRIPS (Trade-Related aspects of Intellectual Property Rights) and the Copyright Treaty of the UN agency World Intellectual Property Organization (WIPO). In a High Court ruling in 2010 specialist IP judge Justice Richard Arnold affirmed that both agreements protect source code.\n  Judge Arnold said that software source code is an \u201cexpression\u201d and that neither TRIPS nor the Copyright Treaty protects \u201cideas, procedures, methods of operation and mathematical concepts as such\u201d \u2013 which either belong to the domain of patents or are not protected at all.\n  The judges view was \u2013 to some extent \u2013 affirmed by Advocate General Yves Bot (yes, thats really his name) at the European Court of Justice late last year. Bot said that the functionalities of a program, and the programming language, cannot be protected by copyright. He added that the source code of a program may be reproduced in order to ensure interoperability with another program, but only if certain conditions are met.\n  Bot\u2019s ruling forms an important plank of Google\u2019s defence.\n  In a court filing that accuses Google of infringing copyright by embedding Java into its Android platform, Oracle said there\u2019s no clear precedent to exclude computer languages from copyright.\n  \u201cCourts have denied copyright protection to codes or systems of symbols only where they lack originality, not because they are inherently uncopyrightable,\u201d Oracle declared in its paperwork last Thursday.\n  Drilling into the definition of computer speak\n  Google, which denies any wrongdoing, maintains that a programming language is \u201can idea, not an expression\u201d and therefore exempt from copyright. Meh, responds Oracle: \u201cA work that represents only one of many ways to perform a function is \u2018the expression of a particular idea, not the idea itself\u2019.\u201d\n  Oracle also cites JRR Tolkiens Elvish as an example of a language that is used outside of its creators works. But Oracle has a parallel argument \u2013 arguing that libraries and their APIs are copyrightable \u201cas a computer program, and their selection, arrangement and structure is copyrightable if sufficiently original and creative.\n  Interfaces to software libraries should not be considered part of the original language in any case, Oracle argues, since they are \u201cdifferent things with different purposes\u201d.\n  The database giant argued: \u201cGoogle was never confused about the distinction between an API [application programming interface] and a programming language when this case began or for long afterwards. Google is now straining to change course to take a position it knows is factually incorrect.\u201d\n  For its part, Google leans on the clarification of the USs Copyright Act of 1976 that an \u201cidea, procedure, process, system, method of operation, concept, principle, or discovery\u201d is not copyrightable \u2013 even if it is in an original work of authorship. A language is not an expression, it is \u201ca system that can be used to express\u201d (Google\u2019s emphasis). The medium of expression, in other words.\n  Google explains this with a hypothetical example worth quoting at length:\n  A set of nonsensical APIs could be created that had exactly the same structure, selection and organization as the Oracle APIs, but that did different things. For example, the sqrt()\n method could always return zero - indeed, every method that returns a number could always return zero, while those that return text could always return the letter a, those that return true or false could always return true\n, and so on, with a default result being used for every variable type.\n  This set of APIs would serve no useful purpose, but would have exactly the same structure, selection and organization as the Oracle APIs. No reasonable jury could ever conclude that the \u201cexpression\u201d in this hypothetical set of APIs is substantially similar to the \u201cexpression\u201d in the Oracle APIs, notwithstanding the \u201ccopied\u201d structure, selection and organization.\n  Thus, Oracle\u2019s infringement theory fails unless it accuses not just the structure.\n   Google says that if Oracle succeeds on this point, it would allow a party to devise a system, and then enforce copyrights in descriptions of that system (the APIs) and implementations (expressions) of that system \u201cto preclude others from practising the system\u201d.\n  (Google\u2019s lawyers, Keker & Van Nest, are as smitten with italics as a teenager is with LOLs.)\n  That\u2019s plainly what Oracle wants to do. Sun (before it was bought by Oracle) was relaxed about clean-room implementations of Java so long as they didn\u2019t threaten Sun - and Google insists the Dalvik VM, which runs Java-written apps on Android devices, is a clean-room implementation.\n  Copyright gives the author such flexibility with regards to pursuing infringements. You can ignore some violations (because you consider them trivial or consider them beneficial) and enforce others. Unlike in trademark law, you don\u2019t need to proactively defend each infringement. One unintended side effect of an Oracle victory would be that this widely accepted and useful flexibility vanishes.\n  In fact, the whole skirmish could be viewed as a catastrophic unintended side effect of taking rigid, dogmatic positions on intellectual property. In recent years, a strangely zealous campaign against patent protection for software has been successful. It has been described as a stalking horse for the all-IP-is-evil activists.\n  But if patents aren\u2019t enforceable, the world\u2019s Oracle Corporations will use whatever other means are available to protect their inventions. In this case, they\u2019ve chosen to use copyright. So the reach of copyright could be extended simply because we\u2019re lacking precedents and case law for a common-sense application of patents. The consequences for interoperability and for legitimate reverse-engineering could be enormous.\n  Both Google and Oracle are culpable here. That Google based Android on Java is not in dispute \u2013 its a faithful knock-off. But as a Google engineer privately admitted, upon being exposed to risk, the Chocolate Factory should then have licensed it:\n  What weve actually been asked to do (by Larry [Page] and Sergey [Brin]) is to investigate what technical alternatives exist to Java for Android and Chrome. Weve been over a bunch of these, and think they all suck. We conclude that we need to negotiate a license for Java under the terms we need.  Google could have avoided this by developing its own language and corresponding virtual machine. Intellectual property exists to encourage innovation, and Google is full of innovative people. Instead, it took a shortcut, and copyright may be extended to areas it really shouldnt be. \u00ae","item_date":"Apr 22 2012 02:29:13","display_item_date":"04-21-2012","url":"http:\/\/www.theregister.co.uk\/2012\/04\/17\/google_oracle_copyright_on_languages\/","source":"www.theregister.co.uk"},{"title":"Why Startups Need an API","details":"When people refer to an API-centric web application, they usually mean that most if not all of the functionality within the app is executed through API calls. That means each system like your databases and your front end application can operate independently but communicate seamlessly. In other words, you can do things like build entirely new interfaces or plug into separate platforms just by using the API calls provided.\n In case you\u2019re unfamiliar, API stands for Application Programming Interface. For most SaaS products today, an API most commonly refers to a set of functions built into an application for use by other external applications, but increasingly companies also use their API internally.  \n At HasOffers, we don\u2019t think of our API as just an extra feature. Our API is our core data layer \u2013 core to our business. This allows us to be consistently enhancing the API and regularly sharing those improvements with clients that use it externally.\n Why API-Centric Applications Are Cool\n For startups looking at building an API, I really recommend approaching your API as your data layer. It should be the first thing you think about as you are building your application.\n Designing your data layer with an API in mind forces you to think about all the possible use cases for your data. It allows you to identify the business problems you need to solve, while also getting you in the mindset of thinking about how others may ultimately interface with your data, including potential partners you may find, or even the companies who may be interested in acquiring you.\n Building your data-layer with an API has other advantages. You create a unified codebase that you can maintain for internal and external facing applications. There\u2019s no additional work in opening the API, since you simply need a ruleset for who has access to which API methods. Ultimately, you can really increase consistency. Your web interface always returns the same results as other applications making API calls, because you\u2019re addressing the data layer using identical methods.\n I learned that we needed to build an API-centric application the hard way. Our original data layer wasn\u2019t written with an API in mind. Our clients using the API had a limited set of features available compared to the universe of features built into our integrated web interface. \n On top of all of the flexibility in functionality, using an API as a data layer really equips you to scale. You can even launch separate systems into cloud architectures, testing as you go. Once you have an internal API, you can easily add access controls and authentication processes to securely support external access.\n Getting Your API Started\n If any of what I\u2019ve said has convinced you to use an API-centric approach to development, you can get started by building your API for internal use first. This really makes sense if your product is brand new and you have no reason to make an external API available to clients. I\u2019d offer advice if you\u2019ve already built your web application without an API. Start building it into your internal process now and eventually it will be complete enough to release for external use. \n The first version of our API needed to encapsulate all functionality we already implemented in the application stack as API calls. When we re-wrote our data layer to be an API two years ago, not only did we have to re-write everything from the ground up, we also documented each API call throughout the process. If you have a large code base, as we did, documenting each call can be daunting.\n Starting with an API also helps guarantee effective documentation.  By starting with an API in mind, you document as you go, allowing you to grow and setting you up to launch a successful external API when you\u2019re ready.\n Stay Current\n Our team just finished updating our API, making this the third full rewrite in three years. We updated it to PHP 5.3.10, increased data capacity, improved management of larger data sets, made our API requests faster and more efficient. We also updated the framework for the affiliate and advertiser APIs. You can learn more about today\u2019s update in our press release.   \n I can tell you that this was no cake walk, but the HasOffers engineering team, led by Lee and myself, believe in building the most scalable foundation we can, greatly improving our ability to adapt to the needs of our clients moving forward. After all of this, I can truly say that our API is one of the major differentiating factors between HasOffers and any competition, which also make things easier for our sales and marketing teams. They understand the value of our API, communicate it to clients, and it truly gives us a huge advantage.  \n Need a new feature? Want to build your own custom front end? Need to integrate with anything under the sun? No problem :)\n \n                       As the CEO of Hasoffers, the industry leading affiliate tracking software, Lucas specializes in performance based online advertising and has dedicated himself to the development of new and innovative technologies to support this quickly growing revenue model.          \n         \n                      You couldn\u2019t be more right Omar. I would be one of them several years ago, but the more we grow and scale, I\u2019m not sure how we could have survived without it.\u00a0\n \n                      Somewhat confused by the article. Has version 3 been released? I couldn\u2019t find any documentation on it. We\u2019ve seen new bugs in the v2 api in the last couple days \u2013 would that be related?\n \n                      Hi Anil, the documentation for v3 is not yet released, but yes, the API you are operating with now is in fact v3. Have we already addressed the bugs you mentioned? \u00a0","item_date":"Apr 21 2012 19:49:06","display_item_date":"04-21-2012","url":"http:\/\/www.hasoffers.com\/blog\/startups-need-api\/","source":"www.hasoffers.com"},{"title":"Oracle, Google Struggle To Explain APIs To Jury","details":"Oracle and Google continued their legal battle in San Francisco federal court on Thursday, beginning the day with a tussle over the authenticity of Java code that Oracle claims to have downloaded from Googles website on March 12.  Oracle says the file shows that Google has not removed the allegedly infringing Android code from its public website as Google claims to have done. Google wants some assurance that the file really was downloaded then.  \n                                               \n  Are you saying it has been forged? Judge William Alsup asked skeptically. \n Not at all, Google trial counsel Daniel Purcell answered. Googles interest in receiving formal verification of the file from Oracle arose out of having only received notice of this file several days ago, he suggested. \n [ Learn more about the case. Read Google: Oracle Wants To Glom Onto Androids Success. ] \n The judge settled the matter by stating that someone will be made available to attest to the legitimacy of the file at a later time. Then he scolded the two sides for their intransigence. \n This is something you shouldve figured out overnight, cmon, he said, reinforcing accounts that have characterized him as a no-nonsense jurist. \n Mark Reinhold, Oracle architect for the Java platform, returned to the witness stand to continue his explanation of the distinction between the Java language--freely available for use--and the Java APIs, which are the detailed specifications that describe valid uses of the Java language. \n Oracle is seeking damages from Google for using a subset of its copyrighted APIs without authorization, and for alleged patent violations. \n Cross-examination from Purcell blurred the clear lines that Reinhold had drawn under questioning from Oracle to distinguish between the Java language and the Java APIs. Purcell twice asked that video depositions of Reinhold be played in which his definitions of the boundaries of Java and its APIs differed from the definition he offered in court. \n Purcell also had Reinhold acknowledge that some 10% to 20% of Javas API have been written by contributors outside of Sun or Oracle, contributors who were not paid by Sun or Oracle. The implication is that perhaps some of the Java API copyrights may not be valid. \n It was enough that Oracle attorney Michael Jacobs asked the court to instruct the jury theres no ownership dispute about Oracles copyrights in the case. But clearly Google is raising that question and the judge wasnt immediately ready to accede to Oracles desire to take the question of ownership off the table. \n In one of its filings Google asserted: Computer programming languages are not copyrightable, and neither are Oracles APIs. So this issue is sure to be raised again. \n After Reinhold was excused, Oracle called Joshua Bloch, chief architect at Google, to question him about his views on the importance of good API design.  Bloch affirmed that programming is creative, which helps support the notion that code should qualify for copyright protection. \n Writing a program is very much a creative process, Bloch said. \n Under questioning from Jacobs, Bloch acknowledged that the Timsort.Java API hed created for Android was virtually identical to a similar range checking API, Arrays.Java, that hed written for Java while working for Sun. \n Is that copyright infringement? Thats what Oracle hopes to establish, but that point has yet to be settled, in part because the boundaries of where the open Java language ends and the proprietary Java APIs begin remain a matter of disagreement. \n Bruce Baber, counsel for Google, countered the notion that Bloch had copied Java APIs by asking him whether he had any creative freedom when re-implementing an API, as Google did with Android.","item_date":"Apr 21 2012 09:28:15","display_item_date":"04-21-2012","url":"http:\/\/www.informationweek.com\/news\/development\/java\/232900594","source":"www.informationweek.com"},{"title":"APIs take center stage at Oracle-Google Trial","details":"The scene of the Oracle-Google trial Thursday was more like a computer science classroom than a courtroom as the witnesses explained the inner workings of Java and APIs.","item_date":"Apr 21 2012 07:57:46","display_item_date":"04-21-2012","url":"http:\/\/news.cnet.com\/8301-1035_3-57416413-94\/apis-take-center-stage-at-oracle-google-trial\/","source":"news.cnet.com"},{"title":"Who can pull this app together for @Chirp? @dougw can make the introductions: http:\/\/apiwiki.twitter.com\/Panorama-for-Chirp","details":"","item_date":"Apr 08 2010 17:42:23","display_item_date":"04-08-2010","url":"https:\/\/twitter.com\/twitterapi\/statuses\/11835068523","source":"twitter.com"},{"title":"There are some changes coming with how we generate tweet IDs. Find out more: http:\/\/bit.ly\/tweet-id-transition ^TS","details":"","item_date":"Mar 26 2010 20:43:58","display_item_date":"03-26-2010","url":"https:\/\/twitter.com\/twitterapi\/statuses\/11108930315","source":"twitter.com"}]}